I0112 00:40:36.734670      21 e2e.go:126] Starting e2e run "57cc750e-de8d-467f-a423-c8fd052edd1e" on Ginkgo node 1
Jan 12 00:40:36.762: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1673484036 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 12 00:40:36.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
E0112 00:40:36.908834      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
Jan 12 00:40:36.910: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 12 00:40:36.933: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 12 00:40:37.016: INFO: 35 / 35 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 12 00:40:37.016: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jan 12 00:40:37.016: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 12 00:40:37.023: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 12 00:40:37.023: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds-amd64' (0 seconds elapsed)
Jan 12 00:40:37.023: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 12 00:40:37.023: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-sriov-device-plugin-amd64' (0 seconds elapsed)
Jan 12 00:40:37.023: INFO: e2e test version: v1.26.0
Jan 12 00:40:37.024: INFO: kube-apiserver version: v1.26.0
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 12 00:40:37.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 00:40:37.028: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.121 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 12 00:40:36.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    E0112 00:40:36.908834      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
    Jan 12 00:40:36.910: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan 12 00:40:36.933: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 12 00:40:37.016: INFO: 35 / 35 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 12 00:40:37.016: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
    Jan 12 00:40:37.016: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 12 00:40:37.023: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jan 12 00:40:37.023: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds-amd64' (0 seconds elapsed)
    Jan 12 00:40:37.023: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jan 12 00:40:37.023: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-sriov-device-plugin-amd64' (0 seconds elapsed)
    Jan 12 00:40:37.023: INFO: e2e test version: v1.26.0
    Jan 12 00:40:37.024: INFO: kube-apiserver version: v1.26.0
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 12 00:40:37.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 00:40:37.028: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:40:37.056
Jan 12 00:40:37.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename subpath 01/12/23 00:40:37.057
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:40:37.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:40:37.076
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/12/23 00:40:37.079
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-2kl6 01/12/23 00:40:37.092
STEP: Creating a pod to test atomic-volume-subpath 01/12/23 00:40:37.092
Jan 12 00:40:37.208: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-2kl6" in namespace "subpath-5669" to be "Succeeded or Failed"
Jan 12 00:40:37.211: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.195221ms
Jan 12 00:40:39.216: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00837664s
Jan 12 00:40:41.217: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 4.008515553s
Jan 12 00:40:43.215: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 6.007007687s
Jan 12 00:40:45.215: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 8.007042505s
Jan 12 00:40:47.217: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 10.008686882s
Jan 12 00:40:49.217: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 12.008852783s
Jan 12 00:40:51.217: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 14.009303118s
Jan 12 00:40:53.215: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 16.007143027s
Jan 12 00:40:55.216: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 18.007696176s
Jan 12 00:40:57.216: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 20.007926998s
Jan 12 00:40:59.217: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=false. Elapsed: 22.008813898s
Jan 12 00:41:01.222: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014229755s
STEP: Saw pod success 01/12/23 00:41:01.222
Jan 12 00:41:01.223: INFO: Pod "pod-subpath-test-secret-2kl6" satisfied condition "Succeeded or Failed"
Jan 12 00:41:01.226: INFO: Trying to get logs from node eqx04-flash06 pod pod-subpath-test-secret-2kl6 container test-container-subpath-secret-2kl6: <nil>
STEP: delete the pod 01/12/23 00:41:01.246
Jan 12 00:41:01.265: INFO: Waiting for pod pod-subpath-test-secret-2kl6 to disappear
Jan 12 00:41:01.267: INFO: Pod pod-subpath-test-secret-2kl6 no longer exists
STEP: Deleting pod pod-subpath-test-secret-2kl6 01/12/23 00:41:01.268
Jan 12 00:41:01.268: INFO: Deleting pod "pod-subpath-test-secret-2kl6" in namespace "subpath-5669"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 12 00:41:01.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5669" for this suite. 01/12/23 00:41:01.287
------------------------------
• [SLOW TEST] [24.281 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:40:37.056
    Jan 12 00:40:37.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename subpath 01/12/23 00:40:37.057
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:40:37.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:40:37.076
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/12/23 00:40:37.079
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-2kl6 01/12/23 00:40:37.092
    STEP: Creating a pod to test atomic-volume-subpath 01/12/23 00:40:37.092
    Jan 12 00:40:37.208: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-2kl6" in namespace "subpath-5669" to be "Succeeded or Failed"
    Jan 12 00:40:37.211: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.195221ms
    Jan 12 00:40:39.216: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00837664s
    Jan 12 00:40:41.217: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 4.008515553s
    Jan 12 00:40:43.215: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 6.007007687s
    Jan 12 00:40:45.215: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 8.007042505s
    Jan 12 00:40:47.217: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 10.008686882s
    Jan 12 00:40:49.217: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 12.008852783s
    Jan 12 00:40:51.217: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 14.009303118s
    Jan 12 00:40:53.215: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 16.007143027s
    Jan 12 00:40:55.216: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 18.007696176s
    Jan 12 00:40:57.216: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=true. Elapsed: 20.007926998s
    Jan 12 00:40:59.217: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Running", Reason="", readiness=false. Elapsed: 22.008813898s
    Jan 12 00:41:01.222: INFO: Pod "pod-subpath-test-secret-2kl6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014229755s
    STEP: Saw pod success 01/12/23 00:41:01.222
    Jan 12 00:41:01.223: INFO: Pod "pod-subpath-test-secret-2kl6" satisfied condition "Succeeded or Failed"
    Jan 12 00:41:01.226: INFO: Trying to get logs from node eqx04-flash06 pod pod-subpath-test-secret-2kl6 container test-container-subpath-secret-2kl6: <nil>
    STEP: delete the pod 01/12/23 00:41:01.246
    Jan 12 00:41:01.265: INFO: Waiting for pod pod-subpath-test-secret-2kl6 to disappear
    Jan 12 00:41:01.267: INFO: Pod pod-subpath-test-secret-2kl6 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-2kl6 01/12/23 00:41:01.268
    Jan 12 00:41:01.268: INFO: Deleting pod "pod-subpath-test-secret-2kl6" in namespace "subpath-5669"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:41:01.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5669" for this suite. 01/12/23 00:41:01.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:41:01.341
Jan 12 00:41:01.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename dns 01/12/23 00:41:01.342
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:41:01.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:41:01.361
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8937.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8937.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/12/23 00:41:01.364
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8937.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8937.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/12/23 00:41:01.365
STEP: creating a pod to probe /etc/hosts 01/12/23 00:41:01.365
STEP: submitting the pod to kubernetes 01/12/23 00:41:01.365
Jan 12 00:41:01.422: INFO: Waiting up to 15m0s for pod "dns-test-b576a331-e898-4c4e-b82a-c140b4d81ba0" in namespace "dns-8937" to be "running"
Jan 12 00:41:01.425: INFO: Pod "dns-test-b576a331-e898-4c4e-b82a-c140b4d81ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.675337ms
Jan 12 00:41:03.430: INFO: Pod "dns-test-b576a331-e898-4c4e-b82a-c140b4d81ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007086409s
Jan 12 00:41:05.430: INFO: Pod "dns-test-b576a331-e898-4c4e-b82a-c140b4d81ba0": Phase="Running", Reason="", readiness=true. Elapsed: 4.007468718s
Jan 12 00:41:05.430: INFO: Pod "dns-test-b576a331-e898-4c4e-b82a-c140b4d81ba0" satisfied condition "running"
STEP: retrieving the pod 01/12/23 00:41:05.43
STEP: looking for the results for each expected name from probers 01/12/23 00:41:05.433
Jan 12 00:41:05.452: INFO: DNS probes using dns-8937/dns-test-b576a331-e898-4c4e-b82a-c140b4d81ba0 succeeded

STEP: deleting the pod 01/12/23 00:41:05.452
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 00:41:05.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8937" for this suite. 01/12/23 00:41:05.475
------------------------------
• [4.163 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:41:01.341
    Jan 12 00:41:01.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename dns 01/12/23 00:41:01.342
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:41:01.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:41:01.361
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8937.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8937.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/12/23 00:41:01.364
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8937.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8937.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/12/23 00:41:01.365
    STEP: creating a pod to probe /etc/hosts 01/12/23 00:41:01.365
    STEP: submitting the pod to kubernetes 01/12/23 00:41:01.365
    Jan 12 00:41:01.422: INFO: Waiting up to 15m0s for pod "dns-test-b576a331-e898-4c4e-b82a-c140b4d81ba0" in namespace "dns-8937" to be "running"
    Jan 12 00:41:01.425: INFO: Pod "dns-test-b576a331-e898-4c4e-b82a-c140b4d81ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.675337ms
    Jan 12 00:41:03.430: INFO: Pod "dns-test-b576a331-e898-4c4e-b82a-c140b4d81ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007086409s
    Jan 12 00:41:05.430: INFO: Pod "dns-test-b576a331-e898-4c4e-b82a-c140b4d81ba0": Phase="Running", Reason="", readiness=true. Elapsed: 4.007468718s
    Jan 12 00:41:05.430: INFO: Pod "dns-test-b576a331-e898-4c4e-b82a-c140b4d81ba0" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 00:41:05.43
    STEP: looking for the results for each expected name from probers 01/12/23 00:41:05.433
    Jan 12 00:41:05.452: INFO: DNS probes using dns-8937/dns-test-b576a331-e898-4c4e-b82a-c140b4d81ba0 succeeded

    STEP: deleting the pod 01/12/23 00:41:05.452
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:41:05.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8937" for this suite. 01/12/23 00:41:05.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:41:05.505
Jan 12 00:41:05.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-runtime 01/12/23 00:41:05.505
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:41:05.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:41:05.524
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 01/12/23 00:41:05.527
STEP: wait for the container to reach Succeeded 01/12/23 00:41:05.557
STEP: get the container status 01/12/23 00:41:10.583
STEP: the container should be terminated 01/12/23 00:41:10.587
STEP: the termination message should be set 01/12/23 00:41:10.587
Jan 12 00:41:10.587: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/12/23 00:41:10.587
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 12 00:41:10.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-117" for this suite. 01/12/23 00:41:10.614
------------------------------
• [SLOW TEST] [5.127 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:41:05.505
    Jan 12 00:41:05.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-runtime 01/12/23 00:41:05.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:41:05.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:41:05.524
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 01/12/23 00:41:05.527
    STEP: wait for the container to reach Succeeded 01/12/23 00:41:05.557
    STEP: get the container status 01/12/23 00:41:10.583
    STEP: the container should be terminated 01/12/23 00:41:10.587
    STEP: the termination message should be set 01/12/23 00:41:10.587
    Jan 12 00:41:10.587: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/12/23 00:41:10.587
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:41:10.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-117" for this suite. 01/12/23 00:41:10.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:41:10.634
Jan 12 00:41:10.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 00:41:10.634
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:41:10.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:41:10.653
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-a8b01fa8-1a7f-4d2d-9728-d60102efef57 01/12/23 00:41:10.66
STEP: Creating the pod 01/12/23 00:41:10.667
Jan 12 00:41:10.734: INFO: Waiting up to 5m0s for pod "pod-configmaps-c400a7be-5876-4d73-9b71-70b29a151670" in namespace "configmap-538" to be "running"
Jan 12 00:41:10.737: INFO: Pod "pod-configmaps-c400a7be-5876-4d73-9b71-70b29a151670": Phase="Pending", Reason="", readiness=false. Elapsed: 2.906705ms
Jan 12 00:41:12.741: INFO: Pod "pod-configmaps-c400a7be-5876-4d73-9b71-70b29a151670": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007141561s
Jan 12 00:41:14.742: INFO: Pod "pod-configmaps-c400a7be-5876-4d73-9b71-70b29a151670": Phase="Running", Reason="", readiness=false. Elapsed: 4.007815186s
Jan 12 00:41:14.742: INFO: Pod "pod-configmaps-c400a7be-5876-4d73-9b71-70b29a151670" satisfied condition "running"
STEP: Waiting for pod with text data 01/12/23 00:41:14.742
STEP: Waiting for pod with binary data 01/12/23 00:41:14.753
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 00:41:14.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-538" for this suite. 01/12/23 00:41:14.765
------------------------------
• [4.149 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:41:10.634
    Jan 12 00:41:10.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 00:41:10.634
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:41:10.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:41:10.653
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-a8b01fa8-1a7f-4d2d-9728-d60102efef57 01/12/23 00:41:10.66
    STEP: Creating the pod 01/12/23 00:41:10.667
    Jan 12 00:41:10.734: INFO: Waiting up to 5m0s for pod "pod-configmaps-c400a7be-5876-4d73-9b71-70b29a151670" in namespace "configmap-538" to be "running"
    Jan 12 00:41:10.737: INFO: Pod "pod-configmaps-c400a7be-5876-4d73-9b71-70b29a151670": Phase="Pending", Reason="", readiness=false. Elapsed: 2.906705ms
    Jan 12 00:41:12.741: INFO: Pod "pod-configmaps-c400a7be-5876-4d73-9b71-70b29a151670": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007141561s
    Jan 12 00:41:14.742: INFO: Pod "pod-configmaps-c400a7be-5876-4d73-9b71-70b29a151670": Phase="Running", Reason="", readiness=false. Elapsed: 4.007815186s
    Jan 12 00:41:14.742: INFO: Pod "pod-configmaps-c400a7be-5876-4d73-9b71-70b29a151670" satisfied condition "running"
    STEP: Waiting for pod with text data 01/12/23 00:41:14.742
    STEP: Waiting for pod with binary data 01/12/23 00:41:14.753
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:41:14.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-538" for this suite. 01/12/23 00:41:14.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:41:14.785
Jan 12 00:41:14.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename endpointslice 01/12/23 00:41:14.785
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:41:14.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:41:14.803
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 01/12/23 00:41:14.806
STEP: getting /apis/discovery.k8s.io 01/12/23 00:41:14.808
STEP: getting /apis/discovery.k8s.iov1 01/12/23 00:41:14.809
STEP: creating 01/12/23 00:41:14.81
STEP: getting 01/12/23 00:41:14.826
STEP: listing 01/12/23 00:41:14.828
STEP: watching 01/12/23 00:41:14.831
Jan 12 00:41:14.831: INFO: starting watch
STEP: cluster-wide listing 01/12/23 00:41:14.832
STEP: cluster-wide watching 01/12/23 00:41:14.838
Jan 12 00:41:14.838: INFO: starting watch
STEP: patching 01/12/23 00:41:14.839
STEP: updating 01/12/23 00:41:14.844
Jan 12 00:41:14.851: INFO: waiting for watch events with expected annotations
Jan 12 00:41:14.851: INFO: saw patched and updated annotations
STEP: deleting 01/12/23 00:41:14.852
STEP: deleting a collection 01/12/23 00:41:14.864
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 12 00:41:14.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5488" for this suite. 01/12/23 00:41:14.884
------------------------------
• [0.114 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:41:14.785
    Jan 12 00:41:14.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename endpointslice 01/12/23 00:41:14.785
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:41:14.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:41:14.803
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 01/12/23 00:41:14.806
    STEP: getting /apis/discovery.k8s.io 01/12/23 00:41:14.808
    STEP: getting /apis/discovery.k8s.iov1 01/12/23 00:41:14.809
    STEP: creating 01/12/23 00:41:14.81
    STEP: getting 01/12/23 00:41:14.826
    STEP: listing 01/12/23 00:41:14.828
    STEP: watching 01/12/23 00:41:14.831
    Jan 12 00:41:14.831: INFO: starting watch
    STEP: cluster-wide listing 01/12/23 00:41:14.832
    STEP: cluster-wide watching 01/12/23 00:41:14.838
    Jan 12 00:41:14.838: INFO: starting watch
    STEP: patching 01/12/23 00:41:14.839
    STEP: updating 01/12/23 00:41:14.844
    Jan 12 00:41:14.851: INFO: waiting for watch events with expected annotations
    Jan 12 00:41:14.851: INFO: saw patched and updated annotations
    STEP: deleting 01/12/23 00:41:14.852
    STEP: deleting a collection 01/12/23 00:41:14.864
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:41:14.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5488" for this suite. 01/12/23 00:41:14.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:41:14.901
Jan 12 00:41:14.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename sched-preemption 01/12/23 00:41:14.901
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:41:14.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:41:14.923
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 12 00:41:14.941: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 12 00:42:15.033: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:42:15.038
Jan 12 00:42:15.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename sched-preemption-path 01/12/23 00:42:15.039
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:42:15.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:42:15.059
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:569
STEP: Finding an available node 01/12/23 00:42:15.066
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/12/23 00:42:15.066
Jan 12 00:42:15.126: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-8022" to be "running"
Jan 12 00:42:15.129: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.976575ms
Jan 12 00:42:17.134: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007743622s
Jan 12 00:42:17.135: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/12/23 00:42:17.139
Jan 12 00:42:17.152: INFO: found a healthy node: eqx04-flash06
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
Jan 12 00:42:29.245: INFO: pods created so far: [1 1 1]
Jan 12 00:42:29.246: INFO: length of pods created so far: 3
Jan 12 00:42:35.283: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jan 12 00:42:42.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:543
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:42:42.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-8022" for this suite. 01/12/23 00:42:42.392
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3798" for this suite. 01/12/23 00:42:42.411
------------------------------
• [SLOW TEST] [87.527 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:531
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:616

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:41:14.901
    Jan 12 00:41:14.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename sched-preemption 01/12/23 00:41:14.901
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:41:14.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:41:14.923
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 12 00:41:14.941: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 12 00:42:15.033: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:42:15.038
    Jan 12 00:42:15.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename sched-preemption-path 01/12/23 00:42:15.039
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:42:15.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:42:15.059
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:569
    STEP: Finding an available node 01/12/23 00:42:15.066
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/12/23 00:42:15.066
    Jan 12 00:42:15.126: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-8022" to be "running"
    Jan 12 00:42:15.129: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.976575ms
    Jan 12 00:42:17.134: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007743622s
    Jan 12 00:42:17.135: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/12/23 00:42:17.139
    Jan 12 00:42:17.152: INFO: found a healthy node: eqx04-flash06
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:616
    Jan 12 00:42:29.245: INFO: pods created so far: [1 1 1]
    Jan 12 00:42:29.246: INFO: length of pods created so far: 3
    Jan 12 00:42:35.283: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:42:42.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:543
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:42:42.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-8022" for this suite. 01/12/23 00:42:42.392
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3798" for this suite. 01/12/23 00:42:42.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:42:42.433
Jan 12 00:42:42.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename replicaset 01/12/23 00:42:42.434
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:42:42.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:42:42.457
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/12/23 00:42:42.46
Jan 12 00:42:42.474: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 12 00:42:47.480: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/12/23 00:42:47.48
STEP: getting scale subresource 01/12/23 00:42:47.481
STEP: updating a scale subresource 01/12/23 00:42:47.487
STEP: verifying the replicaset Spec.Replicas was modified 01/12/23 00:42:47.499
STEP: Patch a scale subresource 01/12/23 00:42:47.503
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 12 00:42:47.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3164" for this suite. 01/12/23 00:42:47.523
------------------------------
• [SLOW TEST] [5.110 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:42:42.433
    Jan 12 00:42:42.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename replicaset 01/12/23 00:42:42.434
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:42:42.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:42:42.457
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/12/23 00:42:42.46
    Jan 12 00:42:42.474: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 12 00:42:47.480: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/12/23 00:42:47.48
    STEP: getting scale subresource 01/12/23 00:42:47.481
    STEP: updating a scale subresource 01/12/23 00:42:47.487
    STEP: verifying the replicaset Spec.Replicas was modified 01/12/23 00:42:47.499
    STEP: Patch a scale subresource 01/12/23 00:42:47.503
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:42:47.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3164" for this suite. 01/12/23 00:42:47.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:42:47.544
Jan 12 00:42:47.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 00:42:47.545
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:42:47.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:42:47.574
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 00:42:47.605
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 00:42:48.074
STEP: Deploying the webhook pod 01/12/23 00:42:48.083
STEP: Wait for the deployment to be ready 01/12/23 00:42:48.151
Jan 12 00:42:48.166: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 12 00:42:50.177: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 42, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 42, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 42, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 42, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 00:42:52.18
STEP: Verifying the service has paired with the endpoint 01/12/23 00:42:52.193
Jan 12 00:42:53.194: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/12/23 00:42:53.198
STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 00:42:53.198
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/12/23 00:42:53.222
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/12/23 00:42:54.237
STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 00:42:54.237
STEP: Having no error when timeout is longer than webhook latency 01/12/23 00:42:55.271
STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 00:42:55.271
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/12/23 00:43:00.31
STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 00:43:00.311
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:43:05.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1779" for this suite. 01/12/23 00:43:05.403
STEP: Destroying namespace "webhook-1779-markers" for this suite. 01/12/23 00:43:05.462
------------------------------
• [SLOW TEST] [17.938 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:42:47.544
    Jan 12 00:42:47.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 00:42:47.545
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:42:47.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:42:47.574
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 00:42:47.605
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 00:42:48.074
    STEP: Deploying the webhook pod 01/12/23 00:42:48.083
    STEP: Wait for the deployment to be ready 01/12/23 00:42:48.151
    Jan 12 00:42:48.166: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 12 00:42:50.177: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 42, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 42, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 42, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 42, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 00:42:52.18
    STEP: Verifying the service has paired with the endpoint 01/12/23 00:42:52.193
    Jan 12 00:42:53.194: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/12/23 00:42:53.198
    STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 00:42:53.198
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/12/23 00:42:53.222
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/12/23 00:42:54.237
    STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 00:42:54.237
    STEP: Having no error when timeout is longer than webhook latency 01/12/23 00:42:55.271
    STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 00:42:55.271
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/12/23 00:43:00.31
    STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 00:43:00.311
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:43:05.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1779" for this suite. 01/12/23 00:43:05.403
    STEP: Destroying namespace "webhook-1779-markers" for this suite. 01/12/23 00:43:05.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:43:05.483
Jan 12 00:43:05.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 00:43:05.484
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:05.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:05.513
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jan 12 00:43:05.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/12/23 00:43:07.947
Jan 12 00:43:07.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 create -f -'
Jan 12 00:43:08.928: INFO: stderr: ""
Jan 12 00:43:08.928: INFO: stdout: "e2e-test-crd-publish-openapi-7362-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 12 00:43:08.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 delete e2e-test-crd-publish-openapi-7362-crds test-foo'
Jan 12 00:43:09.000: INFO: stderr: ""
Jan 12 00:43:09.000: INFO: stdout: "e2e-test-crd-publish-openapi-7362-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 12 00:43:09.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 apply -f -'
Jan 12 00:43:09.232: INFO: stderr: ""
Jan 12 00:43:09.232: INFO: stdout: "e2e-test-crd-publish-openapi-7362-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 12 00:43:09.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 delete e2e-test-crd-publish-openapi-7362-crds test-foo'
Jan 12 00:43:09.305: INFO: stderr: ""
Jan 12 00:43:09.305: INFO: stdout: "e2e-test-crd-publish-openapi-7362-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/12/23 00:43:09.305
Jan 12 00:43:09.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 create -f -'
Jan 12 00:43:09.511: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/12/23 00:43:09.511
Jan 12 00:43:09.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 create -f -'
Jan 12 00:43:10.092: INFO: rc: 1
Jan 12 00:43:10.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 apply -f -'
Jan 12 00:43:10.270: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/12/23 00:43:10.27
Jan 12 00:43:10.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 create -f -'
Jan 12 00:43:11.048: INFO: rc: 1
Jan 12 00:43:11.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 apply -f -'
Jan 12 00:43:11.274: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/12/23 00:43:11.274
Jan 12 00:43:11.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 explain e2e-test-crd-publish-openapi-7362-crds'
Jan 12 00:43:11.448: INFO: stderr: ""
Jan 12 00:43:11.448: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7362-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/12/23 00:43:11.448
Jan 12 00:43:11.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 explain e2e-test-crd-publish-openapi-7362-crds.metadata'
Jan 12 00:43:11.630: INFO: stderr: ""
Jan 12 00:43:11.630: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7362-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 12 00:43:11.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 explain e2e-test-crd-publish-openapi-7362-crds.spec'
Jan 12 00:43:11.802: INFO: stderr: ""
Jan 12 00:43:11.802: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7362-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 12 00:43:11.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 explain e2e-test-crd-publish-openapi-7362-crds.spec.bars'
Jan 12 00:43:11.991: INFO: stderr: ""
Jan 12 00:43:11.991: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7362-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/12/23 00:43:11.991
Jan 12 00:43:11.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 explain e2e-test-crd-publish-openapi-7362-crds.spec.bars2'
Jan 12 00:43:12.172: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:43:14.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3349" for this suite. 01/12/23 00:43:14.585
------------------------------
• [SLOW TEST] [9.244 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:43:05.483
    Jan 12 00:43:05.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 00:43:05.484
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:05.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:05.513
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jan 12 00:43:05.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/12/23 00:43:07.947
    Jan 12 00:43:07.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 create -f -'
    Jan 12 00:43:08.928: INFO: stderr: ""
    Jan 12 00:43:08.928: INFO: stdout: "e2e-test-crd-publish-openapi-7362-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 12 00:43:08.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 delete e2e-test-crd-publish-openapi-7362-crds test-foo'
    Jan 12 00:43:09.000: INFO: stderr: ""
    Jan 12 00:43:09.000: INFO: stdout: "e2e-test-crd-publish-openapi-7362-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 12 00:43:09.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 apply -f -'
    Jan 12 00:43:09.232: INFO: stderr: ""
    Jan 12 00:43:09.232: INFO: stdout: "e2e-test-crd-publish-openapi-7362-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 12 00:43:09.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 delete e2e-test-crd-publish-openapi-7362-crds test-foo'
    Jan 12 00:43:09.305: INFO: stderr: ""
    Jan 12 00:43:09.305: INFO: stdout: "e2e-test-crd-publish-openapi-7362-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/12/23 00:43:09.305
    Jan 12 00:43:09.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 create -f -'
    Jan 12 00:43:09.511: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/12/23 00:43:09.511
    Jan 12 00:43:09.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 create -f -'
    Jan 12 00:43:10.092: INFO: rc: 1
    Jan 12 00:43:10.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 apply -f -'
    Jan 12 00:43:10.270: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/12/23 00:43:10.27
    Jan 12 00:43:10.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 create -f -'
    Jan 12 00:43:11.048: INFO: rc: 1
    Jan 12 00:43:11.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 --namespace=crd-publish-openapi-3349 apply -f -'
    Jan 12 00:43:11.274: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/12/23 00:43:11.274
    Jan 12 00:43:11.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 explain e2e-test-crd-publish-openapi-7362-crds'
    Jan 12 00:43:11.448: INFO: stderr: ""
    Jan 12 00:43:11.448: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7362-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/12/23 00:43:11.448
    Jan 12 00:43:11.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 explain e2e-test-crd-publish-openapi-7362-crds.metadata'
    Jan 12 00:43:11.630: INFO: stderr: ""
    Jan 12 00:43:11.630: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7362-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 12 00:43:11.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 explain e2e-test-crd-publish-openapi-7362-crds.spec'
    Jan 12 00:43:11.802: INFO: stderr: ""
    Jan 12 00:43:11.802: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7362-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 12 00:43:11.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 explain e2e-test-crd-publish-openapi-7362-crds.spec.bars'
    Jan 12 00:43:11.991: INFO: stderr: ""
    Jan 12 00:43:11.991: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7362-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/12/23 00:43:11.991
    Jan 12 00:43:11.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3349 explain e2e-test-crd-publish-openapi-7362-crds.spec.bars2'
    Jan 12 00:43:12.172: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:43:14.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3349" for this suite. 01/12/23 00:43:14.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:43:14.73
Jan 12 00:43:14.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename containers 01/12/23 00:43:14.731
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:14.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:14.753
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 01/12/23 00:43:14.756
Jan 12 00:43:14.927: INFO: Waiting up to 5m0s for pod "client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6" in namespace "containers-4454" to be "Succeeded or Failed"
Jan 12 00:43:14.930: INFO: Pod "client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.662702ms
Jan 12 00:43:16.933: INFO: Pod "client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006013285s
Jan 12 00:43:18.934: INFO: Pod "client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006630461s
Jan 12 00:43:20.934: INFO: Pod "client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006905612s
STEP: Saw pod success 01/12/23 00:43:20.934
Jan 12 00:43:20.934: INFO: Pod "client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6" satisfied condition "Succeeded or Failed"
Jan 12 00:43:20.936: INFO: Trying to get logs from node eqx04-flash06 pod client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 00:43:20.951
Jan 12 00:43:20.986: INFO: Waiting for pod client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6 to disappear
Jan 12 00:43:20.988: INFO: Pod client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 12 00:43:20.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4454" for this suite. 01/12/23 00:43:20.992
------------------------------
• [SLOW TEST] [6.280 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:43:14.73
    Jan 12 00:43:14.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename containers 01/12/23 00:43:14.731
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:14.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:14.753
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 01/12/23 00:43:14.756
    Jan 12 00:43:14.927: INFO: Waiting up to 5m0s for pod "client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6" in namespace "containers-4454" to be "Succeeded or Failed"
    Jan 12 00:43:14.930: INFO: Pod "client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.662702ms
    Jan 12 00:43:16.933: INFO: Pod "client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006013285s
    Jan 12 00:43:18.934: INFO: Pod "client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006630461s
    Jan 12 00:43:20.934: INFO: Pod "client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006905612s
    STEP: Saw pod success 01/12/23 00:43:20.934
    Jan 12 00:43:20.934: INFO: Pod "client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6" satisfied condition "Succeeded or Failed"
    Jan 12 00:43:20.936: INFO: Trying to get logs from node eqx04-flash06 pod client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 00:43:20.951
    Jan 12 00:43:20.986: INFO: Waiting for pod client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6 to disappear
    Jan 12 00:43:20.988: INFO: Pod client-containers-93bf1368-4858-4cae-9fba-c086b5bfcff6 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:43:20.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4454" for this suite. 01/12/23 00:43:20.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:43:21.011
Jan 12 00:43:21.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename dns 01/12/23 00:43:21.012
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:21.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:21.034
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/12/23 00:43:21.036
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4814.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4814.svc.cluster.local;sleep 1; done
 01/12/23 00:43:21.042
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4814.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4814.svc.cluster.local;sleep 1; done
 01/12/23 00:43:21.042
STEP: creating a pod to probe DNS 01/12/23 00:43:21.042
STEP: submitting the pod to kubernetes 01/12/23 00:43:21.042
Jan 12 00:43:21.106: INFO: Waiting up to 15m0s for pod "dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37" in namespace "dns-4814" to be "running"
Jan 12 00:43:21.109: INFO: Pod "dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.411ms
Jan 12 00:43:23.114: INFO: Pod "dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007456495s
Jan 12 00:43:25.113: INFO: Pod "dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37": Phase="Running", Reason="", readiness=true. Elapsed: 4.006477788s
Jan 12 00:43:25.113: INFO: Pod "dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37" satisfied condition "running"
STEP: retrieving the pod 01/12/23 00:43:25.113
STEP: looking for the results for each expected name from probers 01/12/23 00:43:25.119
Jan 12 00:43:25.135: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local from pod dns-4814/dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37: the server could not find the requested resource (get pods dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37)
Jan 12 00:43:25.137: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local from pod dns-4814/dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37: the server could not find the requested resource (get pods dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37)
Jan 12 00:43:25.140: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4814.svc.cluster.local from pod dns-4814/dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37: the server could not find the requested resource (get pods dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37)
Jan 12 00:43:25.142: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4814.svc.cluster.local from pod dns-4814/dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37: the server could not find the requested resource (get pods dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37)
Jan 12 00:43:25.143: INFO: Lookups using dns-4814/dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37 failed for: [jessie_udp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local jessie_udp@dns-test-service-2.dns-4814.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4814.svc.cluster.local]

Jan 12 00:43:30.168: INFO: DNS probes using dns-4814/dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37 succeeded

STEP: deleting the pod 01/12/23 00:43:30.168
STEP: deleting the test headless service 01/12/23 00:43:30.209
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 00:43:30.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4814" for this suite. 01/12/23 00:43:30.253
------------------------------
• [SLOW TEST] [9.264 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:43:21.011
    Jan 12 00:43:21.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename dns 01/12/23 00:43:21.012
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:21.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:21.034
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/12/23 00:43:21.036
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4814.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4814.svc.cluster.local;sleep 1; done
     01/12/23 00:43:21.042
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4814.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4814.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4814.svc.cluster.local;sleep 1; done
     01/12/23 00:43:21.042
    STEP: creating a pod to probe DNS 01/12/23 00:43:21.042
    STEP: submitting the pod to kubernetes 01/12/23 00:43:21.042
    Jan 12 00:43:21.106: INFO: Waiting up to 15m0s for pod "dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37" in namespace "dns-4814" to be "running"
    Jan 12 00:43:21.109: INFO: Pod "dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.411ms
    Jan 12 00:43:23.114: INFO: Pod "dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007456495s
    Jan 12 00:43:25.113: INFO: Pod "dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37": Phase="Running", Reason="", readiness=true. Elapsed: 4.006477788s
    Jan 12 00:43:25.113: INFO: Pod "dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 00:43:25.113
    STEP: looking for the results for each expected name from probers 01/12/23 00:43:25.119
    Jan 12 00:43:25.135: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local from pod dns-4814/dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37: the server could not find the requested resource (get pods dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37)
    Jan 12 00:43:25.137: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local from pod dns-4814/dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37: the server could not find the requested resource (get pods dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37)
    Jan 12 00:43:25.140: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4814.svc.cluster.local from pod dns-4814/dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37: the server could not find the requested resource (get pods dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37)
    Jan 12 00:43:25.142: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4814.svc.cluster.local from pod dns-4814/dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37: the server could not find the requested resource (get pods dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37)
    Jan 12 00:43:25.143: INFO: Lookups using dns-4814/dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37 failed for: [jessie_udp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4814.svc.cluster.local jessie_udp@dns-test-service-2.dns-4814.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4814.svc.cluster.local]

    Jan 12 00:43:30.168: INFO: DNS probes using dns-4814/dns-test-2d069a3a-ae85-434e-997a-b147a2f1db37 succeeded

    STEP: deleting the pod 01/12/23 00:43:30.168
    STEP: deleting the test headless service 01/12/23 00:43:30.209
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:43:30.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4814" for this suite. 01/12/23 00:43:30.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:43:30.279
Jan 12 00:43:30.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 00:43:30.279
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:30.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:30.306
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 01/12/23 00:43:30.308
Jan 12 00:43:30.727: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760" in namespace "downward-api-8094" to be "Succeeded or Failed"
Jan 12 00:43:30.730: INFO: Pod "downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760": Phase="Pending", Reason="", readiness=false. Elapsed: 2.741819ms
Jan 12 00:43:32.733: INFO: Pod "downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006538036s
Jan 12 00:43:34.734: INFO: Pod "downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007436283s
Jan 12 00:43:36.734: INFO: Pod "downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007327466s
STEP: Saw pod success 01/12/23 00:43:36.734
Jan 12 00:43:36.734: INFO: Pod "downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760" satisfied condition "Succeeded or Failed"
Jan 12 00:43:36.737: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760 container client-container: <nil>
STEP: delete the pod 01/12/23 00:43:36.746
Jan 12 00:43:36.775: INFO: Waiting for pod downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760 to disappear
Jan 12 00:43:36.778: INFO: Pod downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 00:43:36.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8094" for this suite. 01/12/23 00:43:36.781
------------------------------
• [SLOW TEST] [6.524 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:43:30.279
    Jan 12 00:43:30.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 00:43:30.279
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:30.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:30.306
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 01/12/23 00:43:30.308
    Jan 12 00:43:30.727: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760" in namespace "downward-api-8094" to be "Succeeded or Failed"
    Jan 12 00:43:30.730: INFO: Pod "downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760": Phase="Pending", Reason="", readiness=false. Elapsed: 2.741819ms
    Jan 12 00:43:32.733: INFO: Pod "downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006538036s
    Jan 12 00:43:34.734: INFO: Pod "downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007436283s
    Jan 12 00:43:36.734: INFO: Pod "downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007327466s
    STEP: Saw pod success 01/12/23 00:43:36.734
    Jan 12 00:43:36.734: INFO: Pod "downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760" satisfied condition "Succeeded or Failed"
    Jan 12 00:43:36.737: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760 container client-container: <nil>
    STEP: delete the pod 01/12/23 00:43:36.746
    Jan 12 00:43:36.775: INFO: Waiting for pod downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760 to disappear
    Jan 12 00:43:36.778: INFO: Pod downwardapi-volume-9f84d8df-0a52-460c-bc92-1ea5fca42760 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:43:36.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8094" for this suite. 01/12/23 00:43:36.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:43:36.804
Jan 12 00:43:36.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename replication-controller 01/12/23 00:43:36.805
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:36.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:36.831
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jan 12 00:43:36.833: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/12/23 00:43:36.854
STEP: Checking rc "condition-test" has the desired failure condition set 01/12/23 00:43:36.863
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/12/23 00:43:37.868
Jan 12 00:43:37.880: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/12/23 00:43:37.88
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 12 00:43:37.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2208" for this suite. 01/12/23 00:43:37.896
------------------------------
• [1.109 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:43:36.804
    Jan 12 00:43:36.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename replication-controller 01/12/23 00:43:36.805
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:36.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:36.831
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jan 12 00:43:36.833: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/12/23 00:43:36.854
    STEP: Checking rc "condition-test" has the desired failure condition set 01/12/23 00:43:36.863
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/12/23 00:43:37.868
    Jan 12 00:43:37.880: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/12/23 00:43:37.88
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:43:37.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2208" for this suite. 01/12/23 00:43:37.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:43:37.915
Jan 12 00:43:37.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 00:43:37.916
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:37.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:37.938
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 01/12/23 00:43:37.941
Jan 12 00:43:37.971: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2" in namespace "downward-api-5314" to be "Succeeded or Failed"
Jan 12 00:43:37.974: INFO: Pod "downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.347305ms
Jan 12 00:43:39.977: INFO: Pod "downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005600849s
Jan 12 00:43:41.977: INFO: Pod "downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005178503s
Jan 12 00:43:43.978: INFO: Pod "downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006144881s
STEP: Saw pod success 01/12/23 00:43:43.978
Jan 12 00:43:43.978: INFO: Pod "downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2" satisfied condition "Succeeded or Failed"
Jan 12 00:43:43.980: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2 container client-container: <nil>
STEP: delete the pod 01/12/23 00:43:43.989
Jan 12 00:43:44.012: INFO: Waiting for pod downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2 to disappear
Jan 12 00:43:44.014: INFO: Pod downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 00:43:44.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5314" for this suite. 01/12/23 00:43:44.018
------------------------------
• [SLOW TEST] [6.181 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:43:37.915
    Jan 12 00:43:37.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 00:43:37.916
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:37.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:37.938
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 01/12/23 00:43:37.941
    Jan 12 00:43:37.971: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2" in namespace "downward-api-5314" to be "Succeeded or Failed"
    Jan 12 00:43:37.974: INFO: Pod "downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.347305ms
    Jan 12 00:43:39.977: INFO: Pod "downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005600849s
    Jan 12 00:43:41.977: INFO: Pod "downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005178503s
    Jan 12 00:43:43.978: INFO: Pod "downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006144881s
    STEP: Saw pod success 01/12/23 00:43:43.978
    Jan 12 00:43:43.978: INFO: Pod "downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2" satisfied condition "Succeeded or Failed"
    Jan 12 00:43:43.980: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2 container client-container: <nil>
    STEP: delete the pod 01/12/23 00:43:43.989
    Jan 12 00:43:44.012: INFO: Waiting for pod downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2 to disappear
    Jan 12 00:43:44.014: INFO: Pod downwardapi-volume-39285160-7fee-49c8-bd8e-b19beda897b2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:43:44.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5314" for this suite. 01/12/23 00:43:44.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:43:44.1
Jan 12 00:43:44.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename var-expansion 01/12/23 00:43:44.101
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:44.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:44.123
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Jan 12 00:43:44.247: INFO: Waiting up to 2m0s for pod "var-expansion-037ed53c-4a9f-460c-ad11-ecf41a66bc4d" in namespace "var-expansion-2177" to be "container 0 failed with reason CreateContainerConfigError"
Jan 12 00:43:44.249: INFO: Pod "var-expansion-037ed53c-4a9f-460c-ad11-ecf41a66bc4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.342965ms
Jan 12 00:43:46.252: INFO: Pod "var-expansion-037ed53c-4a9f-460c-ad11-ecf41a66bc4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005535159s
Jan 12 00:43:46.252: INFO: Pod "var-expansion-037ed53c-4a9f-460c-ad11-ecf41a66bc4d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 12 00:43:46.252: INFO: Deleting pod "var-expansion-037ed53c-4a9f-460c-ad11-ecf41a66bc4d" in namespace "var-expansion-2177"
Jan 12 00:43:46.267: INFO: Wait up to 5m0s for pod "var-expansion-037ed53c-4a9f-460c-ad11-ecf41a66bc4d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 00:43:50.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2177" for this suite. 01/12/23 00:43:50.277
------------------------------
• [SLOW TEST] [6.195 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:43:44.1
    Jan 12 00:43:44.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename var-expansion 01/12/23 00:43:44.101
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:44.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:44.123
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Jan 12 00:43:44.247: INFO: Waiting up to 2m0s for pod "var-expansion-037ed53c-4a9f-460c-ad11-ecf41a66bc4d" in namespace "var-expansion-2177" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 12 00:43:44.249: INFO: Pod "var-expansion-037ed53c-4a9f-460c-ad11-ecf41a66bc4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.342965ms
    Jan 12 00:43:46.252: INFO: Pod "var-expansion-037ed53c-4a9f-460c-ad11-ecf41a66bc4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005535159s
    Jan 12 00:43:46.252: INFO: Pod "var-expansion-037ed53c-4a9f-460c-ad11-ecf41a66bc4d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 12 00:43:46.252: INFO: Deleting pod "var-expansion-037ed53c-4a9f-460c-ad11-ecf41a66bc4d" in namespace "var-expansion-2177"
    Jan 12 00:43:46.267: INFO: Wait up to 5m0s for pod "var-expansion-037ed53c-4a9f-460c-ad11-ecf41a66bc4d" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:43:50.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2177" for this suite. 01/12/23 00:43:50.277
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:43:50.296
Jan 12 00:43:50.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 00:43:50.297
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:50.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:50.329
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 01/12/23 00:43:50.334
Jan 12 00:43:50.334: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 12 00:43:50.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 create -f -'
Jan 12 00:43:51.312: INFO: stderr: ""
Jan 12 00:43:51.312: INFO: stdout: "service/agnhost-replica created\n"
Jan 12 00:43:51.312: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 12 00:43:51.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 create -f -'
Jan 12 00:43:51.879: INFO: stderr: ""
Jan 12 00:43:51.879: INFO: stdout: "service/agnhost-primary created\n"
Jan 12 00:43:51.879: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 12 00:43:51.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 create -f -'
Jan 12 00:43:52.575: INFO: stderr: ""
Jan 12 00:43:52.575: INFO: stdout: "service/frontend created\n"
Jan 12 00:43:52.575: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 12 00:43:52.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 create -f -'
Jan 12 00:43:52.780: INFO: stderr: ""
Jan 12 00:43:52.780: INFO: stdout: "deployment.apps/frontend created\n"
Jan 12 00:43:52.781: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 12 00:43:52.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 create -f -'
Jan 12 00:43:53.037: INFO: stderr: ""
Jan 12 00:43:53.037: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 12 00:43:53.037: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 12 00:43:53.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 create -f -'
Jan 12 00:43:53.294: INFO: stderr: ""
Jan 12 00:43:53.294: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/12/23 00:43:53.294
Jan 12 00:43:53.295: INFO: Waiting for all frontend pods to be Running.
Jan 12 00:43:58.347: INFO: Waiting for frontend to serve content.
Jan 12 00:43:58.365: INFO: Trying to add a new entry to the guestbook.
Jan 12 00:43:58.373: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/12/23 00:43:58.381
Jan 12 00:43:58.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 delete --grace-period=0 --force -f -'
Jan 12 00:43:58.488: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 00:43:58.488: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/12/23 00:43:58.488
Jan 12 00:43:58.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 delete --grace-period=0 --force -f -'
Jan 12 00:43:58.656: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 00:43:58.656: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/12/23 00:43:58.656
Jan 12 00:43:58.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 delete --grace-period=0 --force -f -'
Jan 12 00:43:58.758: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 00:43:58.758: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/12/23 00:43:58.758
Jan 12 00:43:58.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 delete --grace-period=0 --force -f -'
Jan 12 00:43:58.827: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 00:43:58.827: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/12/23 00:43:58.828
Jan 12 00:43:58.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 delete --grace-period=0 --force -f -'
Jan 12 00:43:58.914: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 00:43:58.914: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/12/23 00:43:58.914
Jan 12 00:43:58.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 delete --grace-period=0 --force -f -'
Jan 12 00:43:59.067: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 00:43:59.068: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 00:43:59.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8341" for this suite. 01/12/23 00:43:59.077
------------------------------
• [SLOW TEST] [8.899 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:43:50.296
    Jan 12 00:43:50.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 00:43:50.297
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:50.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:50.329
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 01/12/23 00:43:50.334
    Jan 12 00:43:50.334: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 12 00:43:50.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 create -f -'
    Jan 12 00:43:51.312: INFO: stderr: ""
    Jan 12 00:43:51.312: INFO: stdout: "service/agnhost-replica created\n"
    Jan 12 00:43:51.312: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 12 00:43:51.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 create -f -'
    Jan 12 00:43:51.879: INFO: stderr: ""
    Jan 12 00:43:51.879: INFO: stdout: "service/agnhost-primary created\n"
    Jan 12 00:43:51.879: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 12 00:43:51.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 create -f -'
    Jan 12 00:43:52.575: INFO: stderr: ""
    Jan 12 00:43:52.575: INFO: stdout: "service/frontend created\n"
    Jan 12 00:43:52.575: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 12 00:43:52.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 create -f -'
    Jan 12 00:43:52.780: INFO: stderr: ""
    Jan 12 00:43:52.780: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 12 00:43:52.781: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 12 00:43:52.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 create -f -'
    Jan 12 00:43:53.037: INFO: stderr: ""
    Jan 12 00:43:53.037: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 12 00:43:53.037: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 12 00:43:53.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 create -f -'
    Jan 12 00:43:53.294: INFO: stderr: ""
    Jan 12 00:43:53.294: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/12/23 00:43:53.294
    Jan 12 00:43:53.295: INFO: Waiting for all frontend pods to be Running.
    Jan 12 00:43:58.347: INFO: Waiting for frontend to serve content.
    Jan 12 00:43:58.365: INFO: Trying to add a new entry to the guestbook.
    Jan 12 00:43:58.373: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/12/23 00:43:58.381
    Jan 12 00:43:58.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 delete --grace-period=0 --force -f -'
    Jan 12 00:43:58.488: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 00:43:58.488: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/12/23 00:43:58.488
    Jan 12 00:43:58.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 delete --grace-period=0 --force -f -'
    Jan 12 00:43:58.656: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 00:43:58.656: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/12/23 00:43:58.656
    Jan 12 00:43:58.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 delete --grace-period=0 --force -f -'
    Jan 12 00:43:58.758: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 00:43:58.758: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/12/23 00:43:58.758
    Jan 12 00:43:58.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 delete --grace-period=0 --force -f -'
    Jan 12 00:43:58.827: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 00:43:58.827: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/12/23 00:43:58.828
    Jan 12 00:43:58.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 delete --grace-period=0 --force -f -'
    Jan 12 00:43:58.914: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 00:43:58.914: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/12/23 00:43:58.914
    Jan 12 00:43:58.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8341 delete --grace-period=0 --force -f -'
    Jan 12 00:43:59.067: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 00:43:59.068: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:43:59.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8341" for this suite. 01/12/23 00:43:59.077
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:43:59.196
Jan 12 00:43:59.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-probe 01/12/23 00:43:59.202
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:59.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:59.238
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e in namespace container-probe-3034 01/12/23 00:43:59.247
Jan 12 00:43:59.289: INFO: Waiting up to 5m0s for pod "liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e" in namespace "container-probe-3034" to be "not pending"
Jan 12 00:43:59.291: INFO: Pod "liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315747ms
Jan 12 00:44:01.296: INFO: Pod "liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007173597s
Jan 12 00:44:03.295: INFO: Pod "liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e": Phase="Running", Reason="", readiness=true. Elapsed: 4.006039669s
Jan 12 00:44:03.295: INFO: Pod "liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e" satisfied condition "not pending"
Jan 12 00:44:03.295: INFO: Started pod liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e in namespace container-probe-3034
STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 00:44:03.295
Jan 12 00:44:03.298: INFO: Initial restart count of pod liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e is 0
Jan 12 00:44:21.333: INFO: Restart count of pod container-probe-3034/liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e is now 1 (18.035310349s elapsed)
Jan 12 00:44:41.373: INFO: Restart count of pod container-probe-3034/liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e is now 2 (38.074973204s elapsed)
Jan 12 00:45:01.411: INFO: Restart count of pod container-probe-3034/liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e is now 3 (58.113244862s elapsed)
Jan 12 00:45:21.453: INFO: Restart count of pod container-probe-3034/liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e is now 4 (1m18.154957961s elapsed)
Jan 12 00:46:33.591: INFO: Restart count of pod container-probe-3034/liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e is now 5 (2m30.293133062s elapsed)
STEP: deleting the pod 01/12/23 00:46:33.591
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 00:46:33.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3034" for this suite. 01/12/23 00:46:33.623
------------------------------
• [SLOW TEST] [154.448 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:43:59.196
    Jan 12 00:43:59.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-probe 01/12/23 00:43:59.202
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:43:59.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:43:59.238
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e in namespace container-probe-3034 01/12/23 00:43:59.247
    Jan 12 00:43:59.289: INFO: Waiting up to 5m0s for pod "liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e" in namespace "container-probe-3034" to be "not pending"
    Jan 12 00:43:59.291: INFO: Pod "liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315747ms
    Jan 12 00:44:01.296: INFO: Pod "liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007173597s
    Jan 12 00:44:03.295: INFO: Pod "liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e": Phase="Running", Reason="", readiness=true. Elapsed: 4.006039669s
    Jan 12 00:44:03.295: INFO: Pod "liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e" satisfied condition "not pending"
    Jan 12 00:44:03.295: INFO: Started pod liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e in namespace container-probe-3034
    STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 00:44:03.295
    Jan 12 00:44:03.298: INFO: Initial restart count of pod liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e is 0
    Jan 12 00:44:21.333: INFO: Restart count of pod container-probe-3034/liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e is now 1 (18.035310349s elapsed)
    Jan 12 00:44:41.373: INFO: Restart count of pod container-probe-3034/liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e is now 2 (38.074973204s elapsed)
    Jan 12 00:45:01.411: INFO: Restart count of pod container-probe-3034/liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e is now 3 (58.113244862s elapsed)
    Jan 12 00:45:21.453: INFO: Restart count of pod container-probe-3034/liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e is now 4 (1m18.154957961s elapsed)
    Jan 12 00:46:33.591: INFO: Restart count of pod container-probe-3034/liveness-08fa3ad8-a955-41f1-b2ae-e5f93178786e is now 5 (2m30.293133062s elapsed)
    STEP: deleting the pod 01/12/23 00:46:33.591
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:46:33.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3034" for this suite. 01/12/23 00:46:33.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:46:33.646
Jan 12 00:46:33.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename resourcequota 01/12/23 00:46:33.647
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:46:33.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:46:33.672
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 01/12/23 00:46:33.675
STEP: Creating a ResourceQuota 01/12/23 00:46:38.681
STEP: Ensuring resource quota status is calculated 01/12/23 00:46:38.695
STEP: Creating a Pod that fits quota 01/12/23 00:46:40.7
STEP: Ensuring ResourceQuota status captures the pod usage 01/12/23 00:46:40.765
STEP: Not allowing a pod to be created that exceeds remaining quota 01/12/23 00:46:42.769
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/12/23 00:46:42.798
STEP: Ensuring a pod cannot update its resource requirements 01/12/23 00:46:42.822
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/12/23 00:46:42.826
STEP: Deleting the pod 01/12/23 00:46:44.831
STEP: Ensuring resource quota status released the pod usage 01/12/23 00:46:44.87
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 00:46:46.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1925" for this suite. 01/12/23 00:46:46.878
------------------------------
• [SLOW TEST] [13.253 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:46:33.646
    Jan 12 00:46:33.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename resourcequota 01/12/23 00:46:33.647
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:46:33.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:46:33.672
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 01/12/23 00:46:33.675
    STEP: Creating a ResourceQuota 01/12/23 00:46:38.681
    STEP: Ensuring resource quota status is calculated 01/12/23 00:46:38.695
    STEP: Creating a Pod that fits quota 01/12/23 00:46:40.7
    STEP: Ensuring ResourceQuota status captures the pod usage 01/12/23 00:46:40.765
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/12/23 00:46:42.769
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/12/23 00:46:42.798
    STEP: Ensuring a pod cannot update its resource requirements 01/12/23 00:46:42.822
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/12/23 00:46:42.826
    STEP: Deleting the pod 01/12/23 00:46:44.831
    STEP: Ensuring resource quota status released the pod usage 01/12/23 00:46:44.87
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:46:46.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1925" for this suite. 01/12/23 00:46:46.878
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:46:46.9
Jan 12 00:46:46.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 00:46:46.9
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:46:46.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:46:46.925
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 00:46:46.944
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 00:46:47.373
STEP: Deploying the webhook pod 01/12/23 00:46:47.383
STEP: Wait for the deployment to be ready 01/12/23 00:46:47.636
Jan 12 00:46:47.677: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 12 00:46:49.686: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 46, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 46, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 46, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 46, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 00:46:51.69
STEP: Verifying the service has paired with the endpoint 01/12/23 00:46:51.707
Jan 12 00:46:52.708: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/12/23 00:46:52.711
STEP: create a configmap that should be updated by the webhook 01/12/23 00:46:52.734
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:46:52.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5573" for this suite. 01/12/23 00:46:52.856
STEP: Destroying namespace "webhook-5573-markers" for this suite. 01/12/23 00:46:52.898
------------------------------
• [SLOW TEST] [6.047 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:46:46.9
    Jan 12 00:46:46.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 00:46:46.9
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:46:46.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:46:46.925
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 00:46:46.944
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 00:46:47.373
    STEP: Deploying the webhook pod 01/12/23 00:46:47.383
    STEP: Wait for the deployment to be ready 01/12/23 00:46:47.636
    Jan 12 00:46:47.677: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 12 00:46:49.686: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 46, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 46, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 46, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 46, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 00:46:51.69
    STEP: Verifying the service has paired with the endpoint 01/12/23 00:46:51.707
    Jan 12 00:46:52.708: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/12/23 00:46:52.711
    STEP: create a configmap that should be updated by the webhook 01/12/23 00:46:52.734
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:46:52.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5573" for this suite. 01/12/23 00:46:52.856
    STEP: Destroying namespace "webhook-5573-markers" for this suite. 01/12/23 00:46:52.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:46:52.948
Jan 12 00:46:52.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-probe 01/12/23 00:46:52.949
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:46:52.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:46:52.98
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-277ae1ab-fecd-49df-b1ec-7add4424b363 in namespace container-probe-9934 01/12/23 00:46:52.982
Jan 12 00:46:53.015: INFO: Waiting up to 5m0s for pod "liveness-277ae1ab-fecd-49df-b1ec-7add4424b363" in namespace "container-probe-9934" to be "not pending"
Jan 12 00:46:53.017: INFO: Pod "liveness-277ae1ab-fecd-49df-b1ec-7add4424b363": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260038ms
Jan 12 00:46:55.020: INFO: Pod "liveness-277ae1ab-fecd-49df-b1ec-7add4424b363": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005490166s
Jan 12 00:46:57.020: INFO: Pod "liveness-277ae1ab-fecd-49df-b1ec-7add4424b363": Phase="Running", Reason="", readiness=true. Elapsed: 4.005570237s
Jan 12 00:46:57.020: INFO: Pod "liveness-277ae1ab-fecd-49df-b1ec-7add4424b363" satisfied condition "not pending"
Jan 12 00:46:57.020: INFO: Started pod liveness-277ae1ab-fecd-49df-b1ec-7add4424b363 in namespace container-probe-9934
STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 00:46:57.02
Jan 12 00:46:57.023: INFO: Initial restart count of pod liveness-277ae1ab-fecd-49df-b1ec-7add4424b363 is 0
Jan 12 00:47:15.059: INFO: Restart count of pod container-probe-9934/liveness-277ae1ab-fecd-49df-b1ec-7add4424b363 is now 1 (18.036055468s elapsed)
STEP: deleting the pod 01/12/23 00:47:15.059
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 00:47:15.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9934" for this suite. 01/12/23 00:47:15.095
------------------------------
• [SLOW TEST] [22.225 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:46:52.948
    Jan 12 00:46:52.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-probe 01/12/23 00:46:52.949
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:46:52.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:46:52.98
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-277ae1ab-fecd-49df-b1ec-7add4424b363 in namespace container-probe-9934 01/12/23 00:46:52.982
    Jan 12 00:46:53.015: INFO: Waiting up to 5m0s for pod "liveness-277ae1ab-fecd-49df-b1ec-7add4424b363" in namespace "container-probe-9934" to be "not pending"
    Jan 12 00:46:53.017: INFO: Pod "liveness-277ae1ab-fecd-49df-b1ec-7add4424b363": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260038ms
    Jan 12 00:46:55.020: INFO: Pod "liveness-277ae1ab-fecd-49df-b1ec-7add4424b363": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005490166s
    Jan 12 00:46:57.020: INFO: Pod "liveness-277ae1ab-fecd-49df-b1ec-7add4424b363": Phase="Running", Reason="", readiness=true. Elapsed: 4.005570237s
    Jan 12 00:46:57.020: INFO: Pod "liveness-277ae1ab-fecd-49df-b1ec-7add4424b363" satisfied condition "not pending"
    Jan 12 00:46:57.020: INFO: Started pod liveness-277ae1ab-fecd-49df-b1ec-7add4424b363 in namespace container-probe-9934
    STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 00:46:57.02
    Jan 12 00:46:57.023: INFO: Initial restart count of pod liveness-277ae1ab-fecd-49df-b1ec-7add4424b363 is 0
    Jan 12 00:47:15.059: INFO: Restart count of pod container-probe-9934/liveness-277ae1ab-fecd-49df-b1ec-7add4424b363 is now 1 (18.036055468s elapsed)
    STEP: deleting the pod 01/12/23 00:47:15.059
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:47:15.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9934" for this suite. 01/12/23 00:47:15.095
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:47:15.173
Jan 12 00:47:15.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pods 01/12/23 00:47:15.174
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:15.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:15.198
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 01/12/23 00:47:15.2
STEP: setting up watch 01/12/23 00:47:15.2
STEP: submitting the pod to kubernetes 01/12/23 00:47:15.303
STEP: verifying the pod is in kubernetes 01/12/23 00:47:15.43
STEP: verifying pod creation was observed 01/12/23 00:47:15.435
Jan 12 00:47:15.435: INFO: Waiting up to 5m0s for pod "pod-submit-remove-8775861f-f625-4d60-971c-f6698cbbd5cf" in namespace "pods-2130" to be "running"
Jan 12 00:47:15.437: INFO: Pod "pod-submit-remove-8775861f-f625-4d60-971c-f6698cbbd5cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.578993ms
Jan 12 00:47:17.441: INFO: Pod "pod-submit-remove-8775861f-f625-4d60-971c-f6698cbbd5cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006532662s
Jan 12 00:47:19.441: INFO: Pod "pod-submit-remove-8775861f-f625-4d60-971c-f6698cbbd5cf": Phase="Running", Reason="", readiness=true. Elapsed: 4.006277614s
Jan 12 00:47:19.441: INFO: Pod "pod-submit-remove-8775861f-f625-4d60-971c-f6698cbbd5cf" satisfied condition "running"
STEP: deleting the pod gracefully 01/12/23 00:47:19.443
STEP: verifying pod deletion was observed 01/12/23 00:47:19.46
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 00:47:20.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2130" for this suite. 01/12/23 00:47:20.817
------------------------------
• [SLOW TEST] [5.664 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:47:15.173
    Jan 12 00:47:15.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pods 01/12/23 00:47:15.174
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:15.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:15.198
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 01/12/23 00:47:15.2
    STEP: setting up watch 01/12/23 00:47:15.2
    STEP: submitting the pod to kubernetes 01/12/23 00:47:15.303
    STEP: verifying the pod is in kubernetes 01/12/23 00:47:15.43
    STEP: verifying pod creation was observed 01/12/23 00:47:15.435
    Jan 12 00:47:15.435: INFO: Waiting up to 5m0s for pod "pod-submit-remove-8775861f-f625-4d60-971c-f6698cbbd5cf" in namespace "pods-2130" to be "running"
    Jan 12 00:47:15.437: INFO: Pod "pod-submit-remove-8775861f-f625-4d60-971c-f6698cbbd5cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.578993ms
    Jan 12 00:47:17.441: INFO: Pod "pod-submit-remove-8775861f-f625-4d60-971c-f6698cbbd5cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006532662s
    Jan 12 00:47:19.441: INFO: Pod "pod-submit-remove-8775861f-f625-4d60-971c-f6698cbbd5cf": Phase="Running", Reason="", readiness=true. Elapsed: 4.006277614s
    Jan 12 00:47:19.441: INFO: Pod "pod-submit-remove-8775861f-f625-4d60-971c-f6698cbbd5cf" satisfied condition "running"
    STEP: deleting the pod gracefully 01/12/23 00:47:19.443
    STEP: verifying pod deletion was observed 01/12/23 00:47:19.46
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:47:20.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2130" for this suite. 01/12/23 00:47:20.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:47:20.84
Jan 12 00:47:20.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubelet-test 01/12/23 00:47:20.84
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:20.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:20.859
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 12 00:47:24.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6798" for this suite. 01/12/23 00:47:24.925
------------------------------
• [4.118 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:47:20.84
    Jan 12 00:47:20.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubelet-test 01/12/23 00:47:20.84
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:20.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:20.859
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:47:24.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6798" for this suite. 01/12/23 00:47:24.925
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:47:24.959
Jan 12 00:47:24.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pods 01/12/23 00:47:24.96
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:24.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:24.981
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/12/23 00:47:24.983
STEP: submitting the pod to kubernetes 01/12/23 00:47:24.983
STEP: verifying QOS class is set on the pod 01/12/23 00:47:25.017
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jan 12 00:47:25.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4058" for this suite. 01/12/23 00:47:25.023
------------------------------
• [0.092 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:47:24.959
    Jan 12 00:47:24.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pods 01/12/23 00:47:24.96
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:24.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:24.981
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/12/23 00:47:24.983
    STEP: submitting the pod to kubernetes 01/12/23 00:47:24.983
    STEP: verifying QOS class is set on the pod 01/12/23 00:47:25.017
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:47:25.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4058" for this suite. 01/12/23 00:47:25.023
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:47:25.052
Jan 12 00:47:25.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename secrets 01/12/23 00:47:25.052
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:25.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:25.085
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-d20d1b12-e439-4380-8877-874810bf3225 01/12/23 00:47:25.094
STEP: Creating secret with name s-test-opt-upd-a72d0f26-386e-4c12-bc8e-552facde7420 01/12/23 00:47:25.102
STEP: Creating the pod 01/12/23 00:47:25.11
Jan 12 00:47:25.148: INFO: Waiting up to 5m0s for pod "pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7" in namespace "secrets-4222" to be "running and ready"
Jan 12 00:47:25.151: INFO: Pod "pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.381287ms
Jan 12 00:47:25.151: INFO: The phase of Pod pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:47:27.154: INFO: Pod "pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005943545s
Jan 12 00:47:27.154: INFO: The phase of Pod pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:47:29.155: INFO: Pod "pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7": Phase="Running", Reason="", readiness=true. Elapsed: 4.006312962s
Jan 12 00:47:29.155: INFO: The phase of Pod pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7 is Running (Ready = true)
Jan 12 00:47:29.155: INFO: Pod "pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-d20d1b12-e439-4380-8877-874810bf3225 01/12/23 00:47:29.197
STEP: Updating secret s-test-opt-upd-a72d0f26-386e-4c12-bc8e-552facde7420 01/12/23 00:47:29.207
STEP: Creating secret with name s-test-opt-create-2d686b9b-485a-4d5d-b18e-83eea8cb29db 01/12/23 00:47:29.216
STEP: waiting to observe update in volume 01/12/23 00:47:29.23
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 00:47:31.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4222" for this suite. 01/12/23 00:47:31.268
------------------------------
• [SLOW TEST] [6.284 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:47:25.052
    Jan 12 00:47:25.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename secrets 01/12/23 00:47:25.052
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:25.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:25.085
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-d20d1b12-e439-4380-8877-874810bf3225 01/12/23 00:47:25.094
    STEP: Creating secret with name s-test-opt-upd-a72d0f26-386e-4c12-bc8e-552facde7420 01/12/23 00:47:25.102
    STEP: Creating the pod 01/12/23 00:47:25.11
    Jan 12 00:47:25.148: INFO: Waiting up to 5m0s for pod "pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7" in namespace "secrets-4222" to be "running and ready"
    Jan 12 00:47:25.151: INFO: Pod "pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.381287ms
    Jan 12 00:47:25.151: INFO: The phase of Pod pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:47:27.154: INFO: Pod "pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005943545s
    Jan 12 00:47:27.154: INFO: The phase of Pod pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:47:29.155: INFO: Pod "pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7": Phase="Running", Reason="", readiness=true. Elapsed: 4.006312962s
    Jan 12 00:47:29.155: INFO: The phase of Pod pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7 is Running (Ready = true)
    Jan 12 00:47:29.155: INFO: Pod "pod-secrets-885d35d9-91d1-44e1-9330-f6dde50c5fb7" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-d20d1b12-e439-4380-8877-874810bf3225 01/12/23 00:47:29.197
    STEP: Updating secret s-test-opt-upd-a72d0f26-386e-4c12-bc8e-552facde7420 01/12/23 00:47:29.207
    STEP: Creating secret with name s-test-opt-create-2d686b9b-485a-4d5d-b18e-83eea8cb29db 01/12/23 00:47:29.216
    STEP: waiting to observe update in volume 01/12/23 00:47:29.23
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:47:31.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4222" for this suite. 01/12/23 00:47:31.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:47:31.34
Jan 12 00:47:31.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 00:47:31.341
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:31.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:31.368
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 01/12/23 00:47:31.37
Jan 12 00:47:31.407: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419" in namespace "projected-6577" to be "Succeeded or Failed"
Jan 12 00:47:31.409: INFO: Pod "downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419": Phase="Pending", Reason="", readiness=false. Elapsed: 2.102818ms
Jan 12 00:47:33.413: INFO: Pod "downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005603761s
Jan 12 00:47:35.413: INFO: Pod "downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005504067s
Jan 12 00:47:37.414: INFO: Pod "downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006474438s
STEP: Saw pod success 01/12/23 00:47:37.414
Jan 12 00:47:37.414: INFO: Pod "downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419" satisfied condition "Succeeded or Failed"
Jan 12 00:47:37.416: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419 container client-container: <nil>
STEP: delete the pod 01/12/23 00:47:37.427
Jan 12 00:47:37.453: INFO: Waiting for pod downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419 to disappear
Jan 12 00:47:37.469: INFO: Pod downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 00:47:37.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6577" for this suite. 01/12/23 00:47:37.473
------------------------------
• [SLOW TEST] [6.150 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:47:31.34
    Jan 12 00:47:31.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 00:47:31.341
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:31.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:31.368
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 01/12/23 00:47:31.37
    Jan 12 00:47:31.407: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419" in namespace "projected-6577" to be "Succeeded or Failed"
    Jan 12 00:47:31.409: INFO: Pod "downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419": Phase="Pending", Reason="", readiness=false. Elapsed: 2.102818ms
    Jan 12 00:47:33.413: INFO: Pod "downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005603761s
    Jan 12 00:47:35.413: INFO: Pod "downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005504067s
    Jan 12 00:47:37.414: INFO: Pod "downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006474438s
    STEP: Saw pod success 01/12/23 00:47:37.414
    Jan 12 00:47:37.414: INFO: Pod "downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419" satisfied condition "Succeeded or Failed"
    Jan 12 00:47:37.416: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419 container client-container: <nil>
    STEP: delete the pod 01/12/23 00:47:37.427
    Jan 12 00:47:37.453: INFO: Waiting for pod downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419 to disappear
    Jan 12 00:47:37.469: INFO: Pod downwardapi-volume-aadaa348-b98b-4133-95d0-f6709cf96419 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:47:37.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6577" for this suite. 01/12/23 00:47:37.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:47:37.491
Jan 12 00:47:37.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename lease-test 01/12/23 00:47:37.492
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:37.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:37.507
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jan 12 00:47:37.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-4481" for this suite. 01/12/23 00:47:37.609
------------------------------
• [0.136 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:47:37.491
    Jan 12 00:47:37.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename lease-test 01/12/23 00:47:37.492
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:37.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:37.507
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:47:37.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-4481" for this suite. 01/12/23 00:47:37.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:47:37.628
Jan 12 00:47:37.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename podtemplate 01/12/23 00:47:37.629
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:37.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:37.66
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 12 00:47:37.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6017" for this suite. 01/12/23 00:47:37.723
------------------------------
• [0.111 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:47:37.628
    Jan 12 00:47:37.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename podtemplate 01/12/23 00:47:37.629
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:37.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:37.66
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:47:37.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6017" for this suite. 01/12/23 00:47:37.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:47:37.74
Jan 12 00:47:37.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 00:47:37.741
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:37.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:37.764
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6823 01/12/23 00:47:37.766
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/12/23 00:47:37.791
STEP: creating service externalsvc in namespace services-6823 01/12/23 00:47:37.791
STEP: creating replication controller externalsvc in namespace services-6823 01/12/23 00:47:37.828
I0112 00:47:37.835239      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6823, replica count: 2
I0112 00:47:40.886292      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/12/23 00:47:40.889
Jan 12 00:47:40.913: INFO: Creating new exec pod
Jan 12 00:47:40.953: INFO: Waiting up to 5m0s for pod "execpodnvmn6" in namespace "services-6823" to be "running"
Jan 12 00:47:40.971: INFO: Pod "execpodnvmn6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.516049ms
Jan 12 00:47:42.975: INFO: Pod "execpodnvmn6": Phase="Running", Reason="", readiness=true. Elapsed: 2.021458424s
Jan 12 00:47:42.975: INFO: Pod "execpodnvmn6" satisfied condition "running"
Jan 12 00:47:42.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-6823 exec execpodnvmn6 -- /bin/sh -x -c nslookup clusterip-service.services-6823.svc.cluster.local'
Jan 12 00:47:43.240: INFO: stderr: "+ nslookup clusterip-service.services-6823.svc.cluster.local\n"
Jan 12 00:47:43.240: INFO: stdout: "Server:\t\t172.19.0.10\nAddress:\t172.19.0.10#53\n\nclusterip-service.services-6823.svc.cluster.local\tcanonical name = externalsvc.services-6823.svc.cluster.local.\nName:\texternalsvc.services-6823.svc.cluster.local\nAddress: 172.19.65.53\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6823, will wait for the garbage collector to delete the pods 01/12/23 00:47:43.24
Jan 12 00:47:43.302: INFO: Deleting ReplicationController externalsvc took: 9.313761ms
Jan 12 00:47:43.403: INFO: Terminating ReplicationController externalsvc pods took: 100.919091ms
Jan 12 00:47:46.248: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 00:47:46.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6823" for this suite. 01/12/23 00:47:46.267
------------------------------
• [SLOW TEST] [8.675 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:47:37.74
    Jan 12 00:47:37.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 00:47:37.741
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:37.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:37.764
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6823 01/12/23 00:47:37.766
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/12/23 00:47:37.791
    STEP: creating service externalsvc in namespace services-6823 01/12/23 00:47:37.791
    STEP: creating replication controller externalsvc in namespace services-6823 01/12/23 00:47:37.828
    I0112 00:47:37.835239      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6823, replica count: 2
    I0112 00:47:40.886292      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/12/23 00:47:40.889
    Jan 12 00:47:40.913: INFO: Creating new exec pod
    Jan 12 00:47:40.953: INFO: Waiting up to 5m0s for pod "execpodnvmn6" in namespace "services-6823" to be "running"
    Jan 12 00:47:40.971: INFO: Pod "execpodnvmn6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.516049ms
    Jan 12 00:47:42.975: INFO: Pod "execpodnvmn6": Phase="Running", Reason="", readiness=true. Elapsed: 2.021458424s
    Jan 12 00:47:42.975: INFO: Pod "execpodnvmn6" satisfied condition "running"
    Jan 12 00:47:42.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-6823 exec execpodnvmn6 -- /bin/sh -x -c nslookup clusterip-service.services-6823.svc.cluster.local'
    Jan 12 00:47:43.240: INFO: stderr: "+ nslookup clusterip-service.services-6823.svc.cluster.local\n"
    Jan 12 00:47:43.240: INFO: stdout: "Server:\t\t172.19.0.10\nAddress:\t172.19.0.10#53\n\nclusterip-service.services-6823.svc.cluster.local\tcanonical name = externalsvc.services-6823.svc.cluster.local.\nName:\texternalsvc.services-6823.svc.cluster.local\nAddress: 172.19.65.53\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-6823, will wait for the garbage collector to delete the pods 01/12/23 00:47:43.24
    Jan 12 00:47:43.302: INFO: Deleting ReplicationController externalsvc took: 9.313761ms
    Jan 12 00:47:43.403: INFO: Terminating ReplicationController externalsvc pods took: 100.919091ms
    Jan 12 00:47:46.248: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:47:46.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6823" for this suite. 01/12/23 00:47:46.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:47:46.417
Jan 12 00:47:46.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename dns 01/12/23 00:47:46.417
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:46.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:46.437
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/12/23 00:47:46.439
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1481.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1481.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/12/23 00:47:46.448
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1481.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1481.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/12/23 00:47:46.448
STEP: creating a pod to probe DNS 01/12/23 00:47:46.448
STEP: submitting the pod to kubernetes 01/12/23 00:47:46.448
Jan 12 00:47:46.590: INFO: Waiting up to 15m0s for pod "dns-test-78278220-9b66-4dba-b214-5479e13a08aa" in namespace "dns-1481" to be "running"
Jan 12 00:47:46.592: INFO: Pod "dns-test-78278220-9b66-4dba-b214-5479e13a08aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.248902ms
Jan 12 00:47:48.595: INFO: Pod "dns-test-78278220-9b66-4dba-b214-5479e13a08aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005301666s
Jan 12 00:47:50.597: INFO: Pod "dns-test-78278220-9b66-4dba-b214-5479e13a08aa": Phase="Running", Reason="", readiness=true. Elapsed: 4.007651193s
Jan 12 00:47:50.597: INFO: Pod "dns-test-78278220-9b66-4dba-b214-5479e13a08aa" satisfied condition "running"
STEP: retrieving the pod 01/12/23 00:47:50.597
STEP: looking for the results for each expected name from probers 01/12/23 00:47:50.6
Jan 12 00:47:50.611: INFO: DNS probes using dns-1481/dns-test-78278220-9b66-4dba-b214-5479e13a08aa succeeded

STEP: deleting the pod 01/12/23 00:47:50.611
STEP: deleting the test headless service 01/12/23 00:47:50.638
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 00:47:50.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1481" for this suite. 01/12/23 00:47:50.681
------------------------------
• [4.293 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:47:46.417
    Jan 12 00:47:46.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename dns 01/12/23 00:47:46.417
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:46.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:46.437
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/12/23 00:47:46.439
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1481.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1481.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/12/23 00:47:46.448
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1481.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1481.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/12/23 00:47:46.448
    STEP: creating a pod to probe DNS 01/12/23 00:47:46.448
    STEP: submitting the pod to kubernetes 01/12/23 00:47:46.448
    Jan 12 00:47:46.590: INFO: Waiting up to 15m0s for pod "dns-test-78278220-9b66-4dba-b214-5479e13a08aa" in namespace "dns-1481" to be "running"
    Jan 12 00:47:46.592: INFO: Pod "dns-test-78278220-9b66-4dba-b214-5479e13a08aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.248902ms
    Jan 12 00:47:48.595: INFO: Pod "dns-test-78278220-9b66-4dba-b214-5479e13a08aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005301666s
    Jan 12 00:47:50.597: INFO: Pod "dns-test-78278220-9b66-4dba-b214-5479e13a08aa": Phase="Running", Reason="", readiness=true. Elapsed: 4.007651193s
    Jan 12 00:47:50.597: INFO: Pod "dns-test-78278220-9b66-4dba-b214-5479e13a08aa" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 00:47:50.597
    STEP: looking for the results for each expected name from probers 01/12/23 00:47:50.6
    Jan 12 00:47:50.611: INFO: DNS probes using dns-1481/dns-test-78278220-9b66-4dba-b214-5479e13a08aa succeeded

    STEP: deleting the pod 01/12/23 00:47:50.611
    STEP: deleting the test headless service 01/12/23 00:47:50.638
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:47:50.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1481" for this suite. 01/12/23 00:47:50.681
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:47:50.71
Jan 12 00:47:50.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename namespaces 01/12/23 00:47:50.711
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:50.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:50.727
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 01/12/23 00:47:50.732
Jan 12 00:47:50.734: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/12/23 00:47:50.734
Jan 12 00:47:50.744: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/12/23 00:47:50.744
Jan 12 00:47:50.755: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:47:50.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4309" for this suite. 01/12/23 00:47:50.758
------------------------------
• [0.071 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:47:50.71
    Jan 12 00:47:50.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename namespaces 01/12/23 00:47:50.711
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:50.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:50.727
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 01/12/23 00:47:50.732
    Jan 12 00:47:50.734: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/12/23 00:47:50.734
    Jan 12 00:47:50.744: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/12/23 00:47:50.744
    Jan 12 00:47:50.755: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:47:50.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4309" for this suite. 01/12/23 00:47:50.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:47:50.782
Jan 12 00:47:50.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 00:47:50.782
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:50.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:50.814
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-d45a37c4-07ef-4b0c-9c40-6ab856f2ab80 01/12/23 00:47:50.817
STEP: Creating a pod to test consume configMaps 01/12/23 00:47:50.827
Jan 12 00:47:50.863: INFO: Waiting up to 5m0s for pod "pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210" in namespace "configmap-961" to be "Succeeded or Failed"
Jan 12 00:47:50.865: INFO: Pod "pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048188ms
Jan 12 00:47:52.870: INFO: Pod "pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007293224s
Jan 12 00:47:54.869: INFO: Pod "pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005874059s
Jan 12 00:47:56.869: INFO: Pod "pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006185906s
STEP: Saw pod success 01/12/23 00:47:56.869
Jan 12 00:47:56.869: INFO: Pod "pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210" satisfied condition "Succeeded or Failed"
Jan 12 00:47:56.871: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 00:47:56.88
Jan 12 00:47:56.902: INFO: Waiting for pod pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210 to disappear
Jan 12 00:47:56.904: INFO: Pod pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 00:47:56.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-961" for this suite. 01/12/23 00:47:56.908
------------------------------
• [SLOW TEST] [6.167 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:47:50.782
    Jan 12 00:47:50.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 00:47:50.782
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:50.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:50.814
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-d45a37c4-07ef-4b0c-9c40-6ab856f2ab80 01/12/23 00:47:50.817
    STEP: Creating a pod to test consume configMaps 01/12/23 00:47:50.827
    Jan 12 00:47:50.863: INFO: Waiting up to 5m0s for pod "pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210" in namespace "configmap-961" to be "Succeeded or Failed"
    Jan 12 00:47:50.865: INFO: Pod "pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048188ms
    Jan 12 00:47:52.870: INFO: Pod "pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007293224s
    Jan 12 00:47:54.869: INFO: Pod "pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005874059s
    Jan 12 00:47:56.869: INFO: Pod "pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006185906s
    STEP: Saw pod success 01/12/23 00:47:56.869
    Jan 12 00:47:56.869: INFO: Pod "pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210" satisfied condition "Succeeded or Failed"
    Jan 12 00:47:56.871: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 00:47:56.88
    Jan 12 00:47:56.902: INFO: Waiting for pod pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210 to disappear
    Jan 12 00:47:56.904: INFO: Pod pod-configmaps-60711ca3-c474-47f5-9639-5d349afbe210 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:47:56.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-961" for this suite. 01/12/23 00:47:56.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:47:56.95
Jan 12 00:47:56.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 00:47:56.951
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:56.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:56.981
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 00:47:56.998
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 00:47:57.363
STEP: Deploying the webhook pod 01/12/23 00:47:57.373
STEP: Wait for the deployment to be ready 01/12/23 00:47:57.449
Jan 12 00:47:57.459: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jan 12 00:47:59.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 00:48:01.463
STEP: Verifying the service has paired with the endpoint 01/12/23 00:48:01.496
Jan 12 00:48:02.498: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 01/12/23 00:48:02.632
STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 00:48:02.668
STEP: Deleting the collection of validation webhooks 01/12/23 00:48:02.698
STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 00:48:02.776
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:48:02.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3590" for this suite. 01/12/23 00:48:02.904
STEP: Destroying namespace "webhook-3590-markers" for this suite. 01/12/23 00:48:02.925
------------------------------
• [SLOW TEST] [6.026 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:47:56.95
    Jan 12 00:47:56.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 00:47:56.951
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:47:56.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:47:56.981
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 00:47:56.998
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 00:47:57.363
    STEP: Deploying the webhook pod 01/12/23 00:47:57.373
    STEP: Wait for the deployment to be ready 01/12/23 00:47:57.449
    Jan 12 00:47:57.459: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jan 12 00:47:59.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 47, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 00:48:01.463
    STEP: Verifying the service has paired with the endpoint 01/12/23 00:48:01.496
    Jan 12 00:48:02.498: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 01/12/23 00:48:02.632
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 00:48:02.668
    STEP: Deleting the collection of validation webhooks 01/12/23 00:48:02.698
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 00:48:02.776
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:48:02.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3590" for this suite. 01/12/23 00:48:02.904
    STEP: Destroying namespace "webhook-3590-markers" for this suite. 01/12/23 00:48:02.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:48:02.977
Jan 12 00:48:02.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 00:48:02.978
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:48:02.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:48:02.997
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 01/12/23 00:48:03
Jan 12 00:48:03.043: INFO: Waiting up to 5m0s for pod "pod-1e99d458-148d-4048-9c55-afc3c41f06ed" in namespace "emptydir-7214" to be "Succeeded or Failed"
Jan 12 00:48:03.045: INFO: Pod "pod-1e99d458-148d-4048-9c55-afc3c41f06ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.446042ms
Jan 12 00:48:05.048: INFO: Pod "pod-1e99d458-148d-4048-9c55-afc3c41f06ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005818229s
Jan 12 00:48:07.048: INFO: Pod "pod-1e99d458-148d-4048-9c55-afc3c41f06ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005775332s
Jan 12 00:48:09.049: INFO: Pod "pod-1e99d458-148d-4048-9c55-afc3c41f06ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006669445s
STEP: Saw pod success 01/12/23 00:48:09.049
Jan 12 00:48:09.049: INFO: Pod "pod-1e99d458-148d-4048-9c55-afc3c41f06ed" satisfied condition "Succeeded or Failed"
Jan 12 00:48:09.052: INFO: Trying to get logs from node eqx04-flash06 pod pod-1e99d458-148d-4048-9c55-afc3c41f06ed container test-container: <nil>
STEP: delete the pod 01/12/23 00:48:09.061
Jan 12 00:48:09.084: INFO: Waiting for pod pod-1e99d458-148d-4048-9c55-afc3c41f06ed to disappear
Jan 12 00:48:09.086: INFO: Pod pod-1e99d458-148d-4048-9c55-afc3c41f06ed no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 00:48:09.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7214" for this suite. 01/12/23 00:48:09.09
------------------------------
• [SLOW TEST] [6.146 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:48:02.977
    Jan 12 00:48:02.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 00:48:02.978
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:48:02.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:48:02.997
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/12/23 00:48:03
    Jan 12 00:48:03.043: INFO: Waiting up to 5m0s for pod "pod-1e99d458-148d-4048-9c55-afc3c41f06ed" in namespace "emptydir-7214" to be "Succeeded or Failed"
    Jan 12 00:48:03.045: INFO: Pod "pod-1e99d458-148d-4048-9c55-afc3c41f06ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.446042ms
    Jan 12 00:48:05.048: INFO: Pod "pod-1e99d458-148d-4048-9c55-afc3c41f06ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005818229s
    Jan 12 00:48:07.048: INFO: Pod "pod-1e99d458-148d-4048-9c55-afc3c41f06ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005775332s
    Jan 12 00:48:09.049: INFO: Pod "pod-1e99d458-148d-4048-9c55-afc3c41f06ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006669445s
    STEP: Saw pod success 01/12/23 00:48:09.049
    Jan 12 00:48:09.049: INFO: Pod "pod-1e99d458-148d-4048-9c55-afc3c41f06ed" satisfied condition "Succeeded or Failed"
    Jan 12 00:48:09.052: INFO: Trying to get logs from node eqx04-flash06 pod pod-1e99d458-148d-4048-9c55-afc3c41f06ed container test-container: <nil>
    STEP: delete the pod 01/12/23 00:48:09.061
    Jan 12 00:48:09.084: INFO: Waiting for pod pod-1e99d458-148d-4048-9c55-afc3c41f06ed to disappear
    Jan 12 00:48:09.086: INFO: Pod pod-1e99d458-148d-4048-9c55-afc3c41f06ed no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:48:09.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7214" for this suite. 01/12/23 00:48:09.09
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:48:09.123
Jan 12 00:48:09.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename hostport 01/12/23 00:48:09.124
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:48:09.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:48:09.157
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/12/23 00:48:09.165
Jan 12 00:48:09.198: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-7679" to be "running and ready"
Jan 12 00:48:09.200: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.359616ms
Jan 12 00:48:09.200: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:48:11.204: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005874014s
Jan 12 00:48:11.204: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:48:13.204: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.006396099s
Jan 12 00:48:13.204: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 12 00:48:13.204: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.9.140.106 on the node which pod1 resides and expect scheduled 01/12/23 00:48:13.204
Jan 12 00:48:13.318: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-7679" to be "running and ready"
Jan 12 00:48:13.320: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.359946ms
Jan 12 00:48:13.320: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:48:15.324: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006276523s
Jan 12 00:48:15.324: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:48:17.323: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.005080946s
Jan 12 00:48:17.323: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 12 00:48:17.323: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.9.140.106 but use UDP protocol on the node which pod2 resides 01/12/23 00:48:17.323
Jan 12 00:48:17.450: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-7679" to be "running and ready"
Jan 12 00:48:17.452: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.287928ms
Jan 12 00:48:17.452: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:48:19.456: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.006457773s
Jan 12 00:48:19.456: INFO: The phase of Pod pod3 is Running (Ready = false)
Jan 12 00:48:21.456: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.006240274s
Jan 12 00:48:21.456: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 12 00:48:21.456: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 12 00:48:21.484: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-7679" to be "running and ready"
Jan 12 00:48:21.486: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.278632ms
Jan 12 00:48:21.486: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:48:23.490: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.006604425s
Jan 12 00:48:23.490: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 12 00:48:23.490: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/12/23 00:48:23.493
Jan 12 00:48:23.493: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.9.140.106 http://127.0.0.1:54323/hostname] Namespace:hostport-7679 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 00:48:23.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 00:48:23.493: INFO: ExecWithOptions: Clientset creation
Jan 12 00:48:23.493: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/hostport-7679/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.9.140.106+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.9.140.106, port: 54323 01/12/23 00:48:23.718
Jan 12 00:48:23.718: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.9.140.106:54323/hostname] Namespace:hostport-7679 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 00:48:23.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 00:48:23.719: INFO: ExecWithOptions: Clientset creation
Jan 12 00:48:23.719: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/hostport-7679/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.9.140.106%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.9.140.106, port: 54323 UDP 01/12/23 00:48:23.952
Jan 12 00:48:23.952: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.9.140.106 54323] Namespace:hostport-7679 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 00:48:23.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 00:48:23.952: INFO: ExecWithOptions: Clientset creation
Jan 12 00:48:23.952: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/hostport-7679/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.9.140.106+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jan 12 00:48:29.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-7679" for this suite. 01/12/23 00:48:29.192
------------------------------
• [SLOW TEST] [20.089 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:48:09.123
    Jan 12 00:48:09.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename hostport 01/12/23 00:48:09.124
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:48:09.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:48:09.157
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/12/23 00:48:09.165
    Jan 12 00:48:09.198: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-7679" to be "running and ready"
    Jan 12 00:48:09.200: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.359616ms
    Jan 12 00:48:09.200: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:48:11.204: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005874014s
    Jan 12 00:48:11.204: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:48:13.204: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.006396099s
    Jan 12 00:48:13.204: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 12 00:48:13.204: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.9.140.106 on the node which pod1 resides and expect scheduled 01/12/23 00:48:13.204
    Jan 12 00:48:13.318: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-7679" to be "running and ready"
    Jan 12 00:48:13.320: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.359946ms
    Jan 12 00:48:13.320: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:48:15.324: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006276523s
    Jan 12 00:48:15.324: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:48:17.323: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.005080946s
    Jan 12 00:48:17.323: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 12 00:48:17.323: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.9.140.106 but use UDP protocol on the node which pod2 resides 01/12/23 00:48:17.323
    Jan 12 00:48:17.450: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-7679" to be "running and ready"
    Jan 12 00:48:17.452: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.287928ms
    Jan 12 00:48:17.452: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:48:19.456: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.006457773s
    Jan 12 00:48:19.456: INFO: The phase of Pod pod3 is Running (Ready = false)
    Jan 12 00:48:21.456: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.006240274s
    Jan 12 00:48:21.456: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 12 00:48:21.456: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 12 00:48:21.484: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-7679" to be "running and ready"
    Jan 12 00:48:21.486: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.278632ms
    Jan 12 00:48:21.486: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:48:23.490: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.006604425s
    Jan 12 00:48:23.490: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 12 00:48:23.490: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/12/23 00:48:23.493
    Jan 12 00:48:23.493: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.9.140.106 http://127.0.0.1:54323/hostname] Namespace:hostport-7679 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 00:48:23.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 00:48:23.493: INFO: ExecWithOptions: Clientset creation
    Jan 12 00:48:23.493: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/hostport-7679/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.9.140.106+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.9.140.106, port: 54323 01/12/23 00:48:23.718
    Jan 12 00:48:23.718: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.9.140.106:54323/hostname] Namespace:hostport-7679 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 00:48:23.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 00:48:23.719: INFO: ExecWithOptions: Clientset creation
    Jan 12 00:48:23.719: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/hostport-7679/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.9.140.106%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.9.140.106, port: 54323 UDP 01/12/23 00:48:23.952
    Jan 12 00:48:23.952: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.9.140.106 54323] Namespace:hostport-7679 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 00:48:23.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 00:48:23.952: INFO: ExecWithOptions: Clientset creation
    Jan 12 00:48:23.952: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/hostport-7679/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.9.140.106+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:48:29.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-7679" for this suite. 01/12/23 00:48:29.192
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:48:29.213
Jan 12 00:48:29.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 00:48:29.215
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:48:29.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:48:29.298
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 01/12/23 00:48:29.301
Jan 12 00:48:29.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-946 create -f -'
Jan 12 00:48:29.572: INFO: stderr: ""
Jan 12 00:48:29.572: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/12/23 00:48:29.572
Jan 12 00:48:29.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-946 diff -f -'
Jan 12 00:48:29.780: INFO: rc: 1
Jan 12 00:48:29.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-946 delete -f -'
Jan 12 00:48:29.851: INFO: stderr: ""
Jan 12 00:48:29.851: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 00:48:29.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-946" for this suite. 01/12/23 00:48:29.855
------------------------------
• [0.670 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:48:29.213
    Jan 12 00:48:29.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 00:48:29.215
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:48:29.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:48:29.298
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 01/12/23 00:48:29.301
    Jan 12 00:48:29.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-946 create -f -'
    Jan 12 00:48:29.572: INFO: stderr: ""
    Jan 12 00:48:29.572: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/12/23 00:48:29.572
    Jan 12 00:48:29.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-946 diff -f -'
    Jan 12 00:48:29.780: INFO: rc: 1
    Jan 12 00:48:29.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-946 delete -f -'
    Jan 12 00:48:29.851: INFO: stderr: ""
    Jan 12 00:48:29.851: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:48:29.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-946" for this suite. 01/12/23 00:48:29.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:48:29.886
Jan 12 00:48:29.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 00:48:29.887
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:48:29.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:48:29.912
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 01/12/23 00:48:29.914
Jan 12 00:48:29.986: INFO: Waiting up to 5m0s for pod "downward-api-7491b73c-3844-4028-b8c0-94698e44936d" in namespace "downward-api-9269" to be "Succeeded or Failed"
Jan 12 00:48:29.989: INFO: Pod "downward-api-7491b73c-3844-4028-b8c0-94698e44936d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.429299ms
Jan 12 00:48:31.993: INFO: Pod "downward-api-7491b73c-3844-4028-b8c0-94698e44936d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006435304s
Jan 12 00:48:33.993: INFO: Pod "downward-api-7491b73c-3844-4028-b8c0-94698e44936d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006263811s
Jan 12 00:48:35.994: INFO: Pod "downward-api-7491b73c-3844-4028-b8c0-94698e44936d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007290806s
STEP: Saw pod success 01/12/23 00:48:35.994
Jan 12 00:48:35.994: INFO: Pod "downward-api-7491b73c-3844-4028-b8c0-94698e44936d" satisfied condition "Succeeded or Failed"
Jan 12 00:48:35.996: INFO: Trying to get logs from node eqx04-flash06 pod downward-api-7491b73c-3844-4028-b8c0-94698e44936d container dapi-container: <nil>
STEP: delete the pod 01/12/23 00:48:36.006
Jan 12 00:48:36.033: INFO: Waiting for pod downward-api-7491b73c-3844-4028-b8c0-94698e44936d to disappear
Jan 12 00:48:36.035: INFO: Pod downward-api-7491b73c-3844-4028-b8c0-94698e44936d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 12 00:48:36.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9269" for this suite. 01/12/23 00:48:36.039
------------------------------
• [SLOW TEST] [6.169 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:48:29.886
    Jan 12 00:48:29.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 00:48:29.887
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:48:29.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:48:29.912
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 01/12/23 00:48:29.914
    Jan 12 00:48:29.986: INFO: Waiting up to 5m0s for pod "downward-api-7491b73c-3844-4028-b8c0-94698e44936d" in namespace "downward-api-9269" to be "Succeeded or Failed"
    Jan 12 00:48:29.989: INFO: Pod "downward-api-7491b73c-3844-4028-b8c0-94698e44936d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.429299ms
    Jan 12 00:48:31.993: INFO: Pod "downward-api-7491b73c-3844-4028-b8c0-94698e44936d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006435304s
    Jan 12 00:48:33.993: INFO: Pod "downward-api-7491b73c-3844-4028-b8c0-94698e44936d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006263811s
    Jan 12 00:48:35.994: INFO: Pod "downward-api-7491b73c-3844-4028-b8c0-94698e44936d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007290806s
    STEP: Saw pod success 01/12/23 00:48:35.994
    Jan 12 00:48:35.994: INFO: Pod "downward-api-7491b73c-3844-4028-b8c0-94698e44936d" satisfied condition "Succeeded or Failed"
    Jan 12 00:48:35.996: INFO: Trying to get logs from node eqx04-flash06 pod downward-api-7491b73c-3844-4028-b8c0-94698e44936d container dapi-container: <nil>
    STEP: delete the pod 01/12/23 00:48:36.006
    Jan 12 00:48:36.033: INFO: Waiting for pod downward-api-7491b73c-3844-4028-b8c0-94698e44936d to disappear
    Jan 12 00:48:36.035: INFO: Pod downward-api-7491b73c-3844-4028-b8c0-94698e44936d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:48:36.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9269" for this suite. 01/12/23 00:48:36.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:48:36.057
Jan 12 00:48:36.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 00:48:36.057
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:48:36.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:48:36.073
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-1e845bac-4be2-482e-b3b8-944ad1c0c6fd 01/12/23 00:48:36.082
STEP: Creating configMap with name cm-test-opt-upd-1821d293-daaa-41f2-ba3b-c63a359737d2 01/12/23 00:48:36.088
STEP: Creating the pod 01/12/23 00:48:36.095
Jan 12 00:48:36.135: INFO: Waiting up to 5m0s for pod "pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b" in namespace "configmap-1511" to be "running and ready"
Jan 12 00:48:36.138: INFO: Pod "pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.406636ms
Jan 12 00:48:36.138: INFO: The phase of Pod pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:48:38.141: INFO: Pod "pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005440616s
Jan 12 00:48:38.141: INFO: The phase of Pod pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:48:40.142: INFO: Pod "pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b": Phase="Running", Reason="", readiness=true. Elapsed: 4.006500826s
Jan 12 00:48:40.142: INFO: The phase of Pod pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b is Running (Ready = true)
Jan 12 00:48:40.142: INFO: Pod "pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-1e845bac-4be2-482e-b3b8-944ad1c0c6fd 01/12/23 00:48:40.176
STEP: Updating configmap cm-test-opt-upd-1821d293-daaa-41f2-ba3b-c63a359737d2 01/12/23 00:48:40.183
STEP: Creating configMap with name cm-test-opt-create-4517bdca-cd39-4267-93f8-70a53d7afe6c 01/12/23 00:48:40.191
STEP: waiting to observe update in volume 01/12/23 00:48:40.199
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 00:48:42.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1511" for this suite. 01/12/23 00:48:42.269
------------------------------
• [SLOW TEST] [6.235 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:48:36.057
    Jan 12 00:48:36.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 00:48:36.057
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:48:36.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:48:36.073
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-1e845bac-4be2-482e-b3b8-944ad1c0c6fd 01/12/23 00:48:36.082
    STEP: Creating configMap with name cm-test-opt-upd-1821d293-daaa-41f2-ba3b-c63a359737d2 01/12/23 00:48:36.088
    STEP: Creating the pod 01/12/23 00:48:36.095
    Jan 12 00:48:36.135: INFO: Waiting up to 5m0s for pod "pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b" in namespace "configmap-1511" to be "running and ready"
    Jan 12 00:48:36.138: INFO: Pod "pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.406636ms
    Jan 12 00:48:36.138: INFO: The phase of Pod pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:48:38.141: INFO: Pod "pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005440616s
    Jan 12 00:48:38.141: INFO: The phase of Pod pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:48:40.142: INFO: Pod "pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b": Phase="Running", Reason="", readiness=true. Elapsed: 4.006500826s
    Jan 12 00:48:40.142: INFO: The phase of Pod pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b is Running (Ready = true)
    Jan 12 00:48:40.142: INFO: Pod "pod-configmaps-03f028ac-c5a6-48c0-be20-ec890df7fe1b" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-1e845bac-4be2-482e-b3b8-944ad1c0c6fd 01/12/23 00:48:40.176
    STEP: Updating configmap cm-test-opt-upd-1821d293-daaa-41f2-ba3b-c63a359737d2 01/12/23 00:48:40.183
    STEP: Creating configMap with name cm-test-opt-create-4517bdca-cd39-4267-93f8-70a53d7afe6c 01/12/23 00:48:40.191
    STEP: waiting to observe update in volume 01/12/23 00:48:40.199
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:48:42.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1511" for this suite. 01/12/23 00:48:42.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:48:42.292
Jan 12 00:48:42.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename sched-preemption 01/12/23 00:48:42.293
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:48:42.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:48:42.322
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 12 00:48:42.342: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 12 00:49:42.417: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:49:42.421
Jan 12 00:49:42.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename sched-preemption-path 01/12/23 00:49:42.422
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:49:42.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:49:42.439
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:763
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
Jan 12 00:49:42.459: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 12 00:49:42.462: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jan 12 00:49:42.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:779
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:49:42.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-9752" for this suite. 01/12/23 00:49:42.564
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5609" for this suite. 01/12/23 00:49:42.585
------------------------------
• [SLOW TEST] [60.309 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:756
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:806

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:48:42.292
    Jan 12 00:48:42.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename sched-preemption 01/12/23 00:48:42.293
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:48:42.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:48:42.322
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 12 00:48:42.342: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 12 00:49:42.417: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:49:42.421
    Jan 12 00:49:42.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename sched-preemption-path 01/12/23 00:49:42.422
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:49:42.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:49:42.439
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:763
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:806
    Jan 12 00:49:42.459: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 12 00:49:42.462: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:49:42.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:779
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:49:42.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-9752" for this suite. 01/12/23 00:49:42.564
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5609" for this suite. 01/12/23 00:49:42.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:49:42.602
Jan 12 00:49:42.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename namespaces 01/12/23 00:49:42.603
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:49:42.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:49:42.619
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-dng5s" 01/12/23 00:49:42.632
Jan 12 00:49:42.660: INFO: Namespace "e2e-ns-dng5s-3700" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-dng5s-3700" 01/12/23 00:49:42.66
Jan 12 00:49:42.671: INFO: Namespace "e2e-ns-dng5s-3700" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-dng5s-3700" 01/12/23 00:49:42.671
Jan 12 00:49:42.681: INFO: Namespace "e2e-ns-dng5s-3700" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:49:42.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5405" for this suite. 01/12/23 00:49:42.685
STEP: Destroying namespace "e2e-ns-dng5s-3700" for this suite. 01/12/23 00:49:42.704
------------------------------
• [0.117 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:49:42.602
    Jan 12 00:49:42.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename namespaces 01/12/23 00:49:42.603
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:49:42.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:49:42.619
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-dng5s" 01/12/23 00:49:42.632
    Jan 12 00:49:42.660: INFO: Namespace "e2e-ns-dng5s-3700" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-dng5s-3700" 01/12/23 00:49:42.66
    Jan 12 00:49:42.671: INFO: Namespace "e2e-ns-dng5s-3700" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-dng5s-3700" 01/12/23 00:49:42.671
    Jan 12 00:49:42.681: INFO: Namespace "e2e-ns-dng5s-3700" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:49:42.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5405" for this suite. 01/12/23 00:49:42.685
    STEP: Destroying namespace "e2e-ns-dng5s-3700" for this suite. 01/12/23 00:49:42.704
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:49:42.719
Jan 12 00:49:42.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename discovery 01/12/23 00:49:42.72
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:49:42.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:49:42.747
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/12/23 00:49:42.75
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 12 00:49:43.218: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 12 00:49:43.219: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 12 00:49:43.219: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 12 00:49:43.219: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 12 00:49:43.219: INFO: Checking APIGroup: apps
Jan 12 00:49:43.219: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 12 00:49:43.219: INFO: Versions found [{apps/v1 v1}]
Jan 12 00:49:43.219: INFO: apps/v1 matches apps/v1
Jan 12 00:49:43.219: INFO: Checking APIGroup: events.k8s.io
Jan 12 00:49:43.220: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 12 00:49:43.220: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 12 00:49:43.220: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 12 00:49:43.220: INFO: Checking APIGroup: authentication.k8s.io
Jan 12 00:49:43.221: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 12 00:49:43.221: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 12 00:49:43.221: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 12 00:49:43.221: INFO: Checking APIGroup: authorization.k8s.io
Jan 12 00:49:43.222: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 12 00:49:43.222: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 12 00:49:43.222: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 12 00:49:43.222: INFO: Checking APIGroup: autoscaling
Jan 12 00:49:43.222: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 12 00:49:43.222: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jan 12 00:49:43.222: INFO: autoscaling/v2 matches autoscaling/v2
Jan 12 00:49:43.222: INFO: Checking APIGroup: batch
Jan 12 00:49:43.223: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 12 00:49:43.223: INFO: Versions found [{batch/v1 v1}]
Jan 12 00:49:43.223: INFO: batch/v1 matches batch/v1
Jan 12 00:49:43.223: INFO: Checking APIGroup: certificates.k8s.io
Jan 12 00:49:43.224: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 12 00:49:43.224: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 12 00:49:43.224: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 12 00:49:43.224: INFO: Checking APIGroup: networking.k8s.io
Jan 12 00:49:43.224: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 12 00:49:43.224: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 12 00:49:43.224: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 12 00:49:43.224: INFO: Checking APIGroup: policy
Jan 12 00:49:43.225: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 12 00:49:43.225: INFO: Versions found [{policy/v1 v1}]
Jan 12 00:49:43.225: INFO: policy/v1 matches policy/v1
Jan 12 00:49:43.225: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 12 00:49:43.226: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 12 00:49:43.226: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 12 00:49:43.226: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 12 00:49:43.226: INFO: Checking APIGroup: storage.k8s.io
Jan 12 00:49:43.226: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 12 00:49:43.226: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 12 00:49:43.226: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 12 00:49:43.226: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 12 00:49:43.227: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 12 00:49:43.227: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 12 00:49:43.227: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 12 00:49:43.227: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 12 00:49:43.228: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 12 00:49:43.228: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 12 00:49:43.228: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 12 00:49:43.228: INFO: Checking APIGroup: scheduling.k8s.io
Jan 12 00:49:43.228: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 12 00:49:43.229: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 12 00:49:43.229: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 12 00:49:43.229: INFO: Checking APIGroup: coordination.k8s.io
Jan 12 00:49:43.230: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 12 00:49:43.230: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 12 00:49:43.230: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 12 00:49:43.230: INFO: Checking APIGroup: node.k8s.io
Jan 12 00:49:43.230: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 12 00:49:43.230: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 12 00:49:43.230: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 12 00:49:43.230: INFO: Checking APIGroup: discovery.k8s.io
Jan 12 00:49:43.231: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 12 00:49:43.231: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 12 00:49:43.231: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 12 00:49:43.231: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 12 00:49:43.232: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jan 12 00:49:43.232: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jan 12 00:49:43.232: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Jan 12 00:49:43.232: INFO: Checking APIGroup: acid.zalan.do
Jan 12 00:49:43.232: INFO: PreferredVersion.GroupVersion: acid.zalan.do/v1
Jan 12 00:49:43.232: INFO: Versions found [{acid.zalan.do/v1 v1}]
Jan 12 00:49:43.232: INFO: acid.zalan.do/v1 matches acid.zalan.do/v1
Jan 12 00:49:43.232: INFO: Checking APIGroup: crd.projectcalico.org
Jan 12 00:49:43.233: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 12 00:49:43.233: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 12 00:49:43.233: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan 12 00:49:43.233: INFO: Checking APIGroup: k8s.cni.cncf.io
Jan 12 00:49:43.234: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Jan 12 00:49:43.234: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Jan 12 00:49:43.234: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Jan 12 00:49:43.234: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan 12 00:49:43.234: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan 12 00:49:43.234: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jan 12 00:49:43.234: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jan 12 00:49:43.234: INFO: Checking APIGroup: robin.io
Jan 12 00:49:43.235: INFO: PreferredVersion.GroupVersion: robin.io/v1alpha1
Jan 12 00:49:43.235: INFO: Versions found [{robin.io/v1alpha1 v1alpha1}]
Jan 12 00:49:43.235: INFO: robin.io/v1alpha1 matches robin.io/v1alpha1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jan 12 00:49:43.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-6807" for this suite. 01/12/23 00:49:43.239
------------------------------
• [0.540 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:49:42.719
    Jan 12 00:49:42.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename discovery 01/12/23 00:49:42.72
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:49:42.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:49:42.747
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/12/23 00:49:42.75
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 12 00:49:43.218: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 12 00:49:43.219: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 12 00:49:43.219: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 12 00:49:43.219: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 12 00:49:43.219: INFO: Checking APIGroup: apps
    Jan 12 00:49:43.219: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 12 00:49:43.219: INFO: Versions found [{apps/v1 v1}]
    Jan 12 00:49:43.219: INFO: apps/v1 matches apps/v1
    Jan 12 00:49:43.219: INFO: Checking APIGroup: events.k8s.io
    Jan 12 00:49:43.220: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 12 00:49:43.220: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 12 00:49:43.220: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 12 00:49:43.220: INFO: Checking APIGroup: authentication.k8s.io
    Jan 12 00:49:43.221: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 12 00:49:43.221: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 12 00:49:43.221: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 12 00:49:43.221: INFO: Checking APIGroup: authorization.k8s.io
    Jan 12 00:49:43.222: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 12 00:49:43.222: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 12 00:49:43.222: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 12 00:49:43.222: INFO: Checking APIGroup: autoscaling
    Jan 12 00:49:43.222: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 12 00:49:43.222: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jan 12 00:49:43.222: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 12 00:49:43.222: INFO: Checking APIGroup: batch
    Jan 12 00:49:43.223: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 12 00:49:43.223: INFO: Versions found [{batch/v1 v1}]
    Jan 12 00:49:43.223: INFO: batch/v1 matches batch/v1
    Jan 12 00:49:43.223: INFO: Checking APIGroup: certificates.k8s.io
    Jan 12 00:49:43.224: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 12 00:49:43.224: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 12 00:49:43.224: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 12 00:49:43.224: INFO: Checking APIGroup: networking.k8s.io
    Jan 12 00:49:43.224: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 12 00:49:43.224: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan 12 00:49:43.224: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 12 00:49:43.224: INFO: Checking APIGroup: policy
    Jan 12 00:49:43.225: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 12 00:49:43.225: INFO: Versions found [{policy/v1 v1}]
    Jan 12 00:49:43.225: INFO: policy/v1 matches policy/v1
    Jan 12 00:49:43.225: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 12 00:49:43.226: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 12 00:49:43.226: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 12 00:49:43.226: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 12 00:49:43.226: INFO: Checking APIGroup: storage.k8s.io
    Jan 12 00:49:43.226: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 12 00:49:43.226: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 12 00:49:43.226: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 12 00:49:43.226: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 12 00:49:43.227: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 12 00:49:43.227: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 12 00:49:43.227: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 12 00:49:43.227: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 12 00:49:43.228: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 12 00:49:43.228: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 12 00:49:43.228: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 12 00:49:43.228: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 12 00:49:43.228: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 12 00:49:43.229: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 12 00:49:43.229: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 12 00:49:43.229: INFO: Checking APIGroup: coordination.k8s.io
    Jan 12 00:49:43.230: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 12 00:49:43.230: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 12 00:49:43.230: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 12 00:49:43.230: INFO: Checking APIGroup: node.k8s.io
    Jan 12 00:49:43.230: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 12 00:49:43.230: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 12 00:49:43.230: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 12 00:49:43.230: INFO: Checking APIGroup: discovery.k8s.io
    Jan 12 00:49:43.231: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 12 00:49:43.231: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 12 00:49:43.231: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 12 00:49:43.231: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 12 00:49:43.232: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jan 12 00:49:43.232: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jan 12 00:49:43.232: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Jan 12 00:49:43.232: INFO: Checking APIGroup: acid.zalan.do
    Jan 12 00:49:43.232: INFO: PreferredVersion.GroupVersion: acid.zalan.do/v1
    Jan 12 00:49:43.232: INFO: Versions found [{acid.zalan.do/v1 v1}]
    Jan 12 00:49:43.232: INFO: acid.zalan.do/v1 matches acid.zalan.do/v1
    Jan 12 00:49:43.232: INFO: Checking APIGroup: crd.projectcalico.org
    Jan 12 00:49:43.233: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan 12 00:49:43.233: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan 12 00:49:43.233: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jan 12 00:49:43.233: INFO: Checking APIGroup: k8s.cni.cncf.io
    Jan 12 00:49:43.234: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
    Jan 12 00:49:43.234: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
    Jan 12 00:49:43.234: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
    Jan 12 00:49:43.234: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jan 12 00:49:43.234: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jan 12 00:49:43.234: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    Jan 12 00:49:43.234: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Jan 12 00:49:43.234: INFO: Checking APIGroup: robin.io
    Jan 12 00:49:43.235: INFO: PreferredVersion.GroupVersion: robin.io/v1alpha1
    Jan 12 00:49:43.235: INFO: Versions found [{robin.io/v1alpha1 v1alpha1}]
    Jan 12 00:49:43.235: INFO: robin.io/v1alpha1 matches robin.io/v1alpha1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:49:43.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-6807" for this suite. 01/12/23 00:49:43.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:49:43.259
Jan 12 00:49:43.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename events 01/12/23 00:49:43.26
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:49:43.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:49:43.292
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/12/23 00:49:43.294
STEP: listing events in all namespaces 01/12/23 00:49:43.305
STEP: listing events in test namespace 01/12/23 00:49:43.314
STEP: listing events with field selection filtering on source 01/12/23 00:49:43.316
STEP: listing events with field selection filtering on reportingController 01/12/23 00:49:43.318
STEP: getting the test event 01/12/23 00:49:43.321
STEP: patching the test event 01/12/23 00:49:43.323
STEP: getting the test event 01/12/23 00:49:43.333
STEP: updating the test event 01/12/23 00:49:43.336
STEP: getting the test event 01/12/23 00:49:43.346
STEP: deleting the test event 01/12/23 00:49:43.348
STEP: listing events in all namespaces 01/12/23 00:49:43.356
STEP: listing events in test namespace 01/12/23 00:49:43.365
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 12 00:49:43.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1885" for this suite. 01/12/23 00:49:43.371
------------------------------
• [0.141 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:49:43.259
    Jan 12 00:49:43.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename events 01/12/23 00:49:43.26
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:49:43.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:49:43.292
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/12/23 00:49:43.294
    STEP: listing events in all namespaces 01/12/23 00:49:43.305
    STEP: listing events in test namespace 01/12/23 00:49:43.314
    STEP: listing events with field selection filtering on source 01/12/23 00:49:43.316
    STEP: listing events with field selection filtering on reportingController 01/12/23 00:49:43.318
    STEP: getting the test event 01/12/23 00:49:43.321
    STEP: patching the test event 01/12/23 00:49:43.323
    STEP: getting the test event 01/12/23 00:49:43.333
    STEP: updating the test event 01/12/23 00:49:43.336
    STEP: getting the test event 01/12/23 00:49:43.346
    STEP: deleting the test event 01/12/23 00:49:43.348
    STEP: listing events in all namespaces 01/12/23 00:49:43.356
    STEP: listing events in test namespace 01/12/23 00:49:43.365
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:49:43.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1885" for this suite. 01/12/23 00:49:43.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:49:43.401
Jan 12 00:49:43.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-probe 01/12/23 00:49:43.402
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:49:43.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:49:43.431
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Jan 12 00:49:43.466: INFO: Waiting up to 5m0s for pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4" in namespace "container-probe-8484" to be "running and ready"
Jan 12 00:49:43.468: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.376023ms
Jan 12 00:49:43.468: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:49:45.471: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005410962s
Jan 12 00:49:45.471: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:49:47.478: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 4.011664669s
Jan 12 00:49:47.478: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
Jan 12 00:49:49.472: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 6.005681009s
Jan 12 00:49:49.472: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
Jan 12 00:49:51.472: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 8.005624808s
Jan 12 00:49:51.472: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
Jan 12 00:49:53.472: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 10.006427537s
Jan 12 00:49:53.472: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
Jan 12 00:49:55.472: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 12.005750119s
Jan 12 00:49:55.472: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
Jan 12 00:49:57.472: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 14.00560871s
Jan 12 00:49:57.472: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
Jan 12 00:49:59.471: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 16.005357229s
Jan 12 00:49:59.471: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
Jan 12 00:50:01.472: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 18.005915074s
Jan 12 00:50:01.472: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
Jan 12 00:50:03.474: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 20.007770676s
Jan 12 00:50:03.474: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
Jan 12 00:50:05.473: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=true. Elapsed: 22.006997609s
Jan 12 00:50:05.473: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = true)
Jan 12 00:50:05.473: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4" satisfied condition "running and ready"
Jan 12 00:50:05.476: INFO: Container started at 2023-01-12 00:49:44 +0000 UTC, pod became ready at 2023-01-12 00:50:03 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 00:50:05.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8484" for this suite. 01/12/23 00:50:05.48
------------------------------
• [SLOW TEST] [22.181 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:49:43.401
    Jan 12 00:49:43.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-probe 01/12/23 00:49:43.402
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:49:43.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:49:43.431
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Jan 12 00:49:43.466: INFO: Waiting up to 5m0s for pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4" in namespace "container-probe-8484" to be "running and ready"
    Jan 12 00:49:43.468: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.376023ms
    Jan 12 00:49:43.468: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:49:45.471: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005410962s
    Jan 12 00:49:45.471: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:49:47.478: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 4.011664669s
    Jan 12 00:49:47.478: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
    Jan 12 00:49:49.472: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 6.005681009s
    Jan 12 00:49:49.472: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
    Jan 12 00:49:51.472: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 8.005624808s
    Jan 12 00:49:51.472: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
    Jan 12 00:49:53.472: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 10.006427537s
    Jan 12 00:49:53.472: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
    Jan 12 00:49:55.472: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 12.005750119s
    Jan 12 00:49:55.472: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
    Jan 12 00:49:57.472: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 14.00560871s
    Jan 12 00:49:57.472: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
    Jan 12 00:49:59.471: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 16.005357229s
    Jan 12 00:49:59.471: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
    Jan 12 00:50:01.472: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 18.005915074s
    Jan 12 00:50:01.472: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
    Jan 12 00:50:03.474: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=false. Elapsed: 20.007770676s
    Jan 12 00:50:03.474: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = false)
    Jan 12 00:50:05.473: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4": Phase="Running", Reason="", readiness=true. Elapsed: 22.006997609s
    Jan 12 00:50:05.473: INFO: The phase of Pod test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4 is Running (Ready = true)
    Jan 12 00:50:05.473: INFO: Pod "test-webserver-39853761-91ca-43d2-81d3-0dcba1de0fa4" satisfied condition "running and ready"
    Jan 12 00:50:05.476: INFO: Container started at 2023-01-12 00:49:44 +0000 UTC, pod became ready at 2023-01-12 00:50:03 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:50:05.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8484" for this suite. 01/12/23 00:50:05.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:50:05.583
Jan 12 00:50:05.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 00:50:05.584
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:50:05.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:50:05.611
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 00:50:05.633
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 00:50:06.076
STEP: Deploying the webhook pod 01/12/23 00:50:06.088
STEP: Wait for the deployment to be ready 01/12/23 00:50:06.272
Jan 12 00:50:06.292: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 12 00:50:08.299: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 50, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 50, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 50, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 50, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 00:50:10.313
STEP: Verifying the service has paired with the endpoint 01/12/23 00:50:10.336
Jan 12 00:50:11.337: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 01/12/23 00:50:11.461
STEP: Creating a configMap that should be mutated 01/12/23 00:50:11.473
STEP: Deleting the collection of validation webhooks 01/12/23 00:50:11.5
STEP: Creating a configMap that should not be mutated 01/12/23 00:50:11.588
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:50:11.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7838" for this suite. 01/12/23 00:50:11.687
STEP: Destroying namespace "webhook-7838-markers" for this suite. 01/12/23 00:50:11.783
------------------------------
• [SLOW TEST] [6.248 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:50:05.583
    Jan 12 00:50:05.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 00:50:05.584
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:50:05.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:50:05.611
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 00:50:05.633
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 00:50:06.076
    STEP: Deploying the webhook pod 01/12/23 00:50:06.088
    STEP: Wait for the deployment to be ready 01/12/23 00:50:06.272
    Jan 12 00:50:06.292: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 12 00:50:08.299: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 50, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 50, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 50, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 50, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 00:50:10.313
    STEP: Verifying the service has paired with the endpoint 01/12/23 00:50:10.336
    Jan 12 00:50:11.337: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 01/12/23 00:50:11.461
    STEP: Creating a configMap that should be mutated 01/12/23 00:50:11.473
    STEP: Deleting the collection of validation webhooks 01/12/23 00:50:11.5
    STEP: Creating a configMap that should not be mutated 01/12/23 00:50:11.588
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:50:11.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7838" for this suite. 01/12/23 00:50:11.687
    STEP: Destroying namespace "webhook-7838-markers" for this suite. 01/12/23 00:50:11.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:50:11.832
Jan 12 00:50:11.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 00:50:11.833
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:50:11.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:50:11.857
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 01/12/23 00:50:11.859
Jan 12 00:50:11.859: INFO: namespace kubectl-9378
Jan 12 00:50:11.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-9378 create -f -'
Jan 12 00:50:12.090: INFO: stderr: ""
Jan 12 00:50:12.090: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/12/23 00:50:12.09
Jan 12 00:50:13.094: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 00:50:13.094: INFO: Found 0 / 1
Jan 12 00:50:14.098: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 00:50:14.098: INFO: Found 0 / 1
Jan 12 00:50:15.094: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 00:50:15.094: INFO: Found 1 / 1
Jan 12 00:50:15.094: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 12 00:50:15.096: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 00:50:15.096: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 12 00:50:15.096: INFO: wait on agnhost-primary startup in kubectl-9378 
Jan 12 00:50:15.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-9378 logs agnhost-primary-mzzk9 agnhost-primary'
Jan 12 00:50:15.236: INFO: stderr: ""
Jan 12 00:50:15.236: INFO: stdout: "Paused\n"
STEP: exposing RC 01/12/23 00:50:15.236
Jan 12 00:50:15.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-9378 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 12 00:50:15.348: INFO: stderr: ""
Jan 12 00:50:15.348: INFO: stdout: "service/rm2 exposed\n"
Jan 12 00:50:15.350: INFO: Service rm2 in namespace kubectl-9378 found.
STEP: exposing service 01/12/23 00:50:17.356
Jan 12 00:50:17.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-9378 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 12 00:50:17.444: INFO: stderr: ""
Jan 12 00:50:17.444: INFO: stdout: "service/rm3 exposed\n"
Jan 12 00:50:17.465: INFO: Service rm3 in namespace kubectl-9378 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 00:50:19.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9378" for this suite. 01/12/23 00:50:19.475
------------------------------
• [SLOW TEST] [7.659 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:50:11.832
    Jan 12 00:50:11.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 00:50:11.833
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:50:11.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:50:11.857
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 01/12/23 00:50:11.859
    Jan 12 00:50:11.859: INFO: namespace kubectl-9378
    Jan 12 00:50:11.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-9378 create -f -'
    Jan 12 00:50:12.090: INFO: stderr: ""
    Jan 12 00:50:12.090: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/12/23 00:50:12.09
    Jan 12 00:50:13.094: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 00:50:13.094: INFO: Found 0 / 1
    Jan 12 00:50:14.098: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 00:50:14.098: INFO: Found 0 / 1
    Jan 12 00:50:15.094: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 00:50:15.094: INFO: Found 1 / 1
    Jan 12 00:50:15.094: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 12 00:50:15.096: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 00:50:15.096: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 12 00:50:15.096: INFO: wait on agnhost-primary startup in kubectl-9378 
    Jan 12 00:50:15.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-9378 logs agnhost-primary-mzzk9 agnhost-primary'
    Jan 12 00:50:15.236: INFO: stderr: ""
    Jan 12 00:50:15.236: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/12/23 00:50:15.236
    Jan 12 00:50:15.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-9378 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 12 00:50:15.348: INFO: stderr: ""
    Jan 12 00:50:15.348: INFO: stdout: "service/rm2 exposed\n"
    Jan 12 00:50:15.350: INFO: Service rm2 in namespace kubectl-9378 found.
    STEP: exposing service 01/12/23 00:50:17.356
    Jan 12 00:50:17.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-9378 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 12 00:50:17.444: INFO: stderr: ""
    Jan 12 00:50:17.444: INFO: stdout: "service/rm3 exposed\n"
    Jan 12 00:50:17.465: INFO: Service rm3 in namespace kubectl-9378 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:50:19.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9378" for this suite. 01/12/23 00:50:19.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:50:19.492
Jan 12 00:50:19.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 00:50:19.494
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:50:19.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:50:19.521
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-431/configmap-test-b7b0e3ea-c86c-42e9-affb-f6f3d2983757 01/12/23 00:50:19.523
STEP: Creating a pod to test consume configMaps 01/12/23 00:50:19.531
Jan 12 00:50:19.565: INFO: Waiting up to 5m0s for pod "pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9" in namespace "configmap-431" to be "Succeeded or Failed"
Jan 12 00:50:19.567: INFO: Pod "pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.711967ms
Jan 12 00:50:21.571: INFO: Pod "pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00625953s
Jan 12 00:50:23.571: INFO: Pod "pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006412179s
Jan 12 00:50:25.572: INFO: Pod "pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00765361s
STEP: Saw pod success 01/12/23 00:50:25.572
Jan 12 00:50:25.573: INFO: Pod "pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9" satisfied condition "Succeeded or Failed"
Jan 12 00:50:25.575: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9 container env-test: <nil>
STEP: delete the pod 01/12/23 00:50:25.59
Jan 12 00:50:25.612: INFO: Waiting for pod pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9 to disappear
Jan 12 00:50:25.615: INFO: Pod pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 00:50:25.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-431" for this suite. 01/12/23 00:50:25.618
------------------------------
• [SLOW TEST] [6.148 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:50:19.492
    Jan 12 00:50:19.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 00:50:19.494
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:50:19.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:50:19.521
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-431/configmap-test-b7b0e3ea-c86c-42e9-affb-f6f3d2983757 01/12/23 00:50:19.523
    STEP: Creating a pod to test consume configMaps 01/12/23 00:50:19.531
    Jan 12 00:50:19.565: INFO: Waiting up to 5m0s for pod "pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9" in namespace "configmap-431" to be "Succeeded or Failed"
    Jan 12 00:50:19.567: INFO: Pod "pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.711967ms
    Jan 12 00:50:21.571: INFO: Pod "pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00625953s
    Jan 12 00:50:23.571: INFO: Pod "pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006412179s
    Jan 12 00:50:25.572: INFO: Pod "pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00765361s
    STEP: Saw pod success 01/12/23 00:50:25.572
    Jan 12 00:50:25.573: INFO: Pod "pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9" satisfied condition "Succeeded or Failed"
    Jan 12 00:50:25.575: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9 container env-test: <nil>
    STEP: delete the pod 01/12/23 00:50:25.59
    Jan 12 00:50:25.612: INFO: Waiting for pod pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9 to disappear
    Jan 12 00:50:25.615: INFO: Pod pod-configmaps-84e0535c-d3fd-4a6b-af38-6855323fe1c9 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:50:25.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-431" for this suite. 01/12/23 00:50:25.618
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:50:25.641
Jan 12 00:50:25.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename var-expansion 01/12/23 00:50:25.642
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:50:25.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:50:25.662
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 01/12/23 00:50:25.664
Jan 12 00:50:25.734: INFO: Waiting up to 2m0s for pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077" in namespace "var-expansion-9341" to be "running"
Jan 12 00:50:25.736: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 2.515845ms
Jan 12 00:50:27.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006054746s
Jan 12 00:50:29.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006050597s
Jan 12 00:50:31.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006262633s
Jan 12 00:50:33.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005794126s
Jan 12 00:50:35.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006741016s
Jan 12 00:50:37.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006652821s
Jan 12 00:50:39.743: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008767943s
Jan 12 00:50:41.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 16.00588535s
Jan 12 00:50:43.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006644582s
Jan 12 00:50:45.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005939499s
Jan 12 00:50:47.746: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01215957s
Jan 12 00:50:49.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007038306s
Jan 12 00:50:51.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006412456s
Jan 12 00:50:53.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 28.00670488s
Jan 12 00:50:55.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005727536s
Jan 12 00:50:57.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 32.007250813s
Jan 12 00:50:59.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007188899s
Jan 12 00:51:01.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006197946s
Jan 12 00:51:03.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006915399s
Jan 12 00:51:05.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007388084s
Jan 12 00:51:07.739: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005094006s
Jan 12 00:51:09.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006371841s
Jan 12 00:51:11.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 46.00591064s
Jan 12 00:51:13.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006523883s
Jan 12 00:51:15.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007020753s
Jan 12 00:51:17.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006731231s
Jan 12 00:51:19.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 54.006909602s
Jan 12 00:51:21.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00614883s
Jan 12 00:51:23.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 58.006992922s
Jan 12 00:51:25.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007279013s
Jan 12 00:51:27.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005989389s
Jan 12 00:51:29.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006354565s
Jan 12 00:51:31.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006210217s
Jan 12 00:51:33.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007398421s
Jan 12 00:51:35.742: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007743064s
Jan 12 00:51:37.739: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005551903s
Jan 12 00:51:39.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006138207s
Jan 12 00:51:41.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.006098147s
Jan 12 00:51:43.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007146566s
Jan 12 00:51:45.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007400861s
Jan 12 00:51:47.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00580656s
Jan 12 00:51:49.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006971535s
Jan 12 00:51:51.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006868125s
Jan 12 00:51:53.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007256155s
Jan 12 00:51:55.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.00687105s
Jan 12 00:51:57.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006411624s
Jan 12 00:51:59.742: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007791387s
Jan 12 00:52:01.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.00605344s
Jan 12 00:52:03.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00730778s
Jan 12 00:52:05.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007497342s
Jan 12 00:52:07.746: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011858753s
Jan 12 00:52:09.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006913047s
Jan 12 00:52:11.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005993458s
Jan 12 00:52:13.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006940046s
Jan 12 00:52:15.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006805044s
Jan 12 00:52:17.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006532045s
Jan 12 00:52:19.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.007609914s
Jan 12 00:52:21.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005776753s
Jan 12 00:52:23.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.00702921s
Jan 12 00:52:25.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006240077s
Jan 12 00:52:25.743: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008706456s
STEP: updating the pod 01/12/23 00:52:25.743
Jan 12 00:52:26.260: INFO: Successfully updated pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077"
STEP: waiting for pod running 01/12/23 00:52:26.26
Jan 12 00:52:26.260: INFO: Waiting up to 2m0s for pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077" in namespace "var-expansion-9341" to be "running"
Jan 12 00:52:26.263: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 2.685893ms
Jan 12 00:52:28.267: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Running", Reason="", readiness=true. Elapsed: 2.006755429s
Jan 12 00:52:28.267: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077" satisfied condition "running"
STEP: deleting the pod gracefully 01/12/23 00:52:28.267
Jan 12 00:52:28.267: INFO: Deleting pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077" in namespace "var-expansion-9341"
Jan 12 00:52:28.286: INFO: Wait up to 5m0s for pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 00:53:00.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9341" for this suite. 01/12/23 00:53:00.301
------------------------------
• [SLOW TEST] [154.715 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:50:25.641
    Jan 12 00:50:25.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename var-expansion 01/12/23 00:50:25.642
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:50:25.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:50:25.662
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 01/12/23 00:50:25.664
    Jan 12 00:50:25.734: INFO: Waiting up to 2m0s for pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077" in namespace "var-expansion-9341" to be "running"
    Jan 12 00:50:25.736: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 2.515845ms
    Jan 12 00:50:27.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006054746s
    Jan 12 00:50:29.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006050597s
    Jan 12 00:50:31.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006262633s
    Jan 12 00:50:33.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005794126s
    Jan 12 00:50:35.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006741016s
    Jan 12 00:50:37.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006652821s
    Jan 12 00:50:39.743: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008767943s
    Jan 12 00:50:41.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 16.00588535s
    Jan 12 00:50:43.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006644582s
    Jan 12 00:50:45.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005939499s
    Jan 12 00:50:47.746: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01215957s
    Jan 12 00:50:49.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007038306s
    Jan 12 00:50:51.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006412456s
    Jan 12 00:50:53.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 28.00670488s
    Jan 12 00:50:55.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005727536s
    Jan 12 00:50:57.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 32.007250813s
    Jan 12 00:50:59.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007188899s
    Jan 12 00:51:01.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006197946s
    Jan 12 00:51:03.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006915399s
    Jan 12 00:51:05.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007388084s
    Jan 12 00:51:07.739: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005094006s
    Jan 12 00:51:09.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006371841s
    Jan 12 00:51:11.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 46.00591064s
    Jan 12 00:51:13.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006523883s
    Jan 12 00:51:15.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007020753s
    Jan 12 00:51:17.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006731231s
    Jan 12 00:51:19.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 54.006909602s
    Jan 12 00:51:21.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00614883s
    Jan 12 00:51:23.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 58.006992922s
    Jan 12 00:51:25.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007279013s
    Jan 12 00:51:27.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005989389s
    Jan 12 00:51:29.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006354565s
    Jan 12 00:51:31.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006210217s
    Jan 12 00:51:33.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007398421s
    Jan 12 00:51:35.742: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007743064s
    Jan 12 00:51:37.739: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005551903s
    Jan 12 00:51:39.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006138207s
    Jan 12 00:51:41.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.006098147s
    Jan 12 00:51:43.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007146566s
    Jan 12 00:51:45.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007400861s
    Jan 12 00:51:47.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00580656s
    Jan 12 00:51:49.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006971535s
    Jan 12 00:51:51.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006868125s
    Jan 12 00:51:53.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007256155s
    Jan 12 00:51:55.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.00687105s
    Jan 12 00:51:57.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006411624s
    Jan 12 00:51:59.742: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007791387s
    Jan 12 00:52:01.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.00605344s
    Jan 12 00:52:03.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00730778s
    Jan 12 00:52:05.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007497342s
    Jan 12 00:52:07.746: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011858753s
    Jan 12 00:52:09.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006913047s
    Jan 12 00:52:11.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005993458s
    Jan 12 00:52:13.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006940046s
    Jan 12 00:52:15.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006805044s
    Jan 12 00:52:17.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006532045s
    Jan 12 00:52:19.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.007609914s
    Jan 12 00:52:21.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005776753s
    Jan 12 00:52:23.741: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.00702921s
    Jan 12 00:52:25.740: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006240077s
    Jan 12 00:52:25.743: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008706456s
    STEP: updating the pod 01/12/23 00:52:25.743
    Jan 12 00:52:26.260: INFO: Successfully updated pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077"
    STEP: waiting for pod running 01/12/23 00:52:26.26
    Jan 12 00:52:26.260: INFO: Waiting up to 2m0s for pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077" in namespace "var-expansion-9341" to be "running"
    Jan 12 00:52:26.263: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Pending", Reason="", readiness=false. Elapsed: 2.685893ms
    Jan 12 00:52:28.267: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077": Phase="Running", Reason="", readiness=true. Elapsed: 2.006755429s
    Jan 12 00:52:28.267: INFO: Pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077" satisfied condition "running"
    STEP: deleting the pod gracefully 01/12/23 00:52:28.267
    Jan 12 00:52:28.267: INFO: Deleting pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077" in namespace "var-expansion-9341"
    Jan 12 00:52:28.286: INFO: Wait up to 5m0s for pod "var-expansion-e37019d9-0085-4404-ae71-abdf1d46e077" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:53:00.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9341" for this suite. 01/12/23 00:53:00.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:53:00.358
Jan 12 00:53:00.358: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 00:53:00.358
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:00.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:00.394
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 01/12/23 00:53:00.396
Jan 12 00:53:00.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: rename a version 01/12/23 00:53:05.453
STEP: check the new version name is served 01/12/23 00:53:05.471
STEP: check the old version name is removed 01/12/23 00:53:07.263
STEP: check the other version is not changed 01/12/23 00:53:08.03
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:53:11.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9174" for this suite. 01/12/23 00:53:11.873
------------------------------
• [SLOW TEST] [11.575 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:53:00.358
    Jan 12 00:53:00.358: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 00:53:00.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:00.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:00.394
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 01/12/23 00:53:00.396
    Jan 12 00:53:00.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: rename a version 01/12/23 00:53:05.453
    STEP: check the new version name is served 01/12/23 00:53:05.471
    STEP: check the old version name is removed 01/12/23 00:53:07.263
    STEP: check the other version is not changed 01/12/23 00:53:08.03
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:53:11.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9174" for this suite. 01/12/23 00:53:11.873
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:53:11.933
Jan 12 00:53:11.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 00:53:11.934
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:11.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:11.961
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 01/12/23 00:53:11.964
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 00:53:11.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7535" for this suite. 01/12/23 00:53:11.992
------------------------------
• [0.080 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:53:11.933
    Jan 12 00:53:11.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 00:53:11.934
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:11.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:11.961
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 01/12/23 00:53:11.964
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:53:11.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7535" for this suite. 01/12/23 00:53:11.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:53:12.018
Jan 12 00:53:12.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 00:53:12.019
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:12.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:12.048
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-98566001-a376-4aee-a614-a96f4fda7514 01/12/23 00:53:12.051
STEP: Creating a pod to test consume configMaps 01/12/23 00:53:12.057
Jan 12 00:53:12.101: INFO: Waiting up to 5m0s for pod "pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8" in namespace "configmap-6456" to be "Succeeded or Failed"
Jan 12 00:53:12.103: INFO: Pod "pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.266741ms
Jan 12 00:53:14.106: INFO: Pod "pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005547074s
Jan 12 00:53:16.106: INFO: Pod "pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005564327s
STEP: Saw pod success 01/12/23 00:53:16.106
Jan 12 00:53:16.106: INFO: Pod "pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8" satisfied condition "Succeeded or Failed"
Jan 12 00:53:16.109: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 00:53:16.125
Jan 12 00:53:16.147: INFO: Waiting for pod pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8 to disappear
Jan 12 00:53:16.149: INFO: Pod pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 00:53:16.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6456" for this suite. 01/12/23 00:53:16.153
------------------------------
• [4.223 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:53:12.018
    Jan 12 00:53:12.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 00:53:12.019
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:12.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:12.048
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-98566001-a376-4aee-a614-a96f4fda7514 01/12/23 00:53:12.051
    STEP: Creating a pod to test consume configMaps 01/12/23 00:53:12.057
    Jan 12 00:53:12.101: INFO: Waiting up to 5m0s for pod "pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8" in namespace "configmap-6456" to be "Succeeded or Failed"
    Jan 12 00:53:12.103: INFO: Pod "pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.266741ms
    Jan 12 00:53:14.106: INFO: Pod "pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005547074s
    Jan 12 00:53:16.106: INFO: Pod "pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005564327s
    STEP: Saw pod success 01/12/23 00:53:16.106
    Jan 12 00:53:16.106: INFO: Pod "pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8" satisfied condition "Succeeded or Failed"
    Jan 12 00:53:16.109: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 00:53:16.125
    Jan 12 00:53:16.147: INFO: Waiting for pod pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8 to disappear
    Jan 12 00:53:16.149: INFO: Pod pod-configmaps-75cf1903-452a-4fd9-ba1d-c3ce621965a8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:53:16.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6456" for this suite. 01/12/23 00:53:16.153
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:53:16.242
Jan 12 00:53:16.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 00:53:16.243
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:16.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:16.283
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 01/12/23 00:53:16.285
Jan 12 00:53:16.317: INFO: Waiting up to 5m0s for pod "downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0" in namespace "downward-api-9081" to be "Succeeded or Failed"
Jan 12 00:53:16.319: INFO: Pod "downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.283822ms
Jan 12 00:53:18.323: INFO: Pod "downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005962908s
Jan 12 00:53:20.324: INFO: Pod "downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007398796s
STEP: Saw pod success 01/12/23 00:53:20.324
Jan 12 00:53:20.325: INFO: Pod "downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0" satisfied condition "Succeeded or Failed"
Jan 12 00:53:20.327: INFO: Trying to get logs from node eqx04-flash06 pod downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0 container dapi-container: <nil>
STEP: delete the pod 01/12/23 00:53:20.337
Jan 12 00:53:20.357: INFO: Waiting for pod downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0 to disappear
Jan 12 00:53:20.359: INFO: Pod downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 12 00:53:20.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9081" for this suite. 01/12/23 00:53:20.363
------------------------------
• [4.139 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:53:16.242
    Jan 12 00:53:16.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 00:53:16.243
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:16.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:16.283
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 01/12/23 00:53:16.285
    Jan 12 00:53:16.317: INFO: Waiting up to 5m0s for pod "downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0" in namespace "downward-api-9081" to be "Succeeded or Failed"
    Jan 12 00:53:16.319: INFO: Pod "downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.283822ms
    Jan 12 00:53:18.323: INFO: Pod "downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005962908s
    Jan 12 00:53:20.324: INFO: Pod "downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007398796s
    STEP: Saw pod success 01/12/23 00:53:20.324
    Jan 12 00:53:20.325: INFO: Pod "downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0" satisfied condition "Succeeded or Failed"
    Jan 12 00:53:20.327: INFO: Trying to get logs from node eqx04-flash06 pod downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0 container dapi-container: <nil>
    STEP: delete the pod 01/12/23 00:53:20.337
    Jan 12 00:53:20.357: INFO: Waiting for pod downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0 to disappear
    Jan 12 00:53:20.359: INFO: Pod downward-api-02904774-a02a-41a6-8d40-9fd47d53bec0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:53:20.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9081" for this suite. 01/12/23 00:53:20.363
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:53:20.382
Jan 12 00:53:20.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 00:53:20.382
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:20.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:20.41
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/12/23 00:53:20.411
Jan 12 00:53:20.451: INFO: Waiting up to 5m0s for pod "pod-2324c5cc-9c22-47db-924d-326174177f12" in namespace "emptydir-5318" to be "Succeeded or Failed"
Jan 12 00:53:20.453: INFO: Pod "pod-2324c5cc-9c22-47db-924d-326174177f12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131846ms
Jan 12 00:53:22.457: INFO: Pod "pod-2324c5cc-9c22-47db-924d-326174177f12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005836593s
Jan 12 00:53:24.457: INFO: Pod "pod-2324c5cc-9c22-47db-924d-326174177f12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00577998s
STEP: Saw pod success 01/12/23 00:53:24.457
Jan 12 00:53:24.457: INFO: Pod "pod-2324c5cc-9c22-47db-924d-326174177f12" satisfied condition "Succeeded or Failed"
Jan 12 00:53:24.460: INFO: Trying to get logs from node eqx04-flash06 pod pod-2324c5cc-9c22-47db-924d-326174177f12 container test-container: <nil>
STEP: delete the pod 01/12/23 00:53:24.469
Jan 12 00:53:24.490: INFO: Waiting for pod pod-2324c5cc-9c22-47db-924d-326174177f12 to disappear
Jan 12 00:53:24.492: INFO: Pod pod-2324c5cc-9c22-47db-924d-326174177f12 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 00:53:24.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5318" for this suite. 01/12/23 00:53:24.495
------------------------------
• [4.130 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:53:20.382
    Jan 12 00:53:20.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 00:53:20.382
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:20.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:20.41
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/12/23 00:53:20.411
    Jan 12 00:53:20.451: INFO: Waiting up to 5m0s for pod "pod-2324c5cc-9c22-47db-924d-326174177f12" in namespace "emptydir-5318" to be "Succeeded or Failed"
    Jan 12 00:53:20.453: INFO: Pod "pod-2324c5cc-9c22-47db-924d-326174177f12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.131846ms
    Jan 12 00:53:22.457: INFO: Pod "pod-2324c5cc-9c22-47db-924d-326174177f12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005836593s
    Jan 12 00:53:24.457: INFO: Pod "pod-2324c5cc-9c22-47db-924d-326174177f12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00577998s
    STEP: Saw pod success 01/12/23 00:53:24.457
    Jan 12 00:53:24.457: INFO: Pod "pod-2324c5cc-9c22-47db-924d-326174177f12" satisfied condition "Succeeded or Failed"
    Jan 12 00:53:24.460: INFO: Trying to get logs from node eqx04-flash06 pod pod-2324c5cc-9c22-47db-924d-326174177f12 container test-container: <nil>
    STEP: delete the pod 01/12/23 00:53:24.469
    Jan 12 00:53:24.490: INFO: Waiting for pod pod-2324c5cc-9c22-47db-924d-326174177f12 to disappear
    Jan 12 00:53:24.492: INFO: Pod pod-2324c5cc-9c22-47db-924d-326174177f12 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:53:24.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5318" for this suite. 01/12/23 00:53:24.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:53:24.513
Jan 12 00:53:24.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 00:53:24.514
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:24.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:24.545
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/12/23 00:53:24.551
Jan 12 00:53:24.593: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-190" to be "running and ready"
Jan 12 00:53:24.595: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.197164ms
Jan 12 00:53:24.595: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:53:26.599: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006740767s
Jan 12 00:53:26.599: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:53:28.599: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.006037715s
Jan 12 00:53:28.599: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 12 00:53:28.599: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 01/12/23 00:53:28.601
Jan 12 00:53:28.641: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-190" to be "running and ready"
Jan 12 00:53:28.643: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.139379ms
Jan 12 00:53:28.643: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:53:30.661: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.019908323s
Jan 12 00:53:30.661: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 12 00:53:30.661: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/12/23 00:53:30.675
STEP: delete the pod with lifecycle hook 01/12/23 00:53:30.696
Jan 12 00:53:30.722: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 12 00:53:30.729: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 12 00:53:32.730: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 12 00:53:32.732: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 12 00:53:34.729: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 12 00:53:34.733: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 12 00:53:34.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-190" for this suite. 01/12/23 00:53:34.736
------------------------------
• [SLOW TEST] [10.251 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:53:24.513
    Jan 12 00:53:24.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 00:53:24.514
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:24.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:24.545
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/12/23 00:53:24.551
    Jan 12 00:53:24.593: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-190" to be "running and ready"
    Jan 12 00:53:24.595: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.197164ms
    Jan 12 00:53:24.595: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:53:26.599: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006740767s
    Jan 12 00:53:26.599: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:53:28.599: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.006037715s
    Jan 12 00:53:28.599: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 12 00:53:28.599: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 01/12/23 00:53:28.601
    Jan 12 00:53:28.641: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-190" to be "running and ready"
    Jan 12 00:53:28.643: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.139379ms
    Jan 12 00:53:28.643: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:53:30.661: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.019908323s
    Jan 12 00:53:30.661: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 12 00:53:30.661: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/12/23 00:53:30.675
    STEP: delete the pod with lifecycle hook 01/12/23 00:53:30.696
    Jan 12 00:53:30.722: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 12 00:53:30.729: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 12 00:53:32.730: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 12 00:53:32.732: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 12 00:53:34.729: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 12 00:53:34.733: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:53:34.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-190" for this suite. 01/12/23 00:53:34.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:53:34.766
Jan 12 00:53:34.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 00:53:34.767
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:34.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:34.787
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 01/12/23 00:53:34.789
Jan 12 00:53:34.854: INFO: Waiting up to 5m0s for pod "annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f" in namespace "projected-3507" to be "running and ready"
Jan 12 00:53:34.863: INFO: Pod "annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.911284ms
Jan 12 00:53:34.863: INFO: The phase of Pod annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:53:36.869: INFO: Pod "annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014915876s
Jan 12 00:53:36.869: INFO: The phase of Pod annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:53:38.867: INFO: Pod "annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f": Phase="Running", Reason="", readiness=true. Elapsed: 4.012879885s
Jan 12 00:53:38.867: INFO: The phase of Pod annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f is Running (Ready = true)
Jan 12 00:53:38.867: INFO: Pod "annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f" satisfied condition "running and ready"
Jan 12 00:53:39.405: INFO: Successfully updated pod "annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 00:53:41.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3507" for this suite. 01/12/23 00:53:41.429
------------------------------
• [SLOW TEST] [6.679 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:53:34.766
    Jan 12 00:53:34.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 00:53:34.767
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:34.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:34.787
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 01/12/23 00:53:34.789
    Jan 12 00:53:34.854: INFO: Waiting up to 5m0s for pod "annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f" in namespace "projected-3507" to be "running and ready"
    Jan 12 00:53:34.863: INFO: Pod "annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.911284ms
    Jan 12 00:53:34.863: INFO: The phase of Pod annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:53:36.869: INFO: Pod "annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014915876s
    Jan 12 00:53:36.869: INFO: The phase of Pod annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:53:38.867: INFO: Pod "annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f": Phase="Running", Reason="", readiness=true. Elapsed: 4.012879885s
    Jan 12 00:53:38.867: INFO: The phase of Pod annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f is Running (Ready = true)
    Jan 12 00:53:38.867: INFO: Pod "annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f" satisfied condition "running and ready"
    Jan 12 00:53:39.405: INFO: Successfully updated pod "annotationupdateec74a770-eeef-4bce-a803-c938a5d3e12f"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:53:41.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3507" for this suite. 01/12/23 00:53:41.429
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:53:41.445
Jan 12 00:53:41.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubelet-test 01/12/23 00:53:41.446
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:41.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:41.468
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 12 00:53:41.502: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs49128de3-4729-4235-ad3c-930120c09756" in namespace "kubelet-test-2249" to be "running and ready"
Jan 12 00:53:41.504: INFO: Pod "busybox-readonly-fs49128de3-4729-4235-ad3c-930120c09756": Phase="Pending", Reason="", readiness=false. Elapsed: 2.228966ms
Jan 12 00:53:41.504: INFO: The phase of Pod busybox-readonly-fs49128de3-4729-4235-ad3c-930120c09756 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:53:43.508: INFO: Pod "busybox-readonly-fs49128de3-4729-4235-ad3c-930120c09756": Phase="Running", Reason="", readiness=true. Elapsed: 2.006214052s
Jan 12 00:53:43.508: INFO: The phase of Pod busybox-readonly-fs49128de3-4729-4235-ad3c-930120c09756 is Running (Ready = true)
Jan 12 00:53:43.508: INFO: Pod "busybox-readonly-fs49128de3-4729-4235-ad3c-930120c09756" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 12 00:53:43.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2249" for this suite. 01/12/23 00:53:43.523
------------------------------
• [2.093 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:53:41.445
    Jan 12 00:53:41.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubelet-test 01/12/23 00:53:41.446
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:41.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:41.468
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 12 00:53:41.502: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs49128de3-4729-4235-ad3c-930120c09756" in namespace "kubelet-test-2249" to be "running and ready"
    Jan 12 00:53:41.504: INFO: Pod "busybox-readonly-fs49128de3-4729-4235-ad3c-930120c09756": Phase="Pending", Reason="", readiness=false. Elapsed: 2.228966ms
    Jan 12 00:53:41.504: INFO: The phase of Pod busybox-readonly-fs49128de3-4729-4235-ad3c-930120c09756 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:53:43.508: INFO: Pod "busybox-readonly-fs49128de3-4729-4235-ad3c-930120c09756": Phase="Running", Reason="", readiness=true. Elapsed: 2.006214052s
    Jan 12 00:53:43.508: INFO: The phase of Pod busybox-readonly-fs49128de3-4729-4235-ad3c-930120c09756 is Running (Ready = true)
    Jan 12 00:53:43.508: INFO: Pod "busybox-readonly-fs49128de3-4729-4235-ad3c-930120c09756" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:53:43.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2249" for this suite. 01/12/23 00:53:43.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:53:43.54
Jan 12 00:53:43.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename events 01/12/23 00:53:43.541
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:43.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:43.564
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/12/23 00:53:43.566
Jan 12 00:53:43.572: INFO: created test-event-1
Jan 12 00:53:43.578: INFO: created test-event-2
Jan 12 00:53:43.587: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/12/23 00:53:43.587
STEP: delete collection of events 01/12/23 00:53:43.601
Jan 12 00:53:43.601: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/12/23 00:53:43.632
Jan 12 00:53:43.632: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 12 00:53:43.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-62" for this suite. 01/12/23 00:53:43.638
------------------------------
• [0.117 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:53:43.54
    Jan 12 00:53:43.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename events 01/12/23 00:53:43.541
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:43.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:43.564
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/12/23 00:53:43.566
    Jan 12 00:53:43.572: INFO: created test-event-1
    Jan 12 00:53:43.578: INFO: created test-event-2
    Jan 12 00:53:43.587: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/12/23 00:53:43.587
    STEP: delete collection of events 01/12/23 00:53:43.601
    Jan 12 00:53:43.601: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/12/23 00:53:43.632
    Jan 12 00:53:43.632: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:53:43.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-62" for this suite. 01/12/23 00:53:43.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:53:43.659
Jan 12 00:53:43.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename resourcequota 01/12/23 00:53:43.66
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:43.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:43.679
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 01/12/23 00:53:43.682
STEP: Getting a ResourceQuota 01/12/23 00:53:43.687
STEP: Listing all ResourceQuotas with LabelSelector 01/12/23 00:53:43.69
STEP: Patching the ResourceQuota 01/12/23 00:53:43.697
STEP: Deleting a Collection of ResourceQuotas 01/12/23 00:53:43.716
STEP: Verifying the deleted ResourceQuota 01/12/23 00:53:43.727
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 00:53:43.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7709" for this suite. 01/12/23 00:53:43.733
------------------------------
• [0.094 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:53:43.659
    Jan 12 00:53:43.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename resourcequota 01/12/23 00:53:43.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:43.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:43.679
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 01/12/23 00:53:43.682
    STEP: Getting a ResourceQuota 01/12/23 00:53:43.687
    STEP: Listing all ResourceQuotas with LabelSelector 01/12/23 00:53:43.69
    STEP: Patching the ResourceQuota 01/12/23 00:53:43.697
    STEP: Deleting a Collection of ResourceQuotas 01/12/23 00:53:43.716
    STEP: Verifying the deleted ResourceQuota 01/12/23 00:53:43.727
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:53:43.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7709" for this suite. 01/12/23 00:53:43.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:53:43.755
Jan 12 00:53:43.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 00:53:43.755
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:43.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:43.782
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 00:53:43.801
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 00:53:44.972
STEP: Deploying the webhook pod 01/12/23 00:53:44.984
STEP: Wait for the deployment to be ready 01/12/23 00:53:45.052
Jan 12 00:53:45.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jan 12 00:53:47.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 00:53:49.071
STEP: Verifying the service has paired with the endpoint 01/12/23 00:53:49.106
Jan 12 00:53:50.106: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/12/23 00:53:50.109
STEP: create a pod that should be updated by the webhook 01/12/23 00:53:50.127
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:53:50.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9849" for this suite. 01/12/23 00:53:50.282
STEP: Destroying namespace "webhook-9849-markers" for this suite. 01/12/23 00:53:50.327
------------------------------
• [SLOW TEST] [6.600 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:53:43.755
    Jan 12 00:53:43.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 00:53:43.755
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:43.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:43.782
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 00:53:43.801
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 00:53:44.972
    STEP: Deploying the webhook pod 01/12/23 00:53:44.984
    STEP: Wait for the deployment to be ready 01/12/23 00:53:45.052
    Jan 12 00:53:45.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jan 12 00:53:47.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 53, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 00:53:49.071
    STEP: Verifying the service has paired with the endpoint 01/12/23 00:53:49.106
    Jan 12 00:53:50.106: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/12/23 00:53:50.109
    STEP: create a pod that should be updated by the webhook 01/12/23 00:53:50.127
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:53:50.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9849" for this suite. 01/12/23 00:53:50.282
    STEP: Destroying namespace "webhook-9849-markers" for this suite. 01/12/23 00:53:50.327
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:53:50.355
Jan 12 00:53:50.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename disruption 01/12/23 00:53:50.356
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:50.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:50.38
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 01/12/23 00:53:50.382
STEP: Waiting for the pdb to be processed 01/12/23 00:53:50.388
STEP: First trying to evict a pod which shouldn't be evictable 01/12/23 00:53:52.402
STEP: Waiting for all pods to be running 01/12/23 00:53:52.402
Jan 12 00:53:52.405: INFO: pods: 0 < 3
Jan 12 00:53:54.409: INFO: running pods: 0 < 3
STEP: locating a running pod 01/12/23 00:53:56.41
STEP: Updating the pdb to allow a pod to be evicted 01/12/23 00:53:56.418
STEP: Waiting for the pdb to be processed 01/12/23 00:53:56.427
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/12/23 00:53:58.433
STEP: Waiting for all pods to be running 01/12/23 00:53:58.433
STEP: Waiting for the pdb to observed all healthy pods 01/12/23 00:53:58.435
STEP: Patching the pdb to disallow a pod to be evicted 01/12/23 00:53:58.497
STEP: Waiting for the pdb to be processed 01/12/23 00:53:58.512
STEP: Waiting for all pods to be running 01/12/23 00:53:58.529
Jan 12 00:53:58.532: INFO: running pods: 2 < 3
Jan 12 00:54:00.537: INFO: running pods: 2 < 3
STEP: locating a running pod 01/12/23 00:54:02.537
STEP: Deleting the pdb to allow a pod to be evicted 01/12/23 00:54:02.544
STEP: Waiting for the pdb to be deleted 01/12/23 00:54:02.553
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/12/23 00:54:02.555
STEP: Waiting for all pods to be running 01/12/23 00:54:02.555
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 12 00:54:02.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2461" for this suite. 01/12/23 00:54:02.62
------------------------------
• [SLOW TEST] [12.363 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:53:50.355
    Jan 12 00:53:50.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename disruption 01/12/23 00:53:50.356
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:53:50.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:53:50.38
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 01/12/23 00:53:50.382
    STEP: Waiting for the pdb to be processed 01/12/23 00:53:50.388
    STEP: First trying to evict a pod which shouldn't be evictable 01/12/23 00:53:52.402
    STEP: Waiting for all pods to be running 01/12/23 00:53:52.402
    Jan 12 00:53:52.405: INFO: pods: 0 < 3
    Jan 12 00:53:54.409: INFO: running pods: 0 < 3
    STEP: locating a running pod 01/12/23 00:53:56.41
    STEP: Updating the pdb to allow a pod to be evicted 01/12/23 00:53:56.418
    STEP: Waiting for the pdb to be processed 01/12/23 00:53:56.427
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/12/23 00:53:58.433
    STEP: Waiting for all pods to be running 01/12/23 00:53:58.433
    STEP: Waiting for the pdb to observed all healthy pods 01/12/23 00:53:58.435
    STEP: Patching the pdb to disallow a pod to be evicted 01/12/23 00:53:58.497
    STEP: Waiting for the pdb to be processed 01/12/23 00:53:58.512
    STEP: Waiting for all pods to be running 01/12/23 00:53:58.529
    Jan 12 00:53:58.532: INFO: running pods: 2 < 3
    Jan 12 00:54:00.537: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/12/23 00:54:02.537
    STEP: Deleting the pdb to allow a pod to be evicted 01/12/23 00:54:02.544
    STEP: Waiting for the pdb to be deleted 01/12/23 00:54:02.553
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/12/23 00:54:02.555
    STEP: Waiting for all pods to be running 01/12/23 00:54:02.555
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:54:02.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2461" for this suite. 01/12/23 00:54:02.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:54:02.719
Jan 12 00:54:02.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename resourcequota 01/12/23 00:54:02.72
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:54:02.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:54:02.757
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 01/12/23 00:54:02.759
STEP: Ensuring ResourceQuota status is calculated 01/12/23 00:54:02.779
STEP: Creating a ResourceQuota with not terminating scope 01/12/23 00:54:04.783
STEP: Ensuring ResourceQuota status is calculated 01/12/23 00:54:04.79
STEP: Creating a long running pod 01/12/23 00:54:06.794
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/12/23 00:54:06.835
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/12/23 00:54:08.84
STEP: Deleting the pod 01/12/23 00:54:10.843
STEP: Ensuring resource quota status released the pod usage 01/12/23 00:54:10.864
STEP: Creating a terminating pod 01/12/23 00:54:12.868
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/12/23 00:54:12.916
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/12/23 00:54:14.92
STEP: Deleting the pod 01/12/23 00:54:16.924
STEP: Ensuring resource quota status released the pod usage 01/12/23 00:54:16.952
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 00:54:18.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7744" for this suite. 01/12/23 00:54:18.959
------------------------------
• [SLOW TEST] [16.327 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:54:02.719
    Jan 12 00:54:02.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename resourcequota 01/12/23 00:54:02.72
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:54:02.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:54:02.757
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 01/12/23 00:54:02.759
    STEP: Ensuring ResourceQuota status is calculated 01/12/23 00:54:02.779
    STEP: Creating a ResourceQuota with not terminating scope 01/12/23 00:54:04.783
    STEP: Ensuring ResourceQuota status is calculated 01/12/23 00:54:04.79
    STEP: Creating a long running pod 01/12/23 00:54:06.794
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/12/23 00:54:06.835
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/12/23 00:54:08.84
    STEP: Deleting the pod 01/12/23 00:54:10.843
    STEP: Ensuring resource quota status released the pod usage 01/12/23 00:54:10.864
    STEP: Creating a terminating pod 01/12/23 00:54:12.868
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/12/23 00:54:12.916
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/12/23 00:54:14.92
    STEP: Deleting the pod 01/12/23 00:54:16.924
    STEP: Ensuring resource quota status released the pod usage 01/12/23 00:54:16.952
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:54:18.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7744" for this suite. 01/12/23 00:54:18.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:54:19.067
Jan 12 00:54:19.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename daemonsets 01/12/23 00:54:19.068
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:54:19.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:54:19.101
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Jan 12 00:54:19.121: INFO: Create a RollingUpdate DaemonSet
Jan 12 00:54:19.248: INFO: Check that daemon pods launch on every node of the cluster
Jan 12 00:54:19.259: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:19.259: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:19.259: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:19.262: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 00:54:19.262: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 00:54:20.266: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:20.266: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:20.266: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:20.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 00:54:20.268: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 00:54:21.265: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:21.266: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:21.266: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:21.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 00:54:21.268: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 00:54:22.266: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:22.266: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:22.266: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:22.269: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 00:54:22.269: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan 12 00:54:22.269: INFO: Update the DaemonSet to trigger a rollout
Jan 12 00:54:22.278: INFO: Updating DaemonSet daemon-set
Jan 12 00:54:25.291: INFO: Roll back the DaemonSet before rollout is complete
Jan 12 00:54:25.308: INFO: Updating DaemonSet daemon-set
Jan 12 00:54:25.308: INFO: Make sure DaemonSet rollback is complete
Jan 12 00:54:25.314: INFO: Wrong image for pod: daemon-set-4qr87. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Jan 12 00:54:25.315: INFO: Pod daemon-set-4qr87 is not available
Jan 12 00:54:25.331: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:25.331: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:25.331: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:26.339: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:26.339: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:26.339: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:27.338: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:27.338: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:27.338: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:28.335: INFO: Pod daemon-set-wcq9d is not available
Jan 12 00:54:28.339: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:28.339: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:54:28.339: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/12/23 00:54:28.344
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5523, will wait for the garbage collector to delete the pods 01/12/23 00:54:28.344
Jan 12 00:54:28.405: INFO: Deleting DaemonSet.extensions daemon-set took: 7.197381ms
Jan 12 00:54:28.506: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.139921ms
Jan 12 00:54:31.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 00:54:31.009: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 12 00:54:31.013: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20144471"},"items":null}

Jan 12 00:54:31.015: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20144471"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:54:31.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5523" for this suite. 01/12/23 00:54:31.027
------------------------------
• [SLOW TEST] [12.030 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:54:19.067
    Jan 12 00:54:19.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename daemonsets 01/12/23 00:54:19.068
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:54:19.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:54:19.101
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Jan 12 00:54:19.121: INFO: Create a RollingUpdate DaemonSet
    Jan 12 00:54:19.248: INFO: Check that daemon pods launch on every node of the cluster
    Jan 12 00:54:19.259: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:19.259: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:19.259: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:19.262: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 00:54:19.262: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 00:54:20.266: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:20.266: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:20.266: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:20.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 00:54:20.268: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 00:54:21.265: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:21.266: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:21.266: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:21.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 00:54:21.268: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 00:54:22.266: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:22.266: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:22.266: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:22.269: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 00:54:22.269: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Jan 12 00:54:22.269: INFO: Update the DaemonSet to trigger a rollout
    Jan 12 00:54:22.278: INFO: Updating DaemonSet daemon-set
    Jan 12 00:54:25.291: INFO: Roll back the DaemonSet before rollout is complete
    Jan 12 00:54:25.308: INFO: Updating DaemonSet daemon-set
    Jan 12 00:54:25.308: INFO: Make sure DaemonSet rollback is complete
    Jan 12 00:54:25.314: INFO: Wrong image for pod: daemon-set-4qr87. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Jan 12 00:54:25.315: INFO: Pod daemon-set-4qr87 is not available
    Jan 12 00:54:25.331: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:25.331: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:25.331: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:26.339: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:26.339: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:26.339: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:27.338: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:27.338: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:27.338: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:28.335: INFO: Pod daemon-set-wcq9d is not available
    Jan 12 00:54:28.339: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:28.339: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:54:28.339: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/12/23 00:54:28.344
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5523, will wait for the garbage collector to delete the pods 01/12/23 00:54:28.344
    Jan 12 00:54:28.405: INFO: Deleting DaemonSet.extensions daemon-set took: 7.197381ms
    Jan 12 00:54:28.506: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.139921ms
    Jan 12 00:54:31.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 00:54:31.009: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 12 00:54:31.013: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20144471"},"items":null}

    Jan 12 00:54:31.015: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20144471"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:54:31.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5523" for this suite. 01/12/23 00:54:31.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:54:31.097
Jan 12 00:54:31.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename endpointslice 01/12/23 00:54:31.098
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:54:31.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:54:31.124
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jan 12 00:54:31.136: INFO: Endpoints addresses: [10.9.100.103 10.9.140.107 10.9.40.104] , ports: [6443]
Jan 12 00:54:31.136: INFO: EndpointSlices addresses: [10.9.100.103 10.9.140.107 10.9.40.104] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 12 00:54:31.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-40" for this suite. 01/12/23 00:54:31.141
------------------------------
• [0.068 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:54:31.097
    Jan 12 00:54:31.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename endpointslice 01/12/23 00:54:31.098
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:54:31.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:54:31.124
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jan 12 00:54:31.136: INFO: Endpoints addresses: [10.9.100.103 10.9.140.107 10.9.40.104] , ports: [6443]
    Jan 12 00:54:31.136: INFO: EndpointSlices addresses: [10.9.100.103 10.9.140.107 10.9.40.104] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:54:31.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-40" for this suite. 01/12/23 00:54:31.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:54:31.167
Jan 12 00:54:31.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename var-expansion 01/12/23 00:54:31.168
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:54:31.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:54:31.194
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 01/12/23 00:54:31.196
STEP: waiting for pod running 01/12/23 00:54:31.242
Jan 12 00:54:31.242: INFO: Waiting up to 2m0s for pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89" in namespace "var-expansion-2954" to be "running"
Jan 12 00:54:31.244: INFO: Pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.196029ms
Jan 12 00:54:33.247: INFO: Pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89": Phase="Running", Reason="", readiness=true. Elapsed: 2.005183459s
Jan 12 00:54:33.247: INFO: Pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89" satisfied condition "running"
STEP: creating a file in subpath 01/12/23 00:54:33.247
Jan 12 00:54:33.249: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2954 PodName:var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 00:54:33.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 00:54:33.250: INFO: ExecWithOptions: Clientset creation
Jan 12 00:54:33.250: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/var-expansion-2954/pods/var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/12/23 00:54:33.382
Jan 12 00:54:33.385: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2954 PodName:var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 00:54:33.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 00:54:33.385: INFO: ExecWithOptions: Clientset creation
Jan 12 00:54:33.385: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/var-expansion-2954/pods/var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/12/23 00:54:33.525
Jan 12 00:54:34.046: INFO: Successfully updated pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89"
STEP: waiting for annotated pod running 01/12/23 00:54:34.047
Jan 12 00:54:34.047: INFO: Waiting up to 2m0s for pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89" in namespace "var-expansion-2954" to be "running"
Jan 12 00:54:34.051: INFO: Pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89": Phase="Running", Reason="", readiness=true. Elapsed: 4.44856ms
Jan 12 00:54:34.051: INFO: Pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89" satisfied condition "running"
STEP: deleting the pod gracefully 01/12/23 00:54:34.051
Jan 12 00:54:34.051: INFO: Deleting pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89" in namespace "var-expansion-2954"
Jan 12 00:54:34.067: INFO: Wait up to 5m0s for pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 00:55:08.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2954" for this suite. 01/12/23 00:55:08.078
------------------------------
• [SLOW TEST] [36.927 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:54:31.167
    Jan 12 00:54:31.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename var-expansion 01/12/23 00:54:31.168
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:54:31.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:54:31.194
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 01/12/23 00:54:31.196
    STEP: waiting for pod running 01/12/23 00:54:31.242
    Jan 12 00:54:31.242: INFO: Waiting up to 2m0s for pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89" in namespace "var-expansion-2954" to be "running"
    Jan 12 00:54:31.244: INFO: Pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.196029ms
    Jan 12 00:54:33.247: INFO: Pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89": Phase="Running", Reason="", readiness=true. Elapsed: 2.005183459s
    Jan 12 00:54:33.247: INFO: Pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89" satisfied condition "running"
    STEP: creating a file in subpath 01/12/23 00:54:33.247
    Jan 12 00:54:33.249: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2954 PodName:var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 00:54:33.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 00:54:33.250: INFO: ExecWithOptions: Clientset creation
    Jan 12 00:54:33.250: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/var-expansion-2954/pods/var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/12/23 00:54:33.382
    Jan 12 00:54:33.385: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2954 PodName:var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 00:54:33.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 00:54:33.385: INFO: ExecWithOptions: Clientset creation
    Jan 12 00:54:33.385: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/var-expansion-2954/pods/var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/12/23 00:54:33.525
    Jan 12 00:54:34.046: INFO: Successfully updated pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89"
    STEP: waiting for annotated pod running 01/12/23 00:54:34.047
    Jan 12 00:54:34.047: INFO: Waiting up to 2m0s for pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89" in namespace "var-expansion-2954" to be "running"
    Jan 12 00:54:34.051: INFO: Pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89": Phase="Running", Reason="", readiness=true. Elapsed: 4.44856ms
    Jan 12 00:54:34.051: INFO: Pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89" satisfied condition "running"
    STEP: deleting the pod gracefully 01/12/23 00:54:34.051
    Jan 12 00:54:34.051: INFO: Deleting pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89" in namespace "var-expansion-2954"
    Jan 12 00:54:34.067: INFO: Wait up to 5m0s for pod "var-expansion-4260feb4-6272-40e7-8ae1-aaa258096e89" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:55:08.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2954" for this suite. 01/12/23 00:55:08.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:55:08.095
Jan 12 00:55:08.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename deployment 01/12/23 00:55:08.096
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:08.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:08.118
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/12/23 00:55:08.128
STEP: waiting for Deployment to be created 01/12/23 00:55:08.188
STEP: waiting for all Replicas to be Ready 01/12/23 00:55:08.189
Jan 12 00:55:08.190: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 00:55:08.190: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 00:55:08.205: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 00:55:08.205: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 00:55:08.211: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 00:55:08.211: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 00:55:08.360: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 00:55:08.360: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 00:55:09.929: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 12 00:55:09.929: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 12 00:55:10.384: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/12/23 00:55:10.384
W0112 00:55:10.399134      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 12 00:55:10.400: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/12/23 00:55:10.4
Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
Jan 12 00:55:10.402: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
Jan 12 00:55:10.402: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
Jan 12 00:55:10.413: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
Jan 12 00:55:10.413: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
Jan 12 00:55:10.450: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
Jan 12 00:55:10.450: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
Jan 12 00:55:10.450: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
Jan 12 00:55:10.450: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
Jan 12 00:55:10.496: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
Jan 12 00:55:10.496: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
Jan 12 00:55:12.509: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
Jan 12 00:55:12.509: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
Jan 12 00:55:12.509: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
STEP: listing Deployments 01/12/23 00:55:12.509
Jan 12 00:55:12.519: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/12/23 00:55:12.519
Jan 12 00:55:12.539: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/12/23 00:55:12.539
Jan 12 00:55:12.562: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 00:55:12.562: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 00:55:12.578: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 00:55:12.578: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 00:55:12.650: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 00:55:12.656: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 00:55:14.619: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 00:55:14.647: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 00:55:14.647: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 00:55:14.711: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 00:55:16.360: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/12/23 00:55:16.372
STEP: fetching the DeploymentStatus 01/12/23 00:55:16.378
Jan 12 00:55:16.381: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
Jan 12 00:55:16.381: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
Jan 12 00:55:16.381: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 3
STEP: deleting the Deployment 01/12/23 00:55:16.382
Jan 12 00:55:16.392: INFO: observed event type MODIFIED
Jan 12 00:55:16.392: INFO: observed event type MODIFIED
Jan 12 00:55:16.392: INFO: observed event type MODIFIED
Jan 12 00:55:16.392: INFO: observed event type MODIFIED
Jan 12 00:55:16.393: INFO: observed event type MODIFIED
Jan 12 00:55:16.393: INFO: observed event type MODIFIED
Jan 12 00:55:16.393: INFO: observed event type MODIFIED
Jan 12 00:55:16.393: INFO: observed event type MODIFIED
Jan 12 00:55:16.393: INFO: observed event type MODIFIED
Jan 12 00:55:16.393: INFO: observed event type MODIFIED
Jan 12 00:55:16.393: INFO: observed event type MODIFIED
Jan 12 00:55:16.393: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 00:55:16.407: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 00:55:16.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7245" for this suite. 01/12/23 00:55:16.43
------------------------------
• [SLOW TEST] [8.359 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:55:08.095
    Jan 12 00:55:08.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename deployment 01/12/23 00:55:08.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:08.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:08.118
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/12/23 00:55:08.128
    STEP: waiting for Deployment to be created 01/12/23 00:55:08.188
    STEP: waiting for all Replicas to be Ready 01/12/23 00:55:08.189
    Jan 12 00:55:08.190: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 00:55:08.190: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 00:55:08.205: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 00:55:08.205: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 00:55:08.211: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 00:55:08.211: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 00:55:08.360: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 00:55:08.360: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 00:55:09.929: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 12 00:55:09.929: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 12 00:55:10.384: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/12/23 00:55:10.384
    W0112 00:55:10.399134      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 12 00:55:10.400: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/12/23 00:55:10.4
    Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
    Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
    Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
    Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
    Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
    Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
    Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
    Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 0
    Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
    Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
    Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
    Jan 12 00:55:10.401: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
    Jan 12 00:55:10.402: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
    Jan 12 00:55:10.402: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
    Jan 12 00:55:10.413: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
    Jan 12 00:55:10.413: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
    Jan 12 00:55:10.450: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
    Jan 12 00:55:10.450: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
    Jan 12 00:55:10.450: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
    Jan 12 00:55:10.450: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
    Jan 12 00:55:10.496: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
    Jan 12 00:55:10.496: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
    Jan 12 00:55:12.509: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
    Jan 12 00:55:12.509: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
    Jan 12 00:55:12.509: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
    STEP: listing Deployments 01/12/23 00:55:12.509
    Jan 12 00:55:12.519: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/12/23 00:55:12.519
    Jan 12 00:55:12.539: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/12/23 00:55:12.539
    Jan 12 00:55:12.562: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 00:55:12.562: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 00:55:12.578: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 00:55:12.578: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 00:55:12.650: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 00:55:12.656: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 00:55:14.619: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 00:55:14.647: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 00:55:14.647: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 00:55:14.711: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 00:55:16.360: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/12/23 00:55:16.372
    STEP: fetching the DeploymentStatus 01/12/23 00:55:16.378
    Jan 12 00:55:16.381: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
    Jan 12 00:55:16.381: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
    Jan 12 00:55:16.381: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
    Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
    Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
    Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 1
    Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
    Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
    Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
    Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 2
    Jan 12 00:55:16.382: INFO: observed Deployment test-deployment in namespace deployment-7245 with ReadyReplicas 3
    STEP: deleting the Deployment 01/12/23 00:55:16.382
    Jan 12 00:55:16.392: INFO: observed event type MODIFIED
    Jan 12 00:55:16.392: INFO: observed event type MODIFIED
    Jan 12 00:55:16.392: INFO: observed event type MODIFIED
    Jan 12 00:55:16.392: INFO: observed event type MODIFIED
    Jan 12 00:55:16.393: INFO: observed event type MODIFIED
    Jan 12 00:55:16.393: INFO: observed event type MODIFIED
    Jan 12 00:55:16.393: INFO: observed event type MODIFIED
    Jan 12 00:55:16.393: INFO: observed event type MODIFIED
    Jan 12 00:55:16.393: INFO: observed event type MODIFIED
    Jan 12 00:55:16.393: INFO: observed event type MODIFIED
    Jan 12 00:55:16.393: INFO: observed event type MODIFIED
    Jan 12 00:55:16.393: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 00:55:16.407: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:55:16.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7245" for this suite. 01/12/23 00:55:16.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:55:16.455
Jan 12 00:55:16.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 00:55:16.457
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:16.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:16.491
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-09d7185d-6086-4e0d-9ea6-a64c6d94639c 01/12/23 00:55:16.494
STEP: Creating a pod to test consume configMaps 01/12/23 00:55:16.5
Jan 12 00:55:16.586: INFO: Waiting up to 5m0s for pod "pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4" in namespace "configmap-3654" to be "Succeeded or Failed"
Jan 12 00:55:16.588: INFO: Pod "pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.121334ms
Jan 12 00:55:18.592: INFO: Pod "pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006017764s
Jan 12 00:55:20.593: INFO: Pod "pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007195041s
Jan 12 00:55:22.591: INFO: Pod "pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005531798s
STEP: Saw pod success 01/12/23 00:55:22.591
Jan 12 00:55:22.592: INFO: Pod "pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4" satisfied condition "Succeeded or Failed"
Jan 12 00:55:22.594: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4 container configmap-volume-test: <nil>
STEP: delete the pod 01/12/23 00:55:22.609
Jan 12 00:55:22.634: INFO: Waiting for pod pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4 to disappear
Jan 12 00:55:22.637: INFO: Pod pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 00:55:22.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3654" for this suite. 01/12/23 00:55:22.64
------------------------------
• [SLOW TEST] [6.268 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:55:16.455
    Jan 12 00:55:16.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 00:55:16.457
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:16.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:16.491
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-09d7185d-6086-4e0d-9ea6-a64c6d94639c 01/12/23 00:55:16.494
    STEP: Creating a pod to test consume configMaps 01/12/23 00:55:16.5
    Jan 12 00:55:16.586: INFO: Waiting up to 5m0s for pod "pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4" in namespace "configmap-3654" to be "Succeeded or Failed"
    Jan 12 00:55:16.588: INFO: Pod "pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.121334ms
    Jan 12 00:55:18.592: INFO: Pod "pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006017764s
    Jan 12 00:55:20.593: INFO: Pod "pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007195041s
    Jan 12 00:55:22.591: INFO: Pod "pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005531798s
    STEP: Saw pod success 01/12/23 00:55:22.591
    Jan 12 00:55:22.592: INFO: Pod "pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4" satisfied condition "Succeeded or Failed"
    Jan 12 00:55:22.594: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4 container configmap-volume-test: <nil>
    STEP: delete the pod 01/12/23 00:55:22.609
    Jan 12 00:55:22.634: INFO: Waiting for pod pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4 to disappear
    Jan 12 00:55:22.637: INFO: Pod pod-configmaps-da11036e-0163-4d32-8ed1-7859b0391da4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:55:22.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3654" for this suite. 01/12/23 00:55:22.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:55:22.724
Jan 12 00:55:22.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 00:55:22.725
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:22.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:22.752
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/12/23 00:55:22.754
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/12/23 00:55:22.755
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/12/23 00:55:22.755
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/12/23 00:55:22.755
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/12/23 00:55:22.756
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/12/23 00:55:22.756
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/12/23 00:55:22.757
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:55:22.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2294" for this suite. 01/12/23 00:55:22.761
------------------------------
• [0.058 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:55:22.724
    Jan 12 00:55:22.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 00:55:22.725
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:22.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:22.752
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/12/23 00:55:22.754
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/12/23 00:55:22.755
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/12/23 00:55:22.755
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/12/23 00:55:22.755
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/12/23 00:55:22.756
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/12/23 00:55:22.756
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/12/23 00:55:22.757
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:55:22.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2294" for this suite. 01/12/23 00:55:22.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:55:22.783
Jan 12 00:55:22.783: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename dns 01/12/23 00:55:22.784
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:22.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:22.808
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/12/23 00:55:22.81
Jan 12 00:55:22.842: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1230  dca3668e-6678-4008-80b5-4e00cedff96b 20145044 0 2023-01-12 00:55:22 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-12 00:55:22 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fnhnm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fnhnm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 00:55:22.842: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1230" to be "running and ready"
Jan 12 00:55:22.845: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489157ms
Jan 12 00:55:22.845: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:55:24.848: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005611752s
Jan 12 00:55:24.848: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 12 00:55:26.849: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.006324063s
Jan 12 00:55:26.849: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 12 00:55:26.849: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/12/23 00:55:26.849
Jan 12 00:55:26.849: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1230 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 00:55:26.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 00:55:26.849: INFO: ExecWithOptions: Clientset creation
Jan 12 00:55:26.849: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/dns-1230/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/12/23 00:55:26.999
Jan 12 00:55:26.999: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1230 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 00:55:26.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 00:55:26.999: INFO: ExecWithOptions: Clientset creation
Jan 12 00:55:26.999: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/dns-1230/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 12 00:55:27.171: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 00:55:27.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1230" for this suite. 01/12/23 00:55:27.199
------------------------------
• [4.435 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:55:22.783
    Jan 12 00:55:22.783: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename dns 01/12/23 00:55:22.784
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:22.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:22.808
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/12/23 00:55:22.81
    Jan 12 00:55:22.842: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1230  dca3668e-6678-4008-80b5-4e00cedff96b 20145044 0 2023-01-12 00:55:22 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-12 00:55:22 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fnhnm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fnhnm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 00:55:22.842: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1230" to be "running and ready"
    Jan 12 00:55:22.845: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489157ms
    Jan 12 00:55:22.845: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:55:24.848: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005611752s
    Jan 12 00:55:24.848: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 00:55:26.849: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.006324063s
    Jan 12 00:55:26.849: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 12 00:55:26.849: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/12/23 00:55:26.849
    Jan 12 00:55:26.849: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1230 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 00:55:26.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 00:55:26.849: INFO: ExecWithOptions: Clientset creation
    Jan 12 00:55:26.849: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/dns-1230/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/12/23 00:55:26.999
    Jan 12 00:55:26.999: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1230 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 00:55:26.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 00:55:26.999: INFO: ExecWithOptions: Clientset creation
    Jan 12 00:55:26.999: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/dns-1230/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 12 00:55:27.171: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:55:27.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1230" for this suite. 01/12/23 00:55:27.199
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:55:27.219
Jan 12 00:55:27.219: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 00:55:27.22
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:27.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:27.262
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-06c9f71b-d4b2-49f4-b0aa-ad0b8d967248 01/12/23 00:55:27.264
STEP: Creating a pod to test consume configMaps 01/12/23 00:55:27.274
Jan 12 00:55:27.314: INFO: Waiting up to 5m0s for pod "pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397" in namespace "configmap-7218" to be "Succeeded or Failed"
Jan 12 00:55:27.316: INFO: Pod "pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397": Phase="Pending", Reason="", readiness=false. Elapsed: 2.307828ms
Jan 12 00:55:29.320: INFO: Pod "pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006796847s
Jan 12 00:55:31.320: INFO: Pod "pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00638831s
Jan 12 00:55:33.319: INFO: Pod "pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005234253s
STEP: Saw pod success 01/12/23 00:55:33.319
Jan 12 00:55:33.319: INFO: Pod "pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397" satisfied condition "Succeeded or Failed"
Jan 12 00:55:33.321: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 00:55:33.33
Jan 12 00:55:33.357: INFO: Waiting for pod pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397 to disappear
Jan 12 00:55:33.359: INFO: Pod pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 00:55:33.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7218" for this suite. 01/12/23 00:55:33.363
------------------------------
• [SLOW TEST] [6.162 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:55:27.219
    Jan 12 00:55:27.219: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 00:55:27.22
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:27.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:27.262
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-06c9f71b-d4b2-49f4-b0aa-ad0b8d967248 01/12/23 00:55:27.264
    STEP: Creating a pod to test consume configMaps 01/12/23 00:55:27.274
    Jan 12 00:55:27.314: INFO: Waiting up to 5m0s for pod "pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397" in namespace "configmap-7218" to be "Succeeded or Failed"
    Jan 12 00:55:27.316: INFO: Pod "pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397": Phase="Pending", Reason="", readiness=false. Elapsed: 2.307828ms
    Jan 12 00:55:29.320: INFO: Pod "pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006796847s
    Jan 12 00:55:31.320: INFO: Pod "pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00638831s
    Jan 12 00:55:33.319: INFO: Pod "pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005234253s
    STEP: Saw pod success 01/12/23 00:55:33.319
    Jan 12 00:55:33.319: INFO: Pod "pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397" satisfied condition "Succeeded or Failed"
    Jan 12 00:55:33.321: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 00:55:33.33
    Jan 12 00:55:33.357: INFO: Waiting for pod pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397 to disappear
    Jan 12 00:55:33.359: INFO: Pod pod-configmaps-c209dd89-8e06-45f5-94a4-925e2ce0d397 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:55:33.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7218" for this suite. 01/12/23 00:55:33.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:55:33.382
Jan 12 00:55:33.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename runtimeclass 01/12/23 00:55:33.383
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:33.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:33.404
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 12 00:55:33.463: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6080 to be scheduled
Jan 12 00:55:33.466: INFO: 1 pods are not scheduled: [runtimeclass-6080/test-runtimeclass-runtimeclass-6080-preconfigured-handler-b5d4m(9ddbb9cb-3e7c-4cb3-af4d-83864cad2e7b)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 12 00:55:35.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6080" for this suite. 01/12/23 00:55:35.481
------------------------------
• [2.119 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:55:33.382
    Jan 12 00:55:33.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename runtimeclass 01/12/23 00:55:33.383
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:33.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:33.404
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 12 00:55:33.463: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6080 to be scheduled
    Jan 12 00:55:33.466: INFO: 1 pods are not scheduled: [runtimeclass-6080/test-runtimeclass-runtimeclass-6080-preconfigured-handler-b5d4m(9ddbb9cb-3e7c-4cb3-af4d-83864cad2e7b)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:55:35.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6080" for this suite. 01/12/23 00:55:35.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:55:35.502
Jan 12 00:55:35.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 00:55:35.503
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:35.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:35.529
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 01/12/23 00:55:35.546
STEP: waiting for available Endpoint 01/12/23 00:55:35.561
STEP: listing all Endpoints 01/12/23 00:55:35.563
STEP: updating the Endpoint 01/12/23 00:55:35.566
STEP: fetching the Endpoint 01/12/23 00:55:35.573
STEP: patching the Endpoint 01/12/23 00:55:35.576
STEP: fetching the Endpoint 01/12/23 00:55:35.583
STEP: deleting the Endpoint by Collection 01/12/23 00:55:35.585
STEP: waiting for Endpoint deletion 01/12/23 00:55:35.597
STEP: fetching the Endpoint 01/12/23 00:55:35.598
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 00:55:35.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-528" for this suite. 01/12/23 00:55:35.604
------------------------------
• [0.121 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:55:35.502
    Jan 12 00:55:35.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 00:55:35.503
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:35.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:35.529
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 01/12/23 00:55:35.546
    STEP: waiting for available Endpoint 01/12/23 00:55:35.561
    STEP: listing all Endpoints 01/12/23 00:55:35.563
    STEP: updating the Endpoint 01/12/23 00:55:35.566
    STEP: fetching the Endpoint 01/12/23 00:55:35.573
    STEP: patching the Endpoint 01/12/23 00:55:35.576
    STEP: fetching the Endpoint 01/12/23 00:55:35.583
    STEP: deleting the Endpoint by Collection 01/12/23 00:55:35.585
    STEP: waiting for Endpoint deletion 01/12/23 00:55:35.597
    STEP: fetching the Endpoint 01/12/23 00:55:35.598
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:55:35.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-528" for this suite. 01/12/23 00:55:35.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:55:35.629
Jan 12 00:55:35.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename deployment 01/12/23 00:55:35.629
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:35.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:35.666
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/12/23 00:55:35.674
Jan 12 00:55:35.674: INFO: Creating simple deployment test-deployment-b4b5c
Jan 12 00:55:35.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-deployment-b4b5c-54bc444df\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jan 12 00:55:37.741: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-b4b5c-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 01/12/23 00:55:39.745
Jan 12 00:55:39.749: INFO: Deployment test-deployment-b4b5c has Conditions: [{Available True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:37 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4b5c-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 01/12/23 00:55:39.749
Jan 12 00:55:39.765: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 55, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 55, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 55, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-b4b5c-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/12/23 00:55:39.765
Jan 12 00:55:39.766: INFO: Observed &Deployment event: ADDED
Jan 12 00:55:39.766: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4b5c-54bc444df"}
Jan 12 00:55:39.767: INFO: Observed &Deployment event: MODIFIED
Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4b5c-54bc444df"}
Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 12 00:55:39.767: INFO: Observed &Deployment event: MODIFIED
Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-b4b5c-54bc444df" is progressing.}
Jan 12 00:55:39.767: INFO: Observed &Deployment event: MODIFIED
Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:37 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4b5c-54bc444df" has successfully progressed.}
Jan 12 00:55:39.767: INFO: Observed &Deployment event: MODIFIED
Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:37 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4b5c-54bc444df" has successfully progressed.}
Jan 12 00:55:39.767: INFO: Found Deployment test-deployment-b4b5c in namespace deployment-1043 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 12 00:55:39.767: INFO: Deployment test-deployment-b4b5c has an updated status
STEP: patching the Statefulset Status 01/12/23 00:55:39.767
Jan 12 00:55:39.767: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 12 00:55:39.778: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/12/23 00:55:39.778
Jan 12 00:55:39.779: INFO: Observed &Deployment event: ADDED
Jan 12 00:55:39.779: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4b5c-54bc444df"}
Jan 12 00:55:39.780: INFO: Observed &Deployment event: MODIFIED
Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4b5c-54bc444df"}
Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 12 00:55:39.780: INFO: Observed &Deployment event: MODIFIED
Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-b4b5c-54bc444df" is progressing.}
Jan 12 00:55:39.780: INFO: Observed &Deployment event: MODIFIED
Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:37 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4b5c-54bc444df" has successfully progressed.}
Jan 12 00:55:39.780: INFO: Observed &Deployment event: MODIFIED
Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:37 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4b5c-54bc444df" has successfully progressed.}
Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 12 00:55:39.780: INFO: Observed &Deployment event: MODIFIED
Jan 12 00:55:39.780: INFO: Found deployment test-deployment-b4b5c in namespace deployment-1043 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 12 00:55:39.780: INFO: Deployment test-deployment-b4b5c has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 00:55:39.796: INFO: Deployment "test-deployment-b4b5c":
&Deployment{ObjectMeta:{test-deployment-b4b5c  deployment-1043  2b5b1a58-69f3-40b4-a926-f095a2ccc9ba 20145215 1 2023-01-12 00:55:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-12 00:55:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-12 00:55:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-12 00:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000571648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-b4b5c-54bc444df",LastUpdateTime:2023-01-12 00:55:39 +0000 UTC,LastTransitionTime:2023-01-12 00:55:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 12 00:55:39.798: INFO: New ReplicaSet "test-deployment-b4b5c-54bc444df" of Deployment "test-deployment-b4b5c":
&ReplicaSet{ObjectMeta:{test-deployment-b4b5c-54bc444df  deployment-1043  1332b2b5-cd23-46c7-9fc5-d57dd2cd6b7f 20145198 1 2023-01-12 00:55:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-b4b5c 2b5b1a58-69f3-40b4-a926-f095a2ccc9ba 0xc000571a37 0xc000571a38}] [] [{kube-controller-manager Update apps/v1 2023-01-12 00:55:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b5b1a58-69f3-40b4-a926-f095a2ccc9ba\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 00:55:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000571ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 12 00:55:39.801: INFO: Pod "test-deployment-b4b5c-54bc444df-pbsb8" is available:
&Pod{ObjectMeta:{test-deployment-b4b5c-54bc444df-pbsb8 test-deployment-b4b5c-54bc444df- deployment-1043  cd295ed1-16da-42c2-a9f9-49872053fcc2 20145197 0 2023-01-12 00:55:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:1bd255678b9a4feec2d51ca22aac8d6af3b3add302d12091ef2a1a5caf108391 cni.projectcalico.org/podIP:172.21.88.161/32 cni.projectcalico.org/podIPs:172.21.88.161/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.161"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.161"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-b4b5c-54bc444df 1332b2b5-cd23-46c7-9fc5-d57dd2cd6b7f 0xc000571eb7 0xc000571eb8}] [] [{kube-controller-manager Update v1 2023-01-12 00:55:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1332b2b5-cd23-46c7-9fc5-d57dd2cd6b7f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 00:55:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 00:55:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 00:55:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9p6xg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9p6xg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 00:55:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 00:55:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 00:55:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 00:55:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.161,StartTime:2023-01-12 00:55:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 00:55:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://6e4dc9b55fb369021502c6bb24679f512cc87b1ca0c484454fbb0e0ece82d877,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 00:55:39.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1043" for this suite. 01/12/23 00:55:39.805
------------------------------
• [4.220 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:55:35.629
    Jan 12 00:55:35.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename deployment 01/12/23 00:55:35.629
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:35.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:35.666
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/12/23 00:55:35.674
    Jan 12 00:55:35.674: INFO: Creating simple deployment test-deployment-b4b5c
    Jan 12 00:55:35.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-deployment-b4b5c-54bc444df\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jan 12 00:55:37.741: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-b4b5c-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 01/12/23 00:55:39.745
    Jan 12 00:55:39.749: INFO: Deployment test-deployment-b4b5c has Conditions: [{Available True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:37 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4b5c-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 01/12/23 00:55:39.749
    Jan 12 00:55:39.765: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 55, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 55, 37, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 0, 55, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 0, 55, 35, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-b4b5c-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/12/23 00:55:39.765
    Jan 12 00:55:39.766: INFO: Observed &Deployment event: ADDED
    Jan 12 00:55:39.766: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4b5c-54bc444df"}
    Jan 12 00:55:39.767: INFO: Observed &Deployment event: MODIFIED
    Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4b5c-54bc444df"}
    Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 12 00:55:39.767: INFO: Observed &Deployment event: MODIFIED
    Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-b4b5c-54bc444df" is progressing.}
    Jan 12 00:55:39.767: INFO: Observed &Deployment event: MODIFIED
    Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:37 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4b5c-54bc444df" has successfully progressed.}
    Jan 12 00:55:39.767: INFO: Observed &Deployment event: MODIFIED
    Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:37 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 12 00:55:39.767: INFO: Observed Deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4b5c-54bc444df" has successfully progressed.}
    Jan 12 00:55:39.767: INFO: Found Deployment test-deployment-b4b5c in namespace deployment-1043 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 12 00:55:39.767: INFO: Deployment test-deployment-b4b5c has an updated status
    STEP: patching the Statefulset Status 01/12/23 00:55:39.767
    Jan 12 00:55:39.767: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 12 00:55:39.778: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/12/23 00:55:39.778
    Jan 12 00:55:39.779: INFO: Observed &Deployment event: ADDED
    Jan 12 00:55:39.779: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4b5c-54bc444df"}
    Jan 12 00:55:39.780: INFO: Observed &Deployment event: MODIFIED
    Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-b4b5c-54bc444df"}
    Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 12 00:55:39.780: INFO: Observed &Deployment event: MODIFIED
    Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:35 +0000 UTC 2023-01-12 00:55:35 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-b4b5c-54bc444df" is progressing.}
    Jan 12 00:55:39.780: INFO: Observed &Deployment event: MODIFIED
    Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:37 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4b5c-54bc444df" has successfully progressed.}
    Jan 12 00:55:39.780: INFO: Observed &Deployment event: MODIFIED
    Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:37 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 00:55:37 +0000 UTC 2023-01-12 00:55:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-b4b5c-54bc444df" has successfully progressed.}
    Jan 12 00:55:39.780: INFO: Observed deployment test-deployment-b4b5c in namespace deployment-1043 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 12 00:55:39.780: INFO: Observed &Deployment event: MODIFIED
    Jan 12 00:55:39.780: INFO: Found deployment test-deployment-b4b5c in namespace deployment-1043 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 12 00:55:39.780: INFO: Deployment test-deployment-b4b5c has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 00:55:39.796: INFO: Deployment "test-deployment-b4b5c":
    &Deployment{ObjectMeta:{test-deployment-b4b5c  deployment-1043  2b5b1a58-69f3-40b4-a926-f095a2ccc9ba 20145215 1 2023-01-12 00:55:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-12 00:55:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-12 00:55:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-12 00:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000571648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-b4b5c-54bc444df",LastUpdateTime:2023-01-12 00:55:39 +0000 UTC,LastTransitionTime:2023-01-12 00:55:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 12 00:55:39.798: INFO: New ReplicaSet "test-deployment-b4b5c-54bc444df" of Deployment "test-deployment-b4b5c":
    &ReplicaSet{ObjectMeta:{test-deployment-b4b5c-54bc444df  deployment-1043  1332b2b5-cd23-46c7-9fc5-d57dd2cd6b7f 20145198 1 2023-01-12 00:55:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-b4b5c 2b5b1a58-69f3-40b4-a926-f095a2ccc9ba 0xc000571a37 0xc000571a38}] [] [{kube-controller-manager Update apps/v1 2023-01-12 00:55:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b5b1a58-69f3-40b4-a926-f095a2ccc9ba\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 00:55:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000571ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 00:55:39.801: INFO: Pod "test-deployment-b4b5c-54bc444df-pbsb8" is available:
    &Pod{ObjectMeta:{test-deployment-b4b5c-54bc444df-pbsb8 test-deployment-b4b5c-54bc444df- deployment-1043  cd295ed1-16da-42c2-a9f9-49872053fcc2 20145197 0 2023-01-12 00:55:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:1bd255678b9a4feec2d51ca22aac8d6af3b3add302d12091ef2a1a5caf108391 cni.projectcalico.org/podIP:172.21.88.161/32 cni.projectcalico.org/podIPs:172.21.88.161/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.161"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.161"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-b4b5c-54bc444df 1332b2b5-cd23-46c7-9fc5-d57dd2cd6b7f 0xc000571eb7 0xc000571eb8}] [] [{kube-controller-manager Update v1 2023-01-12 00:55:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1332b2b5-cd23-46c7-9fc5-d57dd2cd6b7f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 00:55:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 00:55:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 00:55:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9p6xg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9p6xg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 00:55:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 00:55:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 00:55:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 00:55:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.161,StartTime:2023-01-12 00:55:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 00:55:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://6e4dc9b55fb369021502c6bb24679f512cc87b1ca0c484454fbb0e0ece82d877,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:55:39.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1043" for this suite. 01/12/23 00:55:39.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:55:39.851
Jan 12 00:55:39.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 00:55:39.852
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:39.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:39.881
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jan 12 00:55:39.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1662 create -f -'
Jan 12 00:55:40.784: INFO: stderr: ""
Jan 12 00:55:40.784: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 12 00:55:40.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1662 create -f -'
Jan 12 00:55:41.326: INFO: stderr: ""
Jan 12 00:55:41.326: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/12/23 00:55:41.326
Jan 12 00:55:42.330: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 00:55:42.330: INFO: Found 0 / 1
Jan 12 00:55:43.330: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 00:55:43.330: INFO: Found 1 / 1
Jan 12 00:55:43.330: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 12 00:55:43.333: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 00:55:43.333: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 12 00:55:43.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1662 describe pod agnhost-primary-mch6t'
Jan 12 00:55:43.407: INFO: stderr: ""
Jan 12 00:55:43.407: INFO: stdout: "Name:             agnhost-primary-mch6t\nNamespace:        kubectl-1662\nPriority:         0\nService Account:  default\nNode:             eqx04-flash06/10.9.40.106\nStart Time:       Thu, 12 Jan 2023 00:55:40 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: fe047e3016b09027706291d667612f9d8c93dddf7fd64ca4899ed73188ac01b4\n                  cni.projectcalico.org/podIP: 172.21.88.166/32\n                  cni.projectcalico.org/podIPs: 172.21.88.166/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"calico\",\n                        \"ips\": [\n                            \"172.21.88.166\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"calico\",\n                        \"ips\": [\n                            \"172.21.88.166\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\nStatus:           Running\nIP:               172.21.88.166\nIPs:\n  IP:           172.21.88.166\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   robin://e4db60ffa9529cb86bf9087911c4f53a774eca9cc7e337e6993fa60d995aa65a\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 12 Jan 2023 00:55:42 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bj7t4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-bj7t4:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       2s    default-scheduler  Successfully assigned kubectl-1662/agnhost-primary-mch6t to eqx04-flash06\n  Normal  AddedInterface  2s    multus             Add eth0 [172.21.88.166/32] from calico\n  Normal  Pulled          2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Jan 12 00:55:43.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1662 describe rc agnhost-primary'
Jan 12 00:55:43.479: INFO: stderr: ""
Jan 12 00:55:43.479: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1662\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-mch6t\n"
Jan 12 00:55:43.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1662 describe service agnhost-primary'
Jan 12 00:55:43.551: INFO: stderr: ""
Jan 12 00:55:43.551: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1662\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.19.104.86\nIPs:               172.19.104.86\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.21.88.166:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 12 00:55:43.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1662 describe node eqx01-flash03'
Jan 12 00:55:43.719: INFO: stderr: ""
Jan 12 00:55:43.719: INFO: stdout: "Name:               eqx01-flash03\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=eqx01-flash03\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    robin.io/domain=ROBIN\n                    robin.io/hostname=eqx01-flash03.robinsystems.com\n                    robin.io/nodetype=robin-node\n                    robin.io/rnodetype=robin-master-node\n                    robin.io/robinhost=eqx01-flash03\n                    robin.io/robinrpool=default\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"robin\":\"eqx01-flash03.robinsystems.com\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.9.100.103/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.21.111.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 03 Oct 2022 18:55:01 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  eqx01-flash03\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 12 Jan 2023 00:55:35 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 11 Jan 2023 08:01:17 +0000   Wed, 11 Jan 2023 08:01:17 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 12 Jan 2023 00:52:56 +0000   Wed, 11 Jan 2023 07:50:51 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 12 Jan 2023 00:52:56 +0000   Wed, 11 Jan 2023 07:50:51 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 12 Jan 2023 00:52:56 +0000   Wed, 11 Jan 2023 07:50:51 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 12 Jan 2023 00:52:56 +0000   Wed, 11 Jan 2023 08:49:09 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.9.100.103\n  Hostname:    eqx01-flash03\nCapacity:\n  cpu:                40\n  ephemeral-storage:  51175Mi\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             263939672Ki\n  pods:               110\nAllocatable:\n  cpu:                39\n  ephemeral-storage:  48294789041\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             263837272Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c523c641e84a4174bf6ba3e4c74852eb\n  System UUID:                00000000-0000-0000-0000-002590F95A7C\n  Boot ID:                    2f0ddc1b-a3ff-4c90-b798-e105ea5f5bdc\n  Kernel Version:             3.10.0-1127.13.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  robin://19.3.9\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nNon-terminated Pods:          (22 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-bgfl4                                          250m (0%)     0 (0%)      0 (0%)           0 (0%)         16h\n  kube-system                 coredns-75b9776576-m9vmm                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     17h\n  kube-system                 etcd-eqx01-flash03                                         100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         17h\n  kube-system                 kube-apiserver-eqx01-flash03                               250m (0%)     0 (0%)      0 (0%)           0 (0%)         16h\n  kube-system                 kube-controller-manager-eqx01-flash03                      200m (0%)     0 (0%)      0 (0%)           0 (0%)         17h\n  kube-system                 kube-multus-ds-amd64-xkrcr                                 100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      93d\n  kube-system                 kube-proxy-5g2wz                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h\n  kube-system                 kube-scheduler-eqx01-flash03                               100m (0%)     0 (0%)      0 (0%)           0 (0%)         16h\n  kube-system                 kube-sriov-device-plugin-amd64-b9cpt                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         93d\n  robinio                     csi-attacher-robin-75b8467f4d-2ww7n                        250m (0%)     0 (0%)      405Mi (0%)       0 (0%)         16h\n  robinio                     csi-nodeplugin-robin-wtcjl                                 250m (0%)     0 (0%)      405Mi (0%)       0 (0%)         16h\n  robinio                     csi-provisioner-robin-644d94f7d-qlgfd                      250m (0%)     0 (0%)      405Mi (0%)       0 (0%)         16h\n  robinio                     csi-resizer-robin-d859c65dd-lvsvh                          250m (0%)     0 (0%)      405Mi (0%)       0 (0%)         16h\n  robinio                     csi-snapshotter-robin-65c6fbcb5-glcck                      250m (0%)     0 (0%)      405Mi (0%)       0 (0%)         16h\n  robinio                     robin-file-server-65dc88554b-6cpbs                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         16h\n  robinio                     robin-master-ff98b76b7-4sgjh                               1 (2%)        0 (0%)      500Mi (0%)       0 (0%)         16h\n  robinio                     robin-nfs-watchdog-vxvtw                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         16h\n  robinio                     robin-patroni-0                                            100m (0%)     500m (1%)   4G (1%)          4G (1%)        16h\n  robinio                     robin-worker-r9kq2                                         1 (2%)        0 (0%)      500Mi (0%)       0 (0%)         16h\n  robinio                     robink8s-serverext-jbnfg                                   100m (0%)     100m (0%)   100Mi (0%)       100Mi (0%)     16h\n  robinio                     snapshot-controller-9dc68bc8d-xb5sk                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         16h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-cvpqz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                4550m (11%)     700m (1%)\n  memory             7331530Ki (2%)  4233930Ki (1%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:              <none>\n"
Jan 12 00:55:43.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1662 describe namespace kubectl-1662'
Jan 12 00:55:43.796: INFO: stderr: ""
Jan 12 00:55:43.796: INFO: stdout: "Name:         kubectl-1662\nLabels:       e2e-framework=kubectl\n              e2e-run=57cc750e-de8d-467f-a423-c8fd052edd1e\n              kubernetes.io/metadata.name=kubectl-1662\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 00:55:43.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1662" for this suite. 01/12/23 00:55:43.8
------------------------------
• [3.996 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:55:39.851
    Jan 12 00:55:39.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 00:55:39.852
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:39.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:39.881
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jan 12 00:55:39.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1662 create -f -'
    Jan 12 00:55:40.784: INFO: stderr: ""
    Jan 12 00:55:40.784: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 12 00:55:40.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1662 create -f -'
    Jan 12 00:55:41.326: INFO: stderr: ""
    Jan 12 00:55:41.326: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/12/23 00:55:41.326
    Jan 12 00:55:42.330: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 00:55:42.330: INFO: Found 0 / 1
    Jan 12 00:55:43.330: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 00:55:43.330: INFO: Found 1 / 1
    Jan 12 00:55:43.330: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 12 00:55:43.333: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 00:55:43.333: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 12 00:55:43.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1662 describe pod agnhost-primary-mch6t'
    Jan 12 00:55:43.407: INFO: stderr: ""
    Jan 12 00:55:43.407: INFO: stdout: "Name:             agnhost-primary-mch6t\nNamespace:        kubectl-1662\nPriority:         0\nService Account:  default\nNode:             eqx04-flash06/10.9.40.106\nStart Time:       Thu, 12 Jan 2023 00:55:40 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: fe047e3016b09027706291d667612f9d8c93dddf7fd64ca4899ed73188ac01b4\n                  cni.projectcalico.org/podIP: 172.21.88.166/32\n                  cni.projectcalico.org/podIPs: 172.21.88.166/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"calico\",\n                        \"ips\": [\n                            \"172.21.88.166\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"calico\",\n                        \"ips\": [\n                            \"172.21.88.166\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\nStatus:           Running\nIP:               172.21.88.166\nIPs:\n  IP:           172.21.88.166\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   robin://e4db60ffa9529cb86bf9087911c4f53a774eca9cc7e337e6993fa60d995aa65a\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 12 Jan 2023 00:55:42 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bj7t4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-bj7t4:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       2s    default-scheduler  Successfully assigned kubectl-1662/agnhost-primary-mch6t to eqx04-flash06\n  Normal  AddedInterface  2s    multus             Add eth0 [172.21.88.166/32] from calico\n  Normal  Pulled          2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
    Jan 12 00:55:43.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1662 describe rc agnhost-primary'
    Jan 12 00:55:43.479: INFO: stderr: ""
    Jan 12 00:55:43.479: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1662\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-mch6t\n"
    Jan 12 00:55:43.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1662 describe service agnhost-primary'
    Jan 12 00:55:43.551: INFO: stderr: ""
    Jan 12 00:55:43.551: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1662\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.19.104.86\nIPs:               172.19.104.86\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.21.88.166:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 12 00:55:43.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1662 describe node eqx01-flash03'
    Jan 12 00:55:43.719: INFO: stderr: ""
    Jan 12 00:55:43.719: INFO: stdout: "Name:               eqx01-flash03\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=eqx01-flash03\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    robin.io/domain=ROBIN\n                    robin.io/hostname=eqx01-flash03.robinsystems.com\n                    robin.io/nodetype=robin-node\n                    robin.io/rnodetype=robin-master-node\n                    robin.io/robinhost=eqx01-flash03\n                    robin.io/robinrpool=default\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"robin\":\"eqx01-flash03.robinsystems.com\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.9.100.103/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.21.111.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 03 Oct 2022 18:55:01 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  eqx01-flash03\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 12 Jan 2023 00:55:35 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 11 Jan 2023 08:01:17 +0000   Wed, 11 Jan 2023 08:01:17 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 12 Jan 2023 00:52:56 +0000   Wed, 11 Jan 2023 07:50:51 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 12 Jan 2023 00:52:56 +0000   Wed, 11 Jan 2023 07:50:51 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 12 Jan 2023 00:52:56 +0000   Wed, 11 Jan 2023 07:50:51 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 12 Jan 2023 00:52:56 +0000   Wed, 11 Jan 2023 08:49:09 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.9.100.103\n  Hostname:    eqx01-flash03\nCapacity:\n  cpu:                40\n  ephemeral-storage:  51175Mi\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             263939672Ki\n  pods:               110\nAllocatable:\n  cpu:                39\n  ephemeral-storage:  48294789041\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             263837272Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c523c641e84a4174bf6ba3e4c74852eb\n  System UUID:                00000000-0000-0000-0000-002590F95A7C\n  Boot ID:                    2f0ddc1b-a3ff-4c90-b798-e105ea5f5bdc\n  Kernel Version:             3.10.0-1127.13.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  robin://19.3.9\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nNon-terminated Pods:          (22 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-bgfl4                                          250m (0%)     0 (0%)      0 (0%)           0 (0%)         16h\n  kube-system                 coredns-75b9776576-m9vmm                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     17h\n  kube-system                 etcd-eqx01-flash03                                         100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         17h\n  kube-system                 kube-apiserver-eqx01-flash03                               250m (0%)     0 (0%)      0 (0%)           0 (0%)         16h\n  kube-system                 kube-controller-manager-eqx01-flash03                      200m (0%)     0 (0%)      0 (0%)           0 (0%)         17h\n  kube-system                 kube-multus-ds-amd64-xkrcr                                 100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      93d\n  kube-system                 kube-proxy-5g2wz                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         17h\n  kube-system                 kube-scheduler-eqx01-flash03                               100m (0%)     0 (0%)      0 (0%)           0 (0%)         16h\n  kube-system                 kube-sriov-device-plugin-amd64-b9cpt                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         93d\n  robinio                     csi-attacher-robin-75b8467f4d-2ww7n                        250m (0%)     0 (0%)      405Mi (0%)       0 (0%)         16h\n  robinio                     csi-nodeplugin-robin-wtcjl                                 250m (0%)     0 (0%)      405Mi (0%)       0 (0%)         16h\n  robinio                     csi-provisioner-robin-644d94f7d-qlgfd                      250m (0%)     0 (0%)      405Mi (0%)       0 (0%)         16h\n  robinio                     csi-resizer-robin-d859c65dd-lvsvh                          250m (0%)     0 (0%)      405Mi (0%)       0 (0%)         16h\n  robinio                     csi-snapshotter-robin-65c6fbcb5-glcck                      250m (0%)     0 (0%)      405Mi (0%)       0 (0%)         16h\n  robinio                     robin-file-server-65dc88554b-6cpbs                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         16h\n  robinio                     robin-master-ff98b76b7-4sgjh                               1 (2%)        0 (0%)      500Mi (0%)       0 (0%)         16h\n  robinio                     robin-nfs-watchdog-vxvtw                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         16h\n  robinio                     robin-patroni-0                                            100m (0%)     500m (1%)   4G (1%)          4G (1%)        16h\n  robinio                     robin-worker-r9kq2                                         1 (2%)        0 (0%)      500Mi (0%)       0 (0%)         16h\n  robinio                     robink8s-serverext-jbnfg                                   100m (0%)     100m (0%)   100Mi (0%)       100Mi (0%)     16h\n  robinio                     snapshot-controller-9dc68bc8d-xb5sk                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         16h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-cvpqz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                4550m (11%)     700m (1%)\n  memory             7331530Ki (2%)  4233930Ki (1%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:              <none>\n"
    Jan 12 00:55:43.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1662 describe namespace kubectl-1662'
    Jan 12 00:55:43.796: INFO: stderr: ""
    Jan 12 00:55:43.796: INFO: stdout: "Name:         kubectl-1662\nLabels:       e2e-framework=kubectl\n              e2e-run=57cc750e-de8d-467f-a423-c8fd052edd1e\n              kubernetes.io/metadata.name=kubectl-1662\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:55:43.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1662" for this suite. 01/12/23 00:55:43.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:55:43.849
Jan 12 00:55:43.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename sched-pred 01/12/23 00:55:43.85
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:43.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:43.885
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 12 00:55:43.887: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 12 00:55:43.894: INFO: Waiting for terminating namespaces to be deleted...
Jan 12 00:55:43.898: INFO: 
Logging pods the apiserver thinks is on node eqx03-flash06 before test
Jan 12 00:55:43.916: INFO: calico-kube-controllers-5c9fd9b888-vc4sl from kube-system started at 2023-01-11 19:29:05 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.916: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 12 00:55:43.916: INFO: calico-node-lwrhk from kube-system started at 2023-01-11 08:01:00 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.916: INFO: 	Container calico-node ready: true, restart count 0
Jan 12 00:55:43.916: INFO: kube-multus-ds-amd64-gsh8m from kube-system started at 2022-10-10 22:16:50 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.916: INFO: 	Container kube-multus ready: true, restart count 2
Jan 12 00:55:43.916: INFO: kube-proxy-h8fqt from kube-system started at 2023-01-11 07:45:11 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.916: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 00:55:43.916: INFO: kube-sriov-device-plugin-amd64-4wg6g from kube-system started at 2022-10-10 22:14:57 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.916: INFO: 	Container kube-sriovdp ready: true, restart count 2
Jan 12 00:55:43.916: INFO: mariadb-1665683911-master-0 from maria started at 2023-01-11 07:55:52 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.916: INFO: 	Container mariadb ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mariadb-1665683911-slave-0 from maria started at 2023-01-11 07:56:03 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mariadb ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1664779250-7dc5656c45-vcb4f from mg started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1664779250 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1664779251-858bf7fdc-6scml from mg started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1664779251 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1664779253-866d78dd7-n8j9t from mg started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1664779253 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1664779256-5c6fc69c89-9kvgw from mg started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1664779256 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: nginx-ingress-1664779263-default-backend-cdc6d499b-ng78h from ng started at 2023-01-11 07:55:44 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 00:55:43.917: INFO: nginx-ingress-1664779266-default-backend-bbdfc88b7-7ln9d from ng started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 00:55:43.917: INFO: nginx-ingress-1664779278-default-backend-5dfb4f9db4-bdjmg from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 00:55:43.917: INFO: nginx-ingress-1664779281-default-backend-d5fb87996-kl2gz from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 00:55:43.917: INFO: nginx-ingress-1664779288-default-backend-594965dbd7-kb8tt from ng started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 00:55:43.917: INFO: nginx-ingress-1664779291-default-backend-66f95c66fc-2vfvf from ng started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 00:55:43.917: INFO: nginx-ingress-1664779294-default-backend-6765dd7578-v796m from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 00:55:43.917: INFO: nginx-ingress-1664779297-default-backend-74df75bf95-n6xwg from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 00:55:43.917: INFO: csi-nodeplugin-robin-hnwgz from robinio started at 2023-01-11 08:44:24 +0000 UTC (3 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container driver-registrar ready: true, restart count 0
Jan 12 00:55:43.917: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 12 00:55:43.917: INFO: 	Container robin ready: true, restart count 0
Jan 12 00:55:43.917: INFO: robin-nfs-watchdog-qrzrq from robinio started at 2023-01-11 08:28:03 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
Jan 12 00:55:43.917: INFO: robin-worker-jlr7d from robinio started at 2023-01-11 08:27:55 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container robinrcm ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089095-66b956f5d-vxj2b from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089095 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089145-598df7974-kn9cv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089145 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089149-5574fb7774-t7pqg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089149 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089152-84f657bf94-dvsbc from sa-ns-user started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089152 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089179-5bf57c5944-tp4xz from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089179 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089182-6c58488f6d-5jn9v from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089182 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089185-677dc7ffb6-b9pl6 from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089185 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089188-58475957bd-5nfbd from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089188 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089191-66859c96dd-lkchj from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089191 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089205-5ff87d5b8d-wch4v from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089205 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089217-76648fcb6f-9hkfm from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089217 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089220-595bdf59bf-wzg6h from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089220 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089231-fd7d54889-stkqh from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089231 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089236-66c6896dcf-jjztv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089236 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089247-845bdb94dc-z5mxg from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089247 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089249-68588bc459-psrj7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089249 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089253-7d549cf945-mhld7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089253 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089257-6c4b6dd79c-tz5d5 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089257 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089259-58c44567c7-jmzbp from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089259 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089262-744cbfcf5c-lzzbl from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089262 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089268-5867478f97-c44xg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089268 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089337-9dbc9475f-zzk9n from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089337 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665089342-69f7c77fd7-9l5qq from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665089342 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665088984-56cbf7747c-8zfh5 from sa-ns started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665088984 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: mysql-1665088986-7785c569-hfqvj from sa-ns started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql-1665088986 ready: true, restart count 0
Jan 12 00:55:43.917: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-ppd9w from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 00:55:43.917: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 12 00:55:43.917: INFO: ravi-ravi-mysql-0 from t001-u000004 started at 2023-01-11 07:55:59 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.917: INFO: 	Container mysql ready: true, restart count 0
Jan 12 00:55:43.917: INFO: 
Logging pods the apiserver thinks is on node eqx04-flash06 before test
Jan 12 00:55:43.934: INFO: test-deployment-b4b5c-54bc444df-pbsb8 from deployment-1043 started at 2023-01-12 00:55:35 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container httpd ready: true, restart count 0
Jan 12 00:55:43.934: INFO: calico-node-wh5zd from kube-system started at 2023-01-11 08:02:20 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container calico-node ready: true, restart count 0
Jan 12 00:55:43.934: INFO: kube-multus-ds-amd64-hgzfz from kube-system started at 2023-01-11 19:29:37 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container kube-multus ready: true, restart count 0
Jan 12 00:55:43.934: INFO: kube-proxy-kvvhz from kube-system started at 2023-01-11 07:45:13 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 00:55:43.934: INFO: kube-sriov-device-plugin-amd64-rpzdk from kube-system started at 2023-01-11 19:29:31 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container kube-sriovdp ready: true, restart count 0
Jan 12 00:55:43.934: INFO: agnhost-primary-mch6t from kubectl-1662 started at 2023-01-12 00:55:40 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container agnhost-primary ready: true, restart count 0
Jan 12 00:55:43.934: INFO: nginx-ingress-1664779263-controller-bf7587b7f-s2gzv from ng started at 2023-01-12 00:37:59 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
Jan 12 00:55:43.934: INFO: nginx-ingress-1664779266-controller-677944959d-qm8mk from ng started at 2023-01-12 00:36:50 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
Jan 12 00:55:43.934: INFO: nginx-ingress-1664779278-controller-5fb4f984c-4xzpx from ng started at 2023-01-12 00:37:04 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
Jan 12 00:55:43.934: INFO: nginx-ingress-1664779281-controller-75b6585875-dtrmw from ng started at 2023-01-12 00:38:10 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
Jan 12 00:55:43.934: INFO: nginx-ingress-1664779288-controller-9744ff446-cbwhr from ng started at 2023-01-12 00:38:44 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
Jan 12 00:55:43.934: INFO: nginx-ingress-1664779291-controller-ff67b7844-8rf6z from ng started at 2023-01-12 00:38:22 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
Jan 12 00:55:43.934: INFO: nginx-ingress-1664779294-controller-84785d75f7-v4d2l from ng started at 2023-01-12 00:37:36 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
Jan 12 00:55:43.934: INFO: nginx-ingress-1664779297-controller-57f654c69d-sblqx from ng started at 2023-01-12 00:37:15 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
Jan 12 00:55:43.934: INFO: csi-nodeplugin-robin-qb8dm from robinio started at 2023-01-11 19:29:39 +0000 UTC (3 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container driver-registrar ready: true, restart count 0
Jan 12 00:55:43.934: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 12 00:55:43.934: INFO: 	Container robin ready: true, restart count 0
Jan 12 00:55:43.934: INFO: robin-nfs-watchdog-dl9f2 from robinio started at 2023-01-11 19:29:32 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
Jan 12 00:55:43.934: INFO: robin-worker-lfptr from robinio started at 2023-01-11 19:29:58 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container robinrcm ready: true, restart count 0
Jan 12 00:55:43.934: INFO: sonobuoy from sonobuoy started at 2023-01-12 00:40:33 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 12 00:55:43.934: INFO: sonobuoy-e2e-job-90575ca5f8b04bb8 from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container e2e ready: true, restart count 0
Jan 12 00:55:43.934: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 00:55:43.934: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-mkhnx from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 00:55:43.934: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 12 00:55:43.934: INFO: cent-1-server-01 from t001-u000004 started at 2023-01-11 19:31:43 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container cent-1-server-01 ready: true, restart count 0
Jan 12 00:55:43.934: INFO: cent-2-server-01 from t001-u000004 started at 2023-01-11 19:31:43 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container cent-2-server-01 ready: true, restart count 0
Jan 12 00:55:43.934: INFO: cl-1-mysql-01 from t001-u000004 started at 2023-01-11 19:31:45 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container cl-1-mysql-01 ready: true, restart count 0
Jan 12 00:55:43.934: INFO: sq-mysql-01 from t001-u000004 started at 2023-01-11 19:31:55 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container sq-mysql-01 ready: true, restart count 0
Jan 12 00:55:43.934: INFO: webhook-to-be-mutated from webhook-9849 started at 2023-01-12 00:53:50 +0000 UTC (1 container statuses recorded)
Jan 12 00:55:43.934: INFO: 	Container example ready: false, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/12/23 00:55:43.934
Jan 12 00:55:43.964: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6892" to be "running"
Jan 12 00:55:43.972: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.388867ms
Jan 12 00:55:45.975: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010966433s
Jan 12 00:55:47.975: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.010729006s
Jan 12 00:55:47.975: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/12/23 00:55:47.977
STEP: Trying to apply a random label on the found node. 01/12/23 00:55:48.007
STEP: verifying the node has the label kubernetes.io/e2e-095dc692-5aa5-4040-b51c-9d058efdaf81 42 01/12/23 00:55:48.029
STEP: Trying to relaunch the pod, now with labels. 01/12/23 00:55:48.032
Jan 12 00:55:48.102: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6892" to be "not pending"
Jan 12 00:55:48.104: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.296786ms
Jan 12 00:55:50.108: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006125703s
Jan 12 00:55:52.108: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.006314316s
Jan 12 00:55:52.108: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-095dc692-5aa5-4040-b51c-9d058efdaf81 off the node eqx04-flash06 01/12/23 00:55:52.111
STEP: verifying the node doesn't have the label kubernetes.io/e2e-095dc692-5aa5-4040-b51c-9d058efdaf81 01/12/23 00:55:52.127
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:55:52.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6892" for this suite. 01/12/23 00:55:52.133
------------------------------
• [SLOW TEST] [8.317 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:55:43.849
    Jan 12 00:55:43.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename sched-pred 01/12/23 00:55:43.85
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:43.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:43.885
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 12 00:55:43.887: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 12 00:55:43.894: INFO: Waiting for terminating namespaces to be deleted...
    Jan 12 00:55:43.898: INFO: 
    Logging pods the apiserver thinks is on node eqx03-flash06 before test
    Jan 12 00:55:43.916: INFO: calico-kube-controllers-5c9fd9b888-vc4sl from kube-system started at 2023-01-11 19:29:05 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.916: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 12 00:55:43.916: INFO: calico-node-lwrhk from kube-system started at 2023-01-11 08:01:00 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.916: INFO: 	Container calico-node ready: true, restart count 0
    Jan 12 00:55:43.916: INFO: kube-multus-ds-amd64-gsh8m from kube-system started at 2022-10-10 22:16:50 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.916: INFO: 	Container kube-multus ready: true, restart count 2
    Jan 12 00:55:43.916: INFO: kube-proxy-h8fqt from kube-system started at 2023-01-11 07:45:11 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.916: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 00:55:43.916: INFO: kube-sriov-device-plugin-amd64-4wg6g from kube-system started at 2022-10-10 22:14:57 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.916: INFO: 	Container kube-sriovdp ready: true, restart count 2
    Jan 12 00:55:43.916: INFO: mariadb-1665683911-master-0 from maria started at 2023-01-11 07:55:52 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.916: INFO: 	Container mariadb ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mariadb-1665683911-slave-0 from maria started at 2023-01-11 07:56:03 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mariadb ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1664779250-7dc5656c45-vcb4f from mg started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1664779250 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1664779251-858bf7fdc-6scml from mg started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1664779251 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1664779253-866d78dd7-n8j9t from mg started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1664779253 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1664779256-5c6fc69c89-9kvgw from mg started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1664779256 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: nginx-ingress-1664779263-default-backend-cdc6d499b-ng78h from ng started at 2023-01-11 07:55:44 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: nginx-ingress-1664779266-default-backend-bbdfc88b7-7ln9d from ng started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: nginx-ingress-1664779278-default-backend-5dfb4f9db4-bdjmg from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: nginx-ingress-1664779281-default-backend-d5fb87996-kl2gz from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: nginx-ingress-1664779288-default-backend-594965dbd7-kb8tt from ng started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: nginx-ingress-1664779291-default-backend-66f95c66fc-2vfvf from ng started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: nginx-ingress-1664779294-default-backend-6765dd7578-v796m from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: nginx-ingress-1664779297-default-backend-74df75bf95-n6xwg from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: csi-nodeplugin-robin-hnwgz from robinio started at 2023-01-11 08:44:24 +0000 UTC (3 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: 	Container robin ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: robin-nfs-watchdog-qrzrq from robinio started at 2023-01-11 08:28:03 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: robin-worker-jlr7d from robinio started at 2023-01-11 08:27:55 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container robinrcm ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089095-66b956f5d-vxj2b from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089095 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089145-598df7974-kn9cv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089145 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089149-5574fb7774-t7pqg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089149 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089152-84f657bf94-dvsbc from sa-ns-user started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089152 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089179-5bf57c5944-tp4xz from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089179 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089182-6c58488f6d-5jn9v from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089182 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089185-677dc7ffb6-b9pl6 from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089185 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089188-58475957bd-5nfbd from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089188 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089191-66859c96dd-lkchj from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089191 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089205-5ff87d5b8d-wch4v from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089205 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089217-76648fcb6f-9hkfm from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089217 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089220-595bdf59bf-wzg6h from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089220 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089231-fd7d54889-stkqh from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089231 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089236-66c6896dcf-jjztv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089236 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089247-845bdb94dc-z5mxg from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089247 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089249-68588bc459-psrj7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089249 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089253-7d549cf945-mhld7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089253 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089257-6c4b6dd79c-tz5d5 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089257 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089259-58c44567c7-jmzbp from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089259 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089262-744cbfcf5c-lzzbl from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089262 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089268-5867478f97-c44xg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089268 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089337-9dbc9475f-zzk9n from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089337 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665089342-69f7c77fd7-9l5qq from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665089342 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665088984-56cbf7747c-8zfh5 from sa-ns started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665088984 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: mysql-1665088986-7785c569-hfqvj from sa-ns started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql-1665088986 ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-ppd9w from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: ravi-ravi-mysql-0 from t001-u000004 started at 2023-01-11 07:55:59 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.917: INFO: 	Container mysql ready: true, restart count 0
    Jan 12 00:55:43.917: INFO: 
    Logging pods the apiserver thinks is on node eqx04-flash06 before test
    Jan 12 00:55:43.934: INFO: test-deployment-b4b5c-54bc444df-pbsb8 from deployment-1043 started at 2023-01-12 00:55:35 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container httpd ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: calico-node-wh5zd from kube-system started at 2023-01-11 08:02:20 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container calico-node ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: kube-multus-ds-amd64-hgzfz from kube-system started at 2023-01-11 19:29:37 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: kube-proxy-kvvhz from kube-system started at 2023-01-11 07:45:13 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: kube-sriov-device-plugin-amd64-rpzdk from kube-system started at 2023-01-11 19:29:31 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container kube-sriovdp ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: agnhost-primary-mch6t from kubectl-1662 started at 2023-01-12 00:55:40 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container agnhost-primary ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: nginx-ingress-1664779263-controller-bf7587b7f-s2gzv from ng started at 2023-01-12 00:37:59 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
    Jan 12 00:55:43.934: INFO: nginx-ingress-1664779266-controller-677944959d-qm8mk from ng started at 2023-01-12 00:36:50 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
    Jan 12 00:55:43.934: INFO: nginx-ingress-1664779278-controller-5fb4f984c-4xzpx from ng started at 2023-01-12 00:37:04 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
    Jan 12 00:55:43.934: INFO: nginx-ingress-1664779281-controller-75b6585875-dtrmw from ng started at 2023-01-12 00:38:10 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
    Jan 12 00:55:43.934: INFO: nginx-ingress-1664779288-controller-9744ff446-cbwhr from ng started at 2023-01-12 00:38:44 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
    Jan 12 00:55:43.934: INFO: nginx-ingress-1664779291-controller-ff67b7844-8rf6z from ng started at 2023-01-12 00:38:22 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
    Jan 12 00:55:43.934: INFO: nginx-ingress-1664779294-controller-84785d75f7-v4d2l from ng started at 2023-01-12 00:37:36 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
    Jan 12 00:55:43.934: INFO: nginx-ingress-1664779297-controller-57f654c69d-sblqx from ng started at 2023-01-12 00:37:15 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container nginx-ingress-controller ready: false, restart count 9
    Jan 12 00:55:43.934: INFO: csi-nodeplugin-robin-qb8dm from robinio started at 2023-01-11 19:29:39 +0000 UTC (3 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: 	Container robin ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: robin-nfs-watchdog-dl9f2 from robinio started at 2023-01-11 19:29:32 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: robin-worker-lfptr from robinio started at 2023-01-11 19:29:58 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container robinrcm ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: sonobuoy from sonobuoy started at 2023-01-12 00:40:33 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: sonobuoy-e2e-job-90575ca5f8b04bb8 from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container e2e ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-mkhnx from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: cent-1-server-01 from t001-u000004 started at 2023-01-11 19:31:43 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container cent-1-server-01 ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: cent-2-server-01 from t001-u000004 started at 2023-01-11 19:31:43 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container cent-2-server-01 ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: cl-1-mysql-01 from t001-u000004 started at 2023-01-11 19:31:45 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container cl-1-mysql-01 ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: sq-mysql-01 from t001-u000004 started at 2023-01-11 19:31:55 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container sq-mysql-01 ready: true, restart count 0
    Jan 12 00:55:43.934: INFO: webhook-to-be-mutated from webhook-9849 started at 2023-01-12 00:53:50 +0000 UTC (1 container statuses recorded)
    Jan 12 00:55:43.934: INFO: 	Container example ready: false, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/12/23 00:55:43.934
    Jan 12 00:55:43.964: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6892" to be "running"
    Jan 12 00:55:43.972: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.388867ms
    Jan 12 00:55:45.975: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010966433s
    Jan 12 00:55:47.975: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.010729006s
    Jan 12 00:55:47.975: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/12/23 00:55:47.977
    STEP: Trying to apply a random label on the found node. 01/12/23 00:55:48.007
    STEP: verifying the node has the label kubernetes.io/e2e-095dc692-5aa5-4040-b51c-9d058efdaf81 42 01/12/23 00:55:48.029
    STEP: Trying to relaunch the pod, now with labels. 01/12/23 00:55:48.032
    Jan 12 00:55:48.102: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6892" to be "not pending"
    Jan 12 00:55:48.104: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.296786ms
    Jan 12 00:55:50.108: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006125703s
    Jan 12 00:55:52.108: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.006314316s
    Jan 12 00:55:52.108: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-095dc692-5aa5-4040-b51c-9d058efdaf81 off the node eqx04-flash06 01/12/23 00:55:52.111
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-095dc692-5aa5-4040-b51c-9d058efdaf81 01/12/23 00:55:52.127
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:55:52.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6892" for this suite. 01/12/23 00:55:52.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:55:52.17
Jan 12 00:55:52.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename crd-watch 01/12/23 00:55:52.171
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:52.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:52.196
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 12 00:55:52.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Creating first CR  01/12/23 00:55:54.285
Jan 12 00:55:54.292: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T00:55:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T00:55:54Z]] name:name1 resourceVersion:20145398 uid:96ebabdf-6fb5-493f-9c94-00b4c4fe9318] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/12/23 00:56:04.293
Jan 12 00:56:04.302: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T00:56:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T00:56:04Z]] name:name2 resourceVersion:20145459 uid:2d163db8-2973-4603-843b-4416ae5966b2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/12/23 00:56:14.302
Jan 12 00:56:14.313: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T00:55:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T00:56:14Z]] name:name1 resourceVersion:20145488 uid:96ebabdf-6fb5-493f-9c94-00b4c4fe9318] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/12/23 00:56:24.314
Jan 12 00:56:24.325: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T00:56:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T00:56:24Z]] name:name2 resourceVersion:20145518 uid:2d163db8-2973-4603-843b-4416ae5966b2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/12/23 00:56:34.326
Jan 12 00:56:34.340: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T00:55:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T00:56:14Z]] name:name1 resourceVersion:20145547 uid:96ebabdf-6fb5-493f-9c94-00b4c4fe9318] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/12/23 00:56:44.34
Jan 12 00:56:44.349: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T00:56:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T00:56:24Z]] name:name2 resourceVersion:20145574 uid:2d163db8-2973-4603-843b-4416ae5966b2] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:56:54.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-6655" for this suite. 01/12/23 00:56:54.868
------------------------------
• [SLOW TEST] [62.716 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:55:52.17
    Jan 12 00:55:52.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename crd-watch 01/12/23 00:55:52.171
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:55:52.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:55:52.196
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 12 00:55:52.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Creating first CR  01/12/23 00:55:54.285
    Jan 12 00:55:54.292: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T00:55:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T00:55:54Z]] name:name1 resourceVersion:20145398 uid:96ebabdf-6fb5-493f-9c94-00b4c4fe9318] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/12/23 00:56:04.293
    Jan 12 00:56:04.302: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T00:56:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T00:56:04Z]] name:name2 resourceVersion:20145459 uid:2d163db8-2973-4603-843b-4416ae5966b2] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/12/23 00:56:14.302
    Jan 12 00:56:14.313: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T00:55:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T00:56:14Z]] name:name1 resourceVersion:20145488 uid:96ebabdf-6fb5-493f-9c94-00b4c4fe9318] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/12/23 00:56:24.314
    Jan 12 00:56:24.325: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T00:56:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T00:56:24Z]] name:name2 resourceVersion:20145518 uid:2d163db8-2973-4603-843b-4416ae5966b2] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/12/23 00:56:34.326
    Jan 12 00:56:34.340: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T00:55:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T00:56:14Z]] name:name1 resourceVersion:20145547 uid:96ebabdf-6fb5-493f-9c94-00b4c4fe9318] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/12/23 00:56:44.34
    Jan 12 00:56:44.349: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T00:56:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T00:56:24Z]] name:name2 resourceVersion:20145574 uid:2d163db8-2973-4603-843b-4416ae5966b2] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:56:54.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-6655" for this suite. 01/12/23 00:56:54.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:56:54.886
Jan 12 00:56:54.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename statefulset 01/12/23 00:56:54.887
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:56:54.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:56:54.912
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2950 01/12/23 00:56:54.914
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-2950 01/12/23 00:56:54.936
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2950 01/12/23 00:56:54.992
Jan 12 00:56:54.995: INFO: Found 0 stateful pods, waiting for 1
Jan 12 00:57:04.999: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/12/23 00:57:04.999
Jan 12 00:57:05.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-2950 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 00:57:05.267: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 00:57:05.267: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 00:57:05.267: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 00:57:05.269: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 12 00:57:15.274: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 00:57:15.274: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 00:57:15.289: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 12 00:57:15.289: INFO: ss-0  eqx04-flash06  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:56:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:56:55 +0000 UTC  }]
Jan 12 00:57:15.289: INFO: 
Jan 12 00:57:15.289: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 12 00:57:16.293: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997261743s
Jan 12 00:57:17.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992596237s
Jan 12 00:57:18.300: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989238684s
Jan 12 00:57:19.304: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985827128s
Jan 12 00:57:20.308: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981752169s
Jan 12 00:57:21.312: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977966632s
Jan 12 00:57:22.317: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973209157s
Jan 12 00:57:23.321: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968561552s
Jan 12 00:57:24.325: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.509403ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2950 01/12/23 00:57:25.325
Jan 12 00:57:25.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-2950 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 00:57:25.540: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 12 00:57:25.540: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 00:57:25.540: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 00:57:25.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-2950 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 00:57:25.828: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 12 00:57:25.829: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 00:57:25.829: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 00:57:25.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-2950 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 00:57:26.042: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 12 00:57:26.042: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 00:57:26.042: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 00:57:26.045: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 12 00:57:36.049: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 00:57:36.049: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 00:57:36.049: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/12/23 00:57:36.049
Jan 12 00:57:36.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-2950 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 00:57:36.259: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 00:57:36.259: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 00:57:36.259: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 00:57:36.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-2950 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 00:57:36.556: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 00:57:36.556: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 00:57:36.556: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 00:57:36.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-2950 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 00:57:36.779: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 00:57:36.779: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 00:57:36.779: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 00:57:36.779: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 00:57:36.798: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 12 00:57:46.805: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 00:57:46.805: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 00:57:46.805: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 00:57:46.817: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 12 00:57:46.817: INFO: ss-0  eqx04-flash06  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:56:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:56:55 +0000 UTC  }]
Jan 12 00:57:46.817: INFO: ss-1  eqx03-flash06  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  }]
Jan 12 00:57:46.817: INFO: ss-2  eqx04-flash06  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  }]
Jan 12 00:57:46.817: INFO: 
Jan 12 00:57:46.817: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 12 00:57:47.821: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 12 00:57:47.821: INFO: ss-0  eqx04-flash06  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:56:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:56:55 +0000 UTC  }]
Jan 12 00:57:47.821: INFO: ss-1  eqx03-flash06  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  }]
Jan 12 00:57:47.821: INFO: ss-2  eqx04-flash06  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  }]
Jan 12 00:57:47.821: INFO: 
Jan 12 00:57:47.821: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 12 00:57:48.824: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992846397s
Jan 12 00:57:49.827: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989926609s
Jan 12 00:57:50.830: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.986670715s
Jan 12 00:57:51.833: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.983965361s
Jan 12 00:57:52.837: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.980947535s
Jan 12 00:57:53.840: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.977180877s
Jan 12 00:57:54.844: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.973807304s
Jan 12 00:57:55.847: INFO: Verifying statefulset ss doesn't scale past 0 for another 970.306773ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2950 01/12/23 00:57:56.847
Jan 12 00:57:56.851: INFO: Scaling statefulset ss to 0
Jan 12 00:57:56.858: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 00:57:56.861: INFO: Deleting all statefulset in ns statefulset-2950
Jan 12 00:57:56.863: INFO: Scaling statefulset ss to 0
Jan 12 00:57:56.870: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 00:57:56.872: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 00:57:56.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2950" for this suite. 01/12/23 00:57:56.889
------------------------------
• [SLOW TEST] [62.049 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:56:54.886
    Jan 12 00:56:54.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename statefulset 01/12/23 00:56:54.887
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:56:54.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:56:54.912
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2950 01/12/23 00:56:54.914
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-2950 01/12/23 00:56:54.936
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2950 01/12/23 00:56:54.992
    Jan 12 00:56:54.995: INFO: Found 0 stateful pods, waiting for 1
    Jan 12 00:57:04.999: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/12/23 00:57:04.999
    Jan 12 00:57:05.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-2950 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 00:57:05.267: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 00:57:05.267: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 00:57:05.267: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 00:57:05.269: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 12 00:57:15.274: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 00:57:15.274: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 00:57:15.289: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Jan 12 00:57:15.289: INFO: ss-0  eqx04-flash06  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:56:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:56:55 +0000 UTC  }]
    Jan 12 00:57:15.289: INFO: 
    Jan 12 00:57:15.289: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 12 00:57:16.293: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997261743s
    Jan 12 00:57:17.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992596237s
    Jan 12 00:57:18.300: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989238684s
    Jan 12 00:57:19.304: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985827128s
    Jan 12 00:57:20.308: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981752169s
    Jan 12 00:57:21.312: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977966632s
    Jan 12 00:57:22.317: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973209157s
    Jan 12 00:57:23.321: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968561552s
    Jan 12 00:57:24.325: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.509403ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2950 01/12/23 00:57:25.325
    Jan 12 00:57:25.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-2950 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 00:57:25.540: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 12 00:57:25.540: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 00:57:25.540: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 00:57:25.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-2950 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 00:57:25.828: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 12 00:57:25.829: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 00:57:25.829: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 00:57:25.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-2950 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 00:57:26.042: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 12 00:57:26.042: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 00:57:26.042: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 00:57:26.045: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan 12 00:57:36.049: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 00:57:36.049: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 00:57:36.049: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/12/23 00:57:36.049
    Jan 12 00:57:36.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-2950 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 00:57:36.259: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 00:57:36.259: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 00:57:36.259: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 00:57:36.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-2950 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 00:57:36.556: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 00:57:36.556: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 00:57:36.556: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 00:57:36.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-2950 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 00:57:36.779: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 00:57:36.779: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 00:57:36.779: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 00:57:36.779: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 00:57:36.798: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan 12 00:57:46.805: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 00:57:46.805: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 00:57:46.805: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 00:57:46.817: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Jan 12 00:57:46.817: INFO: ss-0  eqx04-flash06  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:56:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:56:55 +0000 UTC  }]
    Jan 12 00:57:46.817: INFO: ss-1  eqx03-flash06  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  }]
    Jan 12 00:57:46.817: INFO: ss-2  eqx04-flash06  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  }]
    Jan 12 00:57:46.817: INFO: 
    Jan 12 00:57:46.817: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 12 00:57:47.821: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Jan 12 00:57:47.821: INFO: ss-0  eqx04-flash06  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:56:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:56:55 +0000 UTC  }]
    Jan 12 00:57:47.821: INFO: ss-1  eqx03-flash06  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  }]
    Jan 12 00:57:47.821: INFO: ss-2  eqx04-flash06  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 00:57:15 +0000 UTC  }]
    Jan 12 00:57:47.821: INFO: 
    Jan 12 00:57:47.821: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 12 00:57:48.824: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992846397s
    Jan 12 00:57:49.827: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989926609s
    Jan 12 00:57:50.830: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.986670715s
    Jan 12 00:57:51.833: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.983965361s
    Jan 12 00:57:52.837: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.980947535s
    Jan 12 00:57:53.840: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.977180877s
    Jan 12 00:57:54.844: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.973807304s
    Jan 12 00:57:55.847: INFO: Verifying statefulset ss doesn't scale past 0 for another 970.306773ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2950 01/12/23 00:57:56.847
    Jan 12 00:57:56.851: INFO: Scaling statefulset ss to 0
    Jan 12 00:57:56.858: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 00:57:56.861: INFO: Deleting all statefulset in ns statefulset-2950
    Jan 12 00:57:56.863: INFO: Scaling statefulset ss to 0
    Jan 12 00:57:56.870: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 00:57:56.872: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:57:56.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2950" for this suite. 01/12/23 00:57:56.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:57:56.937
Jan 12 00:57:56.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 00:57:56.938
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:57:56.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:57:56.96
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 01/12/23 00:57:56.962
Jan 12 00:57:57.059: INFO: Waiting up to 5m0s for pod "downward-api-67119d52-17e0-4881-a140-ca5057145bd3" in namespace "downward-api-8751" to be "Succeeded or Failed"
Jan 12 00:57:57.062: INFO: Pod "downward-api-67119d52-17e0-4881-a140-ca5057145bd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.289797ms
Jan 12 00:57:59.066: INFO: Pod "downward-api-67119d52-17e0-4881-a140-ca5057145bd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00683099s
Jan 12 00:58:01.066: INFO: Pod "downward-api-67119d52-17e0-4881-a140-ca5057145bd3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006260404s
Jan 12 00:58:03.066: INFO: Pod "downward-api-67119d52-17e0-4881-a140-ca5057145bd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006381719s
STEP: Saw pod success 01/12/23 00:58:03.066
Jan 12 00:58:03.066: INFO: Pod "downward-api-67119d52-17e0-4881-a140-ca5057145bd3" satisfied condition "Succeeded or Failed"
Jan 12 00:58:03.069: INFO: Trying to get logs from node eqx04-flash06 pod downward-api-67119d52-17e0-4881-a140-ca5057145bd3 container dapi-container: <nil>
STEP: delete the pod 01/12/23 00:58:03.085
Jan 12 00:58:03.105: INFO: Waiting for pod downward-api-67119d52-17e0-4881-a140-ca5057145bd3 to disappear
Jan 12 00:58:03.107: INFO: Pod downward-api-67119d52-17e0-4881-a140-ca5057145bd3 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 12 00:58:03.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8751" for this suite. 01/12/23 00:58:03.111
------------------------------
• [SLOW TEST] [6.200 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:57:56.937
    Jan 12 00:57:56.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 00:57:56.938
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:57:56.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:57:56.96
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 01/12/23 00:57:56.962
    Jan 12 00:57:57.059: INFO: Waiting up to 5m0s for pod "downward-api-67119d52-17e0-4881-a140-ca5057145bd3" in namespace "downward-api-8751" to be "Succeeded or Failed"
    Jan 12 00:57:57.062: INFO: Pod "downward-api-67119d52-17e0-4881-a140-ca5057145bd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.289797ms
    Jan 12 00:57:59.066: INFO: Pod "downward-api-67119d52-17e0-4881-a140-ca5057145bd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00683099s
    Jan 12 00:58:01.066: INFO: Pod "downward-api-67119d52-17e0-4881-a140-ca5057145bd3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006260404s
    Jan 12 00:58:03.066: INFO: Pod "downward-api-67119d52-17e0-4881-a140-ca5057145bd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006381719s
    STEP: Saw pod success 01/12/23 00:58:03.066
    Jan 12 00:58:03.066: INFO: Pod "downward-api-67119d52-17e0-4881-a140-ca5057145bd3" satisfied condition "Succeeded or Failed"
    Jan 12 00:58:03.069: INFO: Trying to get logs from node eqx04-flash06 pod downward-api-67119d52-17e0-4881-a140-ca5057145bd3 container dapi-container: <nil>
    STEP: delete the pod 01/12/23 00:58:03.085
    Jan 12 00:58:03.105: INFO: Waiting for pod downward-api-67119d52-17e0-4881-a140-ca5057145bd3 to disappear
    Jan 12 00:58:03.107: INFO: Pod downward-api-67119d52-17e0-4881-a140-ca5057145bd3 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:58:03.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8751" for this suite. 01/12/23 00:58:03.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:58:03.138
Jan 12 00:58:03.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename daemonsets 01/12/23 00:58:03.139
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:58:03.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:58:03.17
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 01/12/23 00:58:03.195
STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 00:58:03.305
Jan 12 00:58:03.309: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:03.309: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:03.309: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:03.319: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 00:58:03.319: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 00:58:04.323: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:04.323: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:04.323: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:04.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 00:58:04.327: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 00:58:05.324: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:05.324: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:05.324: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:05.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 00:58:05.327: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 00:58:06.324: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:06.324: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:06.324: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:06.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 00:58:06.327: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/12/23 00:58:06.329
Jan 12 00:58:06.373: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:06.373: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:06.373: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:06.376: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 00:58:06.376: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 00:58:07.381: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:07.381: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:07.381: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:07.384: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 00:58:07.384: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 00:58:08.381: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:08.381: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:08.381: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:08.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 00:58:08.383: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 00:58:09.380: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:09.380: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:09.380: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 00:58:09.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 00:58:09.383: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/12/23 00:58:09.383
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/12/23 00:58:09.388
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4261, will wait for the garbage collector to delete the pods 01/12/23 00:58:09.388
Jan 12 00:58:09.450: INFO: Deleting DaemonSet.extensions daemon-set took: 9.15434ms
Jan 12 00:58:09.551: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.73197ms
Jan 12 00:58:11.854: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 00:58:11.854: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 12 00:58:11.857: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20146216"},"items":null}

Jan 12 00:58:11.859: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20146216"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 00:58:11.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4261" for this suite. 01/12/23 00:58:11.876
------------------------------
• [SLOW TEST] [8.762 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:58:03.138
    Jan 12 00:58:03.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename daemonsets 01/12/23 00:58:03.139
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:58:03.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:58:03.17
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 01/12/23 00:58:03.195
    STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 00:58:03.305
    Jan 12 00:58:03.309: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:03.309: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:03.309: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:03.319: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 00:58:03.319: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 00:58:04.323: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:04.323: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:04.323: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:04.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 00:58:04.327: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 00:58:05.324: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:05.324: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:05.324: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:05.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 00:58:05.327: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 00:58:06.324: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:06.324: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:06.324: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:06.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 00:58:06.327: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/12/23 00:58:06.329
    Jan 12 00:58:06.373: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:06.373: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:06.373: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:06.376: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 00:58:06.376: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 00:58:07.381: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:07.381: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:07.381: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:07.384: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 00:58:07.384: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 00:58:08.381: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:08.381: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:08.381: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:08.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 00:58:08.383: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 00:58:09.380: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:09.380: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:09.380: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 00:58:09.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 00:58:09.383: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/12/23 00:58:09.383
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/12/23 00:58:09.388
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4261, will wait for the garbage collector to delete the pods 01/12/23 00:58:09.388
    Jan 12 00:58:09.450: INFO: Deleting DaemonSet.extensions daemon-set took: 9.15434ms
    Jan 12 00:58:09.551: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.73197ms
    Jan 12 00:58:11.854: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 00:58:11.854: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 12 00:58:11.857: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20146216"},"items":null}

    Jan 12 00:58:11.859: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20146216"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:58:11.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4261" for this suite. 01/12/23 00:58:11.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:58:11.901
Jan 12 00:58:11.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 00:58:11.902
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:58:11.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:58:11.925
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 01/12/23 00:58:11.927
Jan 12 00:58:11.961: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221" in namespace "downward-api-6947" to be "Succeeded or Failed"
Jan 12 00:58:11.963: INFO: Pod "downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221": Phase="Pending", Reason="", readiness=false. Elapsed: 2.269177ms
Jan 12 00:58:13.968: INFO: Pod "downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007820364s
Jan 12 00:58:15.976: INFO: Pod "downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015331527s
STEP: Saw pod success 01/12/23 00:58:15.976
Jan 12 00:58:15.976: INFO: Pod "downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221" satisfied condition "Succeeded or Failed"
Jan 12 00:58:15.980: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221 container client-container: <nil>
STEP: delete the pod 01/12/23 00:58:15.99
Jan 12 00:58:16.011: INFO: Waiting for pod downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221 to disappear
Jan 12 00:58:16.013: INFO: Pod downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 00:58:16.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6947" for this suite. 01/12/23 00:58:16.02
------------------------------
• [4.137 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:58:11.901
    Jan 12 00:58:11.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 00:58:11.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:58:11.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:58:11.925
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 01/12/23 00:58:11.927
    Jan 12 00:58:11.961: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221" in namespace "downward-api-6947" to be "Succeeded or Failed"
    Jan 12 00:58:11.963: INFO: Pod "downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221": Phase="Pending", Reason="", readiness=false. Elapsed: 2.269177ms
    Jan 12 00:58:13.968: INFO: Pod "downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007820364s
    Jan 12 00:58:15.976: INFO: Pod "downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015331527s
    STEP: Saw pod success 01/12/23 00:58:15.976
    Jan 12 00:58:15.976: INFO: Pod "downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221" satisfied condition "Succeeded or Failed"
    Jan 12 00:58:15.980: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221 container client-container: <nil>
    STEP: delete the pod 01/12/23 00:58:15.99
    Jan 12 00:58:16.011: INFO: Waiting for pod downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221 to disappear
    Jan 12 00:58:16.013: INFO: Pod downwardapi-volume-e9539a8e-91e0-4eef-bdf4-019607405221 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:58:16.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6947" for this suite. 01/12/23 00:58:16.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:58:16.038
Jan 12 00:58:16.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename runtimeclass 01/12/23 00:58:16.038
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:58:16.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:58:16.077
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 12 00:58:16.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1614" for this suite. 01/12/23 00:58:16.089
------------------------------
• [0.074 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:58:16.038
    Jan 12 00:58:16.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename runtimeclass 01/12/23 00:58:16.038
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:58:16.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:58:16.077
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:58:16.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1614" for this suite. 01/12/23 00:58:16.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:58:16.113
Jan 12 00:58:16.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename statefulset 01/12/23 00:58:16.114
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:58:16.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:58:16.136
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-223 01/12/23 00:58:16.138
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-223 01/12/23 00:58:16.145
Jan 12 00:58:16.182: INFO: Found 0 stateful pods, waiting for 1
Jan 12 00:58:26.186: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/12/23 00:58:26.191
STEP: updating a scale subresource 01/12/23 00:58:26.193
STEP: verifying the statefulset Spec.Replicas was modified 01/12/23 00:58:26.201
STEP: Patch a scale subresource 01/12/23 00:58:26.204
STEP: verifying the statefulset Spec.Replicas was modified 01/12/23 00:58:26.23
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 00:58:26.235: INFO: Deleting all statefulset in ns statefulset-223
Jan 12 00:58:26.240: INFO: Scaling statefulset ss to 0
Jan 12 00:58:36.261: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 00:58:36.263: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 00:58:36.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-223" for this suite. 01/12/23 00:58:36.283
------------------------------
• [SLOW TEST] [20.209 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:58:16.113
    Jan 12 00:58:16.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename statefulset 01/12/23 00:58:16.114
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:58:16.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:58:16.136
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-223 01/12/23 00:58:16.138
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-223 01/12/23 00:58:16.145
    Jan 12 00:58:16.182: INFO: Found 0 stateful pods, waiting for 1
    Jan 12 00:58:26.186: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/12/23 00:58:26.191
    STEP: updating a scale subresource 01/12/23 00:58:26.193
    STEP: verifying the statefulset Spec.Replicas was modified 01/12/23 00:58:26.201
    STEP: Patch a scale subresource 01/12/23 00:58:26.204
    STEP: verifying the statefulset Spec.Replicas was modified 01/12/23 00:58:26.23
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 00:58:26.235: INFO: Deleting all statefulset in ns statefulset-223
    Jan 12 00:58:26.240: INFO: Scaling statefulset ss to 0
    Jan 12 00:58:36.261: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 00:58:36.263: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:58:36.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-223" for this suite. 01/12/23 00:58:36.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:58:36.323
Jan 12 00:58:36.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-runtime 01/12/23 00:58:36.324
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:58:36.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:58:36.349
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 01/12/23 00:58:36.351
STEP: wait for the container to reach Succeeded 01/12/23 00:58:36.485
STEP: get the container status 01/12/23 00:58:41.505
STEP: the container should be terminated 01/12/23 00:58:41.508
STEP: the termination message should be set 01/12/23 00:58:41.508
Jan 12 00:58:41.508: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/12/23 00:58:41.508
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 12 00:58:41.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8271" for this suite. 01/12/23 00:58:41.543
------------------------------
• [SLOW TEST] [5.241 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:58:36.323
    Jan 12 00:58:36.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-runtime 01/12/23 00:58:36.324
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:58:36.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:58:36.349
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 01/12/23 00:58:36.351
    STEP: wait for the container to reach Succeeded 01/12/23 00:58:36.485
    STEP: get the container status 01/12/23 00:58:41.505
    STEP: the container should be terminated 01/12/23 00:58:41.508
    STEP: the termination message should be set 01/12/23 00:58:41.508
    Jan 12 00:58:41.508: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/12/23 00:58:41.508
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:58:41.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8271" for this suite. 01/12/23 00:58:41.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:58:41.566
Jan 12 00:58:41.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename resourcequota 01/12/23 00:58:41.567
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:58:41.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:58:41.586
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 01/12/23 00:58:58.613
STEP: Creating a ResourceQuota 01/12/23 00:59:03.617
STEP: Ensuring resource quota status is calculated 01/12/23 00:59:03.628
STEP: Creating a ConfigMap 01/12/23 00:59:05.632
STEP: Ensuring resource quota status captures configMap creation 01/12/23 00:59:05.65
STEP: Deleting a ConfigMap 01/12/23 00:59:07.656
STEP: Ensuring resource quota status released usage 01/12/23 00:59:07.664
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 00:59:09.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-651" for this suite. 01/12/23 00:59:09.671
------------------------------
• [SLOW TEST] [28.185 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:58:41.566
    Jan 12 00:58:41.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename resourcequota 01/12/23 00:58:41.567
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:58:41.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:58:41.586
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 01/12/23 00:58:58.613
    STEP: Creating a ResourceQuota 01/12/23 00:59:03.617
    STEP: Ensuring resource quota status is calculated 01/12/23 00:59:03.628
    STEP: Creating a ConfigMap 01/12/23 00:59:05.632
    STEP: Ensuring resource quota status captures configMap creation 01/12/23 00:59:05.65
    STEP: Deleting a ConfigMap 01/12/23 00:59:07.656
    STEP: Ensuring resource quota status released usage 01/12/23 00:59:07.664
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 00:59:09.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-651" for this suite. 01/12/23 00:59:09.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 00:59:09.754
Jan 12 00:59:09.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-probe 01/12/23 00:59:09.755
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:59:09.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:59:09.783
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598 in namespace container-probe-2232 01/12/23 00:59:09.785
Jan 12 00:59:09.843: INFO: Waiting up to 5m0s for pod "busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598" in namespace "container-probe-2232" to be "not pending"
Jan 12 00:59:09.845: INFO: Pod "busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598": Phase="Pending", Reason="", readiness=false. Elapsed: 2.22918ms
Jan 12 00:59:11.848: INFO: Pod "busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005295147s
Jan 12 00:59:13.850: INFO: Pod "busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598": Phase="Running", Reason="", readiness=true. Elapsed: 4.007147201s
Jan 12 00:59:13.850: INFO: Pod "busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598" satisfied condition "not pending"
Jan 12 00:59:13.850: INFO: Started pod busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598 in namespace container-probe-2232
STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 00:59:13.85
Jan 12 00:59:13.852: INFO: Initial restart count of pod busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598 is 0
STEP: deleting the pod 01/12/23 01:03:14.35
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 01:03:14.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2232" for this suite. 01/12/23 01:03:14.385
------------------------------
• [SLOW TEST] [244.712 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 00:59:09.754
    Jan 12 00:59:09.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-probe 01/12/23 00:59:09.755
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 00:59:09.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 00:59:09.783
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598 in namespace container-probe-2232 01/12/23 00:59:09.785
    Jan 12 00:59:09.843: INFO: Waiting up to 5m0s for pod "busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598" in namespace "container-probe-2232" to be "not pending"
    Jan 12 00:59:09.845: INFO: Pod "busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598": Phase="Pending", Reason="", readiness=false. Elapsed: 2.22918ms
    Jan 12 00:59:11.848: INFO: Pod "busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005295147s
    Jan 12 00:59:13.850: INFO: Pod "busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598": Phase="Running", Reason="", readiness=true. Elapsed: 4.007147201s
    Jan 12 00:59:13.850: INFO: Pod "busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598" satisfied condition "not pending"
    Jan 12 00:59:13.850: INFO: Started pod busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598 in namespace container-probe-2232
    STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 00:59:13.85
    Jan 12 00:59:13.852: INFO: Initial restart count of pod busybox-197160b5-7cc8-48f0-8aa9-53218f6d8598 is 0
    STEP: deleting the pod 01/12/23 01:03:14.35
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:03:14.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2232" for this suite. 01/12/23 01:03:14.385
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:03:14.466
Jan 12 01:03:14.466: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 01:03:14.467
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:03:14.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:03:14.497
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 01/12/23 01:03:14.499
Jan 12 01:03:14.499: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4805 proxy --unix-socket=/tmp/kubectl-proxy-unix3837374243/test'
STEP: retrieving proxy /api/ output 01/12/23 01:03:14.55
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 01:03:14.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4805" for this suite. 01/12/23 01:03:14.559
------------------------------
• [0.138 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:03:14.466
    Jan 12 01:03:14.466: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 01:03:14.467
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:03:14.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:03:14.497
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 01/12/23 01:03:14.499
    Jan 12 01:03:14.499: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4805 proxy --unix-socket=/tmp/kubectl-proxy-unix3837374243/test'
    STEP: retrieving proxy /api/ output 01/12/23 01:03:14.55
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:03:14.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4805" for this suite. 01/12/23 01:03:14.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:03:14.605
Jan 12 01:03:14.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename disruption 01/12/23 01:03:14.605
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:03:14.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:03:14.625
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 01/12/23 01:03:14.627
STEP: Waiting for the pdb to be processed 01/12/23 01:03:14.643
STEP: updating the pdb 01/12/23 01:03:14.649
STEP: Waiting for the pdb to be processed 01/12/23 01:03:14.67
STEP: patching the pdb 01/12/23 01:03:14.685
STEP: Waiting for the pdb to be processed 01/12/23 01:03:14.695
STEP: Waiting for the pdb to be deleted 01/12/23 01:03:14.716
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 12 01:03:14.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1560" for this suite. 01/12/23 01:03:14.724
------------------------------
• [0.178 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:03:14.605
    Jan 12 01:03:14.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename disruption 01/12/23 01:03:14.605
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:03:14.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:03:14.625
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 01/12/23 01:03:14.627
    STEP: Waiting for the pdb to be processed 01/12/23 01:03:14.643
    STEP: updating the pdb 01/12/23 01:03:14.649
    STEP: Waiting for the pdb to be processed 01/12/23 01:03:14.67
    STEP: patching the pdb 01/12/23 01:03:14.685
    STEP: Waiting for the pdb to be processed 01/12/23 01:03:14.695
    STEP: Waiting for the pdb to be deleted 01/12/23 01:03:14.716
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:03:14.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1560" for this suite. 01/12/23 01:03:14.724
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:03:14.784
Jan 12 01:03:14.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename ingress 01/12/23 01:03:14.784
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:03:14.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:03:14.816
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/12/23 01:03:14.819
STEP: getting /apis/networking.k8s.io 01/12/23 01:03:14.821
STEP: getting /apis/networking.k8s.iov1 01/12/23 01:03:14.821
STEP: creating 01/12/23 01:03:14.822
STEP: getting 01/12/23 01:03:14.851
STEP: listing 01/12/23 01:03:14.854
STEP: watching 01/12/23 01:03:14.856
Jan 12 01:03:14.856: INFO: starting watch
STEP: cluster-wide listing 01/12/23 01:03:14.857
STEP: cluster-wide watching 01/12/23 01:03:14.859
Jan 12 01:03:14.860: INFO: starting watch
STEP: patching 01/12/23 01:03:14.861
STEP: updating 01/12/23 01:03:14.867
Jan 12 01:03:14.876: INFO: waiting for watch events with expected annotations
Jan 12 01:03:14.876: INFO: saw patched and updated annotations
STEP: patching /status 01/12/23 01:03:14.876
STEP: updating /status 01/12/23 01:03:14.89
STEP: get /status 01/12/23 01:03:14.902
STEP: deleting 01/12/23 01:03:14.905
STEP: deleting a collection 01/12/23 01:03:14.919
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jan 12 01:03:14.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-2950" for this suite. 01/12/23 01:03:14.944
------------------------------
• [0.196 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:03:14.784
    Jan 12 01:03:14.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename ingress 01/12/23 01:03:14.784
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:03:14.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:03:14.816
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/12/23 01:03:14.819
    STEP: getting /apis/networking.k8s.io 01/12/23 01:03:14.821
    STEP: getting /apis/networking.k8s.iov1 01/12/23 01:03:14.821
    STEP: creating 01/12/23 01:03:14.822
    STEP: getting 01/12/23 01:03:14.851
    STEP: listing 01/12/23 01:03:14.854
    STEP: watching 01/12/23 01:03:14.856
    Jan 12 01:03:14.856: INFO: starting watch
    STEP: cluster-wide listing 01/12/23 01:03:14.857
    STEP: cluster-wide watching 01/12/23 01:03:14.859
    Jan 12 01:03:14.860: INFO: starting watch
    STEP: patching 01/12/23 01:03:14.861
    STEP: updating 01/12/23 01:03:14.867
    Jan 12 01:03:14.876: INFO: waiting for watch events with expected annotations
    Jan 12 01:03:14.876: INFO: saw patched and updated annotations
    STEP: patching /status 01/12/23 01:03:14.876
    STEP: updating /status 01/12/23 01:03:14.89
    STEP: get /status 01/12/23 01:03:14.902
    STEP: deleting 01/12/23 01:03:14.905
    STEP: deleting a collection 01/12/23 01:03:14.919
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:03:14.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-2950" for this suite. 01/12/23 01:03:14.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:03:14.981
Jan 12 01:03:14.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename gc 01/12/23 01:03:14.981
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:03:15.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:03:15.016
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/12/23 01:03:15.022
STEP: create the rc2 01/12/23 01:03:15.029
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/12/23 01:03:20.064
STEP: delete the rc simpletest-rc-to-be-deleted 01/12/23 01:03:21.044
STEP: wait for the rc to be deleted 01/12/23 01:03:21.053
Jan 12 01:03:26.086: INFO: 68 pods remaining
Jan 12 01:03:26.086: INFO: 68 pods has nil DeletionTimestamp
Jan 12 01:03:26.086: INFO: 
STEP: Gathering metrics 01/12/23 01:03:31.19
Jan 12 01:03:31.243: INFO: Waiting up to 5m0s for pod "kube-controller-manager-eqx04-flash04" in namespace "kube-system" to be "running and ready"
Jan 12 01:03:31.246: INFO: Pod "kube-controller-manager-eqx04-flash04": Phase="Running", Reason="", readiness=true. Elapsed: 3.510188ms
Jan 12 01:03:31.246: INFO: The phase of Pod kube-controller-manager-eqx04-flash04 is Running (Ready = true)
Jan 12 01:03:31.246: INFO: Pod "kube-controller-manager-eqx04-flash04" satisfied condition "running and ready"
Jan 12 01:03:31.365: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 12 01:03:31.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-25ld6" in namespace "gc-6213"
Jan 12 01:03:31.399: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nnp6" in namespace "gc-6213"
Jan 12 01:03:31.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-4llxh" in namespace "gc-6213"
Jan 12 01:03:31.462: INFO: Deleting pod "simpletest-rc-to-be-deleted-4n7gk" in namespace "gc-6213"
Jan 12 01:03:31.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-5757m" in namespace "gc-6213"
Jan 12 01:03:31.518: INFO: Deleting pod "simpletest-rc-to-be-deleted-5x5sk" in namespace "gc-6213"
Jan 12 01:03:31.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-64pkx" in namespace "gc-6213"
Jan 12 01:03:31.575: INFO: Deleting pod "simpletest-rc-to-be-deleted-67bf8" in namespace "gc-6213"
Jan 12 01:03:31.617: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jl9d" in namespace "gc-6213"
Jan 12 01:03:31.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kqhv" in namespace "gc-6213"
Jan 12 01:03:31.720: INFO: Deleting pod "simpletest-rc-to-be-deleted-7mxkv" in namespace "gc-6213"
Jan 12 01:03:31.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nb5g" in namespace "gc-6213"
Jan 12 01:03:31.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nmzc" in namespace "gc-6213"
Jan 12 01:03:31.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-8pvpm" in namespace "gc-6213"
Jan 12 01:03:31.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-98kf2" in namespace "gc-6213"
Jan 12 01:03:31.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-99cp8" in namespace "gc-6213"
Jan 12 01:03:31.900: INFO: Deleting pod "simpletest-rc-to-be-deleted-9l2n7" in namespace "gc-6213"
Jan 12 01:03:31.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mfg9" in namespace "gc-6213"
Jan 12 01:03:31.974: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qlv2" in namespace "gc-6213"
Jan 12 01:03:32.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xxfd" in namespace "gc-6213"
Jan 12 01:03:32.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-bftpr" in namespace "gc-6213"
Jan 12 01:03:32.083: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjv79" in namespace "gc-6213"
Jan 12 01:03:32.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-bk94w" in namespace "gc-6213"
Jan 12 01:03:32.152: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9brk" in namespace "gc-6213"
Jan 12 01:03:32.174: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccqdr" in namespace "gc-6213"
Jan 12 01:03:32.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctsl8" in namespace "gc-6213"
Jan 12 01:03:32.231: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2gms" in namespace "gc-6213"
Jan 12 01:03:32.255: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7qct" in namespace "gc-6213"
Jan 12 01:03:32.287: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcc6w" in namespace "gc-6213"
Jan 12 01:03:32.336: INFO: Deleting pod "simpletest-rc-to-be-deleted-djtlc" in namespace "gc-6213"
Jan 12 01:03:32.358: INFO: Deleting pod "simpletest-rc-to-be-deleted-dw4hq" in namespace "gc-6213"
Jan 12 01:03:32.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffrcn" in namespace "gc-6213"
Jan 12 01:03:32.436: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgcb7" in namespace "gc-6213"
Jan 12 01:03:32.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgd9d" in namespace "gc-6213"
Jan 12 01:03:32.483: INFO: Deleting pod "simpletest-rc-to-be-deleted-flbg6" in namespace "gc-6213"
Jan 12 01:03:32.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpkc7" in namespace "gc-6213"
Jan 12 01:03:32.565: INFO: Deleting pod "simpletest-rc-to-be-deleted-frqzk" in namespace "gc-6213"
Jan 12 01:03:32.588: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7nzq" in namespace "gc-6213"
Jan 12 01:03:32.623: INFO: Deleting pod "simpletest-rc-to-be-deleted-gcdgs" in namespace "gc-6213"
Jan 12 01:03:32.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkqls" in namespace "gc-6213"
Jan 12 01:03:32.708: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzlcl" in namespace "gc-6213"
Jan 12 01:03:32.769: INFO: Deleting pod "simpletest-rc-to-be-deleted-hj2n2" in namespace "gc-6213"
Jan 12 01:03:32.819: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqmrx" in namespace "gc-6213"
Jan 12 01:03:32.869: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwmfk" in namespace "gc-6213"
Jan 12 01:03:32.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzwc2" in namespace "gc-6213"
Jan 12 01:03:32.932: INFO: Deleting pod "simpletest-rc-to-be-deleted-j98hr" in namespace "gc-6213"
Jan 12 01:03:33.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9q2g" in namespace "gc-6213"
Jan 12 01:03:33.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-jfml6" in namespace "gc-6213"
Jan 12 01:03:33.143: INFO: Deleting pod "simpletest-rc-to-be-deleted-jh78q" in namespace "gc-6213"
Jan 12 01:03:33.163: INFO: Deleting pod "simpletest-rc-to-be-deleted-jhmrv" in namespace "gc-6213"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 12 01:03:33.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6213" for this suite. 01/12/23 01:03:33.21
------------------------------
• [SLOW TEST] [18.254 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:03:14.981
    Jan 12 01:03:14.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename gc 01/12/23 01:03:14.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:03:15.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:03:15.016
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/12/23 01:03:15.022
    STEP: create the rc2 01/12/23 01:03:15.029
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/12/23 01:03:20.064
    STEP: delete the rc simpletest-rc-to-be-deleted 01/12/23 01:03:21.044
    STEP: wait for the rc to be deleted 01/12/23 01:03:21.053
    Jan 12 01:03:26.086: INFO: 68 pods remaining
    Jan 12 01:03:26.086: INFO: 68 pods has nil DeletionTimestamp
    Jan 12 01:03:26.086: INFO: 
    STEP: Gathering metrics 01/12/23 01:03:31.19
    Jan 12 01:03:31.243: INFO: Waiting up to 5m0s for pod "kube-controller-manager-eqx04-flash04" in namespace "kube-system" to be "running and ready"
    Jan 12 01:03:31.246: INFO: Pod "kube-controller-manager-eqx04-flash04": Phase="Running", Reason="", readiness=true. Elapsed: 3.510188ms
    Jan 12 01:03:31.246: INFO: The phase of Pod kube-controller-manager-eqx04-flash04 is Running (Ready = true)
    Jan 12 01:03:31.246: INFO: Pod "kube-controller-manager-eqx04-flash04" satisfied condition "running and ready"
    Jan 12 01:03:31.365: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 12 01:03:31.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-25ld6" in namespace "gc-6213"
    Jan 12 01:03:31.399: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nnp6" in namespace "gc-6213"
    Jan 12 01:03:31.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-4llxh" in namespace "gc-6213"
    Jan 12 01:03:31.462: INFO: Deleting pod "simpletest-rc-to-be-deleted-4n7gk" in namespace "gc-6213"
    Jan 12 01:03:31.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-5757m" in namespace "gc-6213"
    Jan 12 01:03:31.518: INFO: Deleting pod "simpletest-rc-to-be-deleted-5x5sk" in namespace "gc-6213"
    Jan 12 01:03:31.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-64pkx" in namespace "gc-6213"
    Jan 12 01:03:31.575: INFO: Deleting pod "simpletest-rc-to-be-deleted-67bf8" in namespace "gc-6213"
    Jan 12 01:03:31.617: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jl9d" in namespace "gc-6213"
    Jan 12 01:03:31.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kqhv" in namespace "gc-6213"
    Jan 12 01:03:31.720: INFO: Deleting pod "simpletest-rc-to-be-deleted-7mxkv" in namespace "gc-6213"
    Jan 12 01:03:31.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nb5g" in namespace "gc-6213"
    Jan 12 01:03:31.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nmzc" in namespace "gc-6213"
    Jan 12 01:03:31.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-8pvpm" in namespace "gc-6213"
    Jan 12 01:03:31.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-98kf2" in namespace "gc-6213"
    Jan 12 01:03:31.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-99cp8" in namespace "gc-6213"
    Jan 12 01:03:31.900: INFO: Deleting pod "simpletest-rc-to-be-deleted-9l2n7" in namespace "gc-6213"
    Jan 12 01:03:31.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mfg9" in namespace "gc-6213"
    Jan 12 01:03:31.974: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qlv2" in namespace "gc-6213"
    Jan 12 01:03:32.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xxfd" in namespace "gc-6213"
    Jan 12 01:03:32.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-bftpr" in namespace "gc-6213"
    Jan 12 01:03:32.083: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjv79" in namespace "gc-6213"
    Jan 12 01:03:32.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-bk94w" in namespace "gc-6213"
    Jan 12 01:03:32.152: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9brk" in namespace "gc-6213"
    Jan 12 01:03:32.174: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccqdr" in namespace "gc-6213"
    Jan 12 01:03:32.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctsl8" in namespace "gc-6213"
    Jan 12 01:03:32.231: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2gms" in namespace "gc-6213"
    Jan 12 01:03:32.255: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7qct" in namespace "gc-6213"
    Jan 12 01:03:32.287: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcc6w" in namespace "gc-6213"
    Jan 12 01:03:32.336: INFO: Deleting pod "simpletest-rc-to-be-deleted-djtlc" in namespace "gc-6213"
    Jan 12 01:03:32.358: INFO: Deleting pod "simpletest-rc-to-be-deleted-dw4hq" in namespace "gc-6213"
    Jan 12 01:03:32.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffrcn" in namespace "gc-6213"
    Jan 12 01:03:32.436: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgcb7" in namespace "gc-6213"
    Jan 12 01:03:32.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgd9d" in namespace "gc-6213"
    Jan 12 01:03:32.483: INFO: Deleting pod "simpletest-rc-to-be-deleted-flbg6" in namespace "gc-6213"
    Jan 12 01:03:32.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpkc7" in namespace "gc-6213"
    Jan 12 01:03:32.565: INFO: Deleting pod "simpletest-rc-to-be-deleted-frqzk" in namespace "gc-6213"
    Jan 12 01:03:32.588: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7nzq" in namespace "gc-6213"
    Jan 12 01:03:32.623: INFO: Deleting pod "simpletest-rc-to-be-deleted-gcdgs" in namespace "gc-6213"
    Jan 12 01:03:32.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkqls" in namespace "gc-6213"
    Jan 12 01:03:32.708: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzlcl" in namespace "gc-6213"
    Jan 12 01:03:32.769: INFO: Deleting pod "simpletest-rc-to-be-deleted-hj2n2" in namespace "gc-6213"
    Jan 12 01:03:32.819: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqmrx" in namespace "gc-6213"
    Jan 12 01:03:32.869: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwmfk" in namespace "gc-6213"
    Jan 12 01:03:32.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzwc2" in namespace "gc-6213"
    Jan 12 01:03:32.932: INFO: Deleting pod "simpletest-rc-to-be-deleted-j98hr" in namespace "gc-6213"
    Jan 12 01:03:33.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9q2g" in namespace "gc-6213"
    Jan 12 01:03:33.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-jfml6" in namespace "gc-6213"
    Jan 12 01:03:33.143: INFO: Deleting pod "simpletest-rc-to-be-deleted-jh78q" in namespace "gc-6213"
    Jan 12 01:03:33.163: INFO: Deleting pod "simpletest-rc-to-be-deleted-jhmrv" in namespace "gc-6213"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:03:33.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6213" for this suite. 01/12/23 01:03:33.21
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:03:33.254
Jan 12 01:03:33.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename secrets 01/12/23 01:03:33.312
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:03:33.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:03:33.533
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 01/12/23 01:03:33.571
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/12/23 01:03:33.581
STEP: patching the secret 01/12/23 01:03:33.655
STEP: deleting the secret using a LabelSelector 01/12/23 01:03:33.679
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/12/23 01:03:33.693
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 01:03:33.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5050" for this suite. 01/12/23 01:03:33.78
------------------------------
• [0.544 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:03:33.254
    Jan 12 01:03:33.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename secrets 01/12/23 01:03:33.312
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:03:33.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:03:33.533
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 01/12/23 01:03:33.571
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/12/23 01:03:33.581
    STEP: patching the secret 01/12/23 01:03:33.655
    STEP: deleting the secret using a LabelSelector 01/12/23 01:03:33.679
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/12/23 01:03:33.693
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:03:33.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5050" for this suite. 01/12/23 01:03:33.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:03:33.804
Jan 12 01:03:33.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename dns 01/12/23 01:03:33.805
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:03:33.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:03:33.828
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/12/23 01:03:33.834
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8839.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8839.svc.cluster.local; sleep 1; done
 01/12/23 01:03:33.847
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8839.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8839.svc.cluster.local; sleep 1; done
 01/12/23 01:03:33.847
STEP: creating a pod to probe DNS 01/12/23 01:03:33.847
STEP: submitting the pod to kubernetes 01/12/23 01:03:33.847
Jan 12 01:03:33.914: INFO: Waiting up to 15m0s for pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010" in namespace "dns-8839" to be "running"
Jan 12 01:03:33.921: INFO: Pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010": Phase="Pending", Reason="", readiness=false. Elapsed: 7.416098ms
Jan 12 01:03:35.973: INFO: Pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059004801s
Jan 12 01:03:37.926: INFO: Pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012082655s
Jan 12 01:03:39.944: INFO: Pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030299354s
Jan 12 01:03:41.925: INFO: Pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011381659s
Jan 12 01:03:43.925: INFO: Pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010": Phase="Running", Reason="", readiness=true. Elapsed: 10.010636418s
Jan 12 01:03:43.925: INFO: Pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010" satisfied condition "running"
STEP: retrieving the pod 01/12/23 01:03:43.925
STEP: looking for the results for each expected name from probers 01/12/23 01:03:43.927
Jan 12 01:03:43.934: INFO: DNS probes using dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010 succeeded

STEP: deleting the pod 01/12/23 01:03:43.934
STEP: changing the externalName to bar.example.com 01/12/23 01:03:43.958
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8839.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8839.svc.cluster.local; sleep 1; done
 01/12/23 01:03:43.973
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8839.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8839.svc.cluster.local; sleep 1; done
 01/12/23 01:03:43.973
STEP: creating a second pod to probe DNS 01/12/23 01:03:43.973
STEP: submitting the pod to kubernetes 01/12/23 01:03:43.973
Jan 12 01:03:44.034: INFO: Waiting up to 15m0s for pod "dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48" in namespace "dns-8839" to be "running"
Jan 12 01:03:44.039: INFO: Pod "dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48": Phase="Pending", Reason="", readiness=false. Elapsed: 5.15779ms
Jan 12 01:03:46.043: INFO: Pod "dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008718983s
Jan 12 01:03:48.042: INFO: Pod "dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008239309s
Jan 12 01:03:50.044: INFO: Pod "dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009902388s
Jan 12 01:03:52.044: INFO: Pod "dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48": Phase="Running", Reason="", readiness=true. Elapsed: 8.009938389s
Jan 12 01:03:52.044: INFO: Pod "dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48" satisfied condition "running"
STEP: retrieving the pod 01/12/23 01:03:52.044
STEP: looking for the results for each expected name from probers 01/12/23 01:03:52.046
Jan 12 01:03:52.053: INFO: DNS probes using dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48 succeeded

STEP: deleting the pod 01/12/23 01:03:52.053
STEP: changing the service to type=ClusterIP 01/12/23 01:03:52.091
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8839.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8839.svc.cluster.local; sleep 1; done
 01/12/23 01:03:52.126
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8839.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8839.svc.cluster.local; sleep 1; done
 01/12/23 01:03:52.126
STEP: creating a third pod to probe DNS 01/12/23 01:03:52.126
STEP: submitting the pod to kubernetes 01/12/23 01:03:52.131
Jan 12 01:03:52.256: INFO: Waiting up to 15m0s for pod "dns-test-b4d8c399-dd17-4adb-aa77-4cff6be0c60f" in namespace "dns-8839" to be "running"
Jan 12 01:03:52.258: INFO: Pod "dns-test-b4d8c399-dd17-4adb-aa77-4cff6be0c60f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.639624ms
Jan 12 01:03:54.262: INFO: Pod "dns-test-b4d8c399-dd17-4adb-aa77-4cff6be0c60f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006077999s
Jan 12 01:03:56.263: INFO: Pod "dns-test-b4d8c399-dd17-4adb-aa77-4cff6be0c60f": Phase="Running", Reason="", readiness=true. Elapsed: 4.0068665s
Jan 12 01:03:56.263: INFO: Pod "dns-test-b4d8c399-dd17-4adb-aa77-4cff6be0c60f" satisfied condition "running"
STEP: retrieving the pod 01/12/23 01:03:56.263
STEP: looking for the results for each expected name from probers 01/12/23 01:03:56.266
Jan 12 01:03:56.272: INFO: DNS probes using dns-test-b4d8c399-dd17-4adb-aa77-4cff6be0c60f succeeded

STEP: deleting the pod 01/12/23 01:03:56.272
STEP: deleting the test externalName service 01/12/23 01:03:56.291
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 01:03:56.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8839" for this suite. 01/12/23 01:03:56.316
------------------------------
• [SLOW TEST] [22.536 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:03:33.804
    Jan 12 01:03:33.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename dns 01/12/23 01:03:33.805
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:03:33.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:03:33.828
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/12/23 01:03:33.834
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8839.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8839.svc.cluster.local; sleep 1; done
     01/12/23 01:03:33.847
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8839.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8839.svc.cluster.local; sleep 1; done
     01/12/23 01:03:33.847
    STEP: creating a pod to probe DNS 01/12/23 01:03:33.847
    STEP: submitting the pod to kubernetes 01/12/23 01:03:33.847
    Jan 12 01:03:33.914: INFO: Waiting up to 15m0s for pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010" in namespace "dns-8839" to be "running"
    Jan 12 01:03:33.921: INFO: Pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010": Phase="Pending", Reason="", readiness=false. Elapsed: 7.416098ms
    Jan 12 01:03:35.973: INFO: Pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059004801s
    Jan 12 01:03:37.926: INFO: Pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012082655s
    Jan 12 01:03:39.944: INFO: Pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030299354s
    Jan 12 01:03:41.925: INFO: Pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011381659s
    Jan 12 01:03:43.925: INFO: Pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010": Phase="Running", Reason="", readiness=true. Elapsed: 10.010636418s
    Jan 12 01:03:43.925: INFO: Pod "dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 01:03:43.925
    STEP: looking for the results for each expected name from probers 01/12/23 01:03:43.927
    Jan 12 01:03:43.934: INFO: DNS probes using dns-test-f9aa187c-3e88-4d23-8e45-84131a43c010 succeeded

    STEP: deleting the pod 01/12/23 01:03:43.934
    STEP: changing the externalName to bar.example.com 01/12/23 01:03:43.958
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8839.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8839.svc.cluster.local; sleep 1; done
     01/12/23 01:03:43.973
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8839.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8839.svc.cluster.local; sleep 1; done
     01/12/23 01:03:43.973
    STEP: creating a second pod to probe DNS 01/12/23 01:03:43.973
    STEP: submitting the pod to kubernetes 01/12/23 01:03:43.973
    Jan 12 01:03:44.034: INFO: Waiting up to 15m0s for pod "dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48" in namespace "dns-8839" to be "running"
    Jan 12 01:03:44.039: INFO: Pod "dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48": Phase="Pending", Reason="", readiness=false. Elapsed: 5.15779ms
    Jan 12 01:03:46.043: INFO: Pod "dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008718983s
    Jan 12 01:03:48.042: INFO: Pod "dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008239309s
    Jan 12 01:03:50.044: INFO: Pod "dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009902388s
    Jan 12 01:03:52.044: INFO: Pod "dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48": Phase="Running", Reason="", readiness=true. Elapsed: 8.009938389s
    Jan 12 01:03:52.044: INFO: Pod "dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 01:03:52.044
    STEP: looking for the results for each expected name from probers 01/12/23 01:03:52.046
    Jan 12 01:03:52.053: INFO: DNS probes using dns-test-843c8dbb-4fde-4838-8253-7cd1b064ba48 succeeded

    STEP: deleting the pod 01/12/23 01:03:52.053
    STEP: changing the service to type=ClusterIP 01/12/23 01:03:52.091
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8839.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8839.svc.cluster.local; sleep 1; done
     01/12/23 01:03:52.126
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8839.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8839.svc.cluster.local; sleep 1; done
     01/12/23 01:03:52.126
    STEP: creating a third pod to probe DNS 01/12/23 01:03:52.126
    STEP: submitting the pod to kubernetes 01/12/23 01:03:52.131
    Jan 12 01:03:52.256: INFO: Waiting up to 15m0s for pod "dns-test-b4d8c399-dd17-4adb-aa77-4cff6be0c60f" in namespace "dns-8839" to be "running"
    Jan 12 01:03:52.258: INFO: Pod "dns-test-b4d8c399-dd17-4adb-aa77-4cff6be0c60f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.639624ms
    Jan 12 01:03:54.262: INFO: Pod "dns-test-b4d8c399-dd17-4adb-aa77-4cff6be0c60f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006077999s
    Jan 12 01:03:56.263: INFO: Pod "dns-test-b4d8c399-dd17-4adb-aa77-4cff6be0c60f": Phase="Running", Reason="", readiness=true. Elapsed: 4.0068665s
    Jan 12 01:03:56.263: INFO: Pod "dns-test-b4d8c399-dd17-4adb-aa77-4cff6be0c60f" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 01:03:56.263
    STEP: looking for the results for each expected name from probers 01/12/23 01:03:56.266
    Jan 12 01:03:56.272: INFO: DNS probes using dns-test-b4d8c399-dd17-4adb-aa77-4cff6be0c60f succeeded

    STEP: deleting the pod 01/12/23 01:03:56.272
    STEP: deleting the test externalName service 01/12/23 01:03:56.291
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:03:56.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8839" for this suite. 01/12/23 01:03:56.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:03:56.342
Jan 12 01:03:56.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename resourcequota 01/12/23 01:03:56.343
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:03:56.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:03:56.381
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 01/12/23 01:03:56.383
STEP: Counting existing ResourceQuota 01/12/23 01:04:01.396
STEP: Creating a ResourceQuota 01/12/23 01:04:06.399
STEP: Ensuring resource quota status is calculated 01/12/23 01:04:06.413
STEP: Creating a Secret 01/12/23 01:04:08.416
STEP: Ensuring resource quota status captures secret creation 01/12/23 01:04:08.439
STEP: Deleting a secret 01/12/23 01:04:10.444
STEP: Ensuring resource quota status released usage 01/12/23 01:04:10.452
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 01:04:12.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2827" for this suite. 01/12/23 01:04:12.459
------------------------------
• [SLOW TEST] [16.145 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:03:56.342
    Jan 12 01:03:56.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename resourcequota 01/12/23 01:03:56.343
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:03:56.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:03:56.381
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 01/12/23 01:03:56.383
    STEP: Counting existing ResourceQuota 01/12/23 01:04:01.396
    STEP: Creating a ResourceQuota 01/12/23 01:04:06.399
    STEP: Ensuring resource quota status is calculated 01/12/23 01:04:06.413
    STEP: Creating a Secret 01/12/23 01:04:08.416
    STEP: Ensuring resource quota status captures secret creation 01/12/23 01:04:08.439
    STEP: Deleting a secret 01/12/23 01:04:10.444
    STEP: Ensuring resource quota status released usage 01/12/23 01:04:10.452
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:04:12.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2827" for this suite. 01/12/23 01:04:12.459
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:04:12.488
Jan 12 01:04:12.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 01:04:12.489
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:04:12.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:04:12.515
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 01/12/23 01:04:12.518
Jan 12 01:04:12.563: INFO: Waiting up to 5m0s for pod "pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924" in namespace "emptydir-7668" to be "Succeeded or Failed"
Jan 12 01:04:12.566: INFO: Pod "pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924": Phase="Pending", Reason="", readiness=false. Elapsed: 2.479188ms
Jan 12 01:04:14.577: INFO: Pod "pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014143422s
Jan 12 01:04:16.569: INFO: Pod "pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005839824s
Jan 12 01:04:18.569: INFO: Pod "pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006184697s
STEP: Saw pod success 01/12/23 01:04:18.569
Jan 12 01:04:18.570: INFO: Pod "pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924" satisfied condition "Succeeded or Failed"
Jan 12 01:04:18.572: INFO: Trying to get logs from node eqx04-flash06 pod pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924 container test-container: <nil>
STEP: delete the pod 01/12/23 01:04:18.591
Jan 12 01:04:18.621: INFO: Waiting for pod pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924 to disappear
Jan 12 01:04:18.624: INFO: Pod pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:04:18.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7668" for this suite. 01/12/23 01:04:18.627
------------------------------
• [SLOW TEST] [6.166 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:04:12.488
    Jan 12 01:04:12.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 01:04:12.489
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:04:12.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:04:12.515
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/12/23 01:04:12.518
    Jan 12 01:04:12.563: INFO: Waiting up to 5m0s for pod "pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924" in namespace "emptydir-7668" to be "Succeeded or Failed"
    Jan 12 01:04:12.566: INFO: Pod "pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924": Phase="Pending", Reason="", readiness=false. Elapsed: 2.479188ms
    Jan 12 01:04:14.577: INFO: Pod "pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014143422s
    Jan 12 01:04:16.569: INFO: Pod "pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005839824s
    Jan 12 01:04:18.569: INFO: Pod "pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006184697s
    STEP: Saw pod success 01/12/23 01:04:18.569
    Jan 12 01:04:18.570: INFO: Pod "pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924" satisfied condition "Succeeded or Failed"
    Jan 12 01:04:18.572: INFO: Trying to get logs from node eqx04-flash06 pod pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924 container test-container: <nil>
    STEP: delete the pod 01/12/23 01:04:18.591
    Jan 12 01:04:18.621: INFO: Waiting for pod pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924 to disappear
    Jan 12 01:04:18.624: INFO: Pod pod-d4593fe6-629e-4ec5-9efe-cdf7c5de4924 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:04:18.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7668" for this suite. 01/12/23 01:04:18.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:04:18.656
Jan 12 01:04:18.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename secrets 01/12/23 01:04:18.656
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:04:18.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:04:18.674
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-90f82422-9216-4809-99c1-188a21b2fc48 01/12/23 01:04:18.676
STEP: Creating a pod to test consume secrets 01/12/23 01:04:18.682
Jan 12 01:04:18.764: INFO: Waiting up to 5m0s for pod "pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6" in namespace "secrets-357" to be "Succeeded or Failed"
Jan 12 01:04:18.766: INFO: Pod "pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.198841ms
Jan 12 01:04:20.770: INFO: Pod "pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005955466s
Jan 12 01:04:22.770: INFO: Pod "pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006264816s
Jan 12 01:04:24.771: INFO: Pod "pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007200175s
STEP: Saw pod success 01/12/23 01:04:24.771
Jan 12 01:04:24.771: INFO: Pod "pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6" satisfied condition "Succeeded or Failed"
Jan 12 01:04:24.774: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6 container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 01:04:24.789
Jan 12 01:04:24.812: INFO: Waiting for pod pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6 to disappear
Jan 12 01:04:24.815: INFO: Pod pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 01:04:24.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-357" for this suite. 01/12/23 01:04:24.819
------------------------------
• [SLOW TEST] [6.182 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:04:18.656
    Jan 12 01:04:18.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename secrets 01/12/23 01:04:18.656
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:04:18.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:04:18.674
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-90f82422-9216-4809-99c1-188a21b2fc48 01/12/23 01:04:18.676
    STEP: Creating a pod to test consume secrets 01/12/23 01:04:18.682
    Jan 12 01:04:18.764: INFO: Waiting up to 5m0s for pod "pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6" in namespace "secrets-357" to be "Succeeded or Failed"
    Jan 12 01:04:18.766: INFO: Pod "pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.198841ms
    Jan 12 01:04:20.770: INFO: Pod "pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005955466s
    Jan 12 01:04:22.770: INFO: Pod "pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006264816s
    Jan 12 01:04:24.771: INFO: Pod "pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007200175s
    STEP: Saw pod success 01/12/23 01:04:24.771
    Jan 12 01:04:24.771: INFO: Pod "pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6" satisfied condition "Succeeded or Failed"
    Jan 12 01:04:24.774: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6 container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 01:04:24.789
    Jan 12 01:04:24.812: INFO: Waiting for pod pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6 to disappear
    Jan 12 01:04:24.815: INFO: Pod pod-secrets-66300dd5-7146-42bc-8ba5-1391a1e5ccc6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:04:24.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-357" for this suite. 01/12/23 01:04:24.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:04:24.839
Jan 12 01:04:24.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename security-context-test 01/12/23 01:04:24.84
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:04:24.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:04:24.868
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Jan 12 01:04:24.935: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ac316481-2a73-42df-b01b-c38555c65622" in namespace "security-context-test-334" to be "Succeeded or Failed"
Jan 12 01:04:24.937: INFO: Pod "busybox-readonly-false-ac316481-2a73-42df-b01b-c38555c65622": Phase="Pending", Reason="", readiness=false. Elapsed: 2.486146ms
Jan 12 01:04:26.942: INFO: Pod "busybox-readonly-false-ac316481-2a73-42df-b01b-c38555c65622": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00694816s
Jan 12 01:04:28.941: INFO: Pod "busybox-readonly-false-ac316481-2a73-42df-b01b-c38555c65622": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00634406s
Jan 12 01:04:30.941: INFO: Pod "busybox-readonly-false-ac316481-2a73-42df-b01b-c38555c65622": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006333851s
Jan 12 01:04:30.941: INFO: Pod "busybox-readonly-false-ac316481-2a73-42df-b01b-c38555c65622" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 12 01:04:30.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-334" for this suite. 01/12/23 01:04:30.945
------------------------------
• [SLOW TEST] [6.124 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:04:24.839
    Jan 12 01:04:24.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename security-context-test 01/12/23 01:04:24.84
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:04:24.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:04:24.868
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Jan 12 01:04:24.935: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ac316481-2a73-42df-b01b-c38555c65622" in namespace "security-context-test-334" to be "Succeeded or Failed"
    Jan 12 01:04:24.937: INFO: Pod "busybox-readonly-false-ac316481-2a73-42df-b01b-c38555c65622": Phase="Pending", Reason="", readiness=false. Elapsed: 2.486146ms
    Jan 12 01:04:26.942: INFO: Pod "busybox-readonly-false-ac316481-2a73-42df-b01b-c38555c65622": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00694816s
    Jan 12 01:04:28.941: INFO: Pod "busybox-readonly-false-ac316481-2a73-42df-b01b-c38555c65622": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00634406s
    Jan 12 01:04:30.941: INFO: Pod "busybox-readonly-false-ac316481-2a73-42df-b01b-c38555c65622": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006333851s
    Jan 12 01:04:30.941: INFO: Pod "busybox-readonly-false-ac316481-2a73-42df-b01b-c38555c65622" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:04:30.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-334" for this suite. 01/12/23 01:04:30.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:04:30.965
Jan 12 01:04:30.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename runtimeclass 01/12/23 01:04:30.966
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:04:30.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:04:30.983
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-3497-delete-me 01/12/23 01:04:30.994
STEP: Waiting for the RuntimeClass to disappear 01/12/23 01:04:31.001
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 12 01:04:31.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3497" for this suite. 01/12/23 01:04:31.012
------------------------------
• [0.070 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:04:30.965
    Jan 12 01:04:30.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename runtimeclass 01/12/23 01:04:30.966
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:04:30.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:04:30.983
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-3497-delete-me 01/12/23 01:04:30.994
    STEP: Waiting for the RuntimeClass to disappear 01/12/23 01:04:31.001
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:04:31.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3497" for this suite. 01/12/23 01:04:31.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:04:31.038
Jan 12 01:04:31.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-probe 01/12/23 01:04:31.039
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:04:31.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:04:31.071
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74 in namespace container-probe-1404 01/12/23 01:04:31.073
Jan 12 01:04:31.107: INFO: Waiting up to 5m0s for pod "test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74" in namespace "container-probe-1404" to be "not pending"
Jan 12 01:04:31.109: INFO: Pod "test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.331346ms
Jan 12 01:04:33.115: INFO: Pod "test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008159916s
Jan 12 01:04:35.113: INFO: Pod "test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74": Phase="Running", Reason="", readiness=true. Elapsed: 4.006185146s
Jan 12 01:04:35.113: INFO: Pod "test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74" satisfied condition "not pending"
Jan 12 01:04:35.113: INFO: Started pod test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74 in namespace container-probe-1404
STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 01:04:35.113
Jan 12 01:04:35.116: INFO: Initial restart count of pod test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74 is 0
STEP: deleting the pod 01/12/23 01:08:35.605
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 01:08:35.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1404" for this suite. 01/12/23 01:08:35.652
------------------------------
• [SLOW TEST] [244.651 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:04:31.038
    Jan 12 01:04:31.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-probe 01/12/23 01:04:31.039
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:04:31.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:04:31.071
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74 in namespace container-probe-1404 01/12/23 01:04:31.073
    Jan 12 01:04:31.107: INFO: Waiting up to 5m0s for pod "test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74" in namespace "container-probe-1404" to be "not pending"
    Jan 12 01:04:31.109: INFO: Pod "test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.331346ms
    Jan 12 01:04:33.115: INFO: Pod "test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008159916s
    Jan 12 01:04:35.113: INFO: Pod "test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74": Phase="Running", Reason="", readiness=true. Elapsed: 4.006185146s
    Jan 12 01:04:35.113: INFO: Pod "test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74" satisfied condition "not pending"
    Jan 12 01:04:35.113: INFO: Started pod test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74 in namespace container-probe-1404
    STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 01:04:35.113
    Jan 12 01:04:35.116: INFO: Initial restart count of pod test-webserver-476f79fd-1274-4e59-8cf9-3455c1224d74 is 0
    STEP: deleting the pod 01/12/23 01:08:35.605
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:08:35.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1404" for this suite. 01/12/23 01:08:35.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:08:35.692
Jan 12 01:08:35.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename csistoragecapacity 01/12/23 01:08:35.694
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:08:35.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:08:35.734
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/12/23 01:08:35.736
STEP: getting /apis/storage.k8s.io 01/12/23 01:08:35.738
STEP: getting /apis/storage.k8s.io/v1 01/12/23 01:08:35.739
STEP: creating 01/12/23 01:08:35.74
STEP: watching 01/12/23 01:08:35.765
Jan 12 01:08:35.765: INFO: starting watch
STEP: getting 01/12/23 01:08:35.772
STEP: listing in namespace 01/12/23 01:08:35.775
STEP: listing across namespaces 01/12/23 01:08:35.778
STEP: patching 01/12/23 01:08:35.78
STEP: updating 01/12/23 01:08:35.795
Jan 12 01:08:35.804: INFO: waiting for watch events with expected annotations in namespace
Jan 12 01:08:35.804: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/12/23 01:08:35.804
STEP: deleting a collection 01/12/23 01:08:35.818
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jan 12 01:08:35.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-3799" for this suite. 01/12/23 01:08:35.844
------------------------------
• [0.178 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:08:35.692
    Jan 12 01:08:35.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename csistoragecapacity 01/12/23 01:08:35.694
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:08:35.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:08:35.734
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/12/23 01:08:35.736
    STEP: getting /apis/storage.k8s.io 01/12/23 01:08:35.738
    STEP: getting /apis/storage.k8s.io/v1 01/12/23 01:08:35.739
    STEP: creating 01/12/23 01:08:35.74
    STEP: watching 01/12/23 01:08:35.765
    Jan 12 01:08:35.765: INFO: starting watch
    STEP: getting 01/12/23 01:08:35.772
    STEP: listing in namespace 01/12/23 01:08:35.775
    STEP: listing across namespaces 01/12/23 01:08:35.778
    STEP: patching 01/12/23 01:08:35.78
    STEP: updating 01/12/23 01:08:35.795
    Jan 12 01:08:35.804: INFO: waiting for watch events with expected annotations in namespace
    Jan 12 01:08:35.804: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/12/23 01:08:35.804
    STEP: deleting a collection 01/12/23 01:08:35.818
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:08:35.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-3799" for this suite. 01/12/23 01:08:35.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:08:35.872
Jan 12 01:08:35.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pods 01/12/23 01:08:35.873
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:08:35.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:08:35.897
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 01/12/23 01:08:35.899
STEP: submitting the pod to kubernetes 01/12/23 01:08:35.899
Jan 12 01:08:35.947: INFO: Waiting up to 5m0s for pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75" in namespace "pods-7954" to be "running and ready"
Jan 12 01:08:35.950: INFO: Pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.279529ms
Jan 12 01:08:35.950: INFO: The phase of Pod pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:08:37.954: INFO: Pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006466871s
Jan 12 01:08:37.954: INFO: The phase of Pod pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:08:39.953: INFO: Pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75": Phase="Running", Reason="", readiness=true. Elapsed: 4.005909709s
Jan 12 01:08:39.953: INFO: The phase of Pod pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75 is Running (Ready = true)
Jan 12 01:08:39.953: INFO: Pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/12/23 01:08:39.956
STEP: updating the pod 01/12/23 01:08:39.958
Jan 12 01:08:40.475: INFO: Successfully updated pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75"
Jan 12 01:08:40.475: INFO: Waiting up to 5m0s for pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75" in namespace "pods-7954" to be "running"
Jan 12 01:08:40.481: INFO: Pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75": Phase="Running", Reason="", readiness=true. Elapsed: 5.36705ms
Jan 12 01:08:40.481: INFO: Pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/12/23 01:08:40.481
Jan 12 01:08:40.484: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 01:08:40.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7954" for this suite. 01/12/23 01:08:40.488
------------------------------
• [4.637 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:08:35.872
    Jan 12 01:08:35.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pods 01/12/23 01:08:35.873
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:08:35.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:08:35.897
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 01/12/23 01:08:35.899
    STEP: submitting the pod to kubernetes 01/12/23 01:08:35.899
    Jan 12 01:08:35.947: INFO: Waiting up to 5m0s for pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75" in namespace "pods-7954" to be "running and ready"
    Jan 12 01:08:35.950: INFO: Pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.279529ms
    Jan 12 01:08:35.950: INFO: The phase of Pod pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:08:37.954: INFO: Pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006466871s
    Jan 12 01:08:37.954: INFO: The phase of Pod pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:08:39.953: INFO: Pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75": Phase="Running", Reason="", readiness=true. Elapsed: 4.005909709s
    Jan 12 01:08:39.953: INFO: The phase of Pod pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75 is Running (Ready = true)
    Jan 12 01:08:39.953: INFO: Pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/12/23 01:08:39.956
    STEP: updating the pod 01/12/23 01:08:39.958
    Jan 12 01:08:40.475: INFO: Successfully updated pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75"
    Jan 12 01:08:40.475: INFO: Waiting up to 5m0s for pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75" in namespace "pods-7954" to be "running"
    Jan 12 01:08:40.481: INFO: Pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75": Phase="Running", Reason="", readiness=true. Elapsed: 5.36705ms
    Jan 12 01:08:40.481: INFO: Pod "pod-update-3c9b8045-75c3-4c98-b91c-af6547fb8e75" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/12/23 01:08:40.481
    Jan 12 01:08:40.484: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:08:40.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7954" for this suite. 01/12/23 01:08:40.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:08:40.51
Jan 12 01:08:40.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename sched-preemption 01/12/23 01:08:40.511
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:08:40.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:08:40.536
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 12 01:08:40.573: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 12 01:09:40.637: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
STEP: Create pods that use 4/5 of node resources. 01/12/23 01:09:40.64
Jan 12 01:09:40.767: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 12 01:09:40.824: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 12 01:09:40.950: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 12 01:09:41.016: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/12/23 01:09:41.016
Jan 12 01:09:41.016: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4639" to be "running"
Jan 12 01:09:41.019: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362328ms
Jan 12 01:09:43.023: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006553329s
Jan 12 01:09:45.022: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006140038s
Jan 12 01:09:47.022: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006044114s
Jan 12 01:09:49.023: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.006679582s
Jan 12 01:09:49.023: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 12 01:09:49.023: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4639" to be "running"
Jan 12 01:09:49.026: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.673986ms
Jan 12 01:09:49.026: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 12 01:09:49.026: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4639" to be "running"
Jan 12 01:09:49.028: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.416888ms
Jan 12 01:09:49.028: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 12 01:09:49.028: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4639" to be "running"
Jan 12 01:09:49.031: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.533918ms
Jan 12 01:09:49.031: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/12/23 01:09:49.031
Jan 12 01:09:49.046: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 12 01:09:49.048: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.231752ms
Jan 12 01:09:51.063: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017624199s
Jan 12 01:09:53.051: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005588995s
Jan 12 01:09:53.051: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:09:53.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4639" for this suite. 01/12/23 01:09:53.145
------------------------------
• [SLOW TEST] [72.728 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:08:40.51
    Jan 12 01:08:40.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename sched-preemption 01/12/23 01:08:40.511
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:08:40.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:08:40.536
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 12 01:08:40.573: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 12 01:09:40.637: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:222
    STEP: Create pods that use 4/5 of node resources. 01/12/23 01:09:40.64
    Jan 12 01:09:40.767: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 12 01:09:40.824: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 12 01:09:40.950: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 12 01:09:41.016: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/12/23 01:09:41.016
    Jan 12 01:09:41.016: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4639" to be "running"
    Jan 12 01:09:41.019: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362328ms
    Jan 12 01:09:43.023: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006553329s
    Jan 12 01:09:45.022: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006140038s
    Jan 12 01:09:47.022: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006044114s
    Jan 12 01:09:49.023: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.006679582s
    Jan 12 01:09:49.023: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 12 01:09:49.023: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4639" to be "running"
    Jan 12 01:09:49.026: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.673986ms
    Jan 12 01:09:49.026: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 12 01:09:49.026: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4639" to be "running"
    Jan 12 01:09:49.028: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.416888ms
    Jan 12 01:09:49.028: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 12 01:09:49.028: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4639" to be "running"
    Jan 12 01:09:49.031: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.533918ms
    Jan 12 01:09:49.031: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/12/23 01:09:49.031
    Jan 12 01:09:49.046: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 12 01:09:49.048: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.231752ms
    Jan 12 01:09:51.063: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017624199s
    Jan 12 01:09:53.051: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005588995s
    Jan 12 01:09:53.051: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:09:53.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4639" for this suite. 01/12/23 01:09:53.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:09:53.239
Jan 12 01:09:53.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pods 01/12/23 01:09:53.24
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:09:53.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:09:53.269
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 01/12/23 01:09:53.271
Jan 12 01:09:53.380: INFO: Waiting up to 5m0s for pod "pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4" in namespace "pods-155" to be "running and ready"
Jan 12 01:09:53.383: INFO: Pod "pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.405013ms
Jan 12 01:09:53.383: INFO: The phase of Pod pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:09:55.390: INFO: Pod "pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009698729s
Jan 12 01:09:55.390: INFO: The phase of Pod pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:09:57.387: INFO: Pod "pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4": Phase="Running", Reason="", readiness=true. Elapsed: 4.006870696s
Jan 12 01:09:57.387: INFO: The phase of Pod pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4 is Running (Ready = true)
Jan 12 01:09:57.387: INFO: Pod "pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4" satisfied condition "running and ready"
Jan 12 01:09:57.392: INFO: Pod pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4 has hostIP: 10.9.40.106
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 01:09:57.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-155" for this suite. 01/12/23 01:09:57.396
------------------------------
• [4.181 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:09:53.239
    Jan 12 01:09:53.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pods 01/12/23 01:09:53.24
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:09:53.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:09:53.269
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 01/12/23 01:09:53.271
    Jan 12 01:09:53.380: INFO: Waiting up to 5m0s for pod "pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4" in namespace "pods-155" to be "running and ready"
    Jan 12 01:09:53.383: INFO: Pod "pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.405013ms
    Jan 12 01:09:53.383: INFO: The phase of Pod pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:09:55.390: INFO: Pod "pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009698729s
    Jan 12 01:09:55.390: INFO: The phase of Pod pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:09:57.387: INFO: Pod "pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4": Phase="Running", Reason="", readiness=true. Elapsed: 4.006870696s
    Jan 12 01:09:57.387: INFO: The phase of Pod pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4 is Running (Ready = true)
    Jan 12 01:09:57.387: INFO: Pod "pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4" satisfied condition "running and ready"
    Jan 12 01:09:57.392: INFO: Pod pod-hostip-1117983d-7c84-48e2-8a0b-fa6e8c3ee8b4 has hostIP: 10.9.40.106
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:09:57.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-155" for this suite. 01/12/23 01:09:57.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:09:57.422
Jan 12 01:09:57.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 01:09:57.423
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:09:57.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:09:57.443
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 01/12/23 01:09:57.445
Jan 12 01:09:57.481: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-eb5b5a9c-3836-4e4d-a319-8222fd798a34" in namespace "emptydir-4691" to be "running"
Jan 12 01:09:57.483: INFO: Pod "pod-sharedvolume-eb5b5a9c-3836-4e4d-a319-8222fd798a34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.158456ms
Jan 12 01:09:59.486: INFO: Pod "pod-sharedvolume-eb5b5a9c-3836-4e4d-a319-8222fd798a34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004886015s
Jan 12 01:10:01.487: INFO: Pod "pod-sharedvolume-eb5b5a9c-3836-4e4d-a319-8222fd798a34": Phase="Running", Reason="", readiness=false. Elapsed: 4.006598155s
Jan 12 01:10:01.487: INFO: Pod "pod-sharedvolume-eb5b5a9c-3836-4e4d-a319-8222fd798a34" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/12/23 01:10:01.487
Jan 12 01:10:01.488: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4691 PodName:pod-sharedvolume-eb5b5a9c-3836-4e4d-a319-8222fd798a34 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:10:01.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:10:01.488: INFO: ExecWithOptions: Clientset creation
Jan 12 01:10:01.488: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/emptydir-4691/pods/pod-sharedvolume-eb5b5a9c-3836-4e4d-a319-8222fd798a34/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 12 01:10:01.624: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:10:01.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4691" for this suite. 01/12/23 01:10:01.628
------------------------------
• [4.279 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:09:57.422
    Jan 12 01:09:57.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 01:09:57.423
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:09:57.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:09:57.443
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 01/12/23 01:09:57.445
    Jan 12 01:09:57.481: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-eb5b5a9c-3836-4e4d-a319-8222fd798a34" in namespace "emptydir-4691" to be "running"
    Jan 12 01:09:57.483: INFO: Pod "pod-sharedvolume-eb5b5a9c-3836-4e4d-a319-8222fd798a34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.158456ms
    Jan 12 01:09:59.486: INFO: Pod "pod-sharedvolume-eb5b5a9c-3836-4e4d-a319-8222fd798a34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004886015s
    Jan 12 01:10:01.487: INFO: Pod "pod-sharedvolume-eb5b5a9c-3836-4e4d-a319-8222fd798a34": Phase="Running", Reason="", readiness=false. Elapsed: 4.006598155s
    Jan 12 01:10:01.487: INFO: Pod "pod-sharedvolume-eb5b5a9c-3836-4e4d-a319-8222fd798a34" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/12/23 01:10:01.487
    Jan 12 01:10:01.488: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4691 PodName:pod-sharedvolume-eb5b5a9c-3836-4e4d-a319-8222fd798a34 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:10:01.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:10:01.488: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:10:01.488: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/emptydir-4691/pods/pod-sharedvolume-eb5b5a9c-3836-4e4d-a319-8222fd798a34/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 12 01:10:01.624: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:10:01.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4691" for this suite. 01/12/23 01:10:01.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:10:01.702
Jan 12 01:10:01.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 01:10:01.702
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:01.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:01.718
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 01:10:01.755
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:10:02.25
STEP: Deploying the webhook pod 01/12/23 01:10:02.264
STEP: Wait for the deployment to be ready 01/12/23 01:10:02.48
Jan 12 01:10:02.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:10:04.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 01:10:06.544
STEP: Verifying the service has paired with the endpoint 01/12/23 01:10:06.565
Jan 12 01:10:07.566: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 01/12/23 01:10:07.569
STEP: create a pod 01/12/23 01:10:07.587
Jan 12 01:10:07.648: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1220" to be "running"
Jan 12 01:10:07.650: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.20713ms
Jan 12 01:10:09.657: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009202281s
Jan 12 01:10:11.653: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005294722s
Jan 12 01:10:11.653: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/12/23 01:10:11.653
Jan 12 01:10:11.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=webhook-1220 attach --namespace=webhook-1220 to-be-attached-pod -i -c=container1'
Jan 12 01:10:11.736: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:10:11.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1220" for this suite. 01/12/23 01:10:11.838
STEP: Destroying namespace "webhook-1220-markers" for this suite. 01/12/23 01:10:11.925
------------------------------
• [SLOW TEST] [10.284 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:10:01.702
    Jan 12 01:10:01.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 01:10:01.702
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:01.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:01.718
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 01:10:01.755
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:10:02.25
    STEP: Deploying the webhook pod 01/12/23 01:10:02.264
    STEP: Wait for the deployment to be ready 01/12/23 01:10:02.48
    Jan 12 01:10:02.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:10:04.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 01:10:06.544
    STEP: Verifying the service has paired with the endpoint 01/12/23 01:10:06.565
    Jan 12 01:10:07.566: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 01/12/23 01:10:07.569
    STEP: create a pod 01/12/23 01:10:07.587
    Jan 12 01:10:07.648: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1220" to be "running"
    Jan 12 01:10:07.650: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.20713ms
    Jan 12 01:10:09.657: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009202281s
    Jan 12 01:10:11.653: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005294722s
    Jan 12 01:10:11.653: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/12/23 01:10:11.653
    Jan 12 01:10:11.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=webhook-1220 attach --namespace=webhook-1220 to-be-attached-pod -i -c=container1'
    Jan 12 01:10:11.736: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:10:11.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1220" for this suite. 01/12/23 01:10:11.838
    STEP: Destroying namespace "webhook-1220-markers" for this suite. 01/12/23 01:10:11.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:10:11.987
Jan 12 01:10:11.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:10:11.988
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:12.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:12.016
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 01/12/23 01:10:12.018
Jan 12 01:10:12.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e" in namespace "projected-7644" to be "Succeeded or Failed"
Jan 12 01:10:12.128: INFO: Pod "downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.335964ms
Jan 12 01:10:14.131: INFO: Pod "downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005780432s
Jan 12 01:10:16.131: INFO: Pod "downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005597076s
Jan 12 01:10:18.131: INFO: Pod "downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005844065s
STEP: Saw pod success 01/12/23 01:10:18.131
Jan 12 01:10:18.131: INFO: Pod "downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e" satisfied condition "Succeeded or Failed"
Jan 12 01:10:18.134: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e container client-container: <nil>
STEP: delete the pod 01/12/23 01:10:18.151
Jan 12 01:10:18.174: INFO: Waiting for pod downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e to disappear
Jan 12 01:10:18.177: INFO: Pod downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 01:10:18.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7644" for this suite. 01/12/23 01:10:18.181
------------------------------
• [SLOW TEST] [6.209 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:10:11.987
    Jan 12 01:10:11.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:10:11.988
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:12.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:12.016
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 01/12/23 01:10:12.018
    Jan 12 01:10:12.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e" in namespace "projected-7644" to be "Succeeded or Failed"
    Jan 12 01:10:12.128: INFO: Pod "downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.335964ms
    Jan 12 01:10:14.131: INFO: Pod "downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005780432s
    Jan 12 01:10:16.131: INFO: Pod "downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005597076s
    Jan 12 01:10:18.131: INFO: Pod "downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005844065s
    STEP: Saw pod success 01/12/23 01:10:18.131
    Jan 12 01:10:18.131: INFO: Pod "downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e" satisfied condition "Succeeded or Failed"
    Jan 12 01:10:18.134: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e container client-container: <nil>
    STEP: delete the pod 01/12/23 01:10:18.151
    Jan 12 01:10:18.174: INFO: Waiting for pod downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e to disappear
    Jan 12 01:10:18.177: INFO: Pod downwardapi-volume-c9342d39-53f0-4255-a91a-5ecbc8b10c1e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:10:18.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7644" for this suite. 01/12/23 01:10:18.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:10:18.199
Jan 12 01:10:18.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename namespaces 01/12/23 01:10:18.199
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:18.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:18.239
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 01/12/23 01:10:18.241
STEP: patching the Namespace 01/12/23 01:10:18.261
STEP: get the Namespace and ensuring it has the label 01/12/23 01:10:18.27
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:10:18.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8345" for this suite. 01/12/23 01:10:18.285
STEP: Destroying namespace "nspatchtest-a6317d7b-d01e-4f28-a75d-94575328c4da-8135" for this suite. 01/12/23 01:10:18.316
------------------------------
• [0.153 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:10:18.199
    Jan 12 01:10:18.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename namespaces 01/12/23 01:10:18.199
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:18.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:18.239
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 01/12/23 01:10:18.241
    STEP: patching the Namespace 01/12/23 01:10:18.261
    STEP: get the Namespace and ensuring it has the label 01/12/23 01:10:18.27
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:10:18.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8345" for this suite. 01/12/23 01:10:18.285
    STEP: Destroying namespace "nspatchtest-a6317d7b-d01e-4f28-a75d-94575328c4da-8135" for this suite. 01/12/23 01:10:18.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:10:18.353
Jan 12 01:10:18.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 01:10:18.354
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:18.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:18.379
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 01:10:18.399
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:10:19.293
STEP: Deploying the webhook pod 01/12/23 01:10:19.303
STEP: Wait for the deployment to be ready 01/12/23 01:10:19.344
Jan 12 01:10:19.384: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 19, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 01:10:21.387
STEP: Verifying the service has paired with the endpoint 01/12/23 01:10:21.407
Jan 12 01:10:22.408: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 01/12/23 01:10:22.41
STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 01:10:22.432
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/12/23 01:10:22.44
STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 01:10:22.458
STEP: Patching a validating webhook configuration's rules to include the create operation 01/12/23 01:10:22.472
STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 01:10:22.482
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:10:22.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-704" for this suite. 01/12/23 01:10:22.587
STEP: Destroying namespace "webhook-704-markers" for this suite. 01/12/23 01:10:22.642
------------------------------
• [4.329 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:10:18.353
    Jan 12 01:10:18.353: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 01:10:18.354
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:18.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:18.379
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 01:10:18.399
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:10:19.293
    STEP: Deploying the webhook pod 01/12/23 01:10:19.303
    STEP: Wait for the deployment to be ready 01/12/23 01:10:19.344
    Jan 12 01:10:19.384: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 19, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 01:10:21.387
    STEP: Verifying the service has paired with the endpoint 01/12/23 01:10:21.407
    Jan 12 01:10:22.408: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 01/12/23 01:10:22.41
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 01:10:22.432
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/12/23 01:10:22.44
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 01:10:22.458
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/12/23 01:10:22.472
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 01:10:22.482
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:10:22.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-704" for this suite. 01/12/23 01:10:22.587
    STEP: Destroying namespace "webhook-704-markers" for this suite. 01/12/23 01:10:22.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:10:22.682
Jan 12 01:10:22.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename secrets 01/12/23 01:10:22.683
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:22.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:22.707
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-ea4bcd30-a29c-43ec-99ea-bffaf0a7a45d 01/12/23 01:10:22.71
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 01:10:22.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7576" for this suite. 01/12/23 01:10:22.715
------------------------------
• [0.086 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:10:22.682
    Jan 12 01:10:22.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename secrets 01/12/23 01:10:22.683
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:22.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:22.707
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-ea4bcd30-a29c-43ec-99ea-bffaf0a7a45d 01/12/23 01:10:22.71
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:10:22.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7576" for this suite. 01/12/23 01:10:22.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:10:22.772
Jan 12 01:10:22.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename job 01/12/23 01:10:22.772
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:22.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:22.798
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 01/12/23 01:10:22.801
STEP: Ensure pods equal to parallelism count is attached to the job 01/12/23 01:10:22.81
STEP: patching /status 01/12/23 01:10:26.815
STEP: updating /status 01/12/23 01:10:26.882
STEP: get /status 01/12/23 01:10:26.888
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 12 01:10:26.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5722" for this suite. 01/12/23 01:10:26.894
------------------------------
• [4.139 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:10:22.772
    Jan 12 01:10:22.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename job 01/12/23 01:10:22.772
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:22.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:22.798
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 01/12/23 01:10:22.801
    STEP: Ensure pods equal to parallelism count is attached to the job 01/12/23 01:10:22.81
    STEP: patching /status 01/12/23 01:10:26.815
    STEP: updating /status 01/12/23 01:10:26.882
    STEP: get /status 01/12/23 01:10:26.888
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:10:26.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5722" for this suite. 01/12/23 01:10:26.894
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:10:26.91
Jan 12 01:10:26.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename job 01/12/23 01:10:26.911
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:26.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:26.945
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 01/12/23 01:10:26.949
STEP: Patching the Job 01/12/23 01:10:26.97
STEP: Watching for Job to be patched 01/12/23 01:10:26.983
Jan 12 01:10:26.984: INFO: Event ADDED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 12 01:10:26.984: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 12 01:10:26.984: INFO: Event MODIFIED found for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/12/23 01:10:26.984
STEP: Watching for Job to be updated 01/12/23 01:10:27
Jan 12 01:10:27.001: INFO: Event MODIFIED found for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 12 01:10:27.001: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/12/23 01:10:27.001
Jan 12 01:10:27.004: INFO: Job: e2e-ftwv2 as labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2]
STEP: Waiting for job to complete 01/12/23 01:10:27.004
STEP: Delete a job collection with a labelselector 01/12/23 01:10:37.007
STEP: Watching for Job to be deleted 01/12/23 01:10:37.017
Jan 12 01:10:37.018: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 12 01:10:37.018: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 12 01:10:37.018: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 12 01:10:37.018: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 12 01:10:37.018: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 12 01:10:37.018: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 12 01:10:37.018: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 12 01:10:37.019: INFO: Event DELETED found for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/12/23 01:10:37.019
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 12 01:10:37.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3194" for this suite. 01/12/23 01:10:37.037
------------------------------
• [SLOW TEST] [10.161 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:10:26.91
    Jan 12 01:10:26.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename job 01/12/23 01:10:26.911
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:26.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:26.945
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 01/12/23 01:10:26.949
    STEP: Patching the Job 01/12/23 01:10:26.97
    STEP: Watching for Job to be patched 01/12/23 01:10:26.983
    Jan 12 01:10:26.984: INFO: Event ADDED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 12 01:10:26.984: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 12 01:10:26.984: INFO: Event MODIFIED found for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/12/23 01:10:26.984
    STEP: Watching for Job to be updated 01/12/23 01:10:27
    Jan 12 01:10:27.001: INFO: Event MODIFIED found for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 12 01:10:27.001: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/12/23 01:10:27.001
    Jan 12 01:10:27.004: INFO: Job: e2e-ftwv2 as labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2]
    STEP: Waiting for job to complete 01/12/23 01:10:27.004
    STEP: Delete a job collection with a labelselector 01/12/23 01:10:37.007
    STEP: Watching for Job to be deleted 01/12/23 01:10:37.017
    Jan 12 01:10:37.018: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 12 01:10:37.018: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 12 01:10:37.018: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 12 01:10:37.018: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 12 01:10:37.018: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 12 01:10:37.018: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 12 01:10:37.018: INFO: Event MODIFIED observed for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 12 01:10:37.019: INFO: Event DELETED found for Job e2e-ftwv2 in namespace job-3194 with labels: map[e2e-ftwv2:patched e2e-job-label:e2e-ftwv2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/12/23 01:10:37.019
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:10:37.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3194" for this suite. 01/12/23 01:10:37.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:10:37.072
Jan 12 01:10:37.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename crd-webhook 01/12/23 01:10:37.073
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:37.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:37.091
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/12/23 01:10:37.092
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/12/23 01:10:38.059
STEP: Deploying the custom resource conversion webhook pod 01/12/23 01:10:38.067
STEP: Wait for the deployment to be ready 01/12/23 01:10:38.112
Jan 12 01:10:38.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-crd-conversion-webhook-deployment-74ff66dd47\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:10:40.152: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 01:10:42.168
STEP: Verifying the service has paired with the endpoint 01/12/23 01:10:42.2
Jan 12 01:10:43.200: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 12 01:10:43.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Creating a v1 custom resource 01/12/23 01:10:45.795
STEP: v2 custom resource should be converted 01/12/23 01:10:45.803
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:10:46.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6266" for this suite. 01/12/23 01:10:46.419
------------------------------
• [SLOW TEST] [9.392 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:10:37.072
    Jan 12 01:10:37.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename crd-webhook 01/12/23 01:10:37.073
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:37.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:37.091
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/12/23 01:10:37.092
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/12/23 01:10:38.059
    STEP: Deploying the custom resource conversion webhook pod 01/12/23 01:10:38.067
    STEP: Wait for the deployment to be ready 01/12/23 01:10:38.112
    Jan 12 01:10:38.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-crd-conversion-webhook-deployment-74ff66dd47\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:10:40.152: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 10, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 01:10:42.168
    STEP: Verifying the service has paired with the endpoint 01/12/23 01:10:42.2
    Jan 12 01:10:43.200: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 12 01:10:43.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Creating a v1 custom resource 01/12/23 01:10:45.795
    STEP: v2 custom resource should be converted 01/12/23 01:10:45.803
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:10:46.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6266" for this suite. 01/12/23 01:10:46.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:10:46.472
Jan 12 01:10:46.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:10:46.473
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:46.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:46.501
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 01/12/23 01:10:46.503
Jan 12 01:10:46.588: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226" in namespace "projected-724" to be "Succeeded or Failed"
Jan 12 01:10:46.591: INFO: Pod "downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362015ms
Jan 12 01:10:48.595: INFO: Pod "downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006329311s
Jan 12 01:10:50.595: INFO: Pod "downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00701987s
Jan 12 01:10:52.595: INFO: Pod "downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006461679s
STEP: Saw pod success 01/12/23 01:10:52.595
Jan 12 01:10:52.595: INFO: Pod "downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226" satisfied condition "Succeeded or Failed"
Jan 12 01:10:52.597: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226 container client-container: <nil>
STEP: delete the pod 01/12/23 01:10:52.606
Jan 12 01:10:52.631: INFO: Waiting for pod downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226 to disappear
Jan 12 01:10:52.633: INFO: Pod downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 01:10:52.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-724" for this suite. 01/12/23 01:10:52.637
------------------------------
• [SLOW TEST] [6.184 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:10:46.472
    Jan 12 01:10:46.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:10:46.473
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:46.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:46.501
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 01/12/23 01:10:46.503
    Jan 12 01:10:46.588: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226" in namespace "projected-724" to be "Succeeded or Failed"
    Jan 12 01:10:46.591: INFO: Pod "downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362015ms
    Jan 12 01:10:48.595: INFO: Pod "downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006329311s
    Jan 12 01:10:50.595: INFO: Pod "downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00701987s
    Jan 12 01:10:52.595: INFO: Pod "downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006461679s
    STEP: Saw pod success 01/12/23 01:10:52.595
    Jan 12 01:10:52.595: INFO: Pod "downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226" satisfied condition "Succeeded or Failed"
    Jan 12 01:10:52.597: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226 container client-container: <nil>
    STEP: delete the pod 01/12/23 01:10:52.606
    Jan 12 01:10:52.631: INFO: Waiting for pod downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226 to disappear
    Jan 12 01:10:52.633: INFO: Pod downwardapi-volume-9da9edc0-c5e3-4b7d-91fc-f2a00419e226 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:10:52.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-724" for this suite. 01/12/23 01:10:52.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:10:52.659
Jan 12 01:10:52.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename svcaccounts 01/12/23 01:10:52.66
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:52.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:52.682
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 01/12/23 01:10:52.684
STEP: watching for the ServiceAccount to be added 01/12/23 01:10:52.693
STEP: patching the ServiceAccount 01/12/23 01:10:52.694
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/12/23 01:10:52.702
STEP: deleting the ServiceAccount 01/12/23 01:10:52.706
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 12 01:10:52.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2976" for this suite. 01/12/23 01:10:52.751
------------------------------
• [0.143 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:10:52.659
    Jan 12 01:10:52.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename svcaccounts 01/12/23 01:10:52.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:52.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:52.682
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 01/12/23 01:10:52.684
    STEP: watching for the ServiceAccount to be added 01/12/23 01:10:52.693
    STEP: patching the ServiceAccount 01/12/23 01:10:52.694
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/12/23 01:10:52.702
    STEP: deleting the ServiceAccount 01/12/23 01:10:52.706
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:10:52.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2976" for this suite. 01/12/23 01:10:52.751
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:10:52.803
Jan 12 01:10:52.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename podtemplate 01/12/23 01:10:52.804
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:52.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:52.83
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/12/23 01:10:52.832
STEP: Replace a pod template 01/12/23 01:10:52.843
Jan 12 01:10:52.852: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 12 01:10:52.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-1289" for this suite. 01/12/23 01:10:52.856
------------------------------
• [0.070 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:10:52.803
    Jan 12 01:10:52.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename podtemplate 01/12/23 01:10:52.804
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:52.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:52.83
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/12/23 01:10:52.832
    STEP: Replace a pod template 01/12/23 01:10:52.843
    Jan 12 01:10:52.852: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:10:52.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-1289" for this suite. 01/12/23 01:10:52.856
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:10:52.874
Jan 12 01:10:52.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:10:52.874
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:52.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:52.908
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-e5a49ea8-a233-4804-8312-d1a91c1e91ab 01/12/23 01:10:52.91
STEP: Creating a pod to test consume configMaps 01/12/23 01:10:52.918
Jan 12 01:10:52.958: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722" in namespace "projected-3941" to be "Succeeded or Failed"
Jan 12 01:10:52.961: INFO: Pod "pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722": Phase="Pending", Reason="", readiness=false. Elapsed: 2.420616ms
Jan 12 01:10:54.965: INFO: Pod "pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006629537s
Jan 12 01:10:56.965: INFO: Pod "pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006567769s
Jan 12 01:10:58.965: INFO: Pod "pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006344185s
STEP: Saw pod success 01/12/23 01:10:58.965
Jan 12 01:10:58.965: INFO: Pod "pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722" satisfied condition "Succeeded or Failed"
Jan 12 01:10:58.967: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 01:10:58.977
Jan 12 01:10:58.997: INFO: Waiting for pod pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722 to disappear
Jan 12 01:10:58.999: INFO: Pod pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:10:58.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3941" for this suite. 01/12/23 01:10:59.003
------------------------------
• [SLOW TEST] [6.150 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:10:52.874
    Jan 12 01:10:52.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:10:52.874
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:52.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:52.908
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-e5a49ea8-a233-4804-8312-d1a91c1e91ab 01/12/23 01:10:52.91
    STEP: Creating a pod to test consume configMaps 01/12/23 01:10:52.918
    Jan 12 01:10:52.958: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722" in namespace "projected-3941" to be "Succeeded or Failed"
    Jan 12 01:10:52.961: INFO: Pod "pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722": Phase="Pending", Reason="", readiness=false. Elapsed: 2.420616ms
    Jan 12 01:10:54.965: INFO: Pod "pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006629537s
    Jan 12 01:10:56.965: INFO: Pod "pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006567769s
    Jan 12 01:10:58.965: INFO: Pod "pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006344185s
    STEP: Saw pod success 01/12/23 01:10:58.965
    Jan 12 01:10:58.965: INFO: Pod "pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722" satisfied condition "Succeeded or Failed"
    Jan 12 01:10:58.967: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 01:10:58.977
    Jan 12 01:10:58.997: INFO: Waiting for pod pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722 to disappear
    Jan 12 01:10:58.999: INFO: Pod pod-projected-configmaps-98c6a9ca-b997-4788-975c-6f88ef5a3722 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:10:58.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3941" for this suite. 01/12/23 01:10:59.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:10:59.025
Jan 12 01:10:59.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename taint-multiple-pods 01/12/23 01:10:59.026
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:59.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:59.069
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jan 12 01:10:59.071: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 12 01:11:59.207: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jan 12 01:11:59.210: INFO: Starting informer...
STEP: Starting pods... 01/12/23 01:11:59.21
Jan 12 01:11:59.749: INFO: Pod1 is running on eqx04-flash06. Tainting Node
Jan 12 01:12:00.359: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3936" to be "running"
Jan 12 01:12:00.362: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.318564ms
Jan 12 01:12:02.366: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006311353s
Jan 12 01:12:02.366: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 12 01:12:02.366: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3936" to be "running"
Jan 12 01:12:02.368: INFO: Pod "taint-eviction-b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.696336ms
Jan 12 01:12:04.374: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00841779s
Jan 12 01:12:04.374: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 12 01:12:04.374: INFO: Pod2 is running on eqx04-flash06. Tainting Node
STEP: Trying to apply a taint on the Node 01/12/23 01:12:04.374
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 01:12:04.392
STEP: Waiting for Pod1 and Pod2 to be deleted 01/12/23 01:12:04.436
Jan 12 01:12:13.442: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 12 01:12:30.387: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 01:12:30.417
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:12:30.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-3936" for this suite. 01/12/23 01:12:30.455
------------------------------
• [SLOW TEST] [91.479 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:10:59.025
    Jan 12 01:10:59.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename taint-multiple-pods 01/12/23 01:10:59.026
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:10:59.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:10:59.069
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jan 12 01:10:59.071: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 12 01:11:59.207: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jan 12 01:11:59.210: INFO: Starting informer...
    STEP: Starting pods... 01/12/23 01:11:59.21
    Jan 12 01:11:59.749: INFO: Pod1 is running on eqx04-flash06. Tainting Node
    Jan 12 01:12:00.359: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3936" to be "running"
    Jan 12 01:12:00.362: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.318564ms
    Jan 12 01:12:02.366: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006311353s
    Jan 12 01:12:02.366: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 12 01:12:02.366: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3936" to be "running"
    Jan 12 01:12:02.368: INFO: Pod "taint-eviction-b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.696336ms
    Jan 12 01:12:04.374: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00841779s
    Jan 12 01:12:04.374: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 12 01:12:04.374: INFO: Pod2 is running on eqx04-flash06. Tainting Node
    STEP: Trying to apply a taint on the Node 01/12/23 01:12:04.374
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 01:12:04.392
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/12/23 01:12:04.436
    Jan 12 01:12:13.442: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 12 01:12:30.387: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 01:12:30.417
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:12:30.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-3936" for this suite. 01/12/23 01:12:30.455
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:12:30.505
Jan 12 01:12:30.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:12:30.506
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:12:30.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:12:30.537
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 01/12/23 01:12:30.539
Jan 12 01:12:30.645: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28" in namespace "projected-1547" to be "Succeeded or Failed"
Jan 12 01:12:30.648: INFO: Pod "downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.218382ms
Jan 12 01:12:32.652: INFO: Pod "downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006014085s
Jan 12 01:12:34.652: INFO: Pod "downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006651319s
Jan 12 01:12:36.651: INFO: Pod "downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005905129s
STEP: Saw pod success 01/12/23 01:12:36.651
Jan 12 01:12:36.652: INFO: Pod "downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28" satisfied condition "Succeeded or Failed"
Jan 12 01:12:36.654: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28 container client-container: <nil>
STEP: delete the pod 01/12/23 01:12:36.671
Jan 12 01:12:36.688: INFO: Waiting for pod downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28 to disappear
Jan 12 01:12:36.691: INFO: Pod downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 01:12:36.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1547" for this suite. 01/12/23 01:12:36.695
------------------------------
• [SLOW TEST] [6.205 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:12:30.505
    Jan 12 01:12:30.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:12:30.506
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:12:30.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:12:30.537
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 01/12/23 01:12:30.539
    Jan 12 01:12:30.645: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28" in namespace "projected-1547" to be "Succeeded or Failed"
    Jan 12 01:12:30.648: INFO: Pod "downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.218382ms
    Jan 12 01:12:32.652: INFO: Pod "downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006014085s
    Jan 12 01:12:34.652: INFO: Pod "downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006651319s
    Jan 12 01:12:36.651: INFO: Pod "downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005905129s
    STEP: Saw pod success 01/12/23 01:12:36.651
    Jan 12 01:12:36.652: INFO: Pod "downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28" satisfied condition "Succeeded or Failed"
    Jan 12 01:12:36.654: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28 container client-container: <nil>
    STEP: delete the pod 01/12/23 01:12:36.671
    Jan 12 01:12:36.688: INFO: Waiting for pod downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28 to disappear
    Jan 12 01:12:36.691: INFO: Pod downwardapi-volume-1ba9863d-64e6-451d-ab35-f0de071fac28 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:12:36.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1547" for this suite. 01/12/23 01:12:36.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:12:36.711
Jan 12 01:12:36.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 01:12:36.713
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:12:36.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:12:36.73
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 12 01:12:36.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:12:37.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7053" for this suite. 01/12/23 01:12:37.775
------------------------------
• [1.126 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:12:36.711
    Jan 12 01:12:36.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 01:12:36.713
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:12:36.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:12:36.73
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 12 01:12:36.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:12:37.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7053" for this suite. 01/12/23 01:12:37.775
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:12:37.842
Jan 12 01:12:37.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 01:12:37.843
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:12:37.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:12:37.872
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 01/12/23 01:12:37.874
STEP: fetching the ConfigMap 01/12/23 01:12:37.883
STEP: patching the ConfigMap 01/12/23 01:12:37.899
STEP: listing all ConfigMaps in all namespaces with a label selector 01/12/23 01:12:37.915
STEP: deleting the ConfigMap by collection with a label selector 01/12/23 01:12:37.92
STEP: listing all ConfigMaps in test namespace 01/12/23 01:12:37.931
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:12:37.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9571" for this suite. 01/12/23 01:12:37.94
------------------------------
• [0.130 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:12:37.842
    Jan 12 01:12:37.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 01:12:37.843
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:12:37.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:12:37.872
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 01/12/23 01:12:37.874
    STEP: fetching the ConfigMap 01/12/23 01:12:37.883
    STEP: patching the ConfigMap 01/12/23 01:12:37.899
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/12/23 01:12:37.915
    STEP: deleting the ConfigMap by collection with a label selector 01/12/23 01:12:37.92
    STEP: listing all ConfigMaps in test namespace 01/12/23 01:12:37.931
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:12:37.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9571" for this suite. 01/12/23 01:12:37.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:12:37.977
Jan 12 01:12:37.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 01:12:37.978
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:12:37.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:12:38
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 01:12:38.018
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:12:38.985
STEP: Deploying the webhook pod 01/12/23 01:12:39.002
STEP: Wait for the deployment to be ready 01/12/23 01:12:39.103
Jan 12 01:12:39.124: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:12:41.136: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:12:43.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 01:12:45.127
STEP: Verifying the service has paired with the endpoint 01/12/23 01:12:45.152
Jan 12 01:12:46.152: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/12/23 01:12:46.155
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/12/23 01:12:46.173
STEP: Creating a dummy validating-webhook-configuration object 01/12/23 01:12:46.189
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/12/23 01:12:46.197
STEP: Creating a dummy mutating-webhook-configuration object 01/12/23 01:12:46.204
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/12/23 01:12:46.216
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:12:46.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8477" for this suite. 01/12/23 01:12:46.338
STEP: Destroying namespace "webhook-8477-markers" for this suite. 01/12/23 01:12:46.385
------------------------------
• [SLOW TEST] [8.441 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:12:37.977
    Jan 12 01:12:37.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 01:12:37.978
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:12:37.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:12:38
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 01:12:38.018
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:12:38.985
    STEP: Deploying the webhook pod 01/12/23 01:12:39.002
    STEP: Wait for the deployment to be ready 01/12/23 01:12:39.103
    Jan 12 01:12:39.124: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:12:41.136: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:12:43.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 12, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 01:12:45.127
    STEP: Verifying the service has paired with the endpoint 01/12/23 01:12:45.152
    Jan 12 01:12:46.152: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/12/23 01:12:46.155
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/12/23 01:12:46.173
    STEP: Creating a dummy validating-webhook-configuration object 01/12/23 01:12:46.189
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/12/23 01:12:46.197
    STEP: Creating a dummy mutating-webhook-configuration object 01/12/23 01:12:46.204
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/12/23 01:12:46.216
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:12:46.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8477" for this suite. 01/12/23 01:12:46.338
    STEP: Destroying namespace "webhook-8477-markers" for this suite. 01/12/23 01:12:46.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:12:46.418
Jan 12 01:12:46.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename proxy 01/12/23 01:12:46.419
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:12:46.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:12:46.443
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 12 01:12:46.445: INFO: Creating pod...
Jan 12 01:12:46.500: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3595" to be "running"
Jan 12 01:12:46.503: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32163ms
Jan 12 01:12:48.507: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006403396s
Jan 12 01:12:50.508: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.007542429s
Jan 12 01:12:50.508: INFO: Pod "agnhost" satisfied condition "running"
Jan 12 01:12:50.508: INFO: Creating service...
Jan 12 01:12:50.538: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/pods/agnhost/proxy?method=DELETE
Jan 12 01:12:50.542: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 12 01:12:50.542: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/pods/agnhost/proxy?method=OPTIONS
Jan 12 01:12:50.544: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 12 01:12:50.544: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/pods/agnhost/proxy?method=PATCH
Jan 12 01:12:50.547: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 12 01:12:50.547: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/pods/agnhost/proxy?method=POST
Jan 12 01:12:50.550: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 12 01:12:50.550: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/pods/agnhost/proxy?method=PUT
Jan 12 01:12:50.552: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 12 01:12:50.552: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 12 01:12:50.557: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 12 01:12:50.557: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 12 01:12:50.560: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 12 01:12:50.561: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 12 01:12:50.564: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 12 01:12:50.564: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/services/e2e-proxy-test-service/proxy?method=POST
Jan 12 01:12:50.567: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 12 01:12:50.567: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/services/e2e-proxy-test-service/proxy?method=PUT
Jan 12 01:12:50.571: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 12 01:12:50.571: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/pods/agnhost/proxy?method=GET
Jan 12 01:12:50.573: INFO: http.Client request:GET StatusCode:301
Jan 12 01:12:50.573: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/services/e2e-proxy-test-service/proxy?method=GET
Jan 12 01:12:50.576: INFO: http.Client request:GET StatusCode:301
Jan 12 01:12:50.576: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/pods/agnhost/proxy?method=HEAD
Jan 12 01:12:50.578: INFO: http.Client request:HEAD StatusCode:301
Jan 12 01:12:50.578: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 12 01:12:50.581: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 12 01:12:50.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3595" for this suite. 01/12/23 01:12:50.584
------------------------------
• [4.240 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:12:46.418
    Jan 12 01:12:46.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename proxy 01/12/23 01:12:46.419
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:12:46.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:12:46.443
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 12 01:12:46.445: INFO: Creating pod...
    Jan 12 01:12:46.500: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3595" to be "running"
    Jan 12 01:12:46.503: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32163ms
    Jan 12 01:12:48.507: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006403396s
    Jan 12 01:12:50.508: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.007542429s
    Jan 12 01:12:50.508: INFO: Pod "agnhost" satisfied condition "running"
    Jan 12 01:12:50.508: INFO: Creating service...
    Jan 12 01:12:50.538: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/pods/agnhost/proxy?method=DELETE
    Jan 12 01:12:50.542: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 12 01:12:50.542: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/pods/agnhost/proxy?method=OPTIONS
    Jan 12 01:12:50.544: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 12 01:12:50.544: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/pods/agnhost/proxy?method=PATCH
    Jan 12 01:12:50.547: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 12 01:12:50.547: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/pods/agnhost/proxy?method=POST
    Jan 12 01:12:50.550: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 12 01:12:50.550: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/pods/agnhost/proxy?method=PUT
    Jan 12 01:12:50.552: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 12 01:12:50.552: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 12 01:12:50.557: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 12 01:12:50.557: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 12 01:12:50.560: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 12 01:12:50.561: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 12 01:12:50.564: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 12 01:12:50.564: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/services/e2e-proxy-test-service/proxy?method=POST
    Jan 12 01:12:50.567: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 12 01:12:50.567: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 12 01:12:50.571: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 12 01:12:50.571: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/pods/agnhost/proxy?method=GET
    Jan 12 01:12:50.573: INFO: http.Client request:GET StatusCode:301
    Jan 12 01:12:50.573: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/services/e2e-proxy-test-service/proxy?method=GET
    Jan 12 01:12:50.576: INFO: http.Client request:GET StatusCode:301
    Jan 12 01:12:50.576: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/pods/agnhost/proxy?method=HEAD
    Jan 12 01:12:50.578: INFO: http.Client request:HEAD StatusCode:301
    Jan 12 01:12:50.578: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3595/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 12 01:12:50.581: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:12:50.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3595" for this suite. 01/12/23 01:12:50.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:12:50.659
Jan 12 01:12:50.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pod-network-test 01/12/23 01:12:50.66
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:12:50.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:12:50.685
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-9211 01/12/23 01:12:50.687
STEP: creating a selector 01/12/23 01:12:50.687
STEP: Creating the service pods in kubernetes 01/12/23 01:12:50.687
Jan 12 01:12:50.687: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 12 01:12:50.977: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9211" to be "running and ready"
Jan 12 01:12:50.980: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.821807ms
Jan 12 01:12:50.980: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:12:52.983: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006104535s
Jan 12 01:12:52.983: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:12:54.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006151191s
Jan 12 01:12:54.983: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:12:56.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006004559s
Jan 12 01:12:56.983: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:12:58.984: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.00644423s
Jan 12 01:12:58.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:13:00.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005830962s
Jan 12 01:13:00.983: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:13:02.984: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.007072215s
Jan 12 01:13:02.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:13:04.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.006179338s
Jan 12 01:13:04.983: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:13:06.984: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.006263073s
Jan 12 01:13:06.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:13:08.984: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.00718648s
Jan 12 01:13:08.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:13:10.984: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00655172s
Jan 12 01:13:10.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:13:12.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005750205s
Jan 12 01:13:12.983: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 12 01:13:12.983: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 12 01:13:12.985: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9211" to be "running and ready"
Jan 12 01:13:12.988: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.285504ms
Jan 12 01:13:12.988: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 12 01:13:12.988: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/12/23 01:13:12.99
Jan 12 01:13:13.027: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9211" to be "running"
Jan 12 01:13:13.029: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.235436ms
Jan 12 01:13:15.033: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005879974s
Jan 12 01:13:17.044: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.01686779s
Jan 12 01:13:17.044: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 12 01:13:17.047: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 12 01:13:17.047: INFO: Breadth first check of 172.21.117.152 on host 10.9.140.106...
Jan 12 01:13:17.049: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.88.129:9080/dial?request=hostname&protocol=http&host=172.21.117.152&port=8083&tries=1'] Namespace:pod-network-test-9211 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:13:17.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:13:17.049: INFO: ExecWithOptions: Clientset creation
Jan 12 01:13:17.049: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-9211/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.88.129%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.21.117.152%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 12 01:13:17.182: INFO: Waiting for responses: map[]
Jan 12 01:13:17.182: INFO: reached 172.21.117.152 after 0/1 tries
Jan 12 01:13:17.182: INFO: Breadth first check of 172.21.88.156 on host 10.9.40.106...
Jan 12 01:13:17.185: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.88.129:9080/dial?request=hostname&protocol=http&host=172.21.88.156&port=8083&tries=1'] Namespace:pod-network-test-9211 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:13:17.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:13:17.185: INFO: ExecWithOptions: Clientset creation
Jan 12 01:13:17.185: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-9211/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.88.129%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.21.88.156%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 12 01:13:17.319: INFO: Waiting for responses: map[]
Jan 12 01:13:17.319: INFO: reached 172.21.88.156 after 0/1 tries
Jan 12 01:13:17.319: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 12 01:13:17.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9211" for this suite. 01/12/23 01:13:17.322
------------------------------
• [SLOW TEST] [26.682 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:12:50.659
    Jan 12 01:12:50.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pod-network-test 01/12/23 01:12:50.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:12:50.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:12:50.685
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-9211 01/12/23 01:12:50.687
    STEP: creating a selector 01/12/23 01:12:50.687
    STEP: Creating the service pods in kubernetes 01/12/23 01:12:50.687
    Jan 12 01:12:50.687: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 12 01:12:50.977: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9211" to be "running and ready"
    Jan 12 01:12:50.980: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.821807ms
    Jan 12 01:12:50.980: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:12:52.983: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006104535s
    Jan 12 01:12:52.983: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:12:54.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006151191s
    Jan 12 01:12:54.983: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:12:56.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006004559s
    Jan 12 01:12:56.983: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:12:58.984: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.00644423s
    Jan 12 01:12:58.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:13:00.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005830962s
    Jan 12 01:13:00.983: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:13:02.984: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.007072215s
    Jan 12 01:13:02.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:13:04.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.006179338s
    Jan 12 01:13:04.983: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:13:06.984: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.006263073s
    Jan 12 01:13:06.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:13:08.984: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.00718648s
    Jan 12 01:13:08.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:13:10.984: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00655172s
    Jan 12 01:13:10.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:13:12.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005750205s
    Jan 12 01:13:12.983: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 12 01:13:12.983: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 12 01:13:12.985: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9211" to be "running and ready"
    Jan 12 01:13:12.988: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.285504ms
    Jan 12 01:13:12.988: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 12 01:13:12.988: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/12/23 01:13:12.99
    Jan 12 01:13:13.027: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9211" to be "running"
    Jan 12 01:13:13.029: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.235436ms
    Jan 12 01:13:15.033: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005879974s
    Jan 12 01:13:17.044: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.01686779s
    Jan 12 01:13:17.044: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 12 01:13:17.047: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 12 01:13:17.047: INFO: Breadth first check of 172.21.117.152 on host 10.9.140.106...
    Jan 12 01:13:17.049: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.88.129:9080/dial?request=hostname&protocol=http&host=172.21.117.152&port=8083&tries=1'] Namespace:pod-network-test-9211 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:13:17.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:13:17.049: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:13:17.049: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-9211/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.88.129%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.21.117.152%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 12 01:13:17.182: INFO: Waiting for responses: map[]
    Jan 12 01:13:17.182: INFO: reached 172.21.117.152 after 0/1 tries
    Jan 12 01:13:17.182: INFO: Breadth first check of 172.21.88.156 on host 10.9.40.106...
    Jan 12 01:13:17.185: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.88.129:9080/dial?request=hostname&protocol=http&host=172.21.88.156&port=8083&tries=1'] Namespace:pod-network-test-9211 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:13:17.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:13:17.185: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:13:17.185: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-9211/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.88.129%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.21.88.156%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 12 01:13:17.319: INFO: Waiting for responses: map[]
    Jan 12 01:13:17.319: INFO: reached 172.21.88.156 after 0/1 tries
    Jan 12 01:13:17.319: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:13:17.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9211" for this suite. 01/12/23 01:13:17.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:13:17.343
Jan 12 01:13:17.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 01:13:17.344
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:17.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:17.381
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jan 12 01:13:17.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6811 version'
Jan 12 01:13:17.460: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 12 01:13:17.460: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:51:45Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 01:13:17.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6811" for this suite. 01/12/23 01:13:17.465
------------------------------
• [0.143 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:13:17.343
    Jan 12 01:13:17.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 01:13:17.344
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:17.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:17.381
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jan 12 01:13:17.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6811 version'
    Jan 12 01:13:17.460: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 12 01:13:17.460: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:51:45Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:13:17.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6811" for this suite. 01/12/23 01:13:17.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:13:17.487
Jan 12 01:13:17.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename gc 01/12/23 01:13:17.488
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:17.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:17.505
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 12 01:13:17.655: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"56c36d92-95f4-41f6-bb33-54d817282436", Controller:(*bool)(0xc004cf508a), BlockOwnerDeletion:(*bool)(0xc004cf508b)}}
Jan 12 01:13:17.662: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a2f034dc-54ba-48e7-90de-255b4fc099ca", Controller:(*bool)(0xc004cf5332), BlockOwnerDeletion:(*bool)(0xc004cf5333)}}
Jan 12 01:13:17.677: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"63c7c326-1df1-4e24-b181-231461ebb298", Controller:(*bool)(0xc004b809a2), BlockOwnerDeletion:(*bool)(0xc004b809a3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 12 01:13:22.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3073" for this suite. 01/12/23 01:13:22.724
------------------------------
• [SLOW TEST] [5.328 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:13:17.487
    Jan 12 01:13:17.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename gc 01/12/23 01:13:17.488
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:17.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:17.505
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 12 01:13:17.655: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"56c36d92-95f4-41f6-bb33-54d817282436", Controller:(*bool)(0xc004cf508a), BlockOwnerDeletion:(*bool)(0xc004cf508b)}}
    Jan 12 01:13:17.662: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a2f034dc-54ba-48e7-90de-255b4fc099ca", Controller:(*bool)(0xc004cf5332), BlockOwnerDeletion:(*bool)(0xc004cf5333)}}
    Jan 12 01:13:17.677: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"63c7c326-1df1-4e24-b181-231461ebb298", Controller:(*bool)(0xc004b809a2), BlockOwnerDeletion:(*bool)(0xc004b809a3)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:13:22.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3073" for this suite. 01/12/23 01:13:22.724
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:13:22.815
Jan 12 01:13:22.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 01:13:22.817
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:22.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:22.889
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 01/12/23 01:13:22.891
Jan 12 01:13:22.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 12 01:13:23.185: INFO: stderr: ""
Jan 12 01:13:23.185: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 01/12/23 01:13:23.185
Jan 12 01:13:23.185: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 12 01:13:23.185: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8059" to be "running and ready, or succeeded"
Jan 12 01:13:23.187: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.607825ms
Jan 12 01:13:23.187: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
Jan 12 01:13:25.228: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043234747s
Jan 12 01:13:25.228: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'eqx04-flash06' to be 'Running' but was 'Pending'
Jan 12 01:13:27.194: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.009307831s
Jan 12 01:13:27.194: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 12 01:13:27.194: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/12/23 01:13:27.194
Jan 12 01:13:27.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 logs logs-generator logs-generator'
Jan 12 01:13:27.275: INFO: stderr: ""
Jan 12 01:13:27.275: INFO: stdout: "I0112 01:13:25.826166       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/pfm 489\nI0112 01:13:26.026517       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/fq4g 576\nI0112 01:13:26.226797       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/prs 463\nI0112 01:13:26.427092       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/z6ph 467\nI0112 01:13:26.626262       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/v9n 542\nI0112 01:13:26.826556       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/b2f 208\nI0112 01:13:27.026925       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/wqqg 253\nI0112 01:13:27.227160       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/xhj5 384\n"
STEP: limiting log lines 01/12/23 01:13:27.275
Jan 12 01:13:27.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 logs logs-generator logs-generator --tail=1'
Jan 12 01:13:27.360: INFO: stderr: ""
Jan 12 01:13:27.360: INFO: stdout: "I0112 01:13:27.227160       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/xhj5 384\n"
Jan 12 01:13:27.360: INFO: got output "I0112 01:13:27.227160       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/xhj5 384\n"
STEP: limiting log bytes 01/12/23 01:13:27.36
Jan 12 01:13:27.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 logs logs-generator logs-generator --limit-bytes=1'
Jan 12 01:13:27.436: INFO: stderr: ""
Jan 12 01:13:27.436: INFO: stdout: "I"
Jan 12 01:13:27.436: INFO: got output "I"
STEP: exposing timestamps 01/12/23 01:13:27.436
Jan 12 01:13:27.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 12 01:13:27.516: INFO: stderr: ""
Jan 12 01:13:27.516: INFO: stdout: "2023-01-12T01:13:27.426559707Z I0112 01:13:27.426476       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4dsj 344\n"
Jan 12 01:13:27.516: INFO: got output "2023-01-12T01:13:27.426559707Z I0112 01:13:27.426476       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4dsj 344\n"
STEP: restricting to a time range 01/12/23 01:13:27.516
Jan 12 01:13:30.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 logs logs-generator logs-generator --since=1s'
Jan 12 01:13:30.103: INFO: stderr: ""
Jan 12 01:13:30.103: INFO: stdout: "I0112 01:13:29.226353       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/wbn 218\nI0112 01:13:29.426630       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/2hh 406\nI0112 01:13:29.626963       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/nwhw 367\nI0112 01:13:29.826211       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/br67 261\nI0112 01:13:30.026525       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/2rb6 585\n"
Jan 12 01:13:30.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 logs logs-generator logs-generator --since=24h'
Jan 12 01:13:30.180: INFO: stderr: ""
Jan 12 01:13:30.180: INFO: stdout: "I0112 01:13:25.826166       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/pfm 489\nI0112 01:13:26.026517       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/fq4g 576\nI0112 01:13:26.226797       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/prs 463\nI0112 01:13:26.427092       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/z6ph 467\nI0112 01:13:26.626262       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/v9n 542\nI0112 01:13:26.826556       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/b2f 208\nI0112 01:13:27.026925       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/wqqg 253\nI0112 01:13:27.227160       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/xhj5 384\nI0112 01:13:27.426476       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4dsj 344\nI0112 01:13:27.626672       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/748 550\nI0112 01:13:27.826961       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/ncrr 398\nI0112 01:13:28.027149       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/6g2 413\nI0112 01:13:28.226391       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/57cg 488\nI0112 01:13:28.426685       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/86l 270\nI0112 01:13:28.626901       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/dhd5 461\nI0112 01:13:28.827192       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/2pmp 257\nI0112 01:13:29.026512       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/gdm4 534\nI0112 01:13:29.226353       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/wbn 218\nI0112 01:13:29.426630       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/2hh 406\nI0112 01:13:29.626963       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/nwhw 367\nI0112 01:13:29.826211       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/br67 261\nI0112 01:13:30.026525       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/2rb6 585\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jan 12 01:13:30.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 delete pod logs-generator'
Jan 12 01:13:31.195: INFO: stderr: ""
Jan 12 01:13:31.195: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 01:13:31.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8059" for this suite. 01/12/23 01:13:31.199
------------------------------
• [SLOW TEST] [8.408 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:13:22.815
    Jan 12 01:13:22.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 01:13:22.817
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:22.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:22.889
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 01/12/23 01:13:22.891
    Jan 12 01:13:22.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 12 01:13:23.185: INFO: stderr: ""
    Jan 12 01:13:23.185: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 01/12/23 01:13:23.185
    Jan 12 01:13:23.185: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 12 01:13:23.185: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8059" to be "running and ready, or succeeded"
    Jan 12 01:13:23.187: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.607825ms
    Jan 12 01:13:23.187: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
    Jan 12 01:13:25.228: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043234747s
    Jan 12 01:13:25.228: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'eqx04-flash06' to be 'Running' but was 'Pending'
    Jan 12 01:13:27.194: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.009307831s
    Jan 12 01:13:27.194: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 12 01:13:27.194: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/12/23 01:13:27.194
    Jan 12 01:13:27.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 logs logs-generator logs-generator'
    Jan 12 01:13:27.275: INFO: stderr: ""
    Jan 12 01:13:27.275: INFO: stdout: "I0112 01:13:25.826166       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/pfm 489\nI0112 01:13:26.026517       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/fq4g 576\nI0112 01:13:26.226797       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/prs 463\nI0112 01:13:26.427092       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/z6ph 467\nI0112 01:13:26.626262       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/v9n 542\nI0112 01:13:26.826556       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/b2f 208\nI0112 01:13:27.026925       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/wqqg 253\nI0112 01:13:27.227160       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/xhj5 384\n"
    STEP: limiting log lines 01/12/23 01:13:27.275
    Jan 12 01:13:27.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 logs logs-generator logs-generator --tail=1'
    Jan 12 01:13:27.360: INFO: stderr: ""
    Jan 12 01:13:27.360: INFO: stdout: "I0112 01:13:27.227160       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/xhj5 384\n"
    Jan 12 01:13:27.360: INFO: got output "I0112 01:13:27.227160       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/xhj5 384\n"
    STEP: limiting log bytes 01/12/23 01:13:27.36
    Jan 12 01:13:27.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 logs logs-generator logs-generator --limit-bytes=1'
    Jan 12 01:13:27.436: INFO: stderr: ""
    Jan 12 01:13:27.436: INFO: stdout: "I"
    Jan 12 01:13:27.436: INFO: got output "I"
    STEP: exposing timestamps 01/12/23 01:13:27.436
    Jan 12 01:13:27.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 12 01:13:27.516: INFO: stderr: ""
    Jan 12 01:13:27.516: INFO: stdout: "2023-01-12T01:13:27.426559707Z I0112 01:13:27.426476       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4dsj 344\n"
    Jan 12 01:13:27.516: INFO: got output "2023-01-12T01:13:27.426559707Z I0112 01:13:27.426476       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4dsj 344\n"
    STEP: restricting to a time range 01/12/23 01:13:27.516
    Jan 12 01:13:30.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 logs logs-generator logs-generator --since=1s'
    Jan 12 01:13:30.103: INFO: stderr: ""
    Jan 12 01:13:30.103: INFO: stdout: "I0112 01:13:29.226353       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/wbn 218\nI0112 01:13:29.426630       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/2hh 406\nI0112 01:13:29.626963       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/nwhw 367\nI0112 01:13:29.826211       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/br67 261\nI0112 01:13:30.026525       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/2rb6 585\n"
    Jan 12 01:13:30.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 logs logs-generator logs-generator --since=24h'
    Jan 12 01:13:30.180: INFO: stderr: ""
    Jan 12 01:13:30.180: INFO: stdout: "I0112 01:13:25.826166       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/pfm 489\nI0112 01:13:26.026517       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/fq4g 576\nI0112 01:13:26.226797       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/prs 463\nI0112 01:13:26.427092       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/z6ph 467\nI0112 01:13:26.626262       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/v9n 542\nI0112 01:13:26.826556       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/b2f 208\nI0112 01:13:27.026925       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/wqqg 253\nI0112 01:13:27.227160       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/xhj5 384\nI0112 01:13:27.426476       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4dsj 344\nI0112 01:13:27.626672       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/748 550\nI0112 01:13:27.826961       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/ncrr 398\nI0112 01:13:28.027149       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/6g2 413\nI0112 01:13:28.226391       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/57cg 488\nI0112 01:13:28.426685       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/86l 270\nI0112 01:13:28.626901       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/dhd5 461\nI0112 01:13:28.827192       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/2pmp 257\nI0112 01:13:29.026512       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/gdm4 534\nI0112 01:13:29.226353       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/wbn 218\nI0112 01:13:29.426630       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/2hh 406\nI0112 01:13:29.626963       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/nwhw 367\nI0112 01:13:29.826211       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/br67 261\nI0112 01:13:30.026525       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/2rb6 585\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jan 12 01:13:30.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-8059 delete pod logs-generator'
    Jan 12 01:13:31.195: INFO: stderr: ""
    Jan 12 01:13:31.195: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:13:31.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8059" for this suite. 01/12/23 01:13:31.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:13:31.225
Jan 12 01:13:31.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 01:13:31.225
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:31.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:31.249
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 12 01:13:31.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:13:31.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9226" for this suite. 01/12/23 01:13:31.853
------------------------------
• [0.653 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:13:31.225
    Jan 12 01:13:31.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 01:13:31.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:31.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:31.249
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 12 01:13:31.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:13:31.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9226" for this suite. 01/12/23 01:13:31.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:13:31.878
Jan 12 01:13:31.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:13:31.879
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:31.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:31.901
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-8ae2f64a-7c12-4eb6-b57c-ac4bc9a07991 01/12/23 01:13:31.903
STEP: Creating a pod to test consume secrets 01/12/23 01:13:31.918
Jan 12 01:13:31.981: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d" in namespace "projected-9236" to be "Succeeded or Failed"
Jan 12 01:13:31.983: INFO: Pod "pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.400587ms
Jan 12 01:13:33.987: INFO: Pod "pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006075934s
Jan 12 01:13:36.012: INFO: Pod "pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031061111s
Jan 12 01:13:37.986: INFO: Pod "pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005744773s
STEP: Saw pod success 01/12/23 01:13:37.986
Jan 12 01:13:37.986: INFO: Pod "pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d" satisfied condition "Succeeded or Failed"
Jan 12 01:13:37.989: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d container projected-secret-volume-test: <nil>
STEP: delete the pod 01/12/23 01:13:37.996
Jan 12 01:13:38.016: INFO: Waiting for pod pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d to disappear
Jan 12 01:13:38.018: INFO: Pod pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 12 01:13:38.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9236" for this suite. 01/12/23 01:13:38.022
------------------------------
• [SLOW TEST] [6.193 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:13:31.878
    Jan 12 01:13:31.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:13:31.879
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:31.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:31.901
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-8ae2f64a-7c12-4eb6-b57c-ac4bc9a07991 01/12/23 01:13:31.903
    STEP: Creating a pod to test consume secrets 01/12/23 01:13:31.918
    Jan 12 01:13:31.981: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d" in namespace "projected-9236" to be "Succeeded or Failed"
    Jan 12 01:13:31.983: INFO: Pod "pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.400587ms
    Jan 12 01:13:33.987: INFO: Pod "pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006075934s
    Jan 12 01:13:36.012: INFO: Pod "pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031061111s
    Jan 12 01:13:37.986: INFO: Pod "pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005744773s
    STEP: Saw pod success 01/12/23 01:13:37.986
    Jan 12 01:13:37.986: INFO: Pod "pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d" satisfied condition "Succeeded or Failed"
    Jan 12 01:13:37.989: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 01:13:37.996
    Jan 12 01:13:38.016: INFO: Waiting for pod pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d to disappear
    Jan 12 01:13:38.018: INFO: Pod pod-projected-secrets-0c7b84ca-3984-4d95-a4c9-5d70ee77284d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:13:38.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9236" for this suite. 01/12/23 01:13:38.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:13:38.073
Jan 12 01:13:38.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:13:38.074
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:38.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:38.097
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-7c699680-9513-4a92-a7e8-73573f525801 01/12/23 01:13:38.099
STEP: Creating a pod to test consume secrets 01/12/23 01:13:38.112
Jan 12 01:13:38.295: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f" in namespace "projected-5655" to be "Succeeded or Failed"
Jan 12 01:13:38.297: INFO: Pod "pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358811ms
Jan 12 01:13:40.302: INFO: Pod "pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006829219s
Jan 12 01:13:42.301: INFO: Pod "pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006183266s
Jan 12 01:13:44.302: INFO: Pod "pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006713688s
STEP: Saw pod success 01/12/23 01:13:44.302
Jan 12 01:13:44.302: INFO: Pod "pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f" satisfied condition "Succeeded or Failed"
Jan 12 01:13:44.304: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f container projected-secret-volume-test: <nil>
STEP: delete the pod 01/12/23 01:13:44.312
Jan 12 01:13:44.347: INFO: Waiting for pod pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f to disappear
Jan 12 01:13:44.350: INFO: Pod pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 12 01:13:44.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5655" for this suite. 01/12/23 01:13:44.354
------------------------------
• [SLOW TEST] [6.299 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:13:38.073
    Jan 12 01:13:38.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:13:38.074
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:38.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:38.097
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-7c699680-9513-4a92-a7e8-73573f525801 01/12/23 01:13:38.099
    STEP: Creating a pod to test consume secrets 01/12/23 01:13:38.112
    Jan 12 01:13:38.295: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f" in namespace "projected-5655" to be "Succeeded or Failed"
    Jan 12 01:13:38.297: INFO: Pod "pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358811ms
    Jan 12 01:13:40.302: INFO: Pod "pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006829219s
    Jan 12 01:13:42.301: INFO: Pod "pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006183266s
    Jan 12 01:13:44.302: INFO: Pod "pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006713688s
    STEP: Saw pod success 01/12/23 01:13:44.302
    Jan 12 01:13:44.302: INFO: Pod "pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f" satisfied condition "Succeeded or Failed"
    Jan 12 01:13:44.304: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 01:13:44.312
    Jan 12 01:13:44.347: INFO: Waiting for pod pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f to disappear
    Jan 12 01:13:44.350: INFO: Pod pod-projected-secrets-ff9f4d56-56ab-4b35-9a7d-e96c0509457f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:13:44.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5655" for this suite. 01/12/23 01:13:44.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:13:44.373
Jan 12 01:13:44.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 01:13:44.374
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:44.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:44.399
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8952 01/12/23 01:13:44.401
STEP: changing the ExternalName service to type=NodePort 01/12/23 01:13:44.418
STEP: creating replication controller externalname-service in namespace services-8952 01/12/23 01:13:44.47
I0112 01:13:44.480220      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8952, replica count: 2
I0112 01:13:47.530981      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 01:13:47.531: INFO: Creating new exec pod
Jan 12 01:13:47.565: INFO: Waiting up to 5m0s for pod "execpodqdznl" in namespace "services-8952" to be "running"
Jan 12 01:13:47.567: INFO: Pod "execpodqdznl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.324479ms
Jan 12 01:13:49.570: INFO: Pod "execpodqdznl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005408321s
Jan 12 01:13:51.571: INFO: Pod "execpodqdznl": Phase="Running", Reason="", readiness=true. Elapsed: 4.006710537s
Jan 12 01:13:51.571: INFO: Pod "execpodqdznl" satisfied condition "running"
Jan 12 01:13:52.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-8952 exec execpodqdznl -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 12 01:13:52.786: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 12 01:13:52.786: INFO: stdout: ""
Jan 12 01:13:52.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-8952 exec execpodqdznl -- /bin/sh -x -c nc -v -z -w 2 172.19.149.230 80'
Jan 12 01:13:53.053: INFO: stderr: "+ nc -v -z -w 2 172.19.149.230 80\nConnection to 172.19.149.230 80 port [tcp/http] succeeded!\n"
Jan 12 01:13:53.053: INFO: stdout: ""
Jan 12 01:13:53.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-8952 exec execpodqdznl -- /bin/sh -x -c nc -v -z -w 2 10.9.140.106 30409'
Jan 12 01:13:53.284: INFO: stderr: "+ nc -v -z -w 2 10.9.140.106 30409\nConnection to 10.9.140.106 30409 port [tcp/*] succeeded!\n"
Jan 12 01:13:53.284: INFO: stdout: ""
Jan 12 01:13:53.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-8952 exec execpodqdznl -- /bin/sh -x -c nc -v -z -w 2 10.9.40.106 30409'
Jan 12 01:13:53.547: INFO: stderr: "+ nc -v -z -w 2 10.9.40.106 30409\nConnection to 10.9.40.106 30409 port [tcp/*] succeeded!\n"
Jan 12 01:13:53.547: INFO: stdout: ""
Jan 12 01:13:53.547: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 01:13:53.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8952" for this suite. 01/12/23 01:13:53.622
------------------------------
• [SLOW TEST] [9.312 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:13:44.373
    Jan 12 01:13:44.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 01:13:44.374
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:44.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:44.399
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8952 01/12/23 01:13:44.401
    STEP: changing the ExternalName service to type=NodePort 01/12/23 01:13:44.418
    STEP: creating replication controller externalname-service in namespace services-8952 01/12/23 01:13:44.47
    I0112 01:13:44.480220      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8952, replica count: 2
    I0112 01:13:47.530981      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 01:13:47.531: INFO: Creating new exec pod
    Jan 12 01:13:47.565: INFO: Waiting up to 5m0s for pod "execpodqdznl" in namespace "services-8952" to be "running"
    Jan 12 01:13:47.567: INFO: Pod "execpodqdznl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.324479ms
    Jan 12 01:13:49.570: INFO: Pod "execpodqdznl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005408321s
    Jan 12 01:13:51.571: INFO: Pod "execpodqdznl": Phase="Running", Reason="", readiness=true. Elapsed: 4.006710537s
    Jan 12 01:13:51.571: INFO: Pod "execpodqdznl" satisfied condition "running"
    Jan 12 01:13:52.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-8952 exec execpodqdznl -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 12 01:13:52.786: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 12 01:13:52.786: INFO: stdout: ""
    Jan 12 01:13:52.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-8952 exec execpodqdznl -- /bin/sh -x -c nc -v -z -w 2 172.19.149.230 80'
    Jan 12 01:13:53.053: INFO: stderr: "+ nc -v -z -w 2 172.19.149.230 80\nConnection to 172.19.149.230 80 port [tcp/http] succeeded!\n"
    Jan 12 01:13:53.053: INFO: stdout: ""
    Jan 12 01:13:53.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-8952 exec execpodqdznl -- /bin/sh -x -c nc -v -z -w 2 10.9.140.106 30409'
    Jan 12 01:13:53.284: INFO: stderr: "+ nc -v -z -w 2 10.9.140.106 30409\nConnection to 10.9.140.106 30409 port [tcp/*] succeeded!\n"
    Jan 12 01:13:53.284: INFO: stdout: ""
    Jan 12 01:13:53.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-8952 exec execpodqdznl -- /bin/sh -x -c nc -v -z -w 2 10.9.40.106 30409'
    Jan 12 01:13:53.547: INFO: stderr: "+ nc -v -z -w 2 10.9.40.106 30409\nConnection to 10.9.40.106 30409 port [tcp/*] succeeded!\n"
    Jan 12 01:13:53.547: INFO: stdout: ""
    Jan 12 01:13:53.547: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:13:53.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8952" for this suite. 01/12/23 01:13:53.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:13:53.688
Jan 12 01:13:53.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pods 01/12/23 01:13:53.69
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:53.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:53.713
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 01/12/23 01:13:53.738
STEP: watching for Pod to be ready 01/12/23 01:13:53.867
Jan 12 01:13:53.869: INFO: observed Pod pod-test in namespace pods-4011 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 12 01:13:54.319: INFO: observed Pod pod-test in namespace pods-4011 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  }]
Jan 12 01:13:54.353: INFO: observed Pod pod-test in namespace pods-4011 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  }]
Jan 12 01:13:55.597: INFO: observed Pod pod-test in namespace pods-4011 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  }]
Jan 12 01:13:55.641: INFO: observed Pod pod-test in namespace pods-4011 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  }]
Jan 12 01:13:56.309: INFO: Found Pod pod-test in namespace pods-4011 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/12/23 01:13:56.312
STEP: getting the Pod and ensuring that it's patched 01/12/23 01:13:56.324
STEP: replacing the Pod's status Ready condition to False 01/12/23 01:13:56.326
STEP: check the Pod again to ensure its Ready conditions are False 01/12/23 01:13:56.357
STEP: deleting the Pod via a Collection with a LabelSelector 01/12/23 01:13:56.357
STEP: watching for the Pod to be deleted 01/12/23 01:13:56.369
Jan 12 01:13:56.370: INFO: observed event type MODIFIED
Jan 12 01:13:58.357: INFO: observed event type MODIFIED
Jan 12 01:13:59.073: INFO: observed event type MODIFIED
Jan 12 01:14:04.129: INFO: observed event type MODIFIED
Jan 12 01:14:04.136: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 01:14:04.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4011" for this suite. 01/12/23 01:14:04.161
------------------------------
• [SLOW TEST] [10.616 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:13:53.688
    Jan 12 01:13:53.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pods 01/12/23 01:13:53.69
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:13:53.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:13:53.713
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 01/12/23 01:13:53.738
    STEP: watching for Pod to be ready 01/12/23 01:13:53.867
    Jan 12 01:13:53.869: INFO: observed Pod pod-test in namespace pods-4011 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 12 01:13:54.319: INFO: observed Pod pod-test in namespace pods-4011 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  }]
    Jan 12 01:13:54.353: INFO: observed Pod pod-test in namespace pods-4011 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  }]
    Jan 12 01:13:55.597: INFO: observed Pod pod-test in namespace pods-4011 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  }]
    Jan 12 01:13:55.641: INFO: observed Pod pod-test in namespace pods-4011 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  }]
    Jan 12 01:13:56.309: INFO: Found Pod pod-test in namespace pods-4011 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:13:54 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/12/23 01:13:56.312
    STEP: getting the Pod and ensuring that it's patched 01/12/23 01:13:56.324
    STEP: replacing the Pod's status Ready condition to False 01/12/23 01:13:56.326
    STEP: check the Pod again to ensure its Ready conditions are False 01/12/23 01:13:56.357
    STEP: deleting the Pod via a Collection with a LabelSelector 01/12/23 01:13:56.357
    STEP: watching for the Pod to be deleted 01/12/23 01:13:56.369
    Jan 12 01:13:56.370: INFO: observed event type MODIFIED
    Jan 12 01:13:58.357: INFO: observed event type MODIFIED
    Jan 12 01:13:59.073: INFO: observed event type MODIFIED
    Jan 12 01:14:04.129: INFO: observed event type MODIFIED
    Jan 12 01:14:04.136: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:14:04.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4011" for this suite. 01/12/23 01:14:04.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:14:04.305
Jan 12 01:14:04.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename replication-controller 01/12/23 01:14:04.305
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:14:04.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:14:04.337
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-nq4pn" 01/12/23 01:14:04.339
Jan 12 01:14:04.354: INFO: Get Replication Controller "e2e-rc-nq4pn" to confirm replicas
Jan 12 01:14:05.366: INFO: Get Replication Controller "e2e-rc-nq4pn" to confirm replicas
Jan 12 01:14:05.369: INFO: Found 1 replicas for "e2e-rc-nq4pn" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-nq4pn" 01/12/23 01:14:05.369
STEP: Updating a scale subresource 01/12/23 01:14:05.371
STEP: Verifying replicas where modified for replication controller "e2e-rc-nq4pn" 01/12/23 01:14:05.38
Jan 12 01:14:05.380: INFO: Get Replication Controller "e2e-rc-nq4pn" to confirm replicas
Jan 12 01:14:06.383: INFO: Get Replication Controller "e2e-rc-nq4pn" to confirm replicas
Jan 12 01:14:06.386: INFO: Found 2 replicas for "e2e-rc-nq4pn" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 12 01:14:06.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7995" for this suite. 01/12/23 01:14:06.39
------------------------------
• [2.102 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:14:04.305
    Jan 12 01:14:04.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename replication-controller 01/12/23 01:14:04.305
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:14:04.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:14:04.337
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-nq4pn" 01/12/23 01:14:04.339
    Jan 12 01:14:04.354: INFO: Get Replication Controller "e2e-rc-nq4pn" to confirm replicas
    Jan 12 01:14:05.366: INFO: Get Replication Controller "e2e-rc-nq4pn" to confirm replicas
    Jan 12 01:14:05.369: INFO: Found 1 replicas for "e2e-rc-nq4pn" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-nq4pn" 01/12/23 01:14:05.369
    STEP: Updating a scale subresource 01/12/23 01:14:05.371
    STEP: Verifying replicas where modified for replication controller "e2e-rc-nq4pn" 01/12/23 01:14:05.38
    Jan 12 01:14:05.380: INFO: Get Replication Controller "e2e-rc-nq4pn" to confirm replicas
    Jan 12 01:14:06.383: INFO: Get Replication Controller "e2e-rc-nq4pn" to confirm replicas
    Jan 12 01:14:06.386: INFO: Found 2 replicas for "e2e-rc-nq4pn" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:14:06.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7995" for this suite. 01/12/23 01:14:06.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:14:06.408
Jan 12 01:14:06.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename replication-controller 01/12/23 01:14:06.409
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:14:06.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:14:06.432
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937 01/12/23 01:14:06.434
Jan 12 01:14:06.447: INFO: Pod name my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937: Found 0 pods out of 1
Jan 12 01:14:11.452: INFO: Pod name my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937: Found 1 pods out of 1
Jan 12 01:14:11.452: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937" are running
Jan 12 01:14:11.452: INFO: Waiting up to 5m0s for pod "my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937-9ckwd" in namespace "replication-controller-2357" to be "running"
Jan 12 01:14:11.454: INFO: Pod "my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937-9ckwd": Phase="Running", Reason="", readiness=true. Elapsed: 2.283598ms
Jan 12 01:14:11.454: INFO: Pod "my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937-9ckwd" satisfied condition "running"
Jan 12 01:14:11.454: INFO: Pod "my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937-9ckwd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:14:06 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:14:08 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:14:08 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:14:06 +0000 UTC Reason: Message:}])
Jan 12 01:14:11.454: INFO: Trying to dial the pod
Jan 12 01:14:16.463: INFO: Controller my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937: Got expected result from replica 1 [my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937-9ckwd]: "my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937-9ckwd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 12 01:14:16.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2357" for this suite. 01/12/23 01:14:16.467
------------------------------
• [SLOW TEST] [10.088 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:14:06.408
    Jan 12 01:14:06.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename replication-controller 01/12/23 01:14:06.409
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:14:06.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:14:06.432
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937 01/12/23 01:14:06.434
    Jan 12 01:14:06.447: INFO: Pod name my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937: Found 0 pods out of 1
    Jan 12 01:14:11.452: INFO: Pod name my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937: Found 1 pods out of 1
    Jan 12 01:14:11.452: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937" are running
    Jan 12 01:14:11.452: INFO: Waiting up to 5m0s for pod "my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937-9ckwd" in namespace "replication-controller-2357" to be "running"
    Jan 12 01:14:11.454: INFO: Pod "my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937-9ckwd": Phase="Running", Reason="", readiness=true. Elapsed: 2.283598ms
    Jan 12 01:14:11.454: INFO: Pod "my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937-9ckwd" satisfied condition "running"
    Jan 12 01:14:11.454: INFO: Pod "my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937-9ckwd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:14:06 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:14:08 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:14:08 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:14:06 +0000 UTC Reason: Message:}])
    Jan 12 01:14:11.454: INFO: Trying to dial the pod
    Jan 12 01:14:16.463: INFO: Controller my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937: Got expected result from replica 1 [my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937-9ckwd]: "my-hostname-basic-b5fe3691-4e7a-4c53-a976-6d0b6d3be937-9ckwd", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:14:16.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2357" for this suite. 01/12/23 01:14:16.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:14:16.499
Jan 12 01:14:16.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:14:16.499
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:14:16.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:14:16.531
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 01/12/23 01:14:16.534
Jan 12 01:14:16.571: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198" in namespace "projected-5589" to be "Succeeded or Failed"
Jan 12 01:14:16.573: INFO: Pod "downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198": Phase="Pending", Reason="", readiness=false. Elapsed: 2.429341ms
Jan 12 01:14:18.581: INFO: Pod "downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01031195s
Jan 12 01:14:20.577: INFO: Pod "downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006435892s
Jan 12 01:14:22.576: INFO: Pod "downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005384783s
STEP: Saw pod success 01/12/23 01:14:22.576
Jan 12 01:14:22.576: INFO: Pod "downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198" satisfied condition "Succeeded or Failed"
Jan 12 01:14:22.578: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198 container client-container: <nil>
STEP: delete the pod 01/12/23 01:14:22.586
Jan 12 01:14:22.626: INFO: Waiting for pod downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198 to disappear
Jan 12 01:14:22.628: INFO: Pod downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 01:14:22.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5589" for this suite. 01/12/23 01:14:22.632
------------------------------
• [SLOW TEST] [6.151 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:14:16.499
    Jan 12 01:14:16.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:14:16.499
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:14:16.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:14:16.531
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 01/12/23 01:14:16.534
    Jan 12 01:14:16.571: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198" in namespace "projected-5589" to be "Succeeded or Failed"
    Jan 12 01:14:16.573: INFO: Pod "downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198": Phase="Pending", Reason="", readiness=false. Elapsed: 2.429341ms
    Jan 12 01:14:18.581: INFO: Pod "downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01031195s
    Jan 12 01:14:20.577: INFO: Pod "downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006435892s
    Jan 12 01:14:22.576: INFO: Pod "downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005384783s
    STEP: Saw pod success 01/12/23 01:14:22.576
    Jan 12 01:14:22.576: INFO: Pod "downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198" satisfied condition "Succeeded or Failed"
    Jan 12 01:14:22.578: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198 container client-container: <nil>
    STEP: delete the pod 01/12/23 01:14:22.586
    Jan 12 01:14:22.626: INFO: Waiting for pod downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198 to disappear
    Jan 12 01:14:22.628: INFO: Pod downwardapi-volume-04a02011-6d5d-4d03-bd80-96b8f63b1198 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:14:22.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5589" for this suite. 01/12/23 01:14:22.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:14:22.65
Jan 12 01:14:22.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename daemonsets 01/12/23 01:14:22.651
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:14:22.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:14:22.667
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Jan 12 01:14:22.683: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/12/23 01:14:22.785
Jan 12 01:14:22.804: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:14:22.804: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/12/23 01:14:22.804
Jan 12 01:14:22.842: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:14:22.843: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:14:23.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:14:23.846: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:14:24.847: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:14:24.847: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:14:25.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 01:14:25.846: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/12/23 01:14:25.849
Jan 12 01:14:25.887: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:14:25.887: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/12/23 01:14:25.887
Jan 12 01:14:25.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:14:25.913: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:14:26.918: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:14:26.918: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:14:27.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:14:27.917: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:14:28.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:14:28.917: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:14:29.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:14:29.917: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:14:30.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 01:14:30.917: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/12/23 01:14:30.922
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1814, will wait for the garbage collector to delete the pods 01/12/23 01:14:30.922
Jan 12 01:14:30.983: INFO: Deleting DaemonSet.extensions daemon-set took: 9.108667ms
Jan 12 01:14:31.084: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.417318ms
Jan 12 01:14:33.588: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:14:33.588: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 12 01:14:33.590: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20154189"},"items":null}

Jan 12 01:14:33.592: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20154189"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:14:33.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1814" for this suite. 01/12/23 01:14:33.624
------------------------------
• [SLOW TEST] [11.052 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:14:22.65
    Jan 12 01:14:22.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename daemonsets 01/12/23 01:14:22.651
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:14:22.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:14:22.667
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Jan 12 01:14:22.683: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/12/23 01:14:22.785
    Jan 12 01:14:22.804: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:14:22.804: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/12/23 01:14:22.804
    Jan 12 01:14:22.842: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:14:22.843: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:14:23.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:14:23.846: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:14:24.847: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:14:24.847: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:14:25.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 01:14:25.846: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/12/23 01:14:25.849
    Jan 12 01:14:25.887: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:14:25.887: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/12/23 01:14:25.887
    Jan 12 01:14:25.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:14:25.913: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:14:26.918: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:14:26.918: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:14:27.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:14:27.917: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:14:28.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:14:28.917: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:14:29.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:14:29.917: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:14:30.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 01:14:30.917: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/12/23 01:14:30.922
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1814, will wait for the garbage collector to delete the pods 01/12/23 01:14:30.922
    Jan 12 01:14:30.983: INFO: Deleting DaemonSet.extensions daemon-set took: 9.108667ms
    Jan 12 01:14:31.084: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.417318ms
    Jan 12 01:14:33.588: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:14:33.588: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 12 01:14:33.590: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20154189"},"items":null}

    Jan 12 01:14:33.592: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20154189"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:14:33.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1814" for this suite. 01/12/23 01:14:33.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:14:33.704
Jan 12 01:14:33.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubelet-test 01/12/23 01:14:33.704
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:14:33.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:14:33.728
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 12 01:14:33.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8532" for this suite. 01/12/23 01:14:33.804
------------------------------
• [0.134 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:14:33.704
    Jan 12 01:14:33.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubelet-test 01/12/23 01:14:33.704
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:14:33.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:14:33.728
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:14:33.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8532" for this suite. 01/12/23 01:14:33.804
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:14:33.838
Jan 12 01:14:33.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 01:14:33.839
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:14:33.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:14:33.862
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/12/23 01:14:33.864
Jan 12 01:14:33.942: INFO: Waiting up to 5m0s for pod "pod-191e5291-4023-4986-8fe9-5eed87157699" in namespace "emptydir-2810" to be "Succeeded or Failed"
Jan 12 01:14:33.944: INFO: Pod "pod-191e5291-4023-4986-8fe9-5eed87157699": Phase="Pending", Reason="", readiness=false. Elapsed: 2.587946ms
Jan 12 01:14:35.952: INFO: Pod "pod-191e5291-4023-4986-8fe9-5eed87157699": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010436421s
Jan 12 01:14:37.948: INFO: Pod "pod-191e5291-4023-4986-8fe9-5eed87157699": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005848431s
Jan 12 01:14:39.950: INFO: Pod "pod-191e5291-4023-4986-8fe9-5eed87157699": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007834153s
STEP: Saw pod success 01/12/23 01:14:39.95
Jan 12 01:14:39.950: INFO: Pod "pod-191e5291-4023-4986-8fe9-5eed87157699" satisfied condition "Succeeded or Failed"
Jan 12 01:14:39.953: INFO: Trying to get logs from node eqx04-flash06 pod pod-191e5291-4023-4986-8fe9-5eed87157699 container test-container: <nil>
STEP: delete the pod 01/12/23 01:14:39.96
Jan 12 01:14:39.976: INFO: Waiting for pod pod-191e5291-4023-4986-8fe9-5eed87157699 to disappear
Jan 12 01:14:39.978: INFO: Pod pod-191e5291-4023-4986-8fe9-5eed87157699 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:14:39.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2810" for this suite. 01/12/23 01:14:39.983
------------------------------
• [SLOW TEST] [6.202 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:14:33.838
    Jan 12 01:14:33.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 01:14:33.839
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:14:33.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:14:33.862
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/12/23 01:14:33.864
    Jan 12 01:14:33.942: INFO: Waiting up to 5m0s for pod "pod-191e5291-4023-4986-8fe9-5eed87157699" in namespace "emptydir-2810" to be "Succeeded or Failed"
    Jan 12 01:14:33.944: INFO: Pod "pod-191e5291-4023-4986-8fe9-5eed87157699": Phase="Pending", Reason="", readiness=false. Elapsed: 2.587946ms
    Jan 12 01:14:35.952: INFO: Pod "pod-191e5291-4023-4986-8fe9-5eed87157699": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010436421s
    Jan 12 01:14:37.948: INFO: Pod "pod-191e5291-4023-4986-8fe9-5eed87157699": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005848431s
    Jan 12 01:14:39.950: INFO: Pod "pod-191e5291-4023-4986-8fe9-5eed87157699": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007834153s
    STEP: Saw pod success 01/12/23 01:14:39.95
    Jan 12 01:14:39.950: INFO: Pod "pod-191e5291-4023-4986-8fe9-5eed87157699" satisfied condition "Succeeded or Failed"
    Jan 12 01:14:39.953: INFO: Trying to get logs from node eqx04-flash06 pod pod-191e5291-4023-4986-8fe9-5eed87157699 container test-container: <nil>
    STEP: delete the pod 01/12/23 01:14:39.96
    Jan 12 01:14:39.976: INFO: Waiting for pod pod-191e5291-4023-4986-8fe9-5eed87157699 to disappear
    Jan 12 01:14:39.978: INFO: Pod pod-191e5291-4023-4986-8fe9-5eed87157699 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:14:39.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2810" for this suite. 01/12/23 01:14:39.983
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:14:40.04
Jan 12 01:14:40.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename subpath 01/12/23 01:14:40.041
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:14:40.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:14:40.076
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/12/23 01:14:40.078
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-986t 01/12/23 01:14:40.092
STEP: Creating a pod to test atomic-volume-subpath 01/12/23 01:14:40.092
Jan 12 01:14:40.195: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-986t" in namespace "subpath-4114" to be "Succeeded or Failed"
Jan 12 01:14:40.199: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Pending", Reason="", readiness=false. Elapsed: 3.580845ms
Jan 12 01:14:42.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007199956s
Jan 12 01:14:44.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 4.007065813s
Jan 12 01:14:46.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 6.007305939s
Jan 12 01:14:48.203: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 8.007782306s
Jan 12 01:14:50.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 10.006745535s
Jan 12 01:14:52.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 12.007406308s
Jan 12 01:14:54.204: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 14.009364535s
Jan 12 01:14:56.220: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 16.025006527s
Jan 12 01:14:58.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 18.006642799s
Jan 12 01:15:00.203: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 20.007569056s
Jan 12 01:15:02.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 22.00656666s
Jan 12 01:15:04.203: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=false. Elapsed: 24.007987504s
Jan 12 01:15:06.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.007148976s
STEP: Saw pod success 01/12/23 01:15:06.202
Jan 12 01:15:06.202: INFO: Pod "pod-subpath-test-configmap-986t" satisfied condition "Succeeded or Failed"
Jan 12 01:15:06.205: INFO: Trying to get logs from node eqx04-flash06 pod pod-subpath-test-configmap-986t container test-container-subpath-configmap-986t: <nil>
STEP: delete the pod 01/12/23 01:15:06.214
Jan 12 01:15:06.241: INFO: Waiting for pod pod-subpath-test-configmap-986t to disappear
Jan 12 01:15:06.244: INFO: Pod pod-subpath-test-configmap-986t no longer exists
STEP: Deleting pod pod-subpath-test-configmap-986t 01/12/23 01:15:06.244
Jan 12 01:15:06.244: INFO: Deleting pod "pod-subpath-test-configmap-986t" in namespace "subpath-4114"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 12 01:15:06.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4114" for this suite. 01/12/23 01:15:06.25
------------------------------
• [SLOW TEST] [26.229 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:14:40.04
    Jan 12 01:14:40.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename subpath 01/12/23 01:14:40.041
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:14:40.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:14:40.076
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/12/23 01:14:40.078
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-986t 01/12/23 01:14:40.092
    STEP: Creating a pod to test atomic-volume-subpath 01/12/23 01:14:40.092
    Jan 12 01:14:40.195: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-986t" in namespace "subpath-4114" to be "Succeeded or Failed"
    Jan 12 01:14:40.199: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Pending", Reason="", readiness=false. Elapsed: 3.580845ms
    Jan 12 01:14:42.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007199956s
    Jan 12 01:14:44.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 4.007065813s
    Jan 12 01:14:46.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 6.007305939s
    Jan 12 01:14:48.203: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 8.007782306s
    Jan 12 01:14:50.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 10.006745535s
    Jan 12 01:14:52.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 12.007406308s
    Jan 12 01:14:54.204: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 14.009364535s
    Jan 12 01:14:56.220: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 16.025006527s
    Jan 12 01:14:58.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 18.006642799s
    Jan 12 01:15:00.203: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 20.007569056s
    Jan 12 01:15:02.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=true. Elapsed: 22.00656666s
    Jan 12 01:15:04.203: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Running", Reason="", readiness=false. Elapsed: 24.007987504s
    Jan 12 01:15:06.202: INFO: Pod "pod-subpath-test-configmap-986t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.007148976s
    STEP: Saw pod success 01/12/23 01:15:06.202
    Jan 12 01:15:06.202: INFO: Pod "pod-subpath-test-configmap-986t" satisfied condition "Succeeded or Failed"
    Jan 12 01:15:06.205: INFO: Trying to get logs from node eqx04-flash06 pod pod-subpath-test-configmap-986t container test-container-subpath-configmap-986t: <nil>
    STEP: delete the pod 01/12/23 01:15:06.214
    Jan 12 01:15:06.241: INFO: Waiting for pod pod-subpath-test-configmap-986t to disappear
    Jan 12 01:15:06.244: INFO: Pod pod-subpath-test-configmap-986t no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-986t 01/12/23 01:15:06.244
    Jan 12 01:15:06.244: INFO: Deleting pod "pod-subpath-test-configmap-986t" in namespace "subpath-4114"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:15:06.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4114" for this suite. 01/12/23 01:15:06.25
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:15:06.27
Jan 12 01:15:06.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename tables 01/12/23 01:15:06.271
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:15:06.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:15:06.297
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jan 12 01:15:06.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-651" for this suite. 01/12/23 01:15:06.305
------------------------------
• [0.097 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:15:06.27
    Jan 12 01:15:06.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename tables 01/12/23 01:15:06.271
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:15:06.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:15:06.297
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:15:06.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-651" for this suite. 01/12/23 01:15:06.305
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:15:06.367
Jan 12 01:15:06.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 01:15:06.369
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:15:06.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:15:06.396
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/12/23 01:15:06.405
Jan 12 01:15:06.565: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7093" to be "running and ready"
Jan 12 01:15:06.567: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.211432ms
Jan 12 01:15:06.567: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:15:08.572: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007646286s
Jan 12 01:15:08.572: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:15:10.571: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.006046575s
Jan 12 01:15:10.571: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 12 01:15:10.571: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 01/12/23 01:15:10.573
Jan 12 01:15:10.719: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7093" to be "running and ready"
Jan 12 01:15:10.721: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.550969ms
Jan 12 01:15:10.721: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:15:12.724: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005624126s
Jan 12 01:15:12.724: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:15:14.727: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.007834677s
Jan 12 01:15:14.727: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 12 01:15:14.727: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/12/23 01:15:14.729
Jan 12 01:15:14.739: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 12 01:15:14.741: INFO: Pod pod-with-prestop-http-hook still exists
Jan 12 01:15:16.742: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 12 01:15:16.746: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/12/23 01:15:16.746
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 12 01:15:16.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7093" for this suite. 01/12/23 01:15:16.77
------------------------------
• [SLOW TEST] [10.436 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:15:06.367
    Jan 12 01:15:06.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 01:15:06.369
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:15:06.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:15:06.396
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/12/23 01:15:06.405
    Jan 12 01:15:06.565: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7093" to be "running and ready"
    Jan 12 01:15:06.567: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.211432ms
    Jan 12 01:15:06.567: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:15:08.572: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007646286s
    Jan 12 01:15:08.572: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:15:10.571: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.006046575s
    Jan 12 01:15:10.571: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 12 01:15:10.571: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 01/12/23 01:15:10.573
    Jan 12 01:15:10.719: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7093" to be "running and ready"
    Jan 12 01:15:10.721: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.550969ms
    Jan 12 01:15:10.721: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:15:12.724: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005624126s
    Jan 12 01:15:12.724: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:15:14.727: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.007834677s
    Jan 12 01:15:14.727: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 12 01:15:14.727: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/12/23 01:15:14.729
    Jan 12 01:15:14.739: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 12 01:15:14.741: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 12 01:15:16.742: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 12 01:15:16.746: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/12/23 01:15:16.746
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:15:16.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7093" for this suite. 01/12/23 01:15:16.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:15:16.805
Jan 12 01:15:16.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:15:16.806
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:15:16.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:15:16.834
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-d1ca8b44-0c54-4ae2-876e-e9b3de7398d0 01/12/23 01:15:16.837
STEP: Creating a pod to test consume configMaps 01/12/23 01:15:16.842
Jan 12 01:15:16.876: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824" in namespace "projected-4600" to be "Succeeded or Failed"
Jan 12 01:15:16.885: INFO: Pod "pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824": Phase="Pending", Reason="", readiness=false. Elapsed: 9.04832ms
Jan 12 01:15:18.889: INFO: Pod "pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012486s
Jan 12 01:15:20.897: INFO: Pod "pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020361407s
STEP: Saw pod success 01/12/23 01:15:20.897
Jan 12 01:15:20.897: INFO: Pod "pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824" satisfied condition "Succeeded or Failed"
Jan 12 01:15:20.899: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 01:15:20.91
Jan 12 01:15:20.928: INFO: Waiting for pod pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824 to disappear
Jan 12 01:15:20.930: INFO: Pod pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:15:20.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4600" for this suite. 01/12/23 01:15:20.934
------------------------------
• [4.145 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:15:16.805
    Jan 12 01:15:16.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:15:16.806
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:15:16.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:15:16.834
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-d1ca8b44-0c54-4ae2-876e-e9b3de7398d0 01/12/23 01:15:16.837
    STEP: Creating a pod to test consume configMaps 01/12/23 01:15:16.842
    Jan 12 01:15:16.876: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824" in namespace "projected-4600" to be "Succeeded or Failed"
    Jan 12 01:15:16.885: INFO: Pod "pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824": Phase="Pending", Reason="", readiness=false. Elapsed: 9.04832ms
    Jan 12 01:15:18.889: INFO: Pod "pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012486s
    Jan 12 01:15:20.897: INFO: Pod "pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020361407s
    STEP: Saw pod success 01/12/23 01:15:20.897
    Jan 12 01:15:20.897: INFO: Pod "pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824" satisfied condition "Succeeded or Failed"
    Jan 12 01:15:20.899: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 01:15:20.91
    Jan 12 01:15:20.928: INFO: Waiting for pod pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824 to disappear
    Jan 12 01:15:20.930: INFO: Pod pod-projected-configmaps-2eeb4af0-507a-4959-9b86-b944564e3824 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:15:20.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4600" for this suite. 01/12/23 01:15:20.934
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:15:20.951
Jan 12 01:15:20.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename statefulset 01/12/23 01:15:20.952
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:15:20.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:15:20.981
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7511 01/12/23 01:15:20.984
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 01/12/23 01:15:20.999
Jan 12 01:15:21.044: INFO: Found 0 stateful pods, waiting for 3
Jan 12 01:15:31.048: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 01:15:31.048: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 01:15:31.048: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/12/23 01:15:31.055
Jan 12 01:15:31.074: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/12/23 01:15:31.074
STEP: Not applying an update when the partition is greater than the number of replicas 01/12/23 01:15:41.108
STEP: Performing a canary update 01/12/23 01:15:41.108
Jan 12 01:15:41.132: INFO: Updating stateful set ss2
Jan 12 01:15:41.165: INFO: Waiting for Pod statefulset-7511/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 01/12/23 01:15:51.175
Jan 12 01:15:51.283: INFO: Found 2 stateful pods, waiting for 3
Jan 12 01:16:01.288: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 01:16:01.288: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 01:16:01.288: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/12/23 01:16:01.293
Jan 12 01:16:01.320: INFO: Updating stateful set ss2
Jan 12 01:16:01.344: INFO: Waiting for Pod statefulset-7511/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jan 12 01:16:11.381: INFO: Updating stateful set ss2
Jan 12 01:16:11.403: INFO: Waiting for StatefulSet statefulset-7511/ss2 to complete update
Jan 12 01:16:11.404: INFO: Waiting for Pod statefulset-7511/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 01:16:21.411: INFO: Deleting all statefulset in ns statefulset-7511
Jan 12 01:16:21.413: INFO: Scaling statefulset ss2 to 0
Jan 12 01:16:31.469: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 01:16:31.472: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 01:16:31.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7511" for this suite. 01/12/23 01:16:31.501
------------------------------
• [SLOW TEST] [70.584 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:15:20.951
    Jan 12 01:15:20.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename statefulset 01/12/23 01:15:20.952
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:15:20.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:15:20.981
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7511 01/12/23 01:15:20.984
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 01/12/23 01:15:20.999
    Jan 12 01:15:21.044: INFO: Found 0 stateful pods, waiting for 3
    Jan 12 01:15:31.048: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 01:15:31.048: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 01:15:31.048: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/12/23 01:15:31.055
    Jan 12 01:15:31.074: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/12/23 01:15:31.074
    STEP: Not applying an update when the partition is greater than the number of replicas 01/12/23 01:15:41.108
    STEP: Performing a canary update 01/12/23 01:15:41.108
    Jan 12 01:15:41.132: INFO: Updating stateful set ss2
    Jan 12 01:15:41.165: INFO: Waiting for Pod statefulset-7511/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 01/12/23 01:15:51.175
    Jan 12 01:15:51.283: INFO: Found 2 stateful pods, waiting for 3
    Jan 12 01:16:01.288: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 01:16:01.288: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 01:16:01.288: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/12/23 01:16:01.293
    Jan 12 01:16:01.320: INFO: Updating stateful set ss2
    Jan 12 01:16:01.344: INFO: Waiting for Pod statefulset-7511/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jan 12 01:16:11.381: INFO: Updating stateful set ss2
    Jan 12 01:16:11.403: INFO: Waiting for StatefulSet statefulset-7511/ss2 to complete update
    Jan 12 01:16:11.404: INFO: Waiting for Pod statefulset-7511/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 01:16:21.411: INFO: Deleting all statefulset in ns statefulset-7511
    Jan 12 01:16:21.413: INFO: Scaling statefulset ss2 to 0
    Jan 12 01:16:31.469: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 01:16:31.472: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:16:31.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7511" for this suite. 01/12/23 01:16:31.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:16:31.536
Jan 12 01:16:31.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 01:16:31.537
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:16:31.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:16:31.563
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 01/12/23 01:16:31.566
Jan 12 01:16:31.632: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80" in namespace "downward-api-5494" to be "Succeeded or Failed"
Jan 12 01:16:31.635: INFO: Pod "downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.488111ms
Jan 12 01:16:33.638: INFO: Pod "downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005797553s
Jan 12 01:16:35.645: INFO: Pod "downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012556018s
STEP: Saw pod success 01/12/23 01:16:35.645
Jan 12 01:16:35.649: INFO: Pod "downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80" satisfied condition "Succeeded or Failed"
Jan 12 01:16:35.656: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80 container client-container: <nil>
STEP: delete the pod 01/12/23 01:16:35.665
Jan 12 01:16:35.682: INFO: Waiting for pod downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80 to disappear
Jan 12 01:16:35.684: INFO: Pod downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 01:16:35.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5494" for this suite. 01/12/23 01:16:35.688
------------------------------
• [4.172 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:16:31.536
    Jan 12 01:16:31.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 01:16:31.537
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:16:31.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:16:31.563
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 01/12/23 01:16:31.566
    Jan 12 01:16:31.632: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80" in namespace "downward-api-5494" to be "Succeeded or Failed"
    Jan 12 01:16:31.635: INFO: Pod "downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.488111ms
    Jan 12 01:16:33.638: INFO: Pod "downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005797553s
    Jan 12 01:16:35.645: INFO: Pod "downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012556018s
    STEP: Saw pod success 01/12/23 01:16:35.645
    Jan 12 01:16:35.649: INFO: Pod "downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80" satisfied condition "Succeeded or Failed"
    Jan 12 01:16:35.656: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80 container client-container: <nil>
    STEP: delete the pod 01/12/23 01:16:35.665
    Jan 12 01:16:35.682: INFO: Waiting for pod downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80 to disappear
    Jan 12 01:16:35.684: INFO: Pod downwardapi-volume-2185734b-2997-444b-93fe-8623018eaa80 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:16:35.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5494" for this suite. 01/12/23 01:16:35.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:16:35.711
Jan 12 01:16:35.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pods 01/12/23 01:16:35.712
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:16:35.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:16:35.743
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jan 12 01:16:35.780: INFO: Waiting up to 5m0s for pod "server-envvars-9d21e22d-a74a-48d4-a8b2-b79acb905d4c" in namespace "pods-4851" to be "running and ready"
Jan 12 01:16:35.782: INFO: Pod "server-envvars-9d21e22d-a74a-48d4-a8b2-b79acb905d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.408857ms
Jan 12 01:16:35.782: INFO: The phase of Pod server-envvars-9d21e22d-a74a-48d4-a8b2-b79acb905d4c is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:16:37.790: INFO: Pod "server-envvars-9d21e22d-a74a-48d4-a8b2-b79acb905d4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010368583s
Jan 12 01:16:37.790: INFO: The phase of Pod server-envvars-9d21e22d-a74a-48d4-a8b2-b79acb905d4c is Running (Ready = true)
Jan 12 01:16:37.790: INFO: Pod "server-envvars-9d21e22d-a74a-48d4-a8b2-b79acb905d4c" satisfied condition "running and ready"
Jan 12 01:16:37.842: INFO: Waiting up to 5m0s for pod "client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2" in namespace "pods-4851" to be "Succeeded or Failed"
Jan 12 01:16:37.844: INFO: Pod "client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281098ms
Jan 12 01:16:39.848: INFO: Pod "client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005813662s
Jan 12 01:16:41.848: INFO: Pod "client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005476683s
Jan 12 01:16:43.849: INFO: Pod "client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006539753s
STEP: Saw pod success 01/12/23 01:16:43.849
Jan 12 01:16:43.849: INFO: Pod "client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2" satisfied condition "Succeeded or Failed"
Jan 12 01:16:43.851: INFO: Trying to get logs from node eqx04-flash06 pod client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2 container env3cont: <nil>
STEP: delete the pod 01/12/23 01:16:43.86
Jan 12 01:16:43.877: INFO: Waiting for pod client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2 to disappear
Jan 12 01:16:43.879: INFO: Pod client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 01:16:43.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4851" for this suite. 01/12/23 01:16:43.883
------------------------------
• [SLOW TEST] [8.255 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:16:35.711
    Jan 12 01:16:35.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pods 01/12/23 01:16:35.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:16:35.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:16:35.743
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jan 12 01:16:35.780: INFO: Waiting up to 5m0s for pod "server-envvars-9d21e22d-a74a-48d4-a8b2-b79acb905d4c" in namespace "pods-4851" to be "running and ready"
    Jan 12 01:16:35.782: INFO: Pod "server-envvars-9d21e22d-a74a-48d4-a8b2-b79acb905d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.408857ms
    Jan 12 01:16:35.782: INFO: The phase of Pod server-envvars-9d21e22d-a74a-48d4-a8b2-b79acb905d4c is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:16:37.790: INFO: Pod "server-envvars-9d21e22d-a74a-48d4-a8b2-b79acb905d4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010368583s
    Jan 12 01:16:37.790: INFO: The phase of Pod server-envvars-9d21e22d-a74a-48d4-a8b2-b79acb905d4c is Running (Ready = true)
    Jan 12 01:16:37.790: INFO: Pod "server-envvars-9d21e22d-a74a-48d4-a8b2-b79acb905d4c" satisfied condition "running and ready"
    Jan 12 01:16:37.842: INFO: Waiting up to 5m0s for pod "client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2" in namespace "pods-4851" to be "Succeeded or Failed"
    Jan 12 01:16:37.844: INFO: Pod "client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281098ms
    Jan 12 01:16:39.848: INFO: Pod "client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005813662s
    Jan 12 01:16:41.848: INFO: Pod "client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005476683s
    Jan 12 01:16:43.849: INFO: Pod "client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006539753s
    STEP: Saw pod success 01/12/23 01:16:43.849
    Jan 12 01:16:43.849: INFO: Pod "client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2" satisfied condition "Succeeded or Failed"
    Jan 12 01:16:43.851: INFO: Trying to get logs from node eqx04-flash06 pod client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2 container env3cont: <nil>
    STEP: delete the pod 01/12/23 01:16:43.86
    Jan 12 01:16:43.877: INFO: Waiting for pod client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2 to disappear
    Jan 12 01:16:43.879: INFO: Pod client-envvars-be988faa-e2a5-4d40-8bf3-8a018f9130b2 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:16:43.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4851" for this suite. 01/12/23 01:16:43.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:16:43.967
Jan 12 01:16:43.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 01:16:43.968
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:16:43.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:16:43.991
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/12/23 01:16:43.993
Jan 12 01:16:44.146: INFO: Waiting up to 5m0s for pod "pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f" in namespace "emptydir-1385" to be "Succeeded or Failed"
Jan 12 01:16:44.149: INFO: Pod "pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407256ms
Jan 12 01:16:46.152: INFO: Pod "pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005425779s
Jan 12 01:16:48.152: INFO: Pod "pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005924361s
Jan 12 01:16:50.152: INFO: Pod "pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005566717s
STEP: Saw pod success 01/12/23 01:16:50.152
Jan 12 01:16:50.152: INFO: Pod "pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f" satisfied condition "Succeeded or Failed"
Jan 12 01:16:50.154: INFO: Trying to get logs from node eqx04-flash06 pod pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f container test-container: <nil>
STEP: delete the pod 01/12/23 01:16:50.162
Jan 12 01:16:50.177: INFO: Waiting for pod pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f to disappear
Jan 12 01:16:50.179: INFO: Pod pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:16:50.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1385" for this suite. 01/12/23 01:16:50.186
------------------------------
• [SLOW TEST] [6.249 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:16:43.967
    Jan 12 01:16:43.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 01:16:43.968
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:16:43.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:16:43.991
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/12/23 01:16:43.993
    Jan 12 01:16:44.146: INFO: Waiting up to 5m0s for pod "pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f" in namespace "emptydir-1385" to be "Succeeded or Failed"
    Jan 12 01:16:44.149: INFO: Pod "pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407256ms
    Jan 12 01:16:46.152: INFO: Pod "pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005425779s
    Jan 12 01:16:48.152: INFO: Pod "pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005924361s
    Jan 12 01:16:50.152: INFO: Pod "pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005566717s
    STEP: Saw pod success 01/12/23 01:16:50.152
    Jan 12 01:16:50.152: INFO: Pod "pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f" satisfied condition "Succeeded or Failed"
    Jan 12 01:16:50.154: INFO: Trying to get logs from node eqx04-flash06 pod pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f container test-container: <nil>
    STEP: delete the pod 01/12/23 01:16:50.162
    Jan 12 01:16:50.177: INFO: Waiting for pod pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f to disappear
    Jan 12 01:16:50.179: INFO: Pod pod-64e47a63-e7c2-4bb3-b710-3cb1a96d2d5f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:16:50.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1385" for this suite. 01/12/23 01:16:50.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:16:50.217
Jan 12 01:16:50.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:16:50.218
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:16:50.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:16:50.241
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-2914fc2b-af81-4965-803d-f565abd00512 01/12/23 01:16:50.244
STEP: Creating a pod to test consume secrets 01/12/23 01:16:50.256
Jan 12 01:16:50.310: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9" in namespace "projected-2362" to be "Succeeded or Failed"
Jan 12 01:16:50.312: INFO: Pod "pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.57369ms
Jan 12 01:16:52.315: INFO: Pod "pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005519455s
Jan 12 01:16:54.316: INFO: Pod "pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006607483s
Jan 12 01:16:56.316: INFO: Pod "pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006329061s
STEP: Saw pod success 01/12/23 01:16:56.316
Jan 12 01:16:56.316: INFO: Pod "pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9" satisfied condition "Succeeded or Failed"
Jan 12 01:16:56.319: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/12/23 01:16:56.33
Jan 12 01:16:56.349: INFO: Waiting for pod pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9 to disappear
Jan 12 01:16:56.351: INFO: Pod pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 12 01:16:56.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2362" for this suite. 01/12/23 01:16:56.355
------------------------------
• [SLOW TEST] [6.162 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:16:50.217
    Jan 12 01:16:50.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:16:50.218
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:16:50.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:16:50.241
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-2914fc2b-af81-4965-803d-f565abd00512 01/12/23 01:16:50.244
    STEP: Creating a pod to test consume secrets 01/12/23 01:16:50.256
    Jan 12 01:16:50.310: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9" in namespace "projected-2362" to be "Succeeded or Failed"
    Jan 12 01:16:50.312: INFO: Pod "pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.57369ms
    Jan 12 01:16:52.315: INFO: Pod "pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005519455s
    Jan 12 01:16:54.316: INFO: Pod "pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006607483s
    Jan 12 01:16:56.316: INFO: Pod "pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006329061s
    STEP: Saw pod success 01/12/23 01:16:56.316
    Jan 12 01:16:56.316: INFO: Pod "pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9" satisfied condition "Succeeded or Failed"
    Jan 12 01:16:56.319: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 01:16:56.33
    Jan 12 01:16:56.349: INFO: Waiting for pod pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9 to disappear
    Jan 12 01:16:56.351: INFO: Pod pod-projected-secrets-eadc6137-58d4-4424-9348-c08caedd82c9 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:16:56.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2362" for this suite. 01/12/23 01:16:56.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:16:56.38
Jan 12 01:16:56.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename resourcequota 01/12/23 01:16:56.38
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:16:56.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:16:56.412
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 01/12/23 01:16:56.414
STEP: Creating a ResourceQuota 01/12/23 01:17:01.428
STEP: Ensuring resource quota status is calculated 01/12/23 01:17:01.434
STEP: Creating a Service 01/12/23 01:17:03.438
STEP: Creating a NodePort Service 01/12/23 01:17:03.475
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/12/23 01:17:03.507
STEP: Ensuring resource quota status captures service creation 01/12/23 01:17:03.559
STEP: Deleting Services 01/12/23 01:17:05.563
STEP: Ensuring resource quota status released usage 01/12/23 01:17:05.641
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 01:17:07.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8963" for this suite. 01/12/23 01:17:07.649
------------------------------
• [SLOW TEST] [11.374 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:16:56.38
    Jan 12 01:16:56.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename resourcequota 01/12/23 01:16:56.38
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:16:56.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:16:56.412
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 01/12/23 01:16:56.414
    STEP: Creating a ResourceQuota 01/12/23 01:17:01.428
    STEP: Ensuring resource quota status is calculated 01/12/23 01:17:01.434
    STEP: Creating a Service 01/12/23 01:17:03.438
    STEP: Creating a NodePort Service 01/12/23 01:17:03.475
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/12/23 01:17:03.507
    STEP: Ensuring resource quota status captures service creation 01/12/23 01:17:03.559
    STEP: Deleting Services 01/12/23 01:17:05.563
    STEP: Ensuring resource quota status released usage 01/12/23 01:17:05.641
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:17:07.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8963" for this suite. 01/12/23 01:17:07.649
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:17:07.755
Jan 12 01:17:07.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename sched-pred 01/12/23 01:17:07.755
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:17:07.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:17:07.786
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 12 01:17:07.789: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 12 01:17:07.811: INFO: Waiting for terminating namespaces to be deleted...
Jan 12 01:17:07.815: INFO: 
Logging pods the apiserver thinks is on node eqx03-flash06 before test
Jan 12 01:17:07.833: INFO: calico-kube-controllers-5c9fd9b888-vc4sl from kube-system started at 2023-01-11 19:29:05 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 12 01:17:07.833: INFO: calico-node-lwrhk from kube-system started at 2023-01-11 08:01:00 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container calico-node ready: true, restart count 0
Jan 12 01:17:07.833: INFO: kube-multus-ds-amd64-gsh8m from kube-system started at 2022-10-10 22:16:50 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container kube-multus ready: true, restart count 2
Jan 12 01:17:07.833: INFO: kube-proxy-h8fqt from kube-system started at 2023-01-11 07:45:11 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 01:17:07.833: INFO: kube-sriov-device-plugin-amd64-4wg6g from kube-system started at 2022-10-10 22:14:57 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container kube-sriovdp ready: true, restart count 2
Jan 12 01:17:07.833: INFO: mariadb-1665683911-master-0 from maria started at 2023-01-11 07:55:52 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container mariadb ready: true, restart count 0
Jan 12 01:17:07.833: INFO: mariadb-1665683911-slave-0 from maria started at 2023-01-11 07:56:03 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container mariadb ready: true, restart count 0
Jan 12 01:17:07.833: INFO: mysql-1664779250-7dc5656c45-vcb4f from mg started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container mysql-1664779250 ready: true, restart count 0
Jan 12 01:17:07.833: INFO: mysql-1664779251-858bf7fdc-6scml from mg started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container mysql-1664779251 ready: true, restart count 0
Jan 12 01:17:07.833: INFO: mysql-1664779253-866d78dd7-n8j9t from mg started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container mysql-1664779253 ready: true, restart count 0
Jan 12 01:17:07.833: INFO: mysql-1664779256-5c6fc69c89-9kvgw from mg started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container mysql-1664779256 ready: true, restart count 0
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779263-controller-bf7587b7f-8f6bc from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779263-default-backend-cdc6d499b-ng78h from ng started at 2023-01-11 07:55:44 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779266-controller-677944959d-dnpbb from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779266-default-backend-bbdfc88b7-7ln9d from ng started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779278-controller-5fb4f984c-cshwh from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779278-default-backend-5dfb4f9db4-bdjmg from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779281-controller-75b6585875-6fwmp from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779281-default-backend-d5fb87996-kl2gz from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779288-controller-9744ff446-rhjs8 from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779288-default-backend-594965dbd7-kb8tt from ng started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779291-controller-ff67b7844-jqvh6 from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779291-default-backend-66f95c66fc-2vfvf from ng started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779294-controller-84785d75f7-x4w9c from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779294-default-backend-6765dd7578-v796m from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779297-controller-57f654c69d-vzn6l from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
Jan 12 01:17:07.833: INFO: nginx-ingress-1664779297-default-backend-74df75bf95-n6xwg from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:17:07.833: INFO: csi-nodeplugin-robin-hnwgz from robinio started at 2023-01-11 08:44:24 +0000 UTC (3 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container driver-registrar ready: true, restart count 0
Jan 12 01:17:07.833: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 12 01:17:07.833: INFO: 	Container robin ready: true, restart count 0
Jan 12 01:17:07.833: INFO: robin-nfs-watchdog-qrzrq from robinio started at 2023-01-11 08:28:03 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
Jan 12 01:17:07.833: INFO: robin-worker-jlr7d from robinio started at 2023-01-11 08:27:55 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container robinrcm ready: true, restart count 0
Jan 12 01:17:07.833: INFO: mysql-1665089095-66b956f5d-vxj2b from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container mysql-1665089095 ready: true, restart count 0
Jan 12 01:17:07.833: INFO: mysql-1665089145-598df7974-kn9cv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container mysql-1665089145 ready: true, restart count 0
Jan 12 01:17:07.833: INFO: mysql-1665089149-5574fb7774-t7pqg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container mysql-1665089149 ready: true, restart count 0
Jan 12 01:17:07.833: INFO: mysql-1665089152-84f657bf94-dvsbc from sa-ns-user started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container mysql-1665089152 ready: true, restart count 0
Jan 12 01:17:07.833: INFO: mysql-1665089179-5bf57c5944-tp4xz from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container mysql-1665089179 ready: true, restart count 0
Jan 12 01:17:07.833: INFO: mysql-1665089182-6c58488f6d-5jn9v from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container mysql-1665089182 ready: true, restart count 0
Jan 12 01:17:07.833: INFO: mysql-1665089185-677dc7ffb6-b9pl6 from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container mysql-1665089185 ready: true, restart count 0
Jan 12 01:17:07.833: INFO: mysql-1665089188-58475957bd-5nfbd from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.833: INFO: 	Container mysql-1665089188 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089191-66859c96dd-lkchj from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089191 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089205-5ff87d5b8d-wch4v from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089205 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089217-76648fcb6f-9hkfm from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089217 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089220-595bdf59bf-wzg6h from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089220 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089231-fd7d54889-stkqh from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089231 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089236-66c6896dcf-jjztv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089236 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089247-845bdb94dc-z5mxg from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089247 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089249-68588bc459-psrj7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089249 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089253-7d549cf945-mhld7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089253 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089257-6c4b6dd79c-tz5d5 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089257 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089259-58c44567c7-jmzbp from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089259 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089262-744cbfcf5c-lzzbl from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089262 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089268-5867478f97-c44xg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089268 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089337-9dbc9475f-zzk9n from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089337 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665089342-69f7c77fd7-9l5qq from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665089342 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665088984-56cbf7747c-8zfh5 from sa-ns started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665088984 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: mysql-1665088986-7785c569-hfqvj from sa-ns started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql-1665088986 ready: true, restart count 0
Jan 12 01:17:07.834: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-ppd9w from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 01:17:07.834: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 12 01:17:07.834: INFO: ravi-ravi-mysql-0 from t001-u000004 started at 2023-01-11 07:55:59 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.834: INFO: 	Container mysql ready: true, restart count 0
Jan 12 01:17:07.834: INFO: 
Logging pods the apiserver thinks is on node eqx04-flash06 before test
Jan 12 01:17:07.851: INFO: calico-node-wh5zd from kube-system started at 2023-01-11 08:02:20 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.851: INFO: 	Container calico-node ready: true, restart count 0
Jan 12 01:17:07.851: INFO: kube-multus-ds-amd64-qllb4 from kube-system started at 2023-01-12 01:12:37 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.851: INFO: 	Container kube-multus ready: true, restart count 0
Jan 12 01:17:07.851: INFO: kube-proxy-kvvhz from kube-system started at 2023-01-11 07:45:13 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.851: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 01:17:07.851: INFO: kube-sriov-device-plugin-amd64-b2hg7 from kube-system started at 2023-01-12 01:12:30 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.851: INFO: 	Container kube-sriovdp ready: true, restart count 0
Jan 12 01:17:07.851: INFO: csi-nodeplugin-robin-z9mk4 from robinio started at 2023-01-12 01:12:37 +0000 UTC (3 container statuses recorded)
Jan 12 01:17:07.851: INFO: 	Container driver-registrar ready: true, restart count 0
Jan 12 01:17:07.851: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 12 01:17:07.851: INFO: 	Container robin ready: true, restart count 0
Jan 12 01:17:07.851: INFO: robin-nfs-watchdog-cpvh5 from robinio started at 2023-01-12 01:12:30 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.851: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
Jan 12 01:17:07.851: INFO: robin-worker-pczc5 from robinio started at 2023-01-12 01:13:06 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.851: INFO: 	Container robinrcm ready: true, restart count 0
Jan 12 01:17:07.851: INFO: sonobuoy from sonobuoy started at 2023-01-12 00:40:33 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.851: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 12 01:17:07.851: INFO: sonobuoy-e2e-job-90575ca5f8b04bb8 from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
Jan 12 01:17:07.851: INFO: 	Container e2e ready: true, restart count 0
Jan 12 01:17:07.851: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 01:17:07.851: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-mkhnx from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
Jan 12 01:17:07.851: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 01:17:07.851: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 12 01:17:07.851: INFO: cent-1-server-01 from t001-u000004 started at 2023-01-12 01:14:56 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.851: INFO: 	Container cent-1-server-01 ready: true, restart count 0
Jan 12 01:17:07.851: INFO: cent-2-server-01 from t001-u000004 started at 2023-01-12 01:14:56 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.851: INFO: 	Container cent-2-server-01 ready: true, restart count 0
Jan 12 01:17:07.851: INFO: sq-mysql-01 from t001-u000004 started at 2023-01-12 01:14:45 +0000 UTC (1 container statuses recorded)
Jan 12 01:17:07.851: INFO: 	Container sq-mysql-01 ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/12/23 01:17:07.851
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17396adb9f021321], Reason = [FailedScheduling], Message = [0/5 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] 01/12/23 01:17:08.056
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:17:09.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2637" for this suite. 01/12/23 01:17:09.053
------------------------------
• [1.568 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:17:07.755
    Jan 12 01:17:07.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename sched-pred 01/12/23 01:17:07.755
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:17:07.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:17:07.786
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 12 01:17:07.789: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 12 01:17:07.811: INFO: Waiting for terminating namespaces to be deleted...
    Jan 12 01:17:07.815: INFO: 
    Logging pods the apiserver thinks is on node eqx03-flash06 before test
    Jan 12 01:17:07.833: INFO: calico-kube-controllers-5c9fd9b888-vc4sl from kube-system started at 2023-01-11 19:29:05 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: calico-node-lwrhk from kube-system started at 2023-01-11 08:01:00 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container calico-node ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: kube-multus-ds-amd64-gsh8m from kube-system started at 2022-10-10 22:16:50 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container kube-multus ready: true, restart count 2
    Jan 12 01:17:07.833: INFO: kube-proxy-h8fqt from kube-system started at 2023-01-11 07:45:11 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: kube-sriov-device-plugin-amd64-4wg6g from kube-system started at 2022-10-10 22:14:57 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container kube-sriovdp ready: true, restart count 2
    Jan 12 01:17:07.833: INFO: mariadb-1665683911-master-0 from maria started at 2023-01-11 07:55:52 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container mariadb ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: mariadb-1665683911-slave-0 from maria started at 2023-01-11 07:56:03 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container mariadb ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: mysql-1664779250-7dc5656c45-vcb4f from mg started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container mysql-1664779250 ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: mysql-1664779251-858bf7fdc-6scml from mg started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container mysql-1664779251 ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: mysql-1664779253-866d78dd7-n8j9t from mg started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container mysql-1664779253 ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: mysql-1664779256-5c6fc69c89-9kvgw from mg started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container mysql-1664779256 ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779263-controller-bf7587b7f-8f6bc from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779263-default-backend-cdc6d499b-ng78h from ng started at 2023-01-11 07:55:44 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779266-controller-677944959d-dnpbb from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779266-default-backend-bbdfc88b7-7ln9d from ng started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779278-controller-5fb4f984c-cshwh from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779278-default-backend-5dfb4f9db4-bdjmg from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779281-controller-75b6585875-6fwmp from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779281-default-backend-d5fb87996-kl2gz from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779288-controller-9744ff446-rhjs8 from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779288-default-backend-594965dbd7-kb8tt from ng started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779291-controller-ff67b7844-jqvh6 from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779291-default-backend-66f95c66fc-2vfvf from ng started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779294-controller-84785d75f7-x4w9c from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779294-default-backend-6765dd7578-v796m from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779297-controller-57f654c69d-vzn6l from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-controller ready: false, restart count 5
    Jan 12 01:17:07.833: INFO: nginx-ingress-1664779297-default-backend-74df75bf95-n6xwg from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: csi-nodeplugin-robin-hnwgz from robinio started at 2023-01-11 08:44:24 +0000 UTC (3 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: 	Container robin ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: robin-nfs-watchdog-qrzrq from robinio started at 2023-01-11 08:28:03 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: robin-worker-jlr7d from robinio started at 2023-01-11 08:27:55 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container robinrcm ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: mysql-1665089095-66b956f5d-vxj2b from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container mysql-1665089095 ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: mysql-1665089145-598df7974-kn9cv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container mysql-1665089145 ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: mysql-1665089149-5574fb7774-t7pqg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container mysql-1665089149 ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: mysql-1665089152-84f657bf94-dvsbc from sa-ns-user started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container mysql-1665089152 ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: mysql-1665089179-5bf57c5944-tp4xz from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container mysql-1665089179 ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: mysql-1665089182-6c58488f6d-5jn9v from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container mysql-1665089182 ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: mysql-1665089185-677dc7ffb6-b9pl6 from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container mysql-1665089185 ready: true, restart count 0
    Jan 12 01:17:07.833: INFO: mysql-1665089188-58475957bd-5nfbd from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.833: INFO: 	Container mysql-1665089188 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089191-66859c96dd-lkchj from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089191 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089205-5ff87d5b8d-wch4v from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089205 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089217-76648fcb6f-9hkfm from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089217 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089220-595bdf59bf-wzg6h from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089220 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089231-fd7d54889-stkqh from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089231 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089236-66c6896dcf-jjztv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089236 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089247-845bdb94dc-z5mxg from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089247 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089249-68588bc459-psrj7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089249 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089253-7d549cf945-mhld7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089253 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089257-6c4b6dd79c-tz5d5 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089257 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089259-58c44567c7-jmzbp from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089259 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089262-744cbfcf5c-lzzbl from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089262 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089268-5867478f97-c44xg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089268 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089337-9dbc9475f-zzk9n from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089337 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665089342-69f7c77fd7-9l5qq from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665089342 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665088984-56cbf7747c-8zfh5 from sa-ns started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665088984 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: mysql-1665088986-7785c569-hfqvj from sa-ns started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql-1665088986 ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-ppd9w from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: ravi-ravi-mysql-0 from t001-u000004 started at 2023-01-11 07:55:59 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.834: INFO: 	Container mysql ready: true, restart count 0
    Jan 12 01:17:07.834: INFO: 
    Logging pods the apiserver thinks is on node eqx04-flash06 before test
    Jan 12 01:17:07.851: INFO: calico-node-wh5zd from kube-system started at 2023-01-11 08:02:20 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.851: INFO: 	Container calico-node ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: kube-multus-ds-amd64-qllb4 from kube-system started at 2023-01-12 01:12:37 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.851: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: kube-proxy-kvvhz from kube-system started at 2023-01-11 07:45:13 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.851: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: kube-sriov-device-plugin-amd64-b2hg7 from kube-system started at 2023-01-12 01:12:30 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.851: INFO: 	Container kube-sriovdp ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: csi-nodeplugin-robin-z9mk4 from robinio started at 2023-01-12 01:12:37 +0000 UTC (3 container statuses recorded)
    Jan 12 01:17:07.851: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: 	Container robin ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: robin-nfs-watchdog-cpvh5 from robinio started at 2023-01-12 01:12:30 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.851: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: robin-worker-pczc5 from robinio started at 2023-01-12 01:13:06 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.851: INFO: 	Container robinrcm ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: sonobuoy from sonobuoy started at 2023-01-12 00:40:33 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.851: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: sonobuoy-e2e-job-90575ca5f8b04bb8 from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
    Jan 12 01:17:07.851: INFO: 	Container e2e ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-mkhnx from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
    Jan 12 01:17:07.851: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: cent-1-server-01 from t001-u000004 started at 2023-01-12 01:14:56 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.851: INFO: 	Container cent-1-server-01 ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: cent-2-server-01 from t001-u000004 started at 2023-01-12 01:14:56 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.851: INFO: 	Container cent-2-server-01 ready: true, restart count 0
    Jan 12 01:17:07.851: INFO: sq-mysql-01 from t001-u000004 started at 2023-01-12 01:14:45 +0000 UTC (1 container statuses recorded)
    Jan 12 01:17:07.851: INFO: 	Container sq-mysql-01 ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/12/23 01:17:07.851
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17396adb9f021321], Reason = [FailedScheduling], Message = [0/5 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] 01/12/23 01:17:08.056
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:17:09.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2637" for this suite. 01/12/23 01:17:09.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:17:09.324
Jan 12 01:17:09.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename svcaccounts 01/12/23 01:17:09.325
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:17:09.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:17:09.349
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-qc9v4"  01/12/23 01:17:09.352
Jan 12 01:17:09.360: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-qc9v4"  01/12/23 01:17:09.36
Jan 12 01:17:09.370: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 12 01:17:09.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3781" for this suite. 01/12/23 01:17:09.383
------------------------------
• [0.126 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:17:09.324
    Jan 12 01:17:09.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename svcaccounts 01/12/23 01:17:09.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:17:09.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:17:09.349
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-qc9v4"  01/12/23 01:17:09.352
    Jan 12 01:17:09.360: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-qc9v4"  01/12/23 01:17:09.36
    Jan 12 01:17:09.370: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:17:09.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3781" for this suite. 01/12/23 01:17:09.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:17:09.451
Jan 12 01:17:09.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubelet-test 01/12/23 01:17:09.452
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:17:09.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:17:09.472
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 12 01:17:09.585: INFO: Waiting up to 5m0s for pod "busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65" in namespace "kubelet-test-5597" to be "running and ready"
Jan 12 01:17:09.587: INFO: Pod "busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.446487ms
Jan 12 01:17:09.587: INFO: The phase of Pod busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:17:11.590: INFO: Pod "busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005420792s
Jan 12 01:17:11.590: INFO: The phase of Pod busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:17:13.592: INFO: Pod "busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65": Phase="Running", Reason="", readiness=true. Elapsed: 4.006629532s
Jan 12 01:17:13.592: INFO: The phase of Pod busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65 is Running (Ready = true)
Jan 12 01:17:13.592: INFO: Pod "busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 12 01:17:13.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5597" for this suite. 01/12/23 01:17:13.606
------------------------------
• [4.191 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:17:09.451
    Jan 12 01:17:09.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubelet-test 01/12/23 01:17:09.452
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:17:09.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:17:09.472
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 12 01:17:09.585: INFO: Waiting up to 5m0s for pod "busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65" in namespace "kubelet-test-5597" to be "running and ready"
    Jan 12 01:17:09.587: INFO: Pod "busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.446487ms
    Jan 12 01:17:09.587: INFO: The phase of Pod busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:17:11.590: INFO: Pod "busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005420792s
    Jan 12 01:17:11.590: INFO: The phase of Pod busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:17:13.592: INFO: Pod "busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65": Phase="Running", Reason="", readiness=true. Elapsed: 4.006629532s
    Jan 12 01:17:13.592: INFO: The phase of Pod busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65 is Running (Ready = true)
    Jan 12 01:17:13.592: INFO: Pod "busybox-scheduling-9a7e8025-fbe7-40e5-91a5-837d59841d65" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:17:13.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5597" for this suite. 01/12/23 01:17:13.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:17:13.643
Jan 12 01:17:13.644: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-probe 01/12/23 01:17:13.644
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:17:13.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:17:13.661
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 01:18:13.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2493" for this suite. 01/12/23 01:18:13.794
------------------------------
• [SLOW TEST] [60.217 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:17:13.643
    Jan 12 01:17:13.644: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-probe 01/12/23 01:17:13.644
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:17:13.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:17:13.661
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:18:13.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2493" for this suite. 01/12/23 01:18:13.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:18:13.864
Jan 12 01:18:13.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename job 01/12/23 01:18:13.864
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:18:13.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:18:13.888
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 01/12/23 01:18:13.89
STEP: Ensuring active pods == parallelism 01/12/23 01:18:13.897
STEP: Orphaning one of the Job's Pods 01/12/23 01:18:17.901
Jan 12 01:18:18.425: INFO: Successfully updated pod "adopt-release-qg95t"
STEP: Checking that the Job readopts the Pod 01/12/23 01:18:18.425
Jan 12 01:18:18.425: INFO: Waiting up to 15m0s for pod "adopt-release-qg95t" in namespace "job-5654" to be "adopted"
Jan 12 01:18:18.437: INFO: Pod "adopt-release-qg95t": Phase="Running", Reason="", readiness=true. Elapsed: 11.824283ms
Jan 12 01:18:20.441: INFO: Pod "adopt-release-qg95t": Phase="Running", Reason="", readiness=true. Elapsed: 2.01571692s
Jan 12 01:18:20.441: INFO: Pod "adopt-release-qg95t" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/12/23 01:18:20.441
Jan 12 01:18:20.959: INFO: Successfully updated pod "adopt-release-qg95t"
STEP: Checking that the Job releases the Pod 01/12/23 01:18:20.959
Jan 12 01:18:20.959: INFO: Waiting up to 15m0s for pod "adopt-release-qg95t" in namespace "job-5654" to be "released"
Jan 12 01:18:20.961: INFO: Pod "adopt-release-qg95t": Phase="Running", Reason="", readiness=true. Elapsed: 2.414661ms
Jan 12 01:18:22.965: INFO: Pod "adopt-release-qg95t": Phase="Running", Reason="", readiness=true. Elapsed: 2.006375821s
Jan 12 01:18:22.965: INFO: Pod "adopt-release-qg95t" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 12 01:18:22.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5654" for this suite. 01/12/23 01:18:22.969
------------------------------
• [SLOW TEST] [9.130 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:18:13.864
    Jan 12 01:18:13.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename job 01/12/23 01:18:13.864
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:18:13.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:18:13.888
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 01/12/23 01:18:13.89
    STEP: Ensuring active pods == parallelism 01/12/23 01:18:13.897
    STEP: Orphaning one of the Job's Pods 01/12/23 01:18:17.901
    Jan 12 01:18:18.425: INFO: Successfully updated pod "adopt-release-qg95t"
    STEP: Checking that the Job readopts the Pod 01/12/23 01:18:18.425
    Jan 12 01:18:18.425: INFO: Waiting up to 15m0s for pod "adopt-release-qg95t" in namespace "job-5654" to be "adopted"
    Jan 12 01:18:18.437: INFO: Pod "adopt-release-qg95t": Phase="Running", Reason="", readiness=true. Elapsed: 11.824283ms
    Jan 12 01:18:20.441: INFO: Pod "adopt-release-qg95t": Phase="Running", Reason="", readiness=true. Elapsed: 2.01571692s
    Jan 12 01:18:20.441: INFO: Pod "adopt-release-qg95t" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/12/23 01:18:20.441
    Jan 12 01:18:20.959: INFO: Successfully updated pod "adopt-release-qg95t"
    STEP: Checking that the Job releases the Pod 01/12/23 01:18:20.959
    Jan 12 01:18:20.959: INFO: Waiting up to 15m0s for pod "adopt-release-qg95t" in namespace "job-5654" to be "released"
    Jan 12 01:18:20.961: INFO: Pod "adopt-release-qg95t": Phase="Running", Reason="", readiness=true. Elapsed: 2.414661ms
    Jan 12 01:18:22.965: INFO: Pod "adopt-release-qg95t": Phase="Running", Reason="", readiness=true. Elapsed: 2.006375821s
    Jan 12 01:18:22.965: INFO: Pod "adopt-release-qg95t" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:18:22.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5654" for this suite. 01/12/23 01:18:22.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:18:22.995
Jan 12 01:18:22.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename svcaccounts 01/12/23 01:18:22.995
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:18:23.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:18:23.014
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Jan 12 01:18:23.091: INFO: created pod
Jan 12 01:18:23.091: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1279" to be "Succeeded or Failed"
Jan 12 01:18:23.094: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.289004ms
Jan 12 01:18:25.098: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006194508s
Jan 12 01:18:27.098: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006187025s
Jan 12 01:18:29.098: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006058741s
STEP: Saw pod success 01/12/23 01:18:29.098
Jan 12 01:18:29.098: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 12 01:18:59.099: INFO: polling logs
Jan 12 01:18:59.113: INFO: Pod logs: 
I0112 01:18:24.657608       1 log.go:198] OK: Got token
I0112 01:18:24.657649       1 log.go:198] validating with in-cluster discovery
I0112 01:18:24.657991       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0112 01:18:24.658014       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1279:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673486903, NotBefore:1673486303, IssuedAt:1673486303, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1279", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"35114e5e-536c-4e30-b77e-41dae0db4853"}}}
I0112 01:18:24.670009       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0112 01:18:24.678477       1 log.go:198] OK: Validated signature on JWT
I0112 01:18:24.678601       1 log.go:198] OK: Got valid claims from token!
I0112 01:18:24.678637       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1279:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673486903, NotBefore:1673486303, IssuedAt:1673486303, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1279", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"35114e5e-536c-4e30-b77e-41dae0db4853"}}}

Jan 12 01:18:59.113: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 12 01:18:59.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1279" for this suite. 01/12/23 01:18:59.125
------------------------------
• [SLOW TEST] [36.152 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:18:22.995
    Jan 12 01:18:22.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename svcaccounts 01/12/23 01:18:22.995
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:18:23.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:18:23.014
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Jan 12 01:18:23.091: INFO: created pod
    Jan 12 01:18:23.091: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1279" to be "Succeeded or Failed"
    Jan 12 01:18:23.094: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.289004ms
    Jan 12 01:18:25.098: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006194508s
    Jan 12 01:18:27.098: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006187025s
    Jan 12 01:18:29.098: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006058741s
    STEP: Saw pod success 01/12/23 01:18:29.098
    Jan 12 01:18:29.098: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 12 01:18:59.099: INFO: polling logs
    Jan 12 01:18:59.113: INFO: Pod logs: 
    I0112 01:18:24.657608       1 log.go:198] OK: Got token
    I0112 01:18:24.657649       1 log.go:198] validating with in-cluster discovery
    I0112 01:18:24.657991       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0112 01:18:24.658014       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1279:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673486903, NotBefore:1673486303, IssuedAt:1673486303, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1279", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"35114e5e-536c-4e30-b77e-41dae0db4853"}}}
    I0112 01:18:24.670009       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0112 01:18:24.678477       1 log.go:198] OK: Validated signature on JWT
    I0112 01:18:24.678601       1 log.go:198] OK: Got valid claims from token!
    I0112 01:18:24.678637       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1279:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673486903, NotBefore:1673486303, IssuedAt:1673486303, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1279", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"35114e5e-536c-4e30-b77e-41dae0db4853"}}}

    Jan 12 01:18:59.113: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:18:59.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1279" for this suite. 01/12/23 01:18:59.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:18:59.147
Jan 12 01:18:59.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename security-context-test 01/12/23 01:18:59.148
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:18:59.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:18:59.171
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Jan 12 01:18:59.209: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6746b6ce-2fa3-43cc-9f06-d5a24e469fdd" in namespace "security-context-test-5112" to be "Succeeded or Failed"
Jan 12 01:18:59.211: INFO: Pod "busybox-privileged-false-6746b6ce-2fa3-43cc-9f06-d5a24e469fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.247538ms
Jan 12 01:19:01.215: INFO: Pod "busybox-privileged-false-6746b6ce-2fa3-43cc-9f06-d5a24e469fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005981462s
Jan 12 01:19:03.214: INFO: Pod "busybox-privileged-false-6746b6ce-2fa3-43cc-9f06-d5a24e469fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005221138s
Jan 12 01:19:05.216: INFO: Pod "busybox-privileged-false-6746b6ce-2fa3-43cc-9f06-d5a24e469fdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007079227s
Jan 12 01:19:05.216: INFO: Pod "busybox-privileged-false-6746b6ce-2fa3-43cc-9f06-d5a24e469fdd" satisfied condition "Succeeded or Failed"
Jan 12 01:19:05.224: INFO: Got logs for pod "busybox-privileged-false-6746b6ce-2fa3-43cc-9f06-d5a24e469fdd": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 12 01:19:05.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5112" for this suite. 01/12/23 01:19:05.228
------------------------------
• [SLOW TEST] [6.098 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:18:59.147
    Jan 12 01:18:59.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename security-context-test 01/12/23 01:18:59.148
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:18:59.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:18:59.171
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Jan 12 01:18:59.209: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6746b6ce-2fa3-43cc-9f06-d5a24e469fdd" in namespace "security-context-test-5112" to be "Succeeded or Failed"
    Jan 12 01:18:59.211: INFO: Pod "busybox-privileged-false-6746b6ce-2fa3-43cc-9f06-d5a24e469fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.247538ms
    Jan 12 01:19:01.215: INFO: Pod "busybox-privileged-false-6746b6ce-2fa3-43cc-9f06-d5a24e469fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005981462s
    Jan 12 01:19:03.214: INFO: Pod "busybox-privileged-false-6746b6ce-2fa3-43cc-9f06-d5a24e469fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005221138s
    Jan 12 01:19:05.216: INFO: Pod "busybox-privileged-false-6746b6ce-2fa3-43cc-9f06-d5a24e469fdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007079227s
    Jan 12 01:19:05.216: INFO: Pod "busybox-privileged-false-6746b6ce-2fa3-43cc-9f06-d5a24e469fdd" satisfied condition "Succeeded or Failed"
    Jan 12 01:19:05.224: INFO: Got logs for pod "busybox-privileged-false-6746b6ce-2fa3-43cc-9f06-d5a24e469fdd": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:19:05.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5112" for this suite. 01/12/23 01:19:05.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:19:05.246
Jan 12 01:19:05.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 01:19:05.247
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:19:05.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:19:05.274
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-8009 01/12/23 01:19:05.277
STEP: creating service affinity-clusterip in namespace services-8009 01/12/23 01:19:05.277
STEP: creating replication controller affinity-clusterip in namespace services-8009 01/12/23 01:19:05.309
I0112 01:19:05.320485      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8009, replica count: 3
I0112 01:19:08.370906      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 01:19:08.376: INFO: Creating new exec pod
Jan 12 01:19:08.415: INFO: Waiting up to 5m0s for pod "execpod-affinityrmtwv" in namespace "services-8009" to be "running"
Jan 12 01:19:08.418: INFO: Pod "execpod-affinityrmtwv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.470786ms
Jan 12 01:19:10.421: INFO: Pod "execpod-affinityrmtwv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006281143s
Jan 12 01:19:12.422: INFO: Pod "execpod-affinityrmtwv": Phase="Running", Reason="", readiness=true. Elapsed: 4.006696442s
Jan 12 01:19:12.422: INFO: Pod "execpod-affinityrmtwv" satisfied condition "running"
Jan 12 01:19:13.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-8009 exec execpod-affinityrmtwv -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan 12 01:19:13.636: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 12 01:19:13.636: INFO: stdout: ""
Jan 12 01:19:13.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-8009 exec execpod-affinityrmtwv -- /bin/sh -x -c nc -v -z -w 2 172.19.8.248 80'
Jan 12 01:19:13.826: INFO: stderr: "+ nc -v -z -w 2 172.19.8.248 80\nConnection to 172.19.8.248 80 port [tcp/http] succeeded!\n"
Jan 12 01:19:13.826: INFO: stdout: ""
Jan 12 01:19:13.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-8009 exec execpod-affinityrmtwv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.8.248:80/ ; done'
Jan 12 01:19:14.107: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n"
Jan 12 01:19:14.107: INFO: stdout: "\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh"
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
Jan 12 01:19:14.107: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-8009, will wait for the garbage collector to delete the pods 01/12/23 01:19:14.134
Jan 12 01:19:14.197: INFO: Deleting ReplicationController affinity-clusterip took: 10.154883ms
Jan 12 01:19:14.298: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.848696ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 01:19:17.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8009" for this suite. 01/12/23 01:19:17.347
------------------------------
• [SLOW TEST] [12.118 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:19:05.246
    Jan 12 01:19:05.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 01:19:05.247
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:19:05.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:19:05.274
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-8009 01/12/23 01:19:05.277
    STEP: creating service affinity-clusterip in namespace services-8009 01/12/23 01:19:05.277
    STEP: creating replication controller affinity-clusterip in namespace services-8009 01/12/23 01:19:05.309
    I0112 01:19:05.320485      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8009, replica count: 3
    I0112 01:19:08.370906      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 01:19:08.376: INFO: Creating new exec pod
    Jan 12 01:19:08.415: INFO: Waiting up to 5m0s for pod "execpod-affinityrmtwv" in namespace "services-8009" to be "running"
    Jan 12 01:19:08.418: INFO: Pod "execpod-affinityrmtwv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.470786ms
    Jan 12 01:19:10.421: INFO: Pod "execpod-affinityrmtwv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006281143s
    Jan 12 01:19:12.422: INFO: Pod "execpod-affinityrmtwv": Phase="Running", Reason="", readiness=true. Elapsed: 4.006696442s
    Jan 12 01:19:12.422: INFO: Pod "execpod-affinityrmtwv" satisfied condition "running"
    Jan 12 01:19:13.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-8009 exec execpod-affinityrmtwv -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan 12 01:19:13.636: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 12 01:19:13.636: INFO: stdout: ""
    Jan 12 01:19:13.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-8009 exec execpod-affinityrmtwv -- /bin/sh -x -c nc -v -z -w 2 172.19.8.248 80'
    Jan 12 01:19:13.826: INFO: stderr: "+ nc -v -z -w 2 172.19.8.248 80\nConnection to 172.19.8.248 80 port [tcp/http] succeeded!\n"
    Jan 12 01:19:13.826: INFO: stdout: ""
    Jan 12 01:19:13.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-8009 exec execpod-affinityrmtwv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.8.248:80/ ; done'
    Jan 12 01:19:14.107: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.8.248:80/\n"
    Jan 12 01:19:14.107: INFO: stdout: "\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh\naffinity-clusterip-t22fh"
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Received response from host: affinity-clusterip-t22fh
    Jan 12 01:19:14.107: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-8009, will wait for the garbage collector to delete the pods 01/12/23 01:19:14.134
    Jan 12 01:19:14.197: INFO: Deleting ReplicationController affinity-clusterip took: 10.154883ms
    Jan 12 01:19:14.298: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.848696ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:19:17.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8009" for this suite. 01/12/23 01:19:17.347
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:19:17.364
Jan 12 01:19:17.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename watch 01/12/23 01:19:17.365
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:19:17.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:19:17.382
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/12/23 01:19:17.384
STEP: starting a background goroutine to produce watch events 01/12/23 01:19:17.386
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/12/23 01:19:17.386
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 12 01:19:20.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1107" for this suite. 01/12/23 01:19:20.222
------------------------------
• [2.927 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:19:17.364
    Jan 12 01:19:17.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename watch 01/12/23 01:19:17.365
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:19:17.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:19:17.382
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/12/23 01:19:17.384
    STEP: starting a background goroutine to produce watch events 01/12/23 01:19:17.386
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/12/23 01:19:17.386
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:19:20.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1107" for this suite. 01/12/23 01:19:20.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:19:20.293
Jan 12 01:19:20.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pod-network-test 01/12/23 01:19:20.294
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:19:20.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:19:20.331
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-9407 01/12/23 01:19:20.333
STEP: creating a selector 01/12/23 01:19:20.333
STEP: Creating the service pods in kubernetes 01/12/23 01:19:20.334
Jan 12 01:19:20.334: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 12 01:19:20.429: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9407" to be "running and ready"
Jan 12 01:19:20.432: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.377937ms
Jan 12 01:19:20.432: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:19:22.435: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005590664s
Jan 12 01:19:22.435: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:19:24.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006216414s
Jan 12 01:19:24.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:19:26.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005684004s
Jan 12 01:19:26.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:19:28.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005929659s
Jan 12 01:19:28.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:19:30.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005686978s
Jan 12 01:19:30.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:19:32.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005728825s
Jan 12 01:19:32.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:19:34.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.006010364s
Jan 12 01:19:34.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:19:36.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.007235263s
Jan 12 01:19:36.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:19:38.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.005970565s
Jan 12 01:19:38.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:19:40.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.006386611s
Jan 12 01:19:40.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:19:42.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005601558s
Jan 12 01:19:42.435: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 12 01:19:42.435: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 12 01:19:42.437: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9407" to be "running and ready"
Jan 12 01:19:42.439: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.201492ms
Jan 12 01:19:42.439: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 12 01:19:42.439: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/12/23 01:19:42.441
Jan 12 01:19:42.563: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9407" to be "running"
Jan 12 01:19:42.566: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.353527ms
Jan 12 01:19:44.570: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006555298s
Jan 12 01:19:46.570: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006957166s
Jan 12 01:19:46.570: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 12 01:19:46.573: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 12 01:19:46.573: INFO: Breadth first check of 172.21.117.151 on host 10.9.140.106...
Jan 12 01:19:46.575: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.88.155:9080/dial?request=hostname&protocol=udp&host=172.21.117.151&port=8081&tries=1'] Namespace:pod-network-test-9407 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:19:46.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:19:46.575: INFO: ExecWithOptions: Clientset creation
Jan 12 01:19:46.575: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-9407/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.88.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.21.117.151%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 12 01:19:46.712: INFO: Waiting for responses: map[]
Jan 12 01:19:46.712: INFO: reached 172.21.117.151 after 0/1 tries
Jan 12 01:19:46.712: INFO: Breadth first check of 172.21.88.137 on host 10.9.40.106...
Jan 12 01:19:46.714: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.88.155:9080/dial?request=hostname&protocol=udp&host=172.21.88.137&port=8081&tries=1'] Namespace:pod-network-test-9407 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:19:46.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:19:46.715: INFO: ExecWithOptions: Clientset creation
Jan 12 01:19:46.715: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-9407/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.88.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.21.88.137%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 12 01:19:46.851: INFO: Waiting for responses: map[]
Jan 12 01:19:46.851: INFO: reached 172.21.88.137 after 0/1 tries
Jan 12 01:19:46.851: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 12 01:19:46.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9407" for this suite. 01/12/23 01:19:46.854
------------------------------
• [SLOW TEST] [26.629 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:19:20.293
    Jan 12 01:19:20.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pod-network-test 01/12/23 01:19:20.294
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:19:20.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:19:20.331
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-9407 01/12/23 01:19:20.333
    STEP: creating a selector 01/12/23 01:19:20.333
    STEP: Creating the service pods in kubernetes 01/12/23 01:19:20.334
    Jan 12 01:19:20.334: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 12 01:19:20.429: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9407" to be "running and ready"
    Jan 12 01:19:20.432: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.377937ms
    Jan 12 01:19:20.432: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:19:22.435: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005590664s
    Jan 12 01:19:22.435: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:19:24.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006216414s
    Jan 12 01:19:24.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:19:26.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005684004s
    Jan 12 01:19:26.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:19:28.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005929659s
    Jan 12 01:19:28.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:19:30.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005686978s
    Jan 12 01:19:30.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:19:32.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005728825s
    Jan 12 01:19:32.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:19:34.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.006010364s
    Jan 12 01:19:34.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:19:36.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.007235263s
    Jan 12 01:19:36.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:19:38.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.005970565s
    Jan 12 01:19:38.435: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:19:40.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.006386611s
    Jan 12 01:19:40.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:19:42.435: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005601558s
    Jan 12 01:19:42.435: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 12 01:19:42.435: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 12 01:19:42.437: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9407" to be "running and ready"
    Jan 12 01:19:42.439: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.201492ms
    Jan 12 01:19:42.439: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 12 01:19:42.439: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/12/23 01:19:42.441
    Jan 12 01:19:42.563: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9407" to be "running"
    Jan 12 01:19:42.566: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.353527ms
    Jan 12 01:19:44.570: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006555298s
    Jan 12 01:19:46.570: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006957166s
    Jan 12 01:19:46.570: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 12 01:19:46.573: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 12 01:19:46.573: INFO: Breadth first check of 172.21.117.151 on host 10.9.140.106...
    Jan 12 01:19:46.575: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.88.155:9080/dial?request=hostname&protocol=udp&host=172.21.117.151&port=8081&tries=1'] Namespace:pod-network-test-9407 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:19:46.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:19:46.575: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:19:46.575: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-9407/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.88.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.21.117.151%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 12 01:19:46.712: INFO: Waiting for responses: map[]
    Jan 12 01:19:46.712: INFO: reached 172.21.117.151 after 0/1 tries
    Jan 12 01:19:46.712: INFO: Breadth first check of 172.21.88.137 on host 10.9.40.106...
    Jan 12 01:19:46.714: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.88.155:9080/dial?request=hostname&protocol=udp&host=172.21.88.137&port=8081&tries=1'] Namespace:pod-network-test-9407 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:19:46.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:19:46.715: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:19:46.715: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-9407/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.88.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.21.88.137%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 12 01:19:46.851: INFO: Waiting for responses: map[]
    Jan 12 01:19:46.851: INFO: reached 172.21.88.137 after 0/1 tries
    Jan 12 01:19:46.851: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:19:46.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9407" for this suite. 01/12/23 01:19:46.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:19:46.923
Jan 12 01:19:46.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename security-context-test 01/12/23 01:19:46.924
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:19:46.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:19:46.961
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Jan 12 01:19:47.016: INFO: Waiting up to 5m0s for pod "busybox-user-65534-1939ea7e-22ff-40f0-aaa7-646d6f3ff564" in namespace "security-context-test-1476" to be "Succeeded or Failed"
Jan 12 01:19:47.018: INFO: Pod "busybox-user-65534-1939ea7e-22ff-40f0-aaa7-646d6f3ff564": Phase="Pending", Reason="", readiness=false. Elapsed: 2.515198ms
Jan 12 01:19:49.021: INFO: Pod "busybox-user-65534-1939ea7e-22ff-40f0-aaa7-646d6f3ff564": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005315699s
Jan 12 01:19:51.022: INFO: Pod "busybox-user-65534-1939ea7e-22ff-40f0-aaa7-646d6f3ff564": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006274925s
Jan 12 01:19:53.022: INFO: Pod "busybox-user-65534-1939ea7e-22ff-40f0-aaa7-646d6f3ff564": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00666926s
Jan 12 01:19:53.023: INFO: Pod "busybox-user-65534-1939ea7e-22ff-40f0-aaa7-646d6f3ff564" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 12 01:19:53.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1476" for this suite. 01/12/23 01:19:53.026
------------------------------
• [SLOW TEST] [6.121 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:19:46.923
    Jan 12 01:19:46.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename security-context-test 01/12/23 01:19:46.924
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:19:46.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:19:46.961
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Jan 12 01:19:47.016: INFO: Waiting up to 5m0s for pod "busybox-user-65534-1939ea7e-22ff-40f0-aaa7-646d6f3ff564" in namespace "security-context-test-1476" to be "Succeeded or Failed"
    Jan 12 01:19:47.018: INFO: Pod "busybox-user-65534-1939ea7e-22ff-40f0-aaa7-646d6f3ff564": Phase="Pending", Reason="", readiness=false. Elapsed: 2.515198ms
    Jan 12 01:19:49.021: INFO: Pod "busybox-user-65534-1939ea7e-22ff-40f0-aaa7-646d6f3ff564": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005315699s
    Jan 12 01:19:51.022: INFO: Pod "busybox-user-65534-1939ea7e-22ff-40f0-aaa7-646d6f3ff564": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006274925s
    Jan 12 01:19:53.022: INFO: Pod "busybox-user-65534-1939ea7e-22ff-40f0-aaa7-646d6f3ff564": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00666926s
    Jan 12 01:19:53.023: INFO: Pod "busybox-user-65534-1939ea7e-22ff-40f0-aaa7-646d6f3ff564" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:19:53.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1476" for this suite. 01/12/23 01:19:53.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:19:53.045
Jan 12 01:19:53.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename watch 01/12/23 01:19:53.045
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:19:53.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:19:53.062
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/12/23 01:19:53.066
STEP: modifying the configmap once 01/12/23 01:19:53.075
STEP: modifying the configmap a second time 01/12/23 01:19:53.093
STEP: deleting the configmap 01/12/23 01:19:53.103
STEP: creating a watch on configmaps from the resource version returned by the first update 01/12/23 01:19:53.113
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/12/23 01:19:53.113
Jan 12 01:19:53.114: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4605  2658681b-8bff-4e6a-8d78-f74a200bf70e 20156859 0 2023-01-12 01:19:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-12 01:19:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 01:19:53.114: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4605  2658681b-8bff-4e6a-8d78-f74a200bf70e 20156860 0 2023-01-12 01:19:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-12 01:19:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 12 01:19:53.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4605" for this suite. 01/12/23 01:19:53.132
------------------------------
• [0.105 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:19:53.045
    Jan 12 01:19:53.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename watch 01/12/23 01:19:53.045
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:19:53.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:19:53.062
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/12/23 01:19:53.066
    STEP: modifying the configmap once 01/12/23 01:19:53.075
    STEP: modifying the configmap a second time 01/12/23 01:19:53.093
    STEP: deleting the configmap 01/12/23 01:19:53.103
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/12/23 01:19:53.113
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/12/23 01:19:53.113
    Jan 12 01:19:53.114: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4605  2658681b-8bff-4e6a-8d78-f74a200bf70e 20156859 0 2023-01-12 01:19:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-12 01:19:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 01:19:53.114: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4605  2658681b-8bff-4e6a-8d78-f74a200bf70e 20156860 0 2023-01-12 01:19:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-12 01:19:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:19:53.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4605" for this suite. 01/12/23 01:19:53.132
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:19:53.15
Jan 12 01:19:53.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename containers 01/12/23 01:19:53.15
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:19:53.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:19:53.19
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Jan 12 01:19:53.226: INFO: Waiting up to 5m0s for pod "client-containers-2c49f300-ee5f-4b70-b899-f84a52b00b5d" in namespace "containers-1001" to be "running"
Jan 12 01:19:53.229: INFO: Pod "client-containers-2c49f300-ee5f-4b70-b899-f84a52b00b5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.257148ms
Jan 12 01:19:55.232: INFO: Pod "client-containers-2c49f300-ee5f-4b70-b899-f84a52b00b5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.00603062s
Jan 12 01:19:55.232: INFO: Pod "client-containers-2c49f300-ee5f-4b70-b899-f84a52b00b5d" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 12 01:19:55.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1001" for this suite. 01/12/23 01:19:55.244
------------------------------
• [2.189 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:19:53.15
    Jan 12 01:19:53.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename containers 01/12/23 01:19:53.15
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:19:53.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:19:53.19
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Jan 12 01:19:53.226: INFO: Waiting up to 5m0s for pod "client-containers-2c49f300-ee5f-4b70-b899-f84a52b00b5d" in namespace "containers-1001" to be "running"
    Jan 12 01:19:53.229: INFO: Pod "client-containers-2c49f300-ee5f-4b70-b899-f84a52b00b5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.257148ms
    Jan 12 01:19:55.232: INFO: Pod "client-containers-2c49f300-ee5f-4b70-b899-f84a52b00b5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.00603062s
    Jan 12 01:19:55.232: INFO: Pod "client-containers-2c49f300-ee5f-4b70-b899-f84a52b00b5d" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:19:55.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1001" for this suite. 01/12/23 01:19:55.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:19:55.342
Jan 12 01:19:55.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename statefulset 01/12/23 01:19:55.343
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:19:55.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:19:55.361
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8109 01/12/23 01:19:55.363
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 01/12/23 01:19:55.369
STEP: Creating pod with conflicting port in namespace statefulset-8109 01/12/23 01:19:55.382
STEP: Waiting until pod test-pod will start running in namespace statefulset-8109 01/12/23 01:19:55.515
Jan 12 01:19:55.515: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8109" to be "running"
Jan 12 01:19:55.518: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.41403ms
Jan 12 01:19:57.521: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005711883s
Jan 12 01:19:59.522: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006338079s
Jan 12 01:19:59.522: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-8109 01/12/23 01:19:59.522
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8109 01/12/23 01:19:59.719
Jan 12 01:19:59.796: INFO: Observed stateful pod in namespace: statefulset-8109, name: ss-0, uid: 3fc9de69-7406-4397-9ae1-7ffdb75e1bfa, status phase: Pending. Waiting for statefulset controller to delete.
Jan 12 01:19:59.824: INFO: Observed stateful pod in namespace: statefulset-8109, name: ss-0, uid: 3fc9de69-7406-4397-9ae1-7ffdb75e1bfa, status phase: Failed. Waiting for statefulset controller to delete.
Jan 12 01:19:59.836: INFO: Observed stateful pod in namespace: statefulset-8109, name: ss-0, uid: 3fc9de69-7406-4397-9ae1-7ffdb75e1bfa, status phase: Failed. Waiting for statefulset controller to delete.
Jan 12 01:19:59.836: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8109
STEP: Removing pod with conflicting port in namespace statefulset-8109 01/12/23 01:19:59.836
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8109 and will be in running state 01/12/23 01:19:59.875
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 01:20:03.891: INFO: Deleting all statefulset in ns statefulset-8109
Jan 12 01:20:03.893: INFO: Scaling statefulset ss to 0
Jan 12 01:20:13.944: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 01:20:13.946: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 01:20:13.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8109" for this suite. 01/12/23 01:20:13.972
------------------------------
• [SLOW TEST] [18.683 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:19:55.342
    Jan 12 01:19:55.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename statefulset 01/12/23 01:19:55.343
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:19:55.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:19:55.361
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8109 01/12/23 01:19:55.363
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 01/12/23 01:19:55.369
    STEP: Creating pod with conflicting port in namespace statefulset-8109 01/12/23 01:19:55.382
    STEP: Waiting until pod test-pod will start running in namespace statefulset-8109 01/12/23 01:19:55.515
    Jan 12 01:19:55.515: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8109" to be "running"
    Jan 12 01:19:55.518: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.41403ms
    Jan 12 01:19:57.521: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005711883s
    Jan 12 01:19:59.522: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006338079s
    Jan 12 01:19:59.522: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-8109 01/12/23 01:19:59.522
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8109 01/12/23 01:19:59.719
    Jan 12 01:19:59.796: INFO: Observed stateful pod in namespace: statefulset-8109, name: ss-0, uid: 3fc9de69-7406-4397-9ae1-7ffdb75e1bfa, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 12 01:19:59.824: INFO: Observed stateful pod in namespace: statefulset-8109, name: ss-0, uid: 3fc9de69-7406-4397-9ae1-7ffdb75e1bfa, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 12 01:19:59.836: INFO: Observed stateful pod in namespace: statefulset-8109, name: ss-0, uid: 3fc9de69-7406-4397-9ae1-7ffdb75e1bfa, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 12 01:19:59.836: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8109
    STEP: Removing pod with conflicting port in namespace statefulset-8109 01/12/23 01:19:59.836
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8109 and will be in running state 01/12/23 01:19:59.875
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 01:20:03.891: INFO: Deleting all statefulset in ns statefulset-8109
    Jan 12 01:20:03.893: INFO: Scaling statefulset ss to 0
    Jan 12 01:20:13.944: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 01:20:13.946: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:20:13.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8109" for this suite. 01/12/23 01:20:13.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:20:14.029
Jan 12 01:20:14.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 01:20:14.03
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:20:14.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:20:14.056
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 01/12/23 01:20:14.058
Jan 12 01:20:14.179: INFO: Waiting up to 5m0s for pod "pod-93f691b8-fca7-4b2f-823f-e35bad542054" in namespace "emptydir-4739" to be "Succeeded or Failed"
Jan 12 01:20:14.182: INFO: Pod "pod-93f691b8-fca7-4b2f-823f-e35bad542054": Phase="Pending", Reason="", readiness=false. Elapsed: 2.682053ms
Jan 12 01:20:16.186: INFO: Pod "pod-93f691b8-fca7-4b2f-823f-e35bad542054": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00694556s
Jan 12 01:20:18.186: INFO: Pod "pod-93f691b8-fca7-4b2f-823f-e35bad542054": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006610431s
Jan 12 01:20:20.193: INFO: Pod "pod-93f691b8-fca7-4b2f-823f-e35bad542054": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013264301s
STEP: Saw pod success 01/12/23 01:20:20.193
Jan 12 01:20:20.193: INFO: Pod "pod-93f691b8-fca7-4b2f-823f-e35bad542054" satisfied condition "Succeeded or Failed"
Jan 12 01:20:20.195: INFO: Trying to get logs from node eqx04-flash06 pod pod-93f691b8-fca7-4b2f-823f-e35bad542054 container test-container: <nil>
STEP: delete the pod 01/12/23 01:20:20.203
Jan 12 01:20:20.229: INFO: Waiting for pod pod-93f691b8-fca7-4b2f-823f-e35bad542054 to disappear
Jan 12 01:20:20.231: INFO: Pod pod-93f691b8-fca7-4b2f-823f-e35bad542054 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:20:20.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4739" for this suite. 01/12/23 01:20:20.235
------------------------------
• [SLOW TEST] [6.282 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:20:14.029
    Jan 12 01:20:14.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 01:20:14.03
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:20:14.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:20:14.056
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/12/23 01:20:14.058
    Jan 12 01:20:14.179: INFO: Waiting up to 5m0s for pod "pod-93f691b8-fca7-4b2f-823f-e35bad542054" in namespace "emptydir-4739" to be "Succeeded or Failed"
    Jan 12 01:20:14.182: INFO: Pod "pod-93f691b8-fca7-4b2f-823f-e35bad542054": Phase="Pending", Reason="", readiness=false. Elapsed: 2.682053ms
    Jan 12 01:20:16.186: INFO: Pod "pod-93f691b8-fca7-4b2f-823f-e35bad542054": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00694556s
    Jan 12 01:20:18.186: INFO: Pod "pod-93f691b8-fca7-4b2f-823f-e35bad542054": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006610431s
    Jan 12 01:20:20.193: INFO: Pod "pod-93f691b8-fca7-4b2f-823f-e35bad542054": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013264301s
    STEP: Saw pod success 01/12/23 01:20:20.193
    Jan 12 01:20:20.193: INFO: Pod "pod-93f691b8-fca7-4b2f-823f-e35bad542054" satisfied condition "Succeeded or Failed"
    Jan 12 01:20:20.195: INFO: Trying to get logs from node eqx04-flash06 pod pod-93f691b8-fca7-4b2f-823f-e35bad542054 container test-container: <nil>
    STEP: delete the pod 01/12/23 01:20:20.203
    Jan 12 01:20:20.229: INFO: Waiting for pod pod-93f691b8-fca7-4b2f-823f-e35bad542054 to disappear
    Jan 12 01:20:20.231: INFO: Pod pod-93f691b8-fca7-4b2f-823f-e35bad542054 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:20:20.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4739" for this suite. 01/12/23 01:20:20.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:20:20.311
Jan 12 01:20:20.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:20:20.312
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:20:20.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:20:20.338
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-db3b930e-16a5-492d-bfcc-9813908ff846 01/12/23 01:20:20.34
STEP: Creating a pod to test consume configMaps 01/12/23 01:20:20.346
Jan 12 01:20:20.505: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de" in namespace "projected-1725" to be "Succeeded or Failed"
Jan 12 01:20:20.508: INFO: Pod "pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.391826ms
Jan 12 01:20:22.513: INFO: Pod "pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007681249s
Jan 12 01:20:24.513: INFO: Pod "pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007527805s
Jan 12 01:20:26.513: INFO: Pod "pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007852085s
STEP: Saw pod success 01/12/23 01:20:26.513
Jan 12 01:20:26.513: INFO: Pod "pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de" satisfied condition "Succeeded or Failed"
Jan 12 01:20:26.516: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de container agnhost-container: <nil>
STEP: delete the pod 01/12/23 01:20:26.526
Jan 12 01:20:26.546: INFO: Waiting for pod pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de to disappear
Jan 12 01:20:26.548: INFO: Pod pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:20:26.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1725" for this suite. 01/12/23 01:20:26.554
------------------------------
• [SLOW TEST] [6.482 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:20:20.311
    Jan 12 01:20:20.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:20:20.312
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:20:20.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:20:20.338
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-db3b930e-16a5-492d-bfcc-9813908ff846 01/12/23 01:20:20.34
    STEP: Creating a pod to test consume configMaps 01/12/23 01:20:20.346
    Jan 12 01:20:20.505: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de" in namespace "projected-1725" to be "Succeeded or Failed"
    Jan 12 01:20:20.508: INFO: Pod "pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.391826ms
    Jan 12 01:20:22.513: INFO: Pod "pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007681249s
    Jan 12 01:20:24.513: INFO: Pod "pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007527805s
    Jan 12 01:20:26.513: INFO: Pod "pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007852085s
    STEP: Saw pod success 01/12/23 01:20:26.513
    Jan 12 01:20:26.513: INFO: Pod "pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de" satisfied condition "Succeeded or Failed"
    Jan 12 01:20:26.516: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 01:20:26.526
    Jan 12 01:20:26.546: INFO: Waiting for pod pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de to disappear
    Jan 12 01:20:26.548: INFO: Pod pod-projected-configmaps-79a8c293-c8fb-45ef-8106-89b55a1f87de no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:20:26.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1725" for this suite. 01/12/23 01:20:26.554
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:20:26.793
Jan 12 01:20:26.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename deployment 01/12/23 01:20:26.794
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:20:26.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:20:26.826
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 12 01:20:26.829: INFO: Creating simple deployment test-new-deployment
Jan 12 01:20:27.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 20, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 20, 26, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-new-deployment-7f5969cbc7\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 20, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 20, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:20:29.018: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 20, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 20, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 20, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 20, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 01/12/23 01:20:31.02
STEP: updating a scale subresource 01/12/23 01:20:31.022
STEP: verifying the deployment Spec.Replicas was modified 01/12/23 01:20:31.036
STEP: Patch a scale subresource 01/12/23 01:20:31.05
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 01:20:31.123: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1218  25d12529-c886-4467-9f2d-3027e83beb6e 20157233 3 2023-01-12 01:20:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-12 01:20:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:20:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047e0498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:3,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-12 01:20:29 +0000 UTC,LastTransitionTime:2023-01-12 01:20:26 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-12 01:20:31 +0000 UTC,LastTransitionTime:2023-01-12 01:20:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 12 01:20:31.126: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-1218  8b75a79f-8114-4e33-b7dc-414e595554f5 20157236 3 2023-01-12 01:20:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 25d12529-c886-4467-9f2d-3027e83beb6e 0xc0049408d7 0xc0049408d8}] [] [{kube-controller-manager Update apps/v1 2023-01-12 01:20:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25d12529-c886-4467-9f2d-3027e83beb6e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:20:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004940968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 12 01:20:31.129: INFO: Pod "test-new-deployment-7f5969cbc7-f62f6" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-f62f6 test-new-deployment-7f5969cbc7- deployment-1218  5b38f0a7-4f71-4111-8d68-ae670eed181f 20157234 0 2023-01-12 01:20:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 8b75a79f-8114-4e33-b7dc-414e595554f5 0xc0047e0887 0xc0047e0888}] [] [{kube-controller-manager Update v1 2023-01-12 01:20:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b75a79f-8114-4e33-b7dc-414e595554f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vv7x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vv7x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 01:20:31.129: INFO: Pod "test-new-deployment-7f5969cbc7-h9ktg" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-h9ktg test-new-deployment-7f5969cbc7- deployment-1218  72bcd9c2-3ee0-4b4a-b460-35bebbcb9abd 20157212 0 2023-01-12 01:20:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c26d26f48144545f8cc06b2c994ee6a22454688b1a113c1cc29ccd72ca35eca6 cni.projectcalico.org/podIP:172.21.88.186/32 cni.projectcalico.org/podIPs:172.21.88.186/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.186"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.186"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 8b75a79f-8114-4e33-b7dc-414e595554f5 0xc0047e09f0 0xc0047e09f1}] [] [{kube-controller-manager Update v1 2023-01-12 01:20:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b75a79f-8114-4e33-b7dc-414e595554f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:20:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:20:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:20:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pz4tv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pz4tv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:20:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:20:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:20:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:20:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.186,StartTime:2023-01-12 01:20:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:20:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://d4a0a9133c33757c2ee550108775f90ba0b5a8d2d330f24e0847f0d04442aa76,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 01:20:31.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1218" for this suite. 01/12/23 01:20:31.133
------------------------------
• [4.375 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:20:26.793
    Jan 12 01:20:26.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename deployment 01/12/23 01:20:26.794
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:20:26.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:20:26.826
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 12 01:20:26.829: INFO: Creating simple deployment test-new-deployment
    Jan 12 01:20:27.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 20, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 20, 26, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-new-deployment-7f5969cbc7\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 20, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 20, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:20:29.018: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 20, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 20, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 20, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 20, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 01/12/23 01:20:31.02
    STEP: updating a scale subresource 01/12/23 01:20:31.022
    STEP: verifying the deployment Spec.Replicas was modified 01/12/23 01:20:31.036
    STEP: Patch a scale subresource 01/12/23 01:20:31.05
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 01:20:31.123: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-1218  25d12529-c886-4467-9f2d-3027e83beb6e 20157233 3 2023-01-12 01:20:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-12 01:20:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:20:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047e0498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:3,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-12 01:20:29 +0000 UTC,LastTransitionTime:2023-01-12 01:20:26 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-12 01:20:31 +0000 UTC,LastTransitionTime:2023-01-12 01:20:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 12 01:20:31.126: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-1218  8b75a79f-8114-4e33-b7dc-414e595554f5 20157236 3 2023-01-12 01:20:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 25d12529-c886-4467-9f2d-3027e83beb6e 0xc0049408d7 0xc0049408d8}] [] [{kube-controller-manager Update apps/v1 2023-01-12 01:20:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25d12529-c886-4467-9f2d-3027e83beb6e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:20:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004940968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 01:20:31.129: INFO: Pod "test-new-deployment-7f5969cbc7-f62f6" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-f62f6 test-new-deployment-7f5969cbc7- deployment-1218  5b38f0a7-4f71-4111-8d68-ae670eed181f 20157234 0 2023-01-12 01:20:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 8b75a79f-8114-4e33-b7dc-414e595554f5 0xc0047e0887 0xc0047e0888}] [] [{kube-controller-manager Update v1 2023-01-12 01:20:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b75a79f-8114-4e33-b7dc-414e595554f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vv7x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vv7x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 01:20:31.129: INFO: Pod "test-new-deployment-7f5969cbc7-h9ktg" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-h9ktg test-new-deployment-7f5969cbc7- deployment-1218  72bcd9c2-3ee0-4b4a-b460-35bebbcb9abd 20157212 0 2023-01-12 01:20:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c26d26f48144545f8cc06b2c994ee6a22454688b1a113c1cc29ccd72ca35eca6 cni.projectcalico.org/podIP:172.21.88.186/32 cni.projectcalico.org/podIPs:172.21.88.186/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.186"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.186"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 8b75a79f-8114-4e33-b7dc-414e595554f5 0xc0047e09f0 0xc0047e09f1}] [] [{kube-controller-manager Update v1 2023-01-12 01:20:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b75a79f-8114-4e33-b7dc-414e595554f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:20:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:20:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:20:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pz4tv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pz4tv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:20:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:20:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:20:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:20:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.186,StartTime:2023-01-12 01:20:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:20:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://d4a0a9133c33757c2ee550108775f90ba0b5a8d2d330f24e0847f0d04442aa76,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:20:31.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1218" for this suite. 01/12/23 01:20:31.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:20:31.169
Jan 12 01:20:31.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename sched-pred 01/12/23 01:20:31.17
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:20:31.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:20:31.193
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 12 01:20:31.195: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 12 01:20:31.202: INFO: Waiting for terminating namespaces to be deleted...
Jan 12 01:20:31.206: INFO: 
Logging pods the apiserver thinks is on node eqx03-flash06 before test
Jan 12 01:20:31.257: INFO: calico-kube-controllers-5c9fd9b888-vc4sl from kube-system started at 2023-01-11 19:29:05 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 12 01:20:31.257: INFO: calico-node-lwrhk from kube-system started at 2023-01-11 08:01:00 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container calico-node ready: true, restart count 0
Jan 12 01:20:31.257: INFO: kube-multus-ds-amd64-gsh8m from kube-system started at 2022-10-10 22:16:50 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container kube-multus ready: true, restart count 2
Jan 12 01:20:31.257: INFO: kube-proxy-h8fqt from kube-system started at 2023-01-11 07:45:11 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 01:20:31.257: INFO: kube-sriov-device-plugin-amd64-4wg6g from kube-system started at 2022-10-10 22:14:57 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container kube-sriovdp ready: true, restart count 2
Jan 12 01:20:31.257: INFO: mariadb-1665683911-master-0 from maria started at 2023-01-11 07:55:52 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mariadb ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mariadb-1665683911-slave-0 from maria started at 2023-01-11 07:56:03 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mariadb ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1664779250-7dc5656c45-vcb4f from mg started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1664779250 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1664779251-858bf7fdc-6scml from mg started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1664779251 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1664779253-866d78dd7-n8j9t from mg started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1664779253 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1664779256-5c6fc69c89-9kvgw from mg started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1664779256 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779263-controller-bf7587b7f-8f6bc from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779263-default-backend-cdc6d499b-ng78h from ng started at 2023-01-11 07:55:44 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779266-controller-677944959d-dnpbb from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779266-default-backend-bbdfc88b7-7ln9d from ng started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779278-controller-5fb4f984c-cshwh from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779278-default-backend-5dfb4f9db4-bdjmg from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779281-controller-75b6585875-6fwmp from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779281-default-backend-d5fb87996-kl2gz from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779288-controller-9744ff446-rhjs8 from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779288-default-backend-594965dbd7-kb8tt from ng started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779291-controller-ff67b7844-jqvh6 from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779291-default-backend-66f95c66fc-2vfvf from ng started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779294-controller-84785d75f7-x4w9c from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779294-default-backend-6765dd7578-v796m from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779297-controller-57f654c69d-vzn6l from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
Jan 12 01:20:31.257: INFO: nginx-ingress-1664779297-default-backend-74df75bf95-n6xwg from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 01:20:31.257: INFO: csi-nodeplugin-robin-hnwgz from robinio started at 2023-01-11 08:44:24 +0000 UTC (3 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container driver-registrar ready: true, restart count 0
Jan 12 01:20:31.257: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 12 01:20:31.257: INFO: 	Container robin ready: true, restart count 0
Jan 12 01:20:31.257: INFO: robin-nfs-watchdog-qrzrq from robinio started at 2023-01-11 08:28:03 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
Jan 12 01:20:31.257: INFO: robin-worker-jlr7d from robinio started at 2023-01-11 08:27:55 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container robinrcm ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089095-66b956f5d-vxj2b from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089095 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089145-598df7974-kn9cv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089145 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089149-5574fb7774-t7pqg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089149 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089152-84f657bf94-dvsbc from sa-ns-user started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089152 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089179-5bf57c5944-tp4xz from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089179 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089182-6c58488f6d-5jn9v from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089182 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089185-677dc7ffb6-b9pl6 from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089185 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089188-58475957bd-5nfbd from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089188 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089191-66859c96dd-lkchj from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089191 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089205-5ff87d5b8d-wch4v from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089205 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089217-76648fcb6f-9hkfm from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089217 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089220-595bdf59bf-wzg6h from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089220 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089231-fd7d54889-stkqh from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089231 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089236-66c6896dcf-jjztv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089236 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089247-845bdb94dc-z5mxg from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089247 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089249-68588bc459-psrj7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089249 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089253-7d549cf945-mhld7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089253 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089257-6c4b6dd79c-tz5d5 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089257 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089259-58c44567c7-jmzbp from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089259 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089262-744cbfcf5c-lzzbl from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089262 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089268-5867478f97-c44xg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089268 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089337-9dbc9475f-zzk9n from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089337 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665089342-69f7c77fd7-9l5qq from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665089342 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665088984-56cbf7747c-8zfh5 from sa-ns started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665088984 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: mysql-1665088986-7785c569-hfqvj from sa-ns started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql-1665088986 ready: true, restart count 0
Jan 12 01:20:31.257: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-ppd9w from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 01:20:31.257: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 12 01:20:31.257: INFO: ravi-ravi-mysql-0 from t001-u000004 started at 2023-01-11 07:55:59 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.257: INFO: 	Container mysql ready: true, restart count 0
Jan 12 01:20:31.257: INFO: 
Logging pods the apiserver thinks is on node eqx04-flash06 before test
Jan 12 01:20:31.273: INFO: test-new-deployment-7f5969cbc7-h9ktg from deployment-1218 started at 2023-01-12 01:20:27 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.273: INFO: 	Container httpd ready: true, restart count 0
Jan 12 01:20:31.273: INFO: calico-node-wh5zd from kube-system started at 2023-01-11 08:02:20 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.273: INFO: 	Container calico-node ready: true, restart count 0
Jan 12 01:20:31.273: INFO: kube-multus-ds-amd64-qllb4 from kube-system started at 2023-01-12 01:12:37 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.273: INFO: 	Container kube-multus ready: true, restart count 0
Jan 12 01:20:31.273: INFO: kube-proxy-kvvhz from kube-system started at 2023-01-11 07:45:13 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.273: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 01:20:31.273: INFO: kube-sriov-device-plugin-amd64-b2hg7 from kube-system started at 2023-01-12 01:12:30 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.273: INFO: 	Container kube-sriovdp ready: true, restart count 0
Jan 12 01:20:31.273: INFO: csi-nodeplugin-robin-z9mk4 from robinio started at 2023-01-12 01:12:37 +0000 UTC (3 container statuses recorded)
Jan 12 01:20:31.273: INFO: 	Container driver-registrar ready: true, restart count 0
Jan 12 01:20:31.273: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 12 01:20:31.273: INFO: 	Container robin ready: true, restart count 0
Jan 12 01:20:31.273: INFO: robin-nfs-watchdog-cpvh5 from robinio started at 2023-01-12 01:12:30 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.273: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
Jan 12 01:20:31.273: INFO: robin-worker-pczc5 from robinio started at 2023-01-12 01:13:06 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.273: INFO: 	Container robinrcm ready: true, restart count 0
Jan 12 01:20:31.273: INFO: sonobuoy from sonobuoy started at 2023-01-12 00:40:33 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.273: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 12 01:20:31.273: INFO: sonobuoy-e2e-job-90575ca5f8b04bb8 from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
Jan 12 01:20:31.273: INFO: 	Container e2e ready: true, restart count 0
Jan 12 01:20:31.273: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 01:20:31.273: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-mkhnx from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
Jan 12 01:20:31.273: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 01:20:31.273: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 12 01:20:31.273: INFO: cent-1-server-01 from t001-u000004 started at 2023-01-12 01:14:56 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.273: INFO: 	Container cent-1-server-01 ready: true, restart count 0
Jan 12 01:20:31.273: INFO: cent-2-server-01 from t001-u000004 started at 2023-01-12 01:14:56 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.273: INFO: 	Container cent-2-server-01 ready: true, restart count 0
Jan 12 01:20:31.273: INFO: sq-mysql-01 from t001-u000004 started at 2023-01-12 01:14:45 +0000 UTC (1 container statuses recorded)
Jan 12 01:20:31.273: INFO: 	Container sq-mysql-01 ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/12/23 01:20:31.273
Jan 12 01:20:31.329: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9634" to be "running"
Jan 12 01:20:31.331: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.376335ms
Jan 12 01:20:33.334: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005569143s
Jan 12 01:20:35.336: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.0068055s
Jan 12 01:20:35.336: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/12/23 01:20:35.338
STEP: Trying to apply a random label on the found node. 01/12/23 01:20:35.358
STEP: verifying the node has the label kubernetes.io/e2e-56164922-fdc6-4bae-b3f4-06d67f7fc2e9 95 01/12/23 01:20:35.38
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/12/23 01:20:35.384
Jan 12 01:20:35.452: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-9634" to be "not pending"
Jan 12 01:20:35.455: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.617393ms
Jan 12 01:20:37.460: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007980615s
Jan 12 01:20:39.458: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.00583182s
Jan 12 01:20:39.458: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.9.40.106 on the node which pod4 resides and expect not scheduled 01/12/23 01:20:39.458
Jan 12 01:20:39.490: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-9634" to be "not pending"
Jan 12 01:20:39.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.428386ms
Jan 12 01:20:41.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006383047s
Jan 12 01:20:43.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006509872s
Jan 12 01:20:45.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00702379s
Jan 12 01:20:47.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005742916s
Jan 12 01:20:49.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006053095s
Jan 12 01:20:51.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006196214s
Jan 12 01:20:53.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007169882s
Jan 12 01:20:55.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006315467s
Jan 12 01:20:57.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007414221s
Jan 12 01:20:59.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006827932s
Jan 12 01:21:01.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.006395579s
Jan 12 01:21:03.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006525417s
Jan 12 01:21:05.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006700741s
Jan 12 01:21:07.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005425218s
Jan 12 01:21:09.501: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011091392s
Jan 12 01:21:11.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006378169s
Jan 12 01:21:13.502: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012022752s
Jan 12 01:21:15.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007229093s
Jan 12 01:21:17.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006141317s
Jan 12 01:21:19.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00662242s
Jan 12 01:21:21.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006242364s
Jan 12 01:21:23.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005586606s
Jan 12 01:21:25.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.006932536s
Jan 12 01:21:27.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.005695611s
Jan 12 01:21:29.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00557703s
Jan 12 01:21:31.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006445316s
Jan 12 01:21:33.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.006567327s
Jan 12 01:21:35.498: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007765839s
Jan 12 01:21:37.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005885041s
Jan 12 01:21:39.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005560244s
Jan 12 01:21:41.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005778338s
Jan 12 01:21:43.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006681296s
Jan 12 01:21:45.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007019345s
Jan 12 01:21:47.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006496284s
Jan 12 01:21:49.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006526983s
Jan 12 01:21:51.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007296097s
Jan 12 01:21:53.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006461102s
Jan 12 01:21:55.499: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009581461s
Jan 12 01:21:57.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006304777s
Jan 12 01:21:59.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.00668297s
Jan 12 01:22:01.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006354015s
Jan 12 01:22:03.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006327911s
Jan 12 01:22:05.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005373193s
Jan 12 01:22:07.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006181615s
Jan 12 01:22:09.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.006903675s
Jan 12 01:22:11.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006857063s
Jan 12 01:22:13.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006723438s
Jan 12 01:22:15.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006270928s
Jan 12 01:22:17.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006383586s
Jan 12 01:22:19.499: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009049107s
Jan 12 01:22:21.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005509661s
Jan 12 01:22:23.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006309304s
Jan 12 01:22:25.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006098743s
Jan 12 01:22:27.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005766344s
Jan 12 01:22:29.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006656633s
Jan 12 01:22:31.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006278584s
Jan 12 01:22:33.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006526417s
Jan 12 01:22:35.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.007506175s
Jan 12 01:22:37.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006131993s
Jan 12 01:22:39.498: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007821171s
Jan 12 01:22:41.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.006260772s
Jan 12 01:22:43.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006129032s
Jan 12 01:22:45.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.007400824s
Jan 12 01:22:47.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005774446s
Jan 12 01:22:49.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.006894188s
Jan 12 01:22:51.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.006108152s
Jan 12 01:22:53.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.005916016s
Jan 12 01:22:55.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.006126882s
Jan 12 01:22:57.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.005644288s
Jan 12 01:22:59.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.006266451s
Jan 12 01:23:01.504: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.014353985s
Jan 12 01:23:03.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.006317768s
Jan 12 01:23:05.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.00569797s
Jan 12 01:23:07.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.016142299s
Jan 12 01:23:09.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.00651538s
Jan 12 01:23:11.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.007143518s
Jan 12 01:23:13.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.006037176s
Jan 12 01:23:15.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.00716736s
Jan 12 01:23:17.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.006361991s
Jan 12 01:23:19.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.007286913s
Jan 12 01:23:21.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.005811778s
Jan 12 01:23:23.504: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.01388486s
Jan 12 01:23:25.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007050933s
Jan 12 01:23:27.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.00556253s
Jan 12 01:23:29.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.005745519s
Jan 12 01:23:31.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.006032899s
Jan 12 01:23:33.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.007212215s
Jan 12 01:23:35.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.005285104s
Jan 12 01:23:37.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.006517599s
Jan 12 01:23:39.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.006159721s
Jan 12 01:23:41.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.006538352s
Jan 12 01:23:43.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.006864162s
Jan 12 01:23:45.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.00686218s
Jan 12 01:23:47.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.005655768s
Jan 12 01:23:49.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.006831775s
Jan 12 01:23:51.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.006886536s
Jan 12 01:23:53.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.006645011s
Jan 12 01:23:55.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.0064609s
Jan 12 01:23:57.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.005696747s
Jan 12 01:23:59.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.006126666s
Jan 12 01:24:01.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.007085363s
Jan 12 01:24:03.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.007007932s
Jan 12 01:24:05.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.006268068s
Jan 12 01:24:07.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.006263182s
Jan 12 01:24:09.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.006581311s
Jan 12 01:24:11.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.006945225s
Jan 12 01:24:13.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.006882941s
Jan 12 01:24:15.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.005661504s
Jan 12 01:24:17.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006235898s
Jan 12 01:24:19.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.007029843s
Jan 12 01:24:21.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.006723724s
Jan 12 01:24:23.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.006864008s
Jan 12 01:24:25.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.006871493s
Jan 12 01:24:27.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.005996505s
Jan 12 01:24:29.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.006908985s
Jan 12 01:24:31.499: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.009200576s
Jan 12 01:24:33.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.00620724s
Jan 12 01:24:35.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007059187s
Jan 12 01:24:37.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.005579246s
Jan 12 01:24:39.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.006573857s
Jan 12 01:24:41.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.005694557s
Jan 12 01:24:43.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.00623799s
Jan 12 01:24:45.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007401898s
Jan 12 01:24:47.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.005834497s
Jan 12 01:24:49.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.006190423s
Jan 12 01:24:51.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.006082544s
Jan 12 01:24:53.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.007590558s
Jan 12 01:24:55.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.005847811s
Jan 12 01:24:57.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005778415s
Jan 12 01:24:59.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.006117716s
Jan 12 01:25:01.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.00648718s
Jan 12 01:25:03.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.005862275s
Jan 12 01:25:05.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.006831627s
Jan 12 01:25:07.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.005530145s
Jan 12 01:25:09.503: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.012854773s
Jan 12 01:25:11.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.00626931s
Jan 12 01:25:13.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.005886123s
Jan 12 01:25:15.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.006286941s
Jan 12 01:25:17.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00568621s
Jan 12 01:25:19.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.005897662s
Jan 12 01:25:21.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.006095482s
Jan 12 01:25:23.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.006128901s
Jan 12 01:25:25.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.006002402s
Jan 12 01:25:27.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.006405133s
Jan 12 01:25:29.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.007048451s
Jan 12 01:25:31.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.006876551s
Jan 12 01:25:33.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007384646s
Jan 12 01:25:35.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.007572919s
Jan 12 01:25:37.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.005617392s
Jan 12 01:25:39.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007425286s
Jan 12 01:25:39.501: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010977595s
STEP: removing the label kubernetes.io/e2e-56164922-fdc6-4bae-b3f4-06d67f7fc2e9 off the node eqx04-flash06 01/12/23 01:25:39.501
STEP: verifying the node doesn't have the label kubernetes.io/e2e-56164922-fdc6-4bae-b3f4-06d67f7fc2e9 01/12/23 01:25:39.518
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:25:39.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9634" for this suite. 01/12/23 01:25:39.527
------------------------------
• [SLOW TEST] [308.400 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:20:31.169
    Jan 12 01:20:31.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename sched-pred 01/12/23 01:20:31.17
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:20:31.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:20:31.193
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 12 01:20:31.195: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 12 01:20:31.202: INFO: Waiting for terminating namespaces to be deleted...
    Jan 12 01:20:31.206: INFO: 
    Logging pods the apiserver thinks is on node eqx03-flash06 before test
    Jan 12 01:20:31.257: INFO: calico-kube-controllers-5c9fd9b888-vc4sl from kube-system started at 2023-01-11 19:29:05 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: calico-node-lwrhk from kube-system started at 2023-01-11 08:01:00 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container calico-node ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: kube-multus-ds-amd64-gsh8m from kube-system started at 2022-10-10 22:16:50 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container kube-multus ready: true, restart count 2
    Jan 12 01:20:31.257: INFO: kube-proxy-h8fqt from kube-system started at 2023-01-11 07:45:11 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: kube-sriov-device-plugin-amd64-4wg6g from kube-system started at 2022-10-10 22:14:57 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container kube-sriovdp ready: true, restart count 2
    Jan 12 01:20:31.257: INFO: mariadb-1665683911-master-0 from maria started at 2023-01-11 07:55:52 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mariadb ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mariadb-1665683911-slave-0 from maria started at 2023-01-11 07:56:03 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mariadb ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1664779250-7dc5656c45-vcb4f from mg started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1664779250 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1664779251-858bf7fdc-6scml from mg started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1664779251 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1664779253-866d78dd7-n8j9t from mg started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1664779253 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1664779256-5c6fc69c89-9kvgw from mg started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1664779256 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779263-controller-bf7587b7f-8f6bc from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779263-default-backend-cdc6d499b-ng78h from ng started at 2023-01-11 07:55:44 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779266-controller-677944959d-dnpbb from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779266-default-backend-bbdfc88b7-7ln9d from ng started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779278-controller-5fb4f984c-cshwh from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779278-default-backend-5dfb4f9db4-bdjmg from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779281-controller-75b6585875-6fwmp from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779281-default-backend-d5fb87996-kl2gz from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779288-controller-9744ff446-rhjs8 from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779288-default-backend-594965dbd7-kb8tt from ng started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779291-controller-ff67b7844-jqvh6 from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779291-default-backend-66f95c66fc-2vfvf from ng started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779294-controller-84785d75f7-x4w9c from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779294-default-backend-6765dd7578-v796m from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779297-controller-57f654c69d-vzn6l from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-controller ready: false, restart count 6
    Jan 12 01:20:31.257: INFO: nginx-ingress-1664779297-default-backend-74df75bf95-n6xwg from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: csi-nodeplugin-robin-hnwgz from robinio started at 2023-01-11 08:44:24 +0000 UTC (3 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: 	Container robin ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: robin-nfs-watchdog-qrzrq from robinio started at 2023-01-11 08:28:03 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: robin-worker-jlr7d from robinio started at 2023-01-11 08:27:55 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container robinrcm ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089095-66b956f5d-vxj2b from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089095 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089145-598df7974-kn9cv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089145 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089149-5574fb7774-t7pqg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089149 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089152-84f657bf94-dvsbc from sa-ns-user started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089152 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089179-5bf57c5944-tp4xz from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089179 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089182-6c58488f6d-5jn9v from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089182 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089185-677dc7ffb6-b9pl6 from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089185 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089188-58475957bd-5nfbd from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089188 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089191-66859c96dd-lkchj from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089191 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089205-5ff87d5b8d-wch4v from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089205 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089217-76648fcb6f-9hkfm from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089217 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089220-595bdf59bf-wzg6h from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089220 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089231-fd7d54889-stkqh from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089231 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089236-66c6896dcf-jjztv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089236 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089247-845bdb94dc-z5mxg from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089247 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089249-68588bc459-psrj7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089249 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089253-7d549cf945-mhld7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089253 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089257-6c4b6dd79c-tz5d5 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089257 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089259-58c44567c7-jmzbp from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089259 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089262-744cbfcf5c-lzzbl from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089262 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089268-5867478f97-c44xg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089268 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089337-9dbc9475f-zzk9n from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089337 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665089342-69f7c77fd7-9l5qq from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665089342 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665088984-56cbf7747c-8zfh5 from sa-ns started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665088984 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: mysql-1665088986-7785c569-hfqvj from sa-ns started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql-1665088986 ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-ppd9w from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: ravi-ravi-mysql-0 from t001-u000004 started at 2023-01-11 07:55:59 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.257: INFO: 	Container mysql ready: true, restart count 0
    Jan 12 01:20:31.257: INFO: 
    Logging pods the apiserver thinks is on node eqx04-flash06 before test
    Jan 12 01:20:31.273: INFO: test-new-deployment-7f5969cbc7-h9ktg from deployment-1218 started at 2023-01-12 01:20:27 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.273: INFO: 	Container httpd ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: calico-node-wh5zd from kube-system started at 2023-01-11 08:02:20 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.273: INFO: 	Container calico-node ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: kube-multus-ds-amd64-qllb4 from kube-system started at 2023-01-12 01:12:37 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.273: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: kube-proxy-kvvhz from kube-system started at 2023-01-11 07:45:13 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.273: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: kube-sriov-device-plugin-amd64-b2hg7 from kube-system started at 2023-01-12 01:12:30 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.273: INFO: 	Container kube-sriovdp ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: csi-nodeplugin-robin-z9mk4 from robinio started at 2023-01-12 01:12:37 +0000 UTC (3 container statuses recorded)
    Jan 12 01:20:31.273: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: 	Container robin ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: robin-nfs-watchdog-cpvh5 from robinio started at 2023-01-12 01:12:30 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.273: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: robin-worker-pczc5 from robinio started at 2023-01-12 01:13:06 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.273: INFO: 	Container robinrcm ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: sonobuoy from sonobuoy started at 2023-01-12 00:40:33 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.273: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: sonobuoy-e2e-job-90575ca5f8b04bb8 from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
    Jan 12 01:20:31.273: INFO: 	Container e2e ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-mkhnx from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
    Jan 12 01:20:31.273: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: cent-1-server-01 from t001-u000004 started at 2023-01-12 01:14:56 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.273: INFO: 	Container cent-1-server-01 ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: cent-2-server-01 from t001-u000004 started at 2023-01-12 01:14:56 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.273: INFO: 	Container cent-2-server-01 ready: true, restart count 0
    Jan 12 01:20:31.273: INFO: sq-mysql-01 from t001-u000004 started at 2023-01-12 01:14:45 +0000 UTC (1 container statuses recorded)
    Jan 12 01:20:31.273: INFO: 	Container sq-mysql-01 ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/12/23 01:20:31.273
    Jan 12 01:20:31.329: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9634" to be "running"
    Jan 12 01:20:31.331: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.376335ms
    Jan 12 01:20:33.334: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005569143s
    Jan 12 01:20:35.336: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.0068055s
    Jan 12 01:20:35.336: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/12/23 01:20:35.338
    STEP: Trying to apply a random label on the found node. 01/12/23 01:20:35.358
    STEP: verifying the node has the label kubernetes.io/e2e-56164922-fdc6-4bae-b3f4-06d67f7fc2e9 95 01/12/23 01:20:35.38
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/12/23 01:20:35.384
    Jan 12 01:20:35.452: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-9634" to be "not pending"
    Jan 12 01:20:35.455: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.617393ms
    Jan 12 01:20:37.460: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007980615s
    Jan 12 01:20:39.458: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.00583182s
    Jan 12 01:20:39.458: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.9.40.106 on the node which pod4 resides and expect not scheduled 01/12/23 01:20:39.458
    Jan 12 01:20:39.490: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-9634" to be "not pending"
    Jan 12 01:20:39.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.428386ms
    Jan 12 01:20:41.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006383047s
    Jan 12 01:20:43.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006509872s
    Jan 12 01:20:45.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00702379s
    Jan 12 01:20:47.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005742916s
    Jan 12 01:20:49.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006053095s
    Jan 12 01:20:51.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006196214s
    Jan 12 01:20:53.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007169882s
    Jan 12 01:20:55.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006315467s
    Jan 12 01:20:57.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007414221s
    Jan 12 01:20:59.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006827932s
    Jan 12 01:21:01.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.006395579s
    Jan 12 01:21:03.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006525417s
    Jan 12 01:21:05.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006700741s
    Jan 12 01:21:07.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005425218s
    Jan 12 01:21:09.501: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011091392s
    Jan 12 01:21:11.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006378169s
    Jan 12 01:21:13.502: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012022752s
    Jan 12 01:21:15.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007229093s
    Jan 12 01:21:17.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006141317s
    Jan 12 01:21:19.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00662242s
    Jan 12 01:21:21.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006242364s
    Jan 12 01:21:23.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005586606s
    Jan 12 01:21:25.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.006932536s
    Jan 12 01:21:27.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.005695611s
    Jan 12 01:21:29.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00557703s
    Jan 12 01:21:31.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006445316s
    Jan 12 01:21:33.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.006567327s
    Jan 12 01:21:35.498: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007765839s
    Jan 12 01:21:37.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005885041s
    Jan 12 01:21:39.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005560244s
    Jan 12 01:21:41.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005778338s
    Jan 12 01:21:43.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006681296s
    Jan 12 01:21:45.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007019345s
    Jan 12 01:21:47.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006496284s
    Jan 12 01:21:49.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006526983s
    Jan 12 01:21:51.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007296097s
    Jan 12 01:21:53.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006461102s
    Jan 12 01:21:55.499: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009581461s
    Jan 12 01:21:57.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006304777s
    Jan 12 01:21:59.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.00668297s
    Jan 12 01:22:01.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006354015s
    Jan 12 01:22:03.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006327911s
    Jan 12 01:22:05.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005373193s
    Jan 12 01:22:07.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006181615s
    Jan 12 01:22:09.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.006903675s
    Jan 12 01:22:11.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006857063s
    Jan 12 01:22:13.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006723438s
    Jan 12 01:22:15.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006270928s
    Jan 12 01:22:17.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006383586s
    Jan 12 01:22:19.499: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.009049107s
    Jan 12 01:22:21.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005509661s
    Jan 12 01:22:23.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006309304s
    Jan 12 01:22:25.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006098743s
    Jan 12 01:22:27.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005766344s
    Jan 12 01:22:29.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006656633s
    Jan 12 01:22:31.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006278584s
    Jan 12 01:22:33.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006526417s
    Jan 12 01:22:35.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.007506175s
    Jan 12 01:22:37.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006131993s
    Jan 12 01:22:39.498: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007821171s
    Jan 12 01:22:41.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.006260772s
    Jan 12 01:22:43.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006129032s
    Jan 12 01:22:45.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.007400824s
    Jan 12 01:22:47.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005774446s
    Jan 12 01:22:49.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.006894188s
    Jan 12 01:22:51.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.006108152s
    Jan 12 01:22:53.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.005916016s
    Jan 12 01:22:55.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.006126882s
    Jan 12 01:22:57.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.005644288s
    Jan 12 01:22:59.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.006266451s
    Jan 12 01:23:01.504: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.014353985s
    Jan 12 01:23:03.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.006317768s
    Jan 12 01:23:05.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.00569797s
    Jan 12 01:23:07.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.016142299s
    Jan 12 01:23:09.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.00651538s
    Jan 12 01:23:11.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.007143518s
    Jan 12 01:23:13.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.006037176s
    Jan 12 01:23:15.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.00716736s
    Jan 12 01:23:17.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.006361991s
    Jan 12 01:23:19.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.007286913s
    Jan 12 01:23:21.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.005811778s
    Jan 12 01:23:23.504: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.01388486s
    Jan 12 01:23:25.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007050933s
    Jan 12 01:23:27.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.00556253s
    Jan 12 01:23:29.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.005745519s
    Jan 12 01:23:31.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.006032899s
    Jan 12 01:23:33.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.007212215s
    Jan 12 01:23:35.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.005285104s
    Jan 12 01:23:37.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.006517599s
    Jan 12 01:23:39.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.006159721s
    Jan 12 01:23:41.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.006538352s
    Jan 12 01:23:43.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.006864162s
    Jan 12 01:23:45.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.00686218s
    Jan 12 01:23:47.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.005655768s
    Jan 12 01:23:49.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.006831775s
    Jan 12 01:23:51.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.006886536s
    Jan 12 01:23:53.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.006645011s
    Jan 12 01:23:55.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.0064609s
    Jan 12 01:23:57.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.005696747s
    Jan 12 01:23:59.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.006126666s
    Jan 12 01:24:01.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.007085363s
    Jan 12 01:24:03.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.007007932s
    Jan 12 01:24:05.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.006268068s
    Jan 12 01:24:07.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.006263182s
    Jan 12 01:24:09.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.006581311s
    Jan 12 01:24:11.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.006945225s
    Jan 12 01:24:13.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.006882941s
    Jan 12 01:24:15.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.005661504s
    Jan 12 01:24:17.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006235898s
    Jan 12 01:24:19.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.007029843s
    Jan 12 01:24:21.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.006723724s
    Jan 12 01:24:23.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.006864008s
    Jan 12 01:24:25.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.006871493s
    Jan 12 01:24:27.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.005996505s
    Jan 12 01:24:29.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.006908985s
    Jan 12 01:24:31.499: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.009200576s
    Jan 12 01:24:33.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.00620724s
    Jan 12 01:24:35.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007059187s
    Jan 12 01:24:37.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.005579246s
    Jan 12 01:24:39.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.006573857s
    Jan 12 01:24:41.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.005694557s
    Jan 12 01:24:43.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.00623799s
    Jan 12 01:24:45.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007401898s
    Jan 12 01:24:47.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.005834497s
    Jan 12 01:24:49.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.006190423s
    Jan 12 01:24:51.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.006082544s
    Jan 12 01:24:53.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.007590558s
    Jan 12 01:24:55.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.005847811s
    Jan 12 01:24:57.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005778415s
    Jan 12 01:24:59.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.006117716s
    Jan 12 01:25:01.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.00648718s
    Jan 12 01:25:03.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.005862275s
    Jan 12 01:25:05.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.006831627s
    Jan 12 01:25:07.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.005530145s
    Jan 12 01:25:09.503: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.012854773s
    Jan 12 01:25:11.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.00626931s
    Jan 12 01:25:13.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.005886123s
    Jan 12 01:25:15.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.006286941s
    Jan 12 01:25:17.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00568621s
    Jan 12 01:25:19.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.005897662s
    Jan 12 01:25:21.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.006095482s
    Jan 12 01:25:23.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.006128901s
    Jan 12 01:25:25.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.006002402s
    Jan 12 01:25:27.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.006405133s
    Jan 12 01:25:29.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.007048451s
    Jan 12 01:25:31.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.006876551s
    Jan 12 01:25:33.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007384646s
    Jan 12 01:25:35.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.007572919s
    Jan 12 01:25:37.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.005617392s
    Jan 12 01:25:39.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007425286s
    Jan 12 01:25:39.501: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010977595s
    STEP: removing the label kubernetes.io/e2e-56164922-fdc6-4bae-b3f4-06d67f7fc2e9 off the node eqx04-flash06 01/12/23 01:25:39.501
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-56164922-fdc6-4bae-b3f4-06d67f7fc2e9 01/12/23 01:25:39.518
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:25:39.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9634" for this suite. 01/12/23 01:25:39.527
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:25:39.573
Jan 12 01:25:39.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-probe 01/12/23 01:25:39.575
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:25:39.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:25:39.595
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-9c2c29e8-e992-4631-8165-8befaca4305b in namespace container-probe-65 01/12/23 01:25:39.608
Jan 12 01:25:39.674: INFO: Waiting up to 5m0s for pod "busybox-9c2c29e8-e992-4631-8165-8befaca4305b" in namespace "container-probe-65" to be "not pending"
Jan 12 01:25:39.677: INFO: Pod "busybox-9c2c29e8-e992-4631-8165-8befaca4305b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.752061ms
Jan 12 01:25:41.681: INFO: Pod "busybox-9c2c29e8-e992-4631-8165-8befaca4305b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006871881s
Jan 12 01:25:43.681: INFO: Pod "busybox-9c2c29e8-e992-4631-8165-8befaca4305b": Phase="Running", Reason="", readiness=true. Elapsed: 4.007167491s
Jan 12 01:25:43.681: INFO: Pod "busybox-9c2c29e8-e992-4631-8165-8befaca4305b" satisfied condition "not pending"
Jan 12 01:25:43.681: INFO: Started pod busybox-9c2c29e8-e992-4631-8165-8befaca4305b in namespace container-probe-65
STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 01:25:43.681
Jan 12 01:25:43.684: INFO: Initial restart count of pod busybox-9c2c29e8-e992-4631-8165-8befaca4305b is 0
Jan 12 01:26:31.786: INFO: Restart count of pod container-probe-65/busybox-9c2c29e8-e992-4631-8165-8befaca4305b is now 1 (48.101899538s elapsed)
STEP: deleting the pod 01/12/23 01:26:31.786
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 01:26:31.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-65" for this suite. 01/12/23 01:26:31.816
------------------------------
• [SLOW TEST] [52.290 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:25:39.573
    Jan 12 01:25:39.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-probe 01/12/23 01:25:39.575
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:25:39.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:25:39.595
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-9c2c29e8-e992-4631-8165-8befaca4305b in namespace container-probe-65 01/12/23 01:25:39.608
    Jan 12 01:25:39.674: INFO: Waiting up to 5m0s for pod "busybox-9c2c29e8-e992-4631-8165-8befaca4305b" in namespace "container-probe-65" to be "not pending"
    Jan 12 01:25:39.677: INFO: Pod "busybox-9c2c29e8-e992-4631-8165-8befaca4305b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.752061ms
    Jan 12 01:25:41.681: INFO: Pod "busybox-9c2c29e8-e992-4631-8165-8befaca4305b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006871881s
    Jan 12 01:25:43.681: INFO: Pod "busybox-9c2c29e8-e992-4631-8165-8befaca4305b": Phase="Running", Reason="", readiness=true. Elapsed: 4.007167491s
    Jan 12 01:25:43.681: INFO: Pod "busybox-9c2c29e8-e992-4631-8165-8befaca4305b" satisfied condition "not pending"
    Jan 12 01:25:43.681: INFO: Started pod busybox-9c2c29e8-e992-4631-8165-8befaca4305b in namespace container-probe-65
    STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 01:25:43.681
    Jan 12 01:25:43.684: INFO: Initial restart count of pod busybox-9c2c29e8-e992-4631-8165-8befaca4305b is 0
    Jan 12 01:26:31.786: INFO: Restart count of pod container-probe-65/busybox-9c2c29e8-e992-4631-8165-8befaca4305b is now 1 (48.101899538s elapsed)
    STEP: deleting the pod 01/12/23 01:26:31.786
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:26:31.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-65" for this suite. 01/12/23 01:26:31.816
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:26:31.864
Jan 12 01:26:31.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pod-network-test 01/12/23 01:26:31.865
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:26:31.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:26:31.892
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-6854 01/12/23 01:26:31.894
STEP: creating a selector 01/12/23 01:26:31.894
STEP: Creating the service pods in kubernetes 01/12/23 01:26:31.895
Jan 12 01:26:31.895: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 12 01:26:32.063: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6854" to be "running and ready"
Jan 12 01:26:32.065: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.512146ms
Jan 12 01:26:32.065: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:26:34.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011611557s
Jan 12 01:26:34.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:26:36.070: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007507998s
Jan 12 01:26:36.070: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:26:38.068: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005637826s
Jan 12 01:26:38.068: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:26:40.084: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.021350728s
Jan 12 01:26:40.084: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:26:42.069: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00631004s
Jan 12 01:26:42.069: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:26:44.070: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.006770296s
Jan 12 01:26:44.070: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:26:46.070: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.006931767s
Jan 12 01:26:46.070: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:26:48.069: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.006533049s
Jan 12 01:26:48.069: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:26:50.069: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.005821676s
Jan 12 01:26:50.069: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:26:52.069: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.006372966s
Jan 12 01:26:52.069: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 01:26:54.069: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005964143s
Jan 12 01:26:54.069: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 12 01:26:54.069: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 12 01:26:54.071: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6854" to be "running and ready"
Jan 12 01:26:54.074: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.301795ms
Jan 12 01:26:54.074: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 12 01:26:54.074: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/12/23 01:26:54.076
Jan 12 01:26:54.165: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6854" to be "running"
Jan 12 01:26:54.167: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.224644ms
Jan 12 01:26:56.173: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008452356s
Jan 12 01:26:58.171: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005864084s
Jan 12 01:26:58.171: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 12 01:26:58.173: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6854" to be "running"
Jan 12 01:26:58.176: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.386263ms
Jan 12 01:26:58.176: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 12 01:26:58.178: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 12 01:26:58.178: INFO: Going to poll 172.21.117.152 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 12 01:26:58.180: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.117.152 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6854 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:26:58.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:26:58.181: INFO: ExecWithOptions: Clientset creation
Jan 12 01:26:58.181: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-6854/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.21.117.152+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 12 01:26:59.327: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 12 01:26:59.327: INFO: Going to poll 172.21.88.180 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 12 01:26:59.331: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.88.180 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6854 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:26:59.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:26:59.331: INFO: ExecWithOptions: Clientset creation
Jan 12 01:26:59.331: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-6854/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.21.88.180+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 12 01:27:00.479: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 12 01:27:00.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6854" for this suite. 01/12/23 01:27:00.484
------------------------------
• [SLOW TEST] [28.645 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:26:31.864
    Jan 12 01:26:31.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pod-network-test 01/12/23 01:26:31.865
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:26:31.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:26:31.892
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-6854 01/12/23 01:26:31.894
    STEP: creating a selector 01/12/23 01:26:31.894
    STEP: Creating the service pods in kubernetes 01/12/23 01:26:31.895
    Jan 12 01:26:31.895: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 12 01:26:32.063: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6854" to be "running and ready"
    Jan 12 01:26:32.065: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.512146ms
    Jan 12 01:26:32.065: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:26:34.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011611557s
    Jan 12 01:26:34.074: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:26:36.070: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007507998s
    Jan 12 01:26:36.070: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:26:38.068: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005637826s
    Jan 12 01:26:38.068: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:26:40.084: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.021350728s
    Jan 12 01:26:40.084: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:26:42.069: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00631004s
    Jan 12 01:26:42.069: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:26:44.070: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.006770296s
    Jan 12 01:26:44.070: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:26:46.070: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.006931767s
    Jan 12 01:26:46.070: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:26:48.069: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.006533049s
    Jan 12 01:26:48.069: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:26:50.069: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.005821676s
    Jan 12 01:26:50.069: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:26:52.069: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.006372966s
    Jan 12 01:26:52.069: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 01:26:54.069: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005964143s
    Jan 12 01:26:54.069: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 12 01:26:54.069: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 12 01:26:54.071: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6854" to be "running and ready"
    Jan 12 01:26:54.074: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.301795ms
    Jan 12 01:26:54.074: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 12 01:26:54.074: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/12/23 01:26:54.076
    Jan 12 01:26:54.165: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6854" to be "running"
    Jan 12 01:26:54.167: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.224644ms
    Jan 12 01:26:56.173: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008452356s
    Jan 12 01:26:58.171: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005864084s
    Jan 12 01:26:58.171: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 12 01:26:58.173: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6854" to be "running"
    Jan 12 01:26:58.176: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.386263ms
    Jan 12 01:26:58.176: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 12 01:26:58.178: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 12 01:26:58.178: INFO: Going to poll 172.21.117.152 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 12 01:26:58.180: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.117.152 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6854 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:26:58.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:26:58.181: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:26:58.181: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-6854/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.21.117.152+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 12 01:26:59.327: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 12 01:26:59.327: INFO: Going to poll 172.21.88.180 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 12 01:26:59.331: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.88.180 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6854 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:26:59.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:26:59.331: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:26:59.331: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-6854/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.21.88.180+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 12 01:27:00.479: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:27:00.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6854" for this suite. 01/12/23 01:27:00.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:27:00.511
Jan 12 01:27:00.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename svc-latency 01/12/23 01:27:00.512
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:27:00.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:27:00.53
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 12 01:27:00.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5890 01/12/23 01:27:00.533
I0112 01:27:00.539778      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5890, replica count: 1
I0112 01:27:01.591147      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0112 01:27:02.591328      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0112 01:27:03.592254      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 01:27:03.733: INFO: Created: latency-svc-q7sl5
Jan 12 01:27:03.733: INFO: Got endpoints: latency-svc-q7sl5 [41.036302ms]
Jan 12 01:27:03.752: INFO: Created: latency-svc-v4klj
Jan 12 01:27:03.759: INFO: Got endpoints: latency-svc-v4klj [25.612862ms]
Jan 12 01:27:03.771: INFO: Created: latency-svc-x7ljn
Jan 12 01:27:03.792: INFO: Got endpoints: latency-svc-x7ljn [58.424121ms]
Jan 12 01:27:03.801: INFO: Created: latency-svc-bc4ph
Jan 12 01:27:03.811: INFO: Got endpoints: latency-svc-bc4ph [76.888657ms]
Jan 12 01:27:03.827: INFO: Created: latency-svc-ggtpz
Jan 12 01:27:03.830: INFO: Got endpoints: latency-svc-ggtpz [95.816719ms]
Jan 12 01:27:03.858: INFO: Created: latency-svc-bnnhn
Jan 12 01:27:03.858: INFO: Got endpoints: latency-svc-bnnhn [124.375366ms]
Jan 12 01:27:03.878: INFO: Created: latency-svc-rr44t
Jan 12 01:27:03.888: INFO: Got endpoints: latency-svc-rr44t [153.970898ms]
Jan 12 01:27:03.913: INFO: Created: latency-svc-crzp5
Jan 12 01:27:03.913: INFO: Got endpoints: latency-svc-crzp5 [179.090924ms]
Jan 12 01:27:03.929: INFO: Created: latency-svc-mzsvh
Jan 12 01:27:03.955: INFO: Got endpoints: latency-svc-mzsvh [220.827967ms]
Jan 12 01:27:03.973: INFO: Created: latency-svc-bfb5z
Jan 12 01:27:03.973: INFO: Got endpoints: latency-svc-bfb5z [238.857772ms]
Jan 12 01:27:03.986: INFO: Created: latency-svc-gjcl9
Jan 12 01:27:04.001: INFO: Got endpoints: latency-svc-gjcl9 [266.292974ms]
Jan 12 01:27:04.005: INFO: Created: latency-svc-66dxr
Jan 12 01:27:04.027: INFO: Got endpoints: latency-svc-66dxr [292.780355ms]
Jan 12 01:27:04.039: INFO: Created: latency-svc-4np54
Jan 12 01:27:04.050: INFO: Got endpoints: latency-svc-4np54 [314.941479ms]
Jan 12 01:27:04.069: INFO: Created: latency-svc-ggjfl
Jan 12 01:27:04.076: INFO: Got endpoints: latency-svc-ggjfl [340.998526ms]
Jan 12 01:27:04.089: INFO: Created: latency-svc-pvzhm
Jan 12 01:27:04.101: INFO: Got endpoints: latency-svc-pvzhm [366.317595ms]
Jan 12 01:27:04.131: INFO: Created: latency-svc-f997q
Jan 12 01:27:04.132: INFO: Got endpoints: latency-svc-f997q [397.15747ms]
Jan 12 01:27:04.144: INFO: Created: latency-svc-hrf6w
Jan 12 01:27:04.158: INFO: Got endpoints: latency-svc-hrf6w [398.570806ms]
Jan 12 01:27:04.162: INFO: Created: latency-svc-pnjtj
Jan 12 01:27:04.187: INFO: Got endpoints: latency-svc-pnjtj [394.532379ms]
Jan 12 01:27:04.199: INFO: Created: latency-svc-jxck9
Jan 12 01:27:04.199: INFO: Got endpoints: latency-svc-jxck9 [386.890968ms]
Jan 12 01:27:04.222: INFO: Created: latency-svc-b6vgg
Jan 12 01:27:04.222: INFO: Got endpoints: latency-svc-b6vgg [392.845671ms]
Jan 12 01:27:04.234: INFO: Created: latency-svc-5hnrx
Jan 12 01:27:04.252: INFO: Got endpoints: latency-svc-5hnrx [393.698128ms]
Jan 12 01:27:04.260: INFO: Created: latency-svc-vvp96
Jan 12 01:27:04.271: INFO: Got endpoints: latency-svc-vvp96 [382.664384ms]
Jan 12 01:27:04.300: INFO: Created: latency-svc-qdw5g
Jan 12 01:27:04.300: INFO: Got endpoints: latency-svc-qdw5g [386.675073ms]
Jan 12 01:27:04.313: INFO: Created: latency-svc-v289s
Jan 12 01:27:04.324: INFO: Got endpoints: latency-svc-v289s [368.852341ms]
Jan 12 01:27:04.344: INFO: Created: latency-svc-2fgh7
Jan 12 01:27:04.344: INFO: Got endpoints: latency-svc-2fgh7 [370.814036ms]
Jan 12 01:27:04.362: INFO: Created: latency-svc-hjw49
Jan 12 01:27:04.371: INFO: Got endpoints: latency-svc-hjw49 [370.304706ms]
Jan 12 01:27:04.383: INFO: Created: latency-svc-l9h8s
Jan 12 01:27:04.397: INFO: Got endpoints: latency-svc-l9h8s [369.889292ms]
Jan 12 01:27:04.418: INFO: Created: latency-svc-jlcdc
Jan 12 01:27:04.418: INFO: Got endpoints: latency-svc-jlcdc [368.891652ms]
Jan 12 01:27:04.448: INFO: Created: latency-svc-v6mm6
Jan 12 01:27:04.448: INFO: Got endpoints: latency-svc-v6mm6 [372.510941ms]
Jan 12 01:27:04.460: INFO: Created: latency-svc-8bxtp
Jan 12 01:27:04.470: INFO: Got endpoints: latency-svc-8bxtp [368.858846ms]
Jan 12 01:27:04.485: INFO: Created: latency-svc-ctqv4
Jan 12 01:27:04.496: INFO: Got endpoints: latency-svc-ctqv4 [363.956032ms]
Jan 12 01:27:04.515: INFO: Created: latency-svc-sqq6v
Jan 12 01:27:04.530: INFO: Got endpoints: latency-svc-sqq6v [372.662818ms]
Jan 12 01:27:04.542: INFO: Created: latency-svc-5g72d
Jan 12 01:27:04.552: INFO: Got endpoints: latency-svc-5g72d [365.505069ms]
Jan 12 01:27:04.570: INFO: Created: latency-svc-9wfdl
Jan 12 01:27:04.570: INFO: Got endpoints: latency-svc-9wfdl [371.360805ms]
Jan 12 01:27:04.599: INFO: Created: latency-svc-6p4xj
Jan 12 01:27:04.599: INFO: Got endpoints: latency-svc-6p4xj [376.468811ms]
Jan 12 01:27:04.611: INFO: Created: latency-svc-kw6zg
Jan 12 01:27:04.625: INFO: Got endpoints: latency-svc-kw6zg [372.680139ms]
Jan 12 01:27:04.629: INFO: Created: latency-svc-sr659
Jan 12 01:27:04.655: INFO: Got endpoints: latency-svc-sr659 [384.402489ms]
Jan 12 01:27:04.673: INFO: Created: latency-svc-qf5s7
Jan 12 01:27:04.673: INFO: Got endpoints: latency-svc-qf5s7 [372.794735ms]
Jan 12 01:27:04.686: INFO: Created: latency-svc-k4dxr
Jan 12 01:27:04.696: INFO: Got endpoints: latency-svc-k4dxr [371.553062ms]
Jan 12 01:27:04.707: INFO: Created: latency-svc-666nm
Jan 12 01:27:04.731: INFO: Got endpoints: latency-svc-666nm [387.829436ms]
Jan 12 01:27:04.739: INFO: Created: latency-svc-nqmpl
Jan 12 01:27:04.749: INFO: Got endpoints: latency-svc-nqmpl [377.770769ms]
Jan 12 01:27:04.771: INFO: Created: latency-svc-bh6rn
Jan 12 01:27:04.773: INFO: Got endpoints: latency-svc-bh6rn [375.909917ms]
Jan 12 01:27:04.802: INFO: Created: latency-svc-fchbd
Jan 12 01:27:04.802: INFO: Got endpoints: latency-svc-fchbd [383.515894ms]
Jan 12 01:27:04.820: INFO: Created: latency-svc-xntgq
Jan 12 01:27:04.830: INFO: Got endpoints: latency-svc-xntgq [381.66856ms]
Jan 12 01:27:04.842: INFO: Created: latency-svc-6jdvv
Jan 12 01:27:04.857: INFO: Got endpoints: latency-svc-6jdvv [386.811767ms]
Jan 12 01:27:04.861: INFO: Created: latency-svc-fns49
Jan 12 01:27:04.882: INFO: Got endpoints: latency-svc-fns49 [385.544122ms]
Jan 12 01:27:04.896: INFO: Created: latency-svc-6z2rs
Jan 12 01:27:04.896: INFO: Got endpoints: latency-svc-6z2rs [365.560542ms]
Jan 12 01:27:04.908: INFO: Created: latency-svc-j68rq
Jan 12 01:27:04.918: INFO: Got endpoints: latency-svc-j68rq [366.307294ms]
Jan 12 01:27:04.932: INFO: Created: latency-svc-6qsnm
Jan 12 01:27:04.943: INFO: Got endpoints: latency-svc-6qsnm [372.814233ms]
Jan 12 01:27:04.956: INFO: Created: latency-svc-7gm6l
Jan 12 01:27:04.971: INFO: Got endpoints: latency-svc-7gm6l [372.279494ms]
Jan 12 01:27:04.976: INFO: Created: latency-svc-9ggbk
Jan 12 01:27:04.998: INFO: Got endpoints: latency-svc-9ggbk [373.447228ms]
Jan 12 01:27:05.006: INFO: Created: latency-svc-mrd75
Jan 12 01:27:05.016: INFO: Got endpoints: latency-svc-mrd75 [361.184838ms]
Jan 12 01:27:05.034: INFO: Created: latency-svc-zx256
Jan 12 01:27:05.041: INFO: Got endpoints: latency-svc-zx256 [368.418537ms]
Jan 12 01:27:05.053: INFO: Created: latency-svc-rlmw9
Jan 12 01:27:05.063: INFO: Got endpoints: latency-svc-rlmw9 [367.682185ms]
Jan 12 01:27:05.084: INFO: Created: latency-svc-bvvv2
Jan 12 01:27:05.090: INFO: Got endpoints: latency-svc-bvvv2 [358.407926ms]
Jan 12 01:27:05.124: INFO: Created: latency-svc-q7wn4
Jan 12 01:27:05.124: INFO: Got endpoints: latency-svc-q7wn4 [375.010815ms]
Jan 12 01:27:05.136: INFO: Created: latency-svc-sf8hq
Jan 12 01:27:05.149: INFO: Got endpoints: latency-svc-sf8hq [375.606521ms]
Jan 12 01:27:05.180: INFO: Created: latency-svc-p4jj6
Jan 12 01:27:05.180: INFO: Got endpoints: latency-svc-p4jj6 [377.839416ms]
Jan 12 01:27:05.196: INFO: Created: latency-svc-7wq7q
Jan 12 01:27:05.211: INFO: Got endpoints: latency-svc-7wq7q [381.273371ms]
Jan 12 01:27:05.229: INFO: Created: latency-svc-g9wh9
Jan 12 01:27:05.234: INFO: Got endpoints: latency-svc-g9wh9 [377.762427ms]
Jan 12 01:27:05.260: INFO: Created: latency-svc-9gt6z
Jan 12 01:27:05.274: INFO: Got endpoints: latency-svc-9gt6z [392.776959ms]
Jan 12 01:27:05.280: INFO: Created: latency-svc-g6mp2
Jan 12 01:27:05.296: INFO: Created: latency-svc-ff4t9
Jan 12 01:27:05.311: INFO: Created: latency-svc-v7z2f
Jan 12 01:27:05.341: INFO: Got endpoints: latency-svc-g6mp2 [445.610966ms]
Jan 12 01:27:05.353: INFO: Created: latency-svc-zn7dp
Jan 12 01:27:05.366: INFO: Created: latency-svc-vxj2b
Jan 12 01:27:05.369: INFO: Got endpoints: latency-svc-ff4t9 [451.057978ms]
Jan 12 01:27:05.385: INFO: Created: latency-svc-p8mkh
Jan 12 01:27:05.407: INFO: Created: latency-svc-s4xjs
Jan 12 01:27:05.430: INFO: Got endpoints: latency-svc-v7z2f [486.493964ms]
Jan 12 01:27:05.436: INFO: Created: latency-svc-z88np
Jan 12 01:27:05.456: INFO: Created: latency-svc-rz7f4
Jan 12 01:27:05.469: INFO: Got endpoints: latency-svc-zn7dp [498.123125ms]
Jan 12 01:27:05.482: INFO: Created: latency-svc-rqk9z
Jan 12 01:27:05.501: INFO: Created: latency-svc-rvjqg
Jan 12 01:27:05.527: INFO: Got endpoints: latency-svc-vxj2b [528.304089ms]
Jan 12 01:27:05.527: INFO: Created: latency-svc-4fmbm
Jan 12 01:27:05.539: INFO: Created: latency-svc-hlp72
Jan 12 01:27:05.551: INFO: Created: latency-svc-rtbnb
Jan 12 01:27:05.582: INFO: Got endpoints: latency-svc-p8mkh [565.61792ms]
Jan 12 01:27:05.598: INFO: Created: latency-svc-2q22r
Jan 12 01:27:05.611: INFO: Created: latency-svc-ghvp6
Jan 12 01:27:05.626: INFO: Got endpoints: latency-svc-s4xjs [585.282365ms]
Jan 12 01:27:05.631: INFO: Created: latency-svc-6rnz5
Jan 12 01:27:05.686: INFO: Got endpoints: latency-svc-z88np [622.580441ms]
Jan 12 01:27:05.687: INFO: Created: latency-svc-6tfwq
Jan 12 01:27:05.733: INFO: Got endpoints: latency-svc-rz7f4 [643.503402ms]
Jan 12 01:27:05.734: INFO: Created: latency-svc-thpfv
Jan 12 01:27:05.759: INFO: Created: latency-svc-brhfd
Jan 12 01:27:05.774: INFO: Got endpoints: latency-svc-rqk9z [650.314152ms]
Jan 12 01:27:05.782: INFO: Created: latency-svc-7s4f2
Jan 12 01:27:05.815: INFO: Created: latency-svc-hbfbp
Jan 12 01:27:05.831: INFO: Got endpoints: latency-svc-rvjqg [681.822308ms]
Jan 12 01:27:05.843: INFO: Created: latency-svc-f78jh
Jan 12 01:27:05.856: INFO: Created: latency-svc-ddpzs
Jan 12 01:27:05.877: INFO: Got endpoints: latency-svc-4fmbm [697.57027ms]
Jan 12 01:27:05.878: INFO: Created: latency-svc-hsf2k
Jan 12 01:27:05.900: INFO: Created: latency-svc-wvx5q
Jan 12 01:27:05.925: INFO: Got endpoints: latency-svc-hlp72 [714.217411ms]
Jan 12 01:27:05.929: INFO: Created: latency-svc-v6ntt
Jan 12 01:27:05.965: INFO: Created: latency-svc-f8z9h
Jan 12 01:27:05.985: INFO: Got endpoints: latency-svc-rtbnb [750.477084ms]
Jan 12 01:27:06.034: INFO: Got endpoints: latency-svc-2q22r [759.355703ms]
Jan 12 01:27:06.034: INFO: Created: latency-svc-2td4x
Jan 12 01:27:06.061: INFO: Created: latency-svc-rq2cp
Jan 12 01:27:06.083: INFO: Got endpoints: latency-svc-ghvp6 [742.035954ms]
Jan 12 01:27:06.128: INFO: Got endpoints: latency-svc-6rnz5 [758.854884ms]
Jan 12 01:27:06.149: INFO: Created: latency-svc-qnwcw
Jan 12 01:27:06.182: INFO: Got endpoints: latency-svc-6tfwq [752.647094ms]
Jan 12 01:27:06.183: INFO: Created: latency-svc-qnv2b
Jan 12 01:27:06.204: INFO: Created: latency-svc-8smlm
Jan 12 01:27:06.219: INFO: Got endpoints: latency-svc-thpfv [749.460245ms]
Jan 12 01:27:06.281: INFO: Got endpoints: latency-svc-brhfd [754.255716ms]
Jan 12 01:27:06.286: INFO: Created: latency-svc-gkdfc
Jan 12 01:27:06.300: INFO: Created: latency-svc-wkqw9
Jan 12 01:27:06.326: INFO: Got endpoints: latency-svc-7s4f2 [743.501395ms]
Jan 12 01:27:06.350: INFO: Created: latency-svc-2pld9
Jan 12 01:27:06.389: INFO: Got endpoints: latency-svc-hbfbp [762.178626ms]
Jan 12 01:27:06.412: INFO: Created: latency-svc-kntq9
Jan 12 01:27:06.422: INFO: Got endpoints: latency-svc-f78jh [736.12824ms]
Jan 12 01:27:06.441: INFO: Created: latency-svc-7mrnp
Jan 12 01:27:06.484: INFO: Got endpoints: latency-svc-ddpzs [751.03513ms]
Jan 12 01:27:06.519: INFO: Created: latency-svc-sxpgx
Jan 12 01:27:06.524: INFO: Got endpoints: latency-svc-hsf2k [749.427777ms]
Jan 12 01:27:06.540: INFO: Created: latency-svc-d87wx
Jan 12 01:27:06.579: INFO: Got endpoints: latency-svc-wvx5q [748.24993ms]
Jan 12 01:27:06.614: INFO: Created: latency-svc-97gkf
Jan 12 01:27:06.632: INFO: Got endpoints: latency-svc-v6ntt [754.631955ms]
Jan 12 01:27:06.648: INFO: Created: latency-svc-wfs8p
Jan 12 01:27:06.673: INFO: Got endpoints: latency-svc-f8z9h [747.299369ms]
Jan 12 01:27:06.697: INFO: Created: latency-svc-mmdch
Jan 12 01:27:06.732: INFO: Got endpoints: latency-svc-2td4x [739.419959ms]
Jan 12 01:27:06.753: INFO: Created: latency-svc-g6lrn
Jan 12 01:27:06.773: INFO: Got endpoints: latency-svc-rq2cp [739.072756ms]
Jan 12 01:27:06.789: INFO: Created: latency-svc-w85qr
Jan 12 01:27:06.826: INFO: Got endpoints: latency-svc-qnwcw [742.37997ms]
Jan 12 01:27:06.859: INFO: Created: latency-svc-wc2rm
Jan 12 01:27:06.875: INFO: Got endpoints: latency-svc-qnv2b [747.082892ms]
Jan 12 01:27:06.894: INFO: Created: latency-svc-cmqz4
Jan 12 01:27:06.919: INFO: Got endpoints: latency-svc-8smlm [735.773374ms]
Jan 12 01:27:06.938: INFO: Created: latency-svc-8wxlp
Jan 12 01:27:06.977: INFO: Got endpoints: latency-svc-gkdfc [758.098624ms]
Jan 12 01:27:06.999: INFO: Created: latency-svc-cmvnm
Jan 12 01:27:07.029: INFO: Got endpoints: latency-svc-wkqw9 [747.82738ms]
Jan 12 01:27:07.045: INFO: Created: latency-svc-pd9rs
Jan 12 01:27:07.085: INFO: Got endpoints: latency-svc-2pld9 [759.204291ms]
Jan 12 01:27:07.109: INFO: Created: latency-svc-2mmqh
Jan 12 01:27:07.126: INFO: Got endpoints: latency-svc-kntq9 [736.093107ms]
Jan 12 01:27:07.148: INFO: Created: latency-svc-524v8
Jan 12 01:27:07.169: INFO: Got endpoints: latency-svc-7mrnp [746.730576ms]
Jan 12 01:27:07.194: INFO: Created: latency-svc-xkztt
Jan 12 01:27:07.239: INFO: Got endpoints: latency-svc-sxpgx [754.908968ms]
Jan 12 01:27:07.273: INFO: Got endpoints: latency-svc-d87wx [748.964951ms]
Jan 12 01:27:07.273: INFO: Created: latency-svc-9k99k
Jan 12 01:27:07.304: INFO: Created: latency-svc-27wmx
Jan 12 01:27:07.328: INFO: Got endpoints: latency-svc-97gkf [748.927725ms]
Jan 12 01:27:07.349: INFO: Created: latency-svc-l5pkc
Jan 12 01:27:07.378: INFO: Got endpoints: latency-svc-wfs8p [745.47885ms]
Jan 12 01:27:07.401: INFO: Created: latency-svc-tdqkd
Jan 12 01:27:07.432: INFO: Got endpoints: latency-svc-mmdch [758.873174ms]
Jan 12 01:27:07.455: INFO: Created: latency-svc-kc24t
Jan 12 01:27:07.481: INFO: Got endpoints: latency-svc-g6lrn [749.517724ms]
Jan 12 01:27:07.497: INFO: Created: latency-svc-jg9vw
Jan 12 01:27:07.525: INFO: Got endpoints: latency-svc-w85qr [751.542125ms]
Jan 12 01:27:07.548: INFO: Created: latency-svc-n5bxz
Jan 12 01:27:07.577: INFO: Got endpoints: latency-svc-wc2rm [751.061633ms]
Jan 12 01:27:07.596: INFO: Created: latency-svc-c8tcx
Jan 12 01:27:07.635: INFO: Got endpoints: latency-svc-cmqz4 [759.688789ms]
Jan 12 01:27:07.671: INFO: Got endpoints: latency-svc-8wxlp [752.309693ms]
Jan 12 01:27:07.678: INFO: Created: latency-svc-v6t4s
Jan 12 01:27:07.701: INFO: Created: latency-svc-flp5s
Jan 12 01:27:07.737: INFO: Got endpoints: latency-svc-cmvnm [759.858963ms]
Jan 12 01:27:07.767: INFO: Created: latency-svc-g8h9l
Jan 12 01:27:07.770: INFO: Got endpoints: latency-svc-pd9rs [741.332654ms]
Jan 12 01:27:07.793: INFO: Created: latency-svc-t8wdl
Jan 12 01:27:07.828: INFO: Got endpoints: latency-svc-2mmqh [743.408168ms]
Jan 12 01:27:07.861: INFO: Created: latency-svc-5kh6h
Jan 12 01:27:07.877: INFO: Got endpoints: latency-svc-524v8 [750.285419ms]
Jan 12 01:27:07.892: INFO: Created: latency-svc-n46qx
Jan 12 01:27:07.933: INFO: Got endpoints: latency-svc-xkztt [764.115864ms]
Jan 12 01:27:07.967: INFO: Created: latency-svc-4srfb
Jan 12 01:27:07.974: INFO: Got endpoints: latency-svc-9k99k [734.892277ms]
Jan 12 01:27:07.990: INFO: Created: latency-svc-d4r6q
Jan 12 01:27:08.025: INFO: Got endpoints: latency-svc-27wmx [752.697501ms]
Jan 12 01:27:08.049: INFO: Created: latency-svc-6m8gl
Jan 12 01:27:08.085: INFO: Got endpoints: latency-svc-l5pkc [757.31765ms]
Jan 12 01:27:08.109: INFO: Created: latency-svc-dc7f2
Jan 12 01:27:08.120: INFO: Got endpoints: latency-svc-tdqkd [742.312934ms]
Jan 12 01:27:08.136: INFO: Created: latency-svc-7wm2g
Jan 12 01:27:08.178: INFO: Got endpoints: latency-svc-kc24t [746.626852ms]
Jan 12 01:27:08.211: INFO: Created: latency-svc-x6qq6
Jan 12 01:27:08.226: INFO: Got endpoints: latency-svc-jg9vw [745.159633ms]
Jan 12 01:27:08.245: INFO: Created: latency-svc-pdtjj
Jan 12 01:27:08.276: INFO: Got endpoints: latency-svc-n5bxz [751.140707ms]
Jan 12 01:27:08.297: INFO: Created: latency-svc-z7vb2
Jan 12 01:27:08.333: INFO: Got endpoints: latency-svc-c8tcx [756.439763ms]
Jan 12 01:27:08.352: INFO: Created: latency-svc-mccxc
Jan 12 01:27:08.375: INFO: Got endpoints: latency-svc-v6t4s [739.96298ms]
Jan 12 01:27:08.391: INFO: Created: latency-svc-pckwg
Jan 12 01:27:08.442: INFO: Got endpoints: latency-svc-flp5s [770.85492ms]
Jan 12 01:27:08.463: INFO: Created: latency-svc-8bbdq
Jan 12 01:27:08.472: INFO: Got endpoints: latency-svc-g8h9l [734.997258ms]
Jan 12 01:27:08.492: INFO: Created: latency-svc-pqrfb
Jan 12 01:27:08.523: INFO: Got endpoints: latency-svc-t8wdl [752.946017ms]
Jan 12 01:27:08.559: INFO: Created: latency-svc-t78c9
Jan 12 01:27:08.580: INFO: Got endpoints: latency-svc-5kh6h [751.472926ms]
Jan 12 01:27:08.596: INFO: Created: latency-svc-vdj9z
Jan 12 01:27:08.626: INFO: Got endpoints: latency-svc-n46qx [749.544364ms]
Jan 12 01:27:08.645: INFO: Created: latency-svc-2b8tw
Jan 12 01:27:08.688: INFO: Got endpoints: latency-svc-4srfb [755.047706ms]
Jan 12 01:27:08.710: INFO: Created: latency-svc-vx8rc
Jan 12 01:27:08.723: INFO: Got endpoints: latency-svc-d4r6q [748.653061ms]
Jan 12 01:27:08.742: INFO: Created: latency-svc-66wvx
Jan 12 01:27:08.775: INFO: Got endpoints: latency-svc-6m8gl [749.70556ms]
Jan 12 01:27:08.811: INFO: Created: latency-svc-fsdk5
Jan 12 01:27:08.829: INFO: Got endpoints: latency-svc-dc7f2 [744.277454ms]
Jan 12 01:27:08.845: INFO: Created: latency-svc-wsxvz
Jan 12 01:27:08.875: INFO: Got endpoints: latency-svc-7wm2g [755.034861ms]
Jan 12 01:27:08.909: INFO: Created: latency-svc-vln76
Jan 12 01:27:08.925: INFO: Got endpoints: latency-svc-x6qq6 [746.959976ms]
Jan 12 01:27:08.945: INFO: Created: latency-svc-2qv7v
Jan 12 01:27:08.970: INFO: Got endpoints: latency-svc-pdtjj [742.996883ms]
Jan 12 01:27:08.990: INFO: Created: latency-svc-pk8jh
Jan 12 01:27:09.032: INFO: Got endpoints: latency-svc-z7vb2 [756.132309ms]
Jan 12 01:27:09.055: INFO: Created: latency-svc-6xjwr
Jan 12 01:27:09.079: INFO: Got endpoints: latency-svc-mccxc [745.380205ms]
Jan 12 01:27:09.095: INFO: Created: latency-svc-j598q
Jan 12 01:27:09.132: INFO: Got endpoints: latency-svc-pckwg [756.696526ms]
Jan 12 01:27:09.156: INFO: Created: latency-svc-d829n
Jan 12 01:27:09.176: INFO: Got endpoints: latency-svc-8bbdq [734.051745ms]
Jan 12 01:27:09.200: INFO: Created: latency-svc-mnp8h
Jan 12 01:27:09.219: INFO: Got endpoints: latency-svc-pqrfb [746.693941ms]
Jan 12 01:27:09.241: INFO: Created: latency-svc-fb8dr
Jan 12 01:27:09.283: INFO: Got endpoints: latency-svc-t78c9 [759.621598ms]
Jan 12 01:27:09.306: INFO: Created: latency-svc-g5647
Jan 12 01:27:09.330: INFO: Got endpoints: latency-svc-vdj9z [749.838186ms]
Jan 12 01:27:09.352: INFO: Created: latency-svc-fnm9n
Jan 12 01:27:09.377: INFO: Got endpoints: latency-svc-2b8tw [750.591533ms]
Jan 12 01:27:09.401: INFO: Created: latency-svc-6mdtf
Jan 12 01:27:09.426: INFO: Got endpoints: latency-svc-vx8rc [737.801335ms]
Jan 12 01:27:09.450: INFO: Created: latency-svc-gxqmq
Jan 12 01:27:09.469: INFO: Got endpoints: latency-svc-66wvx [745.55541ms]
Jan 12 01:27:09.487: INFO: Created: latency-svc-2nsd4
Jan 12 01:27:09.534: INFO: Got endpoints: latency-svc-fsdk5 [758.816479ms]
Jan 12 01:27:09.557: INFO: Created: latency-svc-lv46t
Jan 12 01:27:09.580: INFO: Got endpoints: latency-svc-wsxvz [750.193516ms]
Jan 12 01:27:09.596: INFO: Created: latency-svc-zhbv4
Jan 12 01:27:09.625: INFO: Got endpoints: latency-svc-vln76 [749.65437ms]
Jan 12 01:27:09.650: INFO: Created: latency-svc-954xd
Jan 12 01:27:09.684: INFO: Got endpoints: latency-svc-2qv7v [758.746142ms]
Jan 12 01:27:09.706: INFO: Created: latency-svc-8m585
Jan 12 01:27:09.723: INFO: Got endpoints: latency-svc-pk8jh [753.418735ms]
Jan 12 01:27:09.739: INFO: Created: latency-svc-94d5p
Jan 12 01:27:09.776: INFO: Got endpoints: latency-svc-6xjwr [744.39434ms]
Jan 12 01:27:09.803: INFO: Created: latency-svc-mxs8b
Jan 12 01:27:09.825: INFO: Got endpoints: latency-svc-j598q [746.191005ms]
Jan 12 01:27:09.840: INFO: Created: latency-svc-4vp6m
Jan 12 01:27:09.876: INFO: Got endpoints: latency-svc-d829n [743.911758ms]
Jan 12 01:27:09.910: INFO: Created: latency-svc-vtbc6
Jan 12 01:27:09.928: INFO: Got endpoints: latency-svc-mnp8h [752.109291ms]
Jan 12 01:27:09.944: INFO: Created: latency-svc-qrpmk
Jan 12 01:27:09.972: INFO: Got endpoints: latency-svc-fb8dr [753.392923ms]
Jan 12 01:27:09.998: INFO: Created: latency-svc-xxf9h
Jan 12 01:27:10.037: INFO: Got endpoints: latency-svc-g5647 [753.735462ms]
Jan 12 01:27:10.053: INFO: Created: latency-svc-f9zc6
Jan 12 01:27:10.074: INFO: Got endpoints: latency-svc-fnm9n [744.360477ms]
Jan 12 01:27:10.118: INFO: Created: latency-svc-66kh5
Jan 12 01:27:10.135: INFO: Got endpoints: latency-svc-6mdtf [758.069815ms]
Jan 12 01:27:10.151: INFO: Created: latency-svc-v69ld
Jan 12 01:27:10.169: INFO: Got endpoints: latency-svc-gxqmq [742.865491ms]
Jan 12 01:27:10.201: INFO: Created: latency-svc-dlzkl
Jan 12 01:27:10.221: INFO: Got endpoints: latency-svc-2nsd4 [752.009606ms]
Jan 12 01:27:10.251: INFO: Created: latency-svc-6nt9r
Jan 12 01:27:10.276: INFO: Got endpoints: latency-svc-lv46t [741.746344ms]
Jan 12 01:27:10.293: INFO: Created: latency-svc-lnlmn
Jan 12 01:27:10.334: INFO: Got endpoints: latency-svc-zhbv4 [753.843403ms]
Jan 12 01:27:10.362: INFO: Created: latency-svc-545rx
Jan 12 01:27:10.369: INFO: Got endpoints: latency-svc-954xd [743.939287ms]
Jan 12 01:27:10.389: INFO: Created: latency-svc-24gd4
Jan 12 01:27:10.420: INFO: Got endpoints: latency-svc-8m585 [736.098252ms]
Jan 12 01:27:10.448: INFO: Created: latency-svc-v8llc
Jan 12 01:27:10.482: INFO: Got endpoints: latency-svc-94d5p [759.031768ms]
Jan 12 01:27:10.499: INFO: Created: latency-svc-zw7qf
Jan 12 01:27:10.519: INFO: Got endpoints: latency-svc-mxs8b [742.533389ms]
Jan 12 01:27:10.547: INFO: Created: latency-svc-2462m
Jan 12 01:27:10.588: INFO: Got endpoints: latency-svc-4vp6m [763.141821ms]
Jan 12 01:27:10.622: INFO: Got endpoints: latency-svc-vtbc6 [746.26184ms]
Jan 12 01:27:10.626: INFO: Created: latency-svc-8mvqh
Jan 12 01:27:10.643: INFO: Created: latency-svc-6zwc6
Jan 12 01:27:10.674: INFO: Got endpoints: latency-svc-qrpmk [745.711117ms]
Jan 12 01:27:10.709: INFO: Created: latency-svc-kv6r6
Jan 12 01:27:10.726: INFO: Got endpoints: latency-svc-xxf9h [754.112106ms]
Jan 12 01:27:10.745: INFO: Created: latency-svc-8rr8j
Jan 12 01:27:10.776: INFO: Got endpoints: latency-svc-f9zc6 [739.024284ms]
Jan 12 01:27:10.796: INFO: Created: latency-svc-tfvd5
Jan 12 01:27:10.826: INFO: Got endpoints: latency-svc-66kh5 [751.676829ms]
Jan 12 01:27:10.846: INFO: Created: latency-svc-6jph2
Jan 12 01:27:10.872: INFO: Got endpoints: latency-svc-v69ld [737.067516ms]
Jan 12 01:27:10.888: INFO: Created: latency-svc-kshld
Jan 12 01:27:10.937: INFO: Got endpoints: latency-svc-dlzkl [768.48204ms]
Jan 12 01:27:10.959: INFO: Created: latency-svc-tqlnh
Jan 12 01:27:10.972: INFO: Got endpoints: latency-svc-6nt9r [750.912846ms]
Jan 12 01:27:10.997: INFO: Created: latency-svc-l6m9h
Jan 12 01:27:11.032: INFO: Got endpoints: latency-svc-lnlmn [755.932318ms]
Jan 12 01:27:11.063: INFO: Created: latency-svc-gm756
Jan 12 01:27:11.077: INFO: Got endpoints: latency-svc-545rx [743.784435ms]
Jan 12 01:27:11.095: INFO: Created: latency-svc-tk8sn
Jan 12 01:27:11.127: INFO: Got endpoints: latency-svc-24gd4 [758.523313ms]
Jan 12 01:27:11.164: INFO: Created: latency-svc-624n7
Jan 12 01:27:11.171: INFO: Got endpoints: latency-svc-v8llc [751.141804ms]
Jan 12 01:27:11.190: INFO: Created: latency-svc-b8bpc
Jan 12 01:27:11.222: INFO: Got endpoints: latency-svc-zw7qf [740.40261ms]
Jan 12 01:27:11.248: INFO: Created: latency-svc-bxcv6
Jan 12 01:27:11.291: INFO: Got endpoints: latency-svc-2462m [772.027523ms]
Jan 12 01:27:11.308: INFO: Created: latency-svc-sw7pz
Jan 12 01:27:11.319: INFO: Got endpoints: latency-svc-8mvqh [730.394034ms]
Jan 12 01:27:11.342: INFO: Created: latency-svc-dh6vr
Jan 12 01:27:11.372: INFO: Got endpoints: latency-svc-6zwc6 [749.701016ms]
Jan 12 01:27:11.412: INFO: Created: latency-svc-xnq8c
Jan 12 01:27:11.422: INFO: Got endpoints: latency-svc-kv6r6 [748.140648ms]
Jan 12 01:27:11.441: INFO: Created: latency-svc-2tk6b
Jan 12 01:27:11.475: INFO: Got endpoints: latency-svc-8rr8j [748.834729ms]
Jan 12 01:27:11.512: INFO: Created: latency-svc-r9g5j
Jan 12 01:27:11.529: INFO: Got endpoints: latency-svc-tfvd5 [753.158383ms]
Jan 12 01:27:11.546: INFO: Created: latency-svc-bschq
Jan 12 01:27:11.575: INFO: Got endpoints: latency-svc-6jph2 [748.731059ms]
Jan 12 01:27:11.628: INFO: Got endpoints: latency-svc-kshld [755.600591ms]
Jan 12 01:27:11.672: INFO: Got endpoints: latency-svc-tqlnh [734.164665ms]
Jan 12 01:27:11.734: INFO: Got endpoints: latency-svc-l6m9h [762.508378ms]
Jan 12 01:27:11.768: INFO: Got endpoints: latency-svc-gm756 [736.214945ms]
Jan 12 01:27:11.828: INFO: Got endpoints: latency-svc-tk8sn [750.201728ms]
Jan 12 01:27:11.877: INFO: Got endpoints: latency-svc-624n7 [749.961549ms]
Jan 12 01:27:11.925: INFO: Got endpoints: latency-svc-b8bpc [753.664727ms]
Jan 12 01:27:11.969: INFO: Got endpoints: latency-svc-bxcv6 [746.855891ms]
Jan 12 01:27:12.019: INFO: Got endpoints: latency-svc-sw7pz [728.080501ms]
Jan 12 01:27:12.078: INFO: Got endpoints: latency-svc-dh6vr [759.596906ms]
Jan 12 01:27:12.128: INFO: Got endpoints: latency-svc-xnq8c [756.291066ms]
Jan 12 01:27:12.185: INFO: Got endpoints: latency-svc-2tk6b [763.189917ms]
Jan 12 01:27:12.218: INFO: Got endpoints: latency-svc-r9g5j [743.373486ms]
Jan 12 01:27:12.272: INFO: Got endpoints: latency-svc-bschq [743.353906ms]
Jan 12 01:27:12.272: INFO: Latencies: [25.612862ms 58.424121ms 76.888657ms 95.816719ms 124.375366ms 153.970898ms 179.090924ms 220.827967ms 238.857772ms 266.292974ms 292.780355ms 314.941479ms 340.998526ms 358.407926ms 361.184838ms 363.956032ms 365.505069ms 365.560542ms 366.307294ms 366.317595ms 367.682185ms 368.418537ms 368.852341ms 368.858846ms 368.891652ms 369.889292ms 370.304706ms 370.814036ms 371.360805ms 371.553062ms 372.279494ms 372.510941ms 372.662818ms 372.680139ms 372.794735ms 372.814233ms 373.447228ms 375.010815ms 375.606521ms 375.909917ms 376.468811ms 377.762427ms 377.770769ms 377.839416ms 381.273371ms 381.66856ms 382.664384ms 383.515894ms 384.402489ms 385.544122ms 386.675073ms 386.811767ms 386.890968ms 387.829436ms 392.776959ms 392.845671ms 393.698128ms 394.532379ms 397.15747ms 398.570806ms 445.610966ms 451.057978ms 486.493964ms 498.123125ms 528.304089ms 565.61792ms 585.282365ms 622.580441ms 643.503402ms 650.314152ms 681.822308ms 697.57027ms 714.217411ms 728.080501ms 730.394034ms 734.051745ms 734.164665ms 734.892277ms 734.997258ms 735.773374ms 736.093107ms 736.098252ms 736.12824ms 736.214945ms 737.067516ms 737.801335ms 739.024284ms 739.072756ms 739.419959ms 739.96298ms 740.40261ms 741.332654ms 741.746344ms 742.035954ms 742.312934ms 742.37997ms 742.533389ms 742.865491ms 742.996883ms 743.353906ms 743.373486ms 743.408168ms 743.501395ms 743.784435ms 743.911758ms 743.939287ms 744.277454ms 744.360477ms 744.39434ms 745.159633ms 745.380205ms 745.47885ms 745.55541ms 745.711117ms 746.191005ms 746.26184ms 746.626852ms 746.693941ms 746.730576ms 746.855891ms 746.959976ms 747.082892ms 747.299369ms 747.82738ms 748.140648ms 748.24993ms 748.653061ms 748.731059ms 748.834729ms 748.927725ms 748.964951ms 749.427777ms 749.460245ms 749.517724ms 749.544364ms 749.65437ms 749.701016ms 749.70556ms 749.838186ms 749.961549ms 750.193516ms 750.201728ms 750.285419ms 750.477084ms 750.591533ms 750.912846ms 751.03513ms 751.061633ms 751.140707ms 751.141804ms 751.472926ms 751.542125ms 751.676829ms 752.009606ms 752.109291ms 752.309693ms 752.647094ms 752.697501ms 752.946017ms 753.158383ms 753.392923ms 753.418735ms 753.664727ms 753.735462ms 753.843403ms 754.112106ms 754.255716ms 754.631955ms 754.908968ms 755.034861ms 755.047706ms 755.600591ms 755.932318ms 756.132309ms 756.291066ms 756.439763ms 756.696526ms 757.31765ms 758.069815ms 758.098624ms 758.523313ms 758.746142ms 758.816479ms 758.854884ms 758.873174ms 759.031768ms 759.204291ms 759.355703ms 759.596906ms 759.621598ms 759.688789ms 759.858963ms 762.178626ms 762.508378ms 763.141821ms 763.189917ms 764.115864ms 768.48204ms 770.85492ms 772.027523ms]
Jan 12 01:27:12.273: INFO: 50 %ile: 743.373486ms
Jan 12 01:27:12.273: INFO: 90 %ile: 758.523313ms
Jan 12 01:27:12.273: INFO: 99 %ile: 770.85492ms
Jan 12 01:27:12.273: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jan 12 01:27:12.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-5890" for this suite. 01/12/23 01:27:12.279
------------------------------
• [SLOW TEST] [11.791 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:27:00.511
    Jan 12 01:27:00.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename svc-latency 01/12/23 01:27:00.512
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:27:00.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:27:00.53
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 12 01:27:00.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5890 01/12/23 01:27:00.533
    I0112 01:27:00.539778      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5890, replica count: 1
    I0112 01:27:01.591147      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0112 01:27:02.591328      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0112 01:27:03.592254      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 01:27:03.733: INFO: Created: latency-svc-q7sl5
    Jan 12 01:27:03.733: INFO: Got endpoints: latency-svc-q7sl5 [41.036302ms]
    Jan 12 01:27:03.752: INFO: Created: latency-svc-v4klj
    Jan 12 01:27:03.759: INFO: Got endpoints: latency-svc-v4klj [25.612862ms]
    Jan 12 01:27:03.771: INFO: Created: latency-svc-x7ljn
    Jan 12 01:27:03.792: INFO: Got endpoints: latency-svc-x7ljn [58.424121ms]
    Jan 12 01:27:03.801: INFO: Created: latency-svc-bc4ph
    Jan 12 01:27:03.811: INFO: Got endpoints: latency-svc-bc4ph [76.888657ms]
    Jan 12 01:27:03.827: INFO: Created: latency-svc-ggtpz
    Jan 12 01:27:03.830: INFO: Got endpoints: latency-svc-ggtpz [95.816719ms]
    Jan 12 01:27:03.858: INFO: Created: latency-svc-bnnhn
    Jan 12 01:27:03.858: INFO: Got endpoints: latency-svc-bnnhn [124.375366ms]
    Jan 12 01:27:03.878: INFO: Created: latency-svc-rr44t
    Jan 12 01:27:03.888: INFO: Got endpoints: latency-svc-rr44t [153.970898ms]
    Jan 12 01:27:03.913: INFO: Created: latency-svc-crzp5
    Jan 12 01:27:03.913: INFO: Got endpoints: latency-svc-crzp5 [179.090924ms]
    Jan 12 01:27:03.929: INFO: Created: latency-svc-mzsvh
    Jan 12 01:27:03.955: INFO: Got endpoints: latency-svc-mzsvh [220.827967ms]
    Jan 12 01:27:03.973: INFO: Created: latency-svc-bfb5z
    Jan 12 01:27:03.973: INFO: Got endpoints: latency-svc-bfb5z [238.857772ms]
    Jan 12 01:27:03.986: INFO: Created: latency-svc-gjcl9
    Jan 12 01:27:04.001: INFO: Got endpoints: latency-svc-gjcl9 [266.292974ms]
    Jan 12 01:27:04.005: INFO: Created: latency-svc-66dxr
    Jan 12 01:27:04.027: INFO: Got endpoints: latency-svc-66dxr [292.780355ms]
    Jan 12 01:27:04.039: INFO: Created: latency-svc-4np54
    Jan 12 01:27:04.050: INFO: Got endpoints: latency-svc-4np54 [314.941479ms]
    Jan 12 01:27:04.069: INFO: Created: latency-svc-ggjfl
    Jan 12 01:27:04.076: INFO: Got endpoints: latency-svc-ggjfl [340.998526ms]
    Jan 12 01:27:04.089: INFO: Created: latency-svc-pvzhm
    Jan 12 01:27:04.101: INFO: Got endpoints: latency-svc-pvzhm [366.317595ms]
    Jan 12 01:27:04.131: INFO: Created: latency-svc-f997q
    Jan 12 01:27:04.132: INFO: Got endpoints: latency-svc-f997q [397.15747ms]
    Jan 12 01:27:04.144: INFO: Created: latency-svc-hrf6w
    Jan 12 01:27:04.158: INFO: Got endpoints: latency-svc-hrf6w [398.570806ms]
    Jan 12 01:27:04.162: INFO: Created: latency-svc-pnjtj
    Jan 12 01:27:04.187: INFO: Got endpoints: latency-svc-pnjtj [394.532379ms]
    Jan 12 01:27:04.199: INFO: Created: latency-svc-jxck9
    Jan 12 01:27:04.199: INFO: Got endpoints: latency-svc-jxck9 [386.890968ms]
    Jan 12 01:27:04.222: INFO: Created: latency-svc-b6vgg
    Jan 12 01:27:04.222: INFO: Got endpoints: latency-svc-b6vgg [392.845671ms]
    Jan 12 01:27:04.234: INFO: Created: latency-svc-5hnrx
    Jan 12 01:27:04.252: INFO: Got endpoints: latency-svc-5hnrx [393.698128ms]
    Jan 12 01:27:04.260: INFO: Created: latency-svc-vvp96
    Jan 12 01:27:04.271: INFO: Got endpoints: latency-svc-vvp96 [382.664384ms]
    Jan 12 01:27:04.300: INFO: Created: latency-svc-qdw5g
    Jan 12 01:27:04.300: INFO: Got endpoints: latency-svc-qdw5g [386.675073ms]
    Jan 12 01:27:04.313: INFO: Created: latency-svc-v289s
    Jan 12 01:27:04.324: INFO: Got endpoints: latency-svc-v289s [368.852341ms]
    Jan 12 01:27:04.344: INFO: Created: latency-svc-2fgh7
    Jan 12 01:27:04.344: INFO: Got endpoints: latency-svc-2fgh7 [370.814036ms]
    Jan 12 01:27:04.362: INFO: Created: latency-svc-hjw49
    Jan 12 01:27:04.371: INFO: Got endpoints: latency-svc-hjw49 [370.304706ms]
    Jan 12 01:27:04.383: INFO: Created: latency-svc-l9h8s
    Jan 12 01:27:04.397: INFO: Got endpoints: latency-svc-l9h8s [369.889292ms]
    Jan 12 01:27:04.418: INFO: Created: latency-svc-jlcdc
    Jan 12 01:27:04.418: INFO: Got endpoints: latency-svc-jlcdc [368.891652ms]
    Jan 12 01:27:04.448: INFO: Created: latency-svc-v6mm6
    Jan 12 01:27:04.448: INFO: Got endpoints: latency-svc-v6mm6 [372.510941ms]
    Jan 12 01:27:04.460: INFO: Created: latency-svc-8bxtp
    Jan 12 01:27:04.470: INFO: Got endpoints: latency-svc-8bxtp [368.858846ms]
    Jan 12 01:27:04.485: INFO: Created: latency-svc-ctqv4
    Jan 12 01:27:04.496: INFO: Got endpoints: latency-svc-ctqv4 [363.956032ms]
    Jan 12 01:27:04.515: INFO: Created: latency-svc-sqq6v
    Jan 12 01:27:04.530: INFO: Got endpoints: latency-svc-sqq6v [372.662818ms]
    Jan 12 01:27:04.542: INFO: Created: latency-svc-5g72d
    Jan 12 01:27:04.552: INFO: Got endpoints: latency-svc-5g72d [365.505069ms]
    Jan 12 01:27:04.570: INFO: Created: latency-svc-9wfdl
    Jan 12 01:27:04.570: INFO: Got endpoints: latency-svc-9wfdl [371.360805ms]
    Jan 12 01:27:04.599: INFO: Created: latency-svc-6p4xj
    Jan 12 01:27:04.599: INFO: Got endpoints: latency-svc-6p4xj [376.468811ms]
    Jan 12 01:27:04.611: INFO: Created: latency-svc-kw6zg
    Jan 12 01:27:04.625: INFO: Got endpoints: latency-svc-kw6zg [372.680139ms]
    Jan 12 01:27:04.629: INFO: Created: latency-svc-sr659
    Jan 12 01:27:04.655: INFO: Got endpoints: latency-svc-sr659 [384.402489ms]
    Jan 12 01:27:04.673: INFO: Created: latency-svc-qf5s7
    Jan 12 01:27:04.673: INFO: Got endpoints: latency-svc-qf5s7 [372.794735ms]
    Jan 12 01:27:04.686: INFO: Created: latency-svc-k4dxr
    Jan 12 01:27:04.696: INFO: Got endpoints: latency-svc-k4dxr [371.553062ms]
    Jan 12 01:27:04.707: INFO: Created: latency-svc-666nm
    Jan 12 01:27:04.731: INFO: Got endpoints: latency-svc-666nm [387.829436ms]
    Jan 12 01:27:04.739: INFO: Created: latency-svc-nqmpl
    Jan 12 01:27:04.749: INFO: Got endpoints: latency-svc-nqmpl [377.770769ms]
    Jan 12 01:27:04.771: INFO: Created: latency-svc-bh6rn
    Jan 12 01:27:04.773: INFO: Got endpoints: latency-svc-bh6rn [375.909917ms]
    Jan 12 01:27:04.802: INFO: Created: latency-svc-fchbd
    Jan 12 01:27:04.802: INFO: Got endpoints: latency-svc-fchbd [383.515894ms]
    Jan 12 01:27:04.820: INFO: Created: latency-svc-xntgq
    Jan 12 01:27:04.830: INFO: Got endpoints: latency-svc-xntgq [381.66856ms]
    Jan 12 01:27:04.842: INFO: Created: latency-svc-6jdvv
    Jan 12 01:27:04.857: INFO: Got endpoints: latency-svc-6jdvv [386.811767ms]
    Jan 12 01:27:04.861: INFO: Created: latency-svc-fns49
    Jan 12 01:27:04.882: INFO: Got endpoints: latency-svc-fns49 [385.544122ms]
    Jan 12 01:27:04.896: INFO: Created: latency-svc-6z2rs
    Jan 12 01:27:04.896: INFO: Got endpoints: latency-svc-6z2rs [365.560542ms]
    Jan 12 01:27:04.908: INFO: Created: latency-svc-j68rq
    Jan 12 01:27:04.918: INFO: Got endpoints: latency-svc-j68rq [366.307294ms]
    Jan 12 01:27:04.932: INFO: Created: latency-svc-6qsnm
    Jan 12 01:27:04.943: INFO: Got endpoints: latency-svc-6qsnm [372.814233ms]
    Jan 12 01:27:04.956: INFO: Created: latency-svc-7gm6l
    Jan 12 01:27:04.971: INFO: Got endpoints: latency-svc-7gm6l [372.279494ms]
    Jan 12 01:27:04.976: INFO: Created: latency-svc-9ggbk
    Jan 12 01:27:04.998: INFO: Got endpoints: latency-svc-9ggbk [373.447228ms]
    Jan 12 01:27:05.006: INFO: Created: latency-svc-mrd75
    Jan 12 01:27:05.016: INFO: Got endpoints: latency-svc-mrd75 [361.184838ms]
    Jan 12 01:27:05.034: INFO: Created: latency-svc-zx256
    Jan 12 01:27:05.041: INFO: Got endpoints: latency-svc-zx256 [368.418537ms]
    Jan 12 01:27:05.053: INFO: Created: latency-svc-rlmw9
    Jan 12 01:27:05.063: INFO: Got endpoints: latency-svc-rlmw9 [367.682185ms]
    Jan 12 01:27:05.084: INFO: Created: latency-svc-bvvv2
    Jan 12 01:27:05.090: INFO: Got endpoints: latency-svc-bvvv2 [358.407926ms]
    Jan 12 01:27:05.124: INFO: Created: latency-svc-q7wn4
    Jan 12 01:27:05.124: INFO: Got endpoints: latency-svc-q7wn4 [375.010815ms]
    Jan 12 01:27:05.136: INFO: Created: latency-svc-sf8hq
    Jan 12 01:27:05.149: INFO: Got endpoints: latency-svc-sf8hq [375.606521ms]
    Jan 12 01:27:05.180: INFO: Created: latency-svc-p4jj6
    Jan 12 01:27:05.180: INFO: Got endpoints: latency-svc-p4jj6 [377.839416ms]
    Jan 12 01:27:05.196: INFO: Created: latency-svc-7wq7q
    Jan 12 01:27:05.211: INFO: Got endpoints: latency-svc-7wq7q [381.273371ms]
    Jan 12 01:27:05.229: INFO: Created: latency-svc-g9wh9
    Jan 12 01:27:05.234: INFO: Got endpoints: latency-svc-g9wh9 [377.762427ms]
    Jan 12 01:27:05.260: INFO: Created: latency-svc-9gt6z
    Jan 12 01:27:05.274: INFO: Got endpoints: latency-svc-9gt6z [392.776959ms]
    Jan 12 01:27:05.280: INFO: Created: latency-svc-g6mp2
    Jan 12 01:27:05.296: INFO: Created: latency-svc-ff4t9
    Jan 12 01:27:05.311: INFO: Created: latency-svc-v7z2f
    Jan 12 01:27:05.341: INFO: Got endpoints: latency-svc-g6mp2 [445.610966ms]
    Jan 12 01:27:05.353: INFO: Created: latency-svc-zn7dp
    Jan 12 01:27:05.366: INFO: Created: latency-svc-vxj2b
    Jan 12 01:27:05.369: INFO: Got endpoints: latency-svc-ff4t9 [451.057978ms]
    Jan 12 01:27:05.385: INFO: Created: latency-svc-p8mkh
    Jan 12 01:27:05.407: INFO: Created: latency-svc-s4xjs
    Jan 12 01:27:05.430: INFO: Got endpoints: latency-svc-v7z2f [486.493964ms]
    Jan 12 01:27:05.436: INFO: Created: latency-svc-z88np
    Jan 12 01:27:05.456: INFO: Created: latency-svc-rz7f4
    Jan 12 01:27:05.469: INFO: Got endpoints: latency-svc-zn7dp [498.123125ms]
    Jan 12 01:27:05.482: INFO: Created: latency-svc-rqk9z
    Jan 12 01:27:05.501: INFO: Created: latency-svc-rvjqg
    Jan 12 01:27:05.527: INFO: Got endpoints: latency-svc-vxj2b [528.304089ms]
    Jan 12 01:27:05.527: INFO: Created: latency-svc-4fmbm
    Jan 12 01:27:05.539: INFO: Created: latency-svc-hlp72
    Jan 12 01:27:05.551: INFO: Created: latency-svc-rtbnb
    Jan 12 01:27:05.582: INFO: Got endpoints: latency-svc-p8mkh [565.61792ms]
    Jan 12 01:27:05.598: INFO: Created: latency-svc-2q22r
    Jan 12 01:27:05.611: INFO: Created: latency-svc-ghvp6
    Jan 12 01:27:05.626: INFO: Got endpoints: latency-svc-s4xjs [585.282365ms]
    Jan 12 01:27:05.631: INFO: Created: latency-svc-6rnz5
    Jan 12 01:27:05.686: INFO: Got endpoints: latency-svc-z88np [622.580441ms]
    Jan 12 01:27:05.687: INFO: Created: latency-svc-6tfwq
    Jan 12 01:27:05.733: INFO: Got endpoints: latency-svc-rz7f4 [643.503402ms]
    Jan 12 01:27:05.734: INFO: Created: latency-svc-thpfv
    Jan 12 01:27:05.759: INFO: Created: latency-svc-brhfd
    Jan 12 01:27:05.774: INFO: Got endpoints: latency-svc-rqk9z [650.314152ms]
    Jan 12 01:27:05.782: INFO: Created: latency-svc-7s4f2
    Jan 12 01:27:05.815: INFO: Created: latency-svc-hbfbp
    Jan 12 01:27:05.831: INFO: Got endpoints: latency-svc-rvjqg [681.822308ms]
    Jan 12 01:27:05.843: INFO: Created: latency-svc-f78jh
    Jan 12 01:27:05.856: INFO: Created: latency-svc-ddpzs
    Jan 12 01:27:05.877: INFO: Got endpoints: latency-svc-4fmbm [697.57027ms]
    Jan 12 01:27:05.878: INFO: Created: latency-svc-hsf2k
    Jan 12 01:27:05.900: INFO: Created: latency-svc-wvx5q
    Jan 12 01:27:05.925: INFO: Got endpoints: latency-svc-hlp72 [714.217411ms]
    Jan 12 01:27:05.929: INFO: Created: latency-svc-v6ntt
    Jan 12 01:27:05.965: INFO: Created: latency-svc-f8z9h
    Jan 12 01:27:05.985: INFO: Got endpoints: latency-svc-rtbnb [750.477084ms]
    Jan 12 01:27:06.034: INFO: Got endpoints: latency-svc-2q22r [759.355703ms]
    Jan 12 01:27:06.034: INFO: Created: latency-svc-2td4x
    Jan 12 01:27:06.061: INFO: Created: latency-svc-rq2cp
    Jan 12 01:27:06.083: INFO: Got endpoints: latency-svc-ghvp6 [742.035954ms]
    Jan 12 01:27:06.128: INFO: Got endpoints: latency-svc-6rnz5 [758.854884ms]
    Jan 12 01:27:06.149: INFO: Created: latency-svc-qnwcw
    Jan 12 01:27:06.182: INFO: Got endpoints: latency-svc-6tfwq [752.647094ms]
    Jan 12 01:27:06.183: INFO: Created: latency-svc-qnv2b
    Jan 12 01:27:06.204: INFO: Created: latency-svc-8smlm
    Jan 12 01:27:06.219: INFO: Got endpoints: latency-svc-thpfv [749.460245ms]
    Jan 12 01:27:06.281: INFO: Got endpoints: latency-svc-brhfd [754.255716ms]
    Jan 12 01:27:06.286: INFO: Created: latency-svc-gkdfc
    Jan 12 01:27:06.300: INFO: Created: latency-svc-wkqw9
    Jan 12 01:27:06.326: INFO: Got endpoints: latency-svc-7s4f2 [743.501395ms]
    Jan 12 01:27:06.350: INFO: Created: latency-svc-2pld9
    Jan 12 01:27:06.389: INFO: Got endpoints: latency-svc-hbfbp [762.178626ms]
    Jan 12 01:27:06.412: INFO: Created: latency-svc-kntq9
    Jan 12 01:27:06.422: INFO: Got endpoints: latency-svc-f78jh [736.12824ms]
    Jan 12 01:27:06.441: INFO: Created: latency-svc-7mrnp
    Jan 12 01:27:06.484: INFO: Got endpoints: latency-svc-ddpzs [751.03513ms]
    Jan 12 01:27:06.519: INFO: Created: latency-svc-sxpgx
    Jan 12 01:27:06.524: INFO: Got endpoints: latency-svc-hsf2k [749.427777ms]
    Jan 12 01:27:06.540: INFO: Created: latency-svc-d87wx
    Jan 12 01:27:06.579: INFO: Got endpoints: latency-svc-wvx5q [748.24993ms]
    Jan 12 01:27:06.614: INFO: Created: latency-svc-97gkf
    Jan 12 01:27:06.632: INFO: Got endpoints: latency-svc-v6ntt [754.631955ms]
    Jan 12 01:27:06.648: INFO: Created: latency-svc-wfs8p
    Jan 12 01:27:06.673: INFO: Got endpoints: latency-svc-f8z9h [747.299369ms]
    Jan 12 01:27:06.697: INFO: Created: latency-svc-mmdch
    Jan 12 01:27:06.732: INFO: Got endpoints: latency-svc-2td4x [739.419959ms]
    Jan 12 01:27:06.753: INFO: Created: latency-svc-g6lrn
    Jan 12 01:27:06.773: INFO: Got endpoints: latency-svc-rq2cp [739.072756ms]
    Jan 12 01:27:06.789: INFO: Created: latency-svc-w85qr
    Jan 12 01:27:06.826: INFO: Got endpoints: latency-svc-qnwcw [742.37997ms]
    Jan 12 01:27:06.859: INFO: Created: latency-svc-wc2rm
    Jan 12 01:27:06.875: INFO: Got endpoints: latency-svc-qnv2b [747.082892ms]
    Jan 12 01:27:06.894: INFO: Created: latency-svc-cmqz4
    Jan 12 01:27:06.919: INFO: Got endpoints: latency-svc-8smlm [735.773374ms]
    Jan 12 01:27:06.938: INFO: Created: latency-svc-8wxlp
    Jan 12 01:27:06.977: INFO: Got endpoints: latency-svc-gkdfc [758.098624ms]
    Jan 12 01:27:06.999: INFO: Created: latency-svc-cmvnm
    Jan 12 01:27:07.029: INFO: Got endpoints: latency-svc-wkqw9 [747.82738ms]
    Jan 12 01:27:07.045: INFO: Created: latency-svc-pd9rs
    Jan 12 01:27:07.085: INFO: Got endpoints: latency-svc-2pld9 [759.204291ms]
    Jan 12 01:27:07.109: INFO: Created: latency-svc-2mmqh
    Jan 12 01:27:07.126: INFO: Got endpoints: latency-svc-kntq9 [736.093107ms]
    Jan 12 01:27:07.148: INFO: Created: latency-svc-524v8
    Jan 12 01:27:07.169: INFO: Got endpoints: latency-svc-7mrnp [746.730576ms]
    Jan 12 01:27:07.194: INFO: Created: latency-svc-xkztt
    Jan 12 01:27:07.239: INFO: Got endpoints: latency-svc-sxpgx [754.908968ms]
    Jan 12 01:27:07.273: INFO: Got endpoints: latency-svc-d87wx [748.964951ms]
    Jan 12 01:27:07.273: INFO: Created: latency-svc-9k99k
    Jan 12 01:27:07.304: INFO: Created: latency-svc-27wmx
    Jan 12 01:27:07.328: INFO: Got endpoints: latency-svc-97gkf [748.927725ms]
    Jan 12 01:27:07.349: INFO: Created: latency-svc-l5pkc
    Jan 12 01:27:07.378: INFO: Got endpoints: latency-svc-wfs8p [745.47885ms]
    Jan 12 01:27:07.401: INFO: Created: latency-svc-tdqkd
    Jan 12 01:27:07.432: INFO: Got endpoints: latency-svc-mmdch [758.873174ms]
    Jan 12 01:27:07.455: INFO: Created: latency-svc-kc24t
    Jan 12 01:27:07.481: INFO: Got endpoints: latency-svc-g6lrn [749.517724ms]
    Jan 12 01:27:07.497: INFO: Created: latency-svc-jg9vw
    Jan 12 01:27:07.525: INFO: Got endpoints: latency-svc-w85qr [751.542125ms]
    Jan 12 01:27:07.548: INFO: Created: latency-svc-n5bxz
    Jan 12 01:27:07.577: INFO: Got endpoints: latency-svc-wc2rm [751.061633ms]
    Jan 12 01:27:07.596: INFO: Created: latency-svc-c8tcx
    Jan 12 01:27:07.635: INFO: Got endpoints: latency-svc-cmqz4 [759.688789ms]
    Jan 12 01:27:07.671: INFO: Got endpoints: latency-svc-8wxlp [752.309693ms]
    Jan 12 01:27:07.678: INFO: Created: latency-svc-v6t4s
    Jan 12 01:27:07.701: INFO: Created: latency-svc-flp5s
    Jan 12 01:27:07.737: INFO: Got endpoints: latency-svc-cmvnm [759.858963ms]
    Jan 12 01:27:07.767: INFO: Created: latency-svc-g8h9l
    Jan 12 01:27:07.770: INFO: Got endpoints: latency-svc-pd9rs [741.332654ms]
    Jan 12 01:27:07.793: INFO: Created: latency-svc-t8wdl
    Jan 12 01:27:07.828: INFO: Got endpoints: latency-svc-2mmqh [743.408168ms]
    Jan 12 01:27:07.861: INFO: Created: latency-svc-5kh6h
    Jan 12 01:27:07.877: INFO: Got endpoints: latency-svc-524v8 [750.285419ms]
    Jan 12 01:27:07.892: INFO: Created: latency-svc-n46qx
    Jan 12 01:27:07.933: INFO: Got endpoints: latency-svc-xkztt [764.115864ms]
    Jan 12 01:27:07.967: INFO: Created: latency-svc-4srfb
    Jan 12 01:27:07.974: INFO: Got endpoints: latency-svc-9k99k [734.892277ms]
    Jan 12 01:27:07.990: INFO: Created: latency-svc-d4r6q
    Jan 12 01:27:08.025: INFO: Got endpoints: latency-svc-27wmx [752.697501ms]
    Jan 12 01:27:08.049: INFO: Created: latency-svc-6m8gl
    Jan 12 01:27:08.085: INFO: Got endpoints: latency-svc-l5pkc [757.31765ms]
    Jan 12 01:27:08.109: INFO: Created: latency-svc-dc7f2
    Jan 12 01:27:08.120: INFO: Got endpoints: latency-svc-tdqkd [742.312934ms]
    Jan 12 01:27:08.136: INFO: Created: latency-svc-7wm2g
    Jan 12 01:27:08.178: INFO: Got endpoints: latency-svc-kc24t [746.626852ms]
    Jan 12 01:27:08.211: INFO: Created: latency-svc-x6qq6
    Jan 12 01:27:08.226: INFO: Got endpoints: latency-svc-jg9vw [745.159633ms]
    Jan 12 01:27:08.245: INFO: Created: latency-svc-pdtjj
    Jan 12 01:27:08.276: INFO: Got endpoints: latency-svc-n5bxz [751.140707ms]
    Jan 12 01:27:08.297: INFO: Created: latency-svc-z7vb2
    Jan 12 01:27:08.333: INFO: Got endpoints: latency-svc-c8tcx [756.439763ms]
    Jan 12 01:27:08.352: INFO: Created: latency-svc-mccxc
    Jan 12 01:27:08.375: INFO: Got endpoints: latency-svc-v6t4s [739.96298ms]
    Jan 12 01:27:08.391: INFO: Created: latency-svc-pckwg
    Jan 12 01:27:08.442: INFO: Got endpoints: latency-svc-flp5s [770.85492ms]
    Jan 12 01:27:08.463: INFO: Created: latency-svc-8bbdq
    Jan 12 01:27:08.472: INFO: Got endpoints: latency-svc-g8h9l [734.997258ms]
    Jan 12 01:27:08.492: INFO: Created: latency-svc-pqrfb
    Jan 12 01:27:08.523: INFO: Got endpoints: latency-svc-t8wdl [752.946017ms]
    Jan 12 01:27:08.559: INFO: Created: latency-svc-t78c9
    Jan 12 01:27:08.580: INFO: Got endpoints: latency-svc-5kh6h [751.472926ms]
    Jan 12 01:27:08.596: INFO: Created: latency-svc-vdj9z
    Jan 12 01:27:08.626: INFO: Got endpoints: latency-svc-n46qx [749.544364ms]
    Jan 12 01:27:08.645: INFO: Created: latency-svc-2b8tw
    Jan 12 01:27:08.688: INFO: Got endpoints: latency-svc-4srfb [755.047706ms]
    Jan 12 01:27:08.710: INFO: Created: latency-svc-vx8rc
    Jan 12 01:27:08.723: INFO: Got endpoints: latency-svc-d4r6q [748.653061ms]
    Jan 12 01:27:08.742: INFO: Created: latency-svc-66wvx
    Jan 12 01:27:08.775: INFO: Got endpoints: latency-svc-6m8gl [749.70556ms]
    Jan 12 01:27:08.811: INFO: Created: latency-svc-fsdk5
    Jan 12 01:27:08.829: INFO: Got endpoints: latency-svc-dc7f2 [744.277454ms]
    Jan 12 01:27:08.845: INFO: Created: latency-svc-wsxvz
    Jan 12 01:27:08.875: INFO: Got endpoints: latency-svc-7wm2g [755.034861ms]
    Jan 12 01:27:08.909: INFO: Created: latency-svc-vln76
    Jan 12 01:27:08.925: INFO: Got endpoints: latency-svc-x6qq6 [746.959976ms]
    Jan 12 01:27:08.945: INFO: Created: latency-svc-2qv7v
    Jan 12 01:27:08.970: INFO: Got endpoints: latency-svc-pdtjj [742.996883ms]
    Jan 12 01:27:08.990: INFO: Created: latency-svc-pk8jh
    Jan 12 01:27:09.032: INFO: Got endpoints: latency-svc-z7vb2 [756.132309ms]
    Jan 12 01:27:09.055: INFO: Created: latency-svc-6xjwr
    Jan 12 01:27:09.079: INFO: Got endpoints: latency-svc-mccxc [745.380205ms]
    Jan 12 01:27:09.095: INFO: Created: latency-svc-j598q
    Jan 12 01:27:09.132: INFO: Got endpoints: latency-svc-pckwg [756.696526ms]
    Jan 12 01:27:09.156: INFO: Created: latency-svc-d829n
    Jan 12 01:27:09.176: INFO: Got endpoints: latency-svc-8bbdq [734.051745ms]
    Jan 12 01:27:09.200: INFO: Created: latency-svc-mnp8h
    Jan 12 01:27:09.219: INFO: Got endpoints: latency-svc-pqrfb [746.693941ms]
    Jan 12 01:27:09.241: INFO: Created: latency-svc-fb8dr
    Jan 12 01:27:09.283: INFO: Got endpoints: latency-svc-t78c9 [759.621598ms]
    Jan 12 01:27:09.306: INFO: Created: latency-svc-g5647
    Jan 12 01:27:09.330: INFO: Got endpoints: latency-svc-vdj9z [749.838186ms]
    Jan 12 01:27:09.352: INFO: Created: latency-svc-fnm9n
    Jan 12 01:27:09.377: INFO: Got endpoints: latency-svc-2b8tw [750.591533ms]
    Jan 12 01:27:09.401: INFO: Created: latency-svc-6mdtf
    Jan 12 01:27:09.426: INFO: Got endpoints: latency-svc-vx8rc [737.801335ms]
    Jan 12 01:27:09.450: INFO: Created: latency-svc-gxqmq
    Jan 12 01:27:09.469: INFO: Got endpoints: latency-svc-66wvx [745.55541ms]
    Jan 12 01:27:09.487: INFO: Created: latency-svc-2nsd4
    Jan 12 01:27:09.534: INFO: Got endpoints: latency-svc-fsdk5 [758.816479ms]
    Jan 12 01:27:09.557: INFO: Created: latency-svc-lv46t
    Jan 12 01:27:09.580: INFO: Got endpoints: latency-svc-wsxvz [750.193516ms]
    Jan 12 01:27:09.596: INFO: Created: latency-svc-zhbv4
    Jan 12 01:27:09.625: INFO: Got endpoints: latency-svc-vln76 [749.65437ms]
    Jan 12 01:27:09.650: INFO: Created: latency-svc-954xd
    Jan 12 01:27:09.684: INFO: Got endpoints: latency-svc-2qv7v [758.746142ms]
    Jan 12 01:27:09.706: INFO: Created: latency-svc-8m585
    Jan 12 01:27:09.723: INFO: Got endpoints: latency-svc-pk8jh [753.418735ms]
    Jan 12 01:27:09.739: INFO: Created: latency-svc-94d5p
    Jan 12 01:27:09.776: INFO: Got endpoints: latency-svc-6xjwr [744.39434ms]
    Jan 12 01:27:09.803: INFO: Created: latency-svc-mxs8b
    Jan 12 01:27:09.825: INFO: Got endpoints: latency-svc-j598q [746.191005ms]
    Jan 12 01:27:09.840: INFO: Created: latency-svc-4vp6m
    Jan 12 01:27:09.876: INFO: Got endpoints: latency-svc-d829n [743.911758ms]
    Jan 12 01:27:09.910: INFO: Created: latency-svc-vtbc6
    Jan 12 01:27:09.928: INFO: Got endpoints: latency-svc-mnp8h [752.109291ms]
    Jan 12 01:27:09.944: INFO: Created: latency-svc-qrpmk
    Jan 12 01:27:09.972: INFO: Got endpoints: latency-svc-fb8dr [753.392923ms]
    Jan 12 01:27:09.998: INFO: Created: latency-svc-xxf9h
    Jan 12 01:27:10.037: INFO: Got endpoints: latency-svc-g5647 [753.735462ms]
    Jan 12 01:27:10.053: INFO: Created: latency-svc-f9zc6
    Jan 12 01:27:10.074: INFO: Got endpoints: latency-svc-fnm9n [744.360477ms]
    Jan 12 01:27:10.118: INFO: Created: latency-svc-66kh5
    Jan 12 01:27:10.135: INFO: Got endpoints: latency-svc-6mdtf [758.069815ms]
    Jan 12 01:27:10.151: INFO: Created: latency-svc-v69ld
    Jan 12 01:27:10.169: INFO: Got endpoints: latency-svc-gxqmq [742.865491ms]
    Jan 12 01:27:10.201: INFO: Created: latency-svc-dlzkl
    Jan 12 01:27:10.221: INFO: Got endpoints: latency-svc-2nsd4 [752.009606ms]
    Jan 12 01:27:10.251: INFO: Created: latency-svc-6nt9r
    Jan 12 01:27:10.276: INFO: Got endpoints: latency-svc-lv46t [741.746344ms]
    Jan 12 01:27:10.293: INFO: Created: latency-svc-lnlmn
    Jan 12 01:27:10.334: INFO: Got endpoints: latency-svc-zhbv4 [753.843403ms]
    Jan 12 01:27:10.362: INFO: Created: latency-svc-545rx
    Jan 12 01:27:10.369: INFO: Got endpoints: latency-svc-954xd [743.939287ms]
    Jan 12 01:27:10.389: INFO: Created: latency-svc-24gd4
    Jan 12 01:27:10.420: INFO: Got endpoints: latency-svc-8m585 [736.098252ms]
    Jan 12 01:27:10.448: INFO: Created: latency-svc-v8llc
    Jan 12 01:27:10.482: INFO: Got endpoints: latency-svc-94d5p [759.031768ms]
    Jan 12 01:27:10.499: INFO: Created: latency-svc-zw7qf
    Jan 12 01:27:10.519: INFO: Got endpoints: latency-svc-mxs8b [742.533389ms]
    Jan 12 01:27:10.547: INFO: Created: latency-svc-2462m
    Jan 12 01:27:10.588: INFO: Got endpoints: latency-svc-4vp6m [763.141821ms]
    Jan 12 01:27:10.622: INFO: Got endpoints: latency-svc-vtbc6 [746.26184ms]
    Jan 12 01:27:10.626: INFO: Created: latency-svc-8mvqh
    Jan 12 01:27:10.643: INFO: Created: latency-svc-6zwc6
    Jan 12 01:27:10.674: INFO: Got endpoints: latency-svc-qrpmk [745.711117ms]
    Jan 12 01:27:10.709: INFO: Created: latency-svc-kv6r6
    Jan 12 01:27:10.726: INFO: Got endpoints: latency-svc-xxf9h [754.112106ms]
    Jan 12 01:27:10.745: INFO: Created: latency-svc-8rr8j
    Jan 12 01:27:10.776: INFO: Got endpoints: latency-svc-f9zc6 [739.024284ms]
    Jan 12 01:27:10.796: INFO: Created: latency-svc-tfvd5
    Jan 12 01:27:10.826: INFO: Got endpoints: latency-svc-66kh5 [751.676829ms]
    Jan 12 01:27:10.846: INFO: Created: latency-svc-6jph2
    Jan 12 01:27:10.872: INFO: Got endpoints: latency-svc-v69ld [737.067516ms]
    Jan 12 01:27:10.888: INFO: Created: latency-svc-kshld
    Jan 12 01:27:10.937: INFO: Got endpoints: latency-svc-dlzkl [768.48204ms]
    Jan 12 01:27:10.959: INFO: Created: latency-svc-tqlnh
    Jan 12 01:27:10.972: INFO: Got endpoints: latency-svc-6nt9r [750.912846ms]
    Jan 12 01:27:10.997: INFO: Created: latency-svc-l6m9h
    Jan 12 01:27:11.032: INFO: Got endpoints: latency-svc-lnlmn [755.932318ms]
    Jan 12 01:27:11.063: INFO: Created: latency-svc-gm756
    Jan 12 01:27:11.077: INFO: Got endpoints: latency-svc-545rx [743.784435ms]
    Jan 12 01:27:11.095: INFO: Created: latency-svc-tk8sn
    Jan 12 01:27:11.127: INFO: Got endpoints: latency-svc-24gd4 [758.523313ms]
    Jan 12 01:27:11.164: INFO: Created: latency-svc-624n7
    Jan 12 01:27:11.171: INFO: Got endpoints: latency-svc-v8llc [751.141804ms]
    Jan 12 01:27:11.190: INFO: Created: latency-svc-b8bpc
    Jan 12 01:27:11.222: INFO: Got endpoints: latency-svc-zw7qf [740.40261ms]
    Jan 12 01:27:11.248: INFO: Created: latency-svc-bxcv6
    Jan 12 01:27:11.291: INFO: Got endpoints: latency-svc-2462m [772.027523ms]
    Jan 12 01:27:11.308: INFO: Created: latency-svc-sw7pz
    Jan 12 01:27:11.319: INFO: Got endpoints: latency-svc-8mvqh [730.394034ms]
    Jan 12 01:27:11.342: INFO: Created: latency-svc-dh6vr
    Jan 12 01:27:11.372: INFO: Got endpoints: latency-svc-6zwc6 [749.701016ms]
    Jan 12 01:27:11.412: INFO: Created: latency-svc-xnq8c
    Jan 12 01:27:11.422: INFO: Got endpoints: latency-svc-kv6r6 [748.140648ms]
    Jan 12 01:27:11.441: INFO: Created: latency-svc-2tk6b
    Jan 12 01:27:11.475: INFO: Got endpoints: latency-svc-8rr8j [748.834729ms]
    Jan 12 01:27:11.512: INFO: Created: latency-svc-r9g5j
    Jan 12 01:27:11.529: INFO: Got endpoints: latency-svc-tfvd5 [753.158383ms]
    Jan 12 01:27:11.546: INFO: Created: latency-svc-bschq
    Jan 12 01:27:11.575: INFO: Got endpoints: latency-svc-6jph2 [748.731059ms]
    Jan 12 01:27:11.628: INFO: Got endpoints: latency-svc-kshld [755.600591ms]
    Jan 12 01:27:11.672: INFO: Got endpoints: latency-svc-tqlnh [734.164665ms]
    Jan 12 01:27:11.734: INFO: Got endpoints: latency-svc-l6m9h [762.508378ms]
    Jan 12 01:27:11.768: INFO: Got endpoints: latency-svc-gm756 [736.214945ms]
    Jan 12 01:27:11.828: INFO: Got endpoints: latency-svc-tk8sn [750.201728ms]
    Jan 12 01:27:11.877: INFO: Got endpoints: latency-svc-624n7 [749.961549ms]
    Jan 12 01:27:11.925: INFO: Got endpoints: latency-svc-b8bpc [753.664727ms]
    Jan 12 01:27:11.969: INFO: Got endpoints: latency-svc-bxcv6 [746.855891ms]
    Jan 12 01:27:12.019: INFO: Got endpoints: latency-svc-sw7pz [728.080501ms]
    Jan 12 01:27:12.078: INFO: Got endpoints: latency-svc-dh6vr [759.596906ms]
    Jan 12 01:27:12.128: INFO: Got endpoints: latency-svc-xnq8c [756.291066ms]
    Jan 12 01:27:12.185: INFO: Got endpoints: latency-svc-2tk6b [763.189917ms]
    Jan 12 01:27:12.218: INFO: Got endpoints: latency-svc-r9g5j [743.373486ms]
    Jan 12 01:27:12.272: INFO: Got endpoints: latency-svc-bschq [743.353906ms]
    Jan 12 01:27:12.272: INFO: Latencies: [25.612862ms 58.424121ms 76.888657ms 95.816719ms 124.375366ms 153.970898ms 179.090924ms 220.827967ms 238.857772ms 266.292974ms 292.780355ms 314.941479ms 340.998526ms 358.407926ms 361.184838ms 363.956032ms 365.505069ms 365.560542ms 366.307294ms 366.317595ms 367.682185ms 368.418537ms 368.852341ms 368.858846ms 368.891652ms 369.889292ms 370.304706ms 370.814036ms 371.360805ms 371.553062ms 372.279494ms 372.510941ms 372.662818ms 372.680139ms 372.794735ms 372.814233ms 373.447228ms 375.010815ms 375.606521ms 375.909917ms 376.468811ms 377.762427ms 377.770769ms 377.839416ms 381.273371ms 381.66856ms 382.664384ms 383.515894ms 384.402489ms 385.544122ms 386.675073ms 386.811767ms 386.890968ms 387.829436ms 392.776959ms 392.845671ms 393.698128ms 394.532379ms 397.15747ms 398.570806ms 445.610966ms 451.057978ms 486.493964ms 498.123125ms 528.304089ms 565.61792ms 585.282365ms 622.580441ms 643.503402ms 650.314152ms 681.822308ms 697.57027ms 714.217411ms 728.080501ms 730.394034ms 734.051745ms 734.164665ms 734.892277ms 734.997258ms 735.773374ms 736.093107ms 736.098252ms 736.12824ms 736.214945ms 737.067516ms 737.801335ms 739.024284ms 739.072756ms 739.419959ms 739.96298ms 740.40261ms 741.332654ms 741.746344ms 742.035954ms 742.312934ms 742.37997ms 742.533389ms 742.865491ms 742.996883ms 743.353906ms 743.373486ms 743.408168ms 743.501395ms 743.784435ms 743.911758ms 743.939287ms 744.277454ms 744.360477ms 744.39434ms 745.159633ms 745.380205ms 745.47885ms 745.55541ms 745.711117ms 746.191005ms 746.26184ms 746.626852ms 746.693941ms 746.730576ms 746.855891ms 746.959976ms 747.082892ms 747.299369ms 747.82738ms 748.140648ms 748.24993ms 748.653061ms 748.731059ms 748.834729ms 748.927725ms 748.964951ms 749.427777ms 749.460245ms 749.517724ms 749.544364ms 749.65437ms 749.701016ms 749.70556ms 749.838186ms 749.961549ms 750.193516ms 750.201728ms 750.285419ms 750.477084ms 750.591533ms 750.912846ms 751.03513ms 751.061633ms 751.140707ms 751.141804ms 751.472926ms 751.542125ms 751.676829ms 752.009606ms 752.109291ms 752.309693ms 752.647094ms 752.697501ms 752.946017ms 753.158383ms 753.392923ms 753.418735ms 753.664727ms 753.735462ms 753.843403ms 754.112106ms 754.255716ms 754.631955ms 754.908968ms 755.034861ms 755.047706ms 755.600591ms 755.932318ms 756.132309ms 756.291066ms 756.439763ms 756.696526ms 757.31765ms 758.069815ms 758.098624ms 758.523313ms 758.746142ms 758.816479ms 758.854884ms 758.873174ms 759.031768ms 759.204291ms 759.355703ms 759.596906ms 759.621598ms 759.688789ms 759.858963ms 762.178626ms 762.508378ms 763.141821ms 763.189917ms 764.115864ms 768.48204ms 770.85492ms 772.027523ms]
    Jan 12 01:27:12.273: INFO: 50 %ile: 743.373486ms
    Jan 12 01:27:12.273: INFO: 90 %ile: 758.523313ms
    Jan 12 01:27:12.273: INFO: 99 %ile: 770.85492ms
    Jan 12 01:27:12.273: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:27:12.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-5890" for this suite. 01/12/23 01:27:12.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:27:12.305
Jan 12 01:27:12.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 01:27:12.305
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:27:12.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:27:12.333
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 01:27:12.354
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:27:12.857
STEP: Deploying the webhook pod 01/12/23 01:27:12.872
STEP: Wait for the deployment to be ready 01/12/23 01:27:12.917
Jan 12 01:27:12.921: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 12 01:27:14.930: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 27, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 27, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 27, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 27, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 01:27:16.933
STEP: Verifying the service has paired with the endpoint 01/12/23 01:27:16.949
Jan 12 01:27:17.949: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jan 12 01:27:17.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3867-crds.webhook.example.com via the AdmissionRegistration API 01/12/23 01:27:18.49
STEP: Creating a custom resource that should be mutated by the webhook 01/12/23 01:27:18.544
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:27:21.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8592" for this suite. 01/12/23 01:27:21.889
STEP: Destroying namespace "webhook-8592-markers" for this suite. 01/12/23 01:27:21.924
------------------------------
• [SLOW TEST] [9.671 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:27:12.305
    Jan 12 01:27:12.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 01:27:12.305
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:27:12.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:27:12.333
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 01:27:12.354
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:27:12.857
    STEP: Deploying the webhook pod 01/12/23 01:27:12.872
    STEP: Wait for the deployment to be ready 01/12/23 01:27:12.917
    Jan 12 01:27:12.921: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 12 01:27:14.930: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 27, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 27, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 27, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 27, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 01:27:16.933
    STEP: Verifying the service has paired with the endpoint 01/12/23 01:27:16.949
    Jan 12 01:27:17.949: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jan 12 01:27:17.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3867-crds.webhook.example.com via the AdmissionRegistration API 01/12/23 01:27:18.49
    STEP: Creating a custom resource that should be mutated by the webhook 01/12/23 01:27:18.544
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:27:21.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8592" for this suite. 01/12/23 01:27:21.889
    STEP: Destroying namespace "webhook-8592-markers" for this suite. 01/12/23 01:27:21.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:27:21.978
Jan 12 01:27:21.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename ephemeral-containers-test 01/12/23 01:27:21.979
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:27:22.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:27:22.017
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/12/23 01:27:22.019
Jan 12 01:27:22.049: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-88" to be "running and ready"
Jan 12 01:27:22.051: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.26475ms
Jan 12 01:27:22.051: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:27:24.055: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00612963s
Jan 12 01:27:24.055: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:27:26.056: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006952241s
Jan 12 01:27:26.056: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 12 01:27:26.056: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/12/23 01:27:26.058
Jan 12 01:27:26.082: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-88" to be "container debugger running"
Jan 12 01:27:26.087: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.065176ms
Jan 12 01:27:28.091: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008307034s
Jan 12 01:27:28.091: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/12/23 01:27:28.091
Jan 12 01:27:28.091: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-88 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:27:28.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:27:28.091: INFO: ExecWithOptions: Clientset creation
Jan 12 01:27:28.092: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/ephemeral-containers-test-88/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 12 01:27:28.232: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:27:28.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-88" for this suite. 01/12/23 01:27:28.251
------------------------------
• [SLOW TEST] [6.296 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:27:21.978
    Jan 12 01:27:21.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/12/23 01:27:21.979
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:27:22.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:27:22.017
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/12/23 01:27:22.019
    Jan 12 01:27:22.049: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-88" to be "running and ready"
    Jan 12 01:27:22.051: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.26475ms
    Jan 12 01:27:22.051: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:27:24.055: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00612963s
    Jan 12 01:27:24.055: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:27:26.056: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006952241s
    Jan 12 01:27:26.056: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 12 01:27:26.056: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/12/23 01:27:26.058
    Jan 12 01:27:26.082: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-88" to be "container debugger running"
    Jan 12 01:27:26.087: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.065176ms
    Jan 12 01:27:28.091: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008307034s
    Jan 12 01:27:28.091: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/12/23 01:27:28.091
    Jan 12 01:27:28.091: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-88 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:27:28.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:27:28.091: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:27:28.092: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/ephemeral-containers-test-88/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 12 01:27:28.232: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:27:28.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-88" for this suite. 01/12/23 01:27:28.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:27:28.276
Jan 12 01:27:28.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 01:27:28.276
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:27:28.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:27:28.296
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/12/23 01:27:28.302
Jan 12 01:27:28.334: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4420" to be "running and ready"
Jan 12 01:27:28.338: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.324888ms
Jan 12 01:27:28.338: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:27:30.341: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006717922s
Jan 12 01:27:30.341: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:27:32.341: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.007154339s
Jan 12 01:27:32.341: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 12 01:27:32.341: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 01/12/23 01:27:32.344
Jan 12 01:27:32.460: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4420" to be "running and ready"
Jan 12 01:27:32.462: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.468115ms
Jan 12 01:27:32.462: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:27:34.466: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006003231s
Jan 12 01:27:34.466: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:27:36.466: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.006567176s
Jan 12 01:27:36.467: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 12 01:27:36.467: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/12/23 01:27:36.469
Jan 12 01:27:36.487: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 12 01:27:36.490: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 12 01:27:38.491: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 12 01:27:38.495: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/12/23 01:27:38.495
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 12 01:27:38.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4420" for this suite. 01/12/23 01:27:38.538
------------------------------
• [SLOW TEST] [10.289 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:27:28.276
    Jan 12 01:27:28.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 01:27:28.276
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:27:28.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:27:28.296
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/12/23 01:27:28.302
    Jan 12 01:27:28.334: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4420" to be "running and ready"
    Jan 12 01:27:28.338: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.324888ms
    Jan 12 01:27:28.338: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:27:30.341: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006717922s
    Jan 12 01:27:30.341: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:27:32.341: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.007154339s
    Jan 12 01:27:32.341: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 12 01:27:32.341: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 01/12/23 01:27:32.344
    Jan 12 01:27:32.460: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4420" to be "running and ready"
    Jan 12 01:27:32.462: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.468115ms
    Jan 12 01:27:32.462: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:27:34.466: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006003231s
    Jan 12 01:27:34.466: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:27:36.466: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.006567176s
    Jan 12 01:27:36.467: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 12 01:27:36.467: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/12/23 01:27:36.469
    Jan 12 01:27:36.487: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 12 01:27:36.490: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 12 01:27:38.491: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 12 01:27:38.495: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/12/23 01:27:38.495
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:27:38.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4420" for this suite. 01/12/23 01:27:38.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:27:38.572
Jan 12 01:27:38.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename gc 01/12/23 01:27:38.573
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:27:38.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:27:38.593
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/12/23 01:27:38.599
STEP: delete the rc 01/12/23 01:27:43.619
STEP: wait for the rc to be deleted 01/12/23 01:27:43.642
Jan 12 01:27:44.666: INFO: 80 pods remaining
Jan 12 01:27:44.666: INFO: 80 pods has nil DeletionTimestamp
Jan 12 01:27:44.666: INFO: 
Jan 12 01:27:45.777: INFO: 68 pods remaining
Jan 12 01:27:45.777: INFO: 68 pods has nil DeletionTimestamp
Jan 12 01:27:45.777: INFO: 
Jan 12 01:27:46.703: INFO: 60 pods remaining
Jan 12 01:27:46.703: INFO: 60 pods has nil DeletionTimestamp
Jan 12 01:27:46.703: INFO: 
Jan 12 01:27:47.702: INFO: 40 pods remaining
Jan 12 01:27:47.703: INFO: 40 pods has nil DeletionTimestamp
Jan 12 01:27:47.703: INFO: 
Jan 12 01:27:48.704: INFO: 29 pods remaining
Jan 12 01:27:48.704: INFO: 29 pods has nil DeletionTimestamp
Jan 12 01:27:48.704: INFO: 
Jan 12 01:27:49.661: INFO: 20 pods remaining
Jan 12 01:27:49.661: INFO: 20 pods has nil DeletionTimestamp
Jan 12 01:27:49.661: INFO: 
STEP: Gathering metrics 01/12/23 01:27:50.703
Jan 12 01:27:50.773: INFO: Waiting up to 5m0s for pod "kube-controller-manager-eqx04-flash04" in namespace "kube-system" to be "running and ready"
Jan 12 01:27:50.784: INFO: Pod "kube-controller-manager-eqx04-flash04": Phase="Running", Reason="", readiness=true. Elapsed: 10.263791ms
Jan 12 01:27:50.784: INFO: The phase of Pod kube-controller-manager-eqx04-flash04 is Running (Ready = true)
Jan 12 01:27:50.784: INFO: Pod "kube-controller-manager-eqx04-flash04" satisfied condition "running and ready"
Jan 12 01:27:50.903: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 12 01:27:50.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2708" for this suite. 01/12/23 01:27:50.911
------------------------------
• [SLOW TEST] [12.445 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:27:38.572
    Jan 12 01:27:38.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename gc 01/12/23 01:27:38.573
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:27:38.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:27:38.593
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/12/23 01:27:38.599
    STEP: delete the rc 01/12/23 01:27:43.619
    STEP: wait for the rc to be deleted 01/12/23 01:27:43.642
    Jan 12 01:27:44.666: INFO: 80 pods remaining
    Jan 12 01:27:44.666: INFO: 80 pods has nil DeletionTimestamp
    Jan 12 01:27:44.666: INFO: 
    Jan 12 01:27:45.777: INFO: 68 pods remaining
    Jan 12 01:27:45.777: INFO: 68 pods has nil DeletionTimestamp
    Jan 12 01:27:45.777: INFO: 
    Jan 12 01:27:46.703: INFO: 60 pods remaining
    Jan 12 01:27:46.703: INFO: 60 pods has nil DeletionTimestamp
    Jan 12 01:27:46.703: INFO: 
    Jan 12 01:27:47.702: INFO: 40 pods remaining
    Jan 12 01:27:47.703: INFO: 40 pods has nil DeletionTimestamp
    Jan 12 01:27:47.703: INFO: 
    Jan 12 01:27:48.704: INFO: 29 pods remaining
    Jan 12 01:27:48.704: INFO: 29 pods has nil DeletionTimestamp
    Jan 12 01:27:48.704: INFO: 
    Jan 12 01:27:49.661: INFO: 20 pods remaining
    Jan 12 01:27:49.661: INFO: 20 pods has nil DeletionTimestamp
    Jan 12 01:27:49.661: INFO: 
    STEP: Gathering metrics 01/12/23 01:27:50.703
    Jan 12 01:27:50.773: INFO: Waiting up to 5m0s for pod "kube-controller-manager-eqx04-flash04" in namespace "kube-system" to be "running and ready"
    Jan 12 01:27:50.784: INFO: Pod "kube-controller-manager-eqx04-flash04": Phase="Running", Reason="", readiness=true. Elapsed: 10.263791ms
    Jan 12 01:27:50.784: INFO: The phase of Pod kube-controller-manager-eqx04-flash04 is Running (Ready = true)
    Jan 12 01:27:50.784: INFO: Pod "kube-controller-manager-eqx04-flash04" satisfied condition "running and ready"
    Jan 12 01:27:50.903: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:27:50.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2708" for this suite. 01/12/23 01:27:50.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:27:51.019
Jan 12 01:27:51.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 01:27:51.02
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:27:51.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:27:51.042
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-1025 01/12/23 01:27:51.05
STEP: creating service affinity-nodeport-transition in namespace services-1025 01/12/23 01:27:51.05
STEP: creating replication controller affinity-nodeport-transition in namespace services-1025 01/12/23 01:27:51.076
I0112 01:27:51.096600      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1025, replica count: 3
I0112 01:27:54.149132      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0112 01:27:57.150079      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0112 01:28:00.150675      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0112 01:28:03.151735      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0112 01:28:06.152398      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 01:28:06.161: INFO: Creating new exec pod
Jan 12 01:28:06.267: INFO: Waiting up to 5m0s for pod "execpod-affinityxkd68" in namespace "services-1025" to be "running"
Jan 12 01:28:06.270: INFO: Pod "execpod-affinityxkd68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.22002ms
Jan 12 01:28:08.273: INFO: Pod "execpod-affinityxkd68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005468393s
Jan 12 01:28:10.273: INFO: Pod "execpod-affinityxkd68": Phase="Running", Reason="", readiness=true. Elapsed: 4.00589839s
Jan 12 01:28:10.273: INFO: Pod "execpod-affinityxkd68" satisfied condition "running"
Jan 12 01:28:11.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-1025 exec execpod-affinityxkd68 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan 12 01:28:11.505: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 12 01:28:11.505: INFO: stdout: ""
Jan 12 01:28:11.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-1025 exec execpod-affinityxkd68 -- /bin/sh -x -c nc -v -z -w 2 172.19.149.204 80'
Jan 12 01:28:11.705: INFO: stderr: "+ nc -v -z -w 2 172.19.149.204 80\nConnection to 172.19.149.204 80 port [tcp/http] succeeded!\n"
Jan 12 01:28:11.705: INFO: stdout: ""
Jan 12 01:28:11.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-1025 exec execpod-affinityxkd68 -- /bin/sh -x -c nc -v -z -w 2 10.9.140.106 32243'
Jan 12 01:28:11.904: INFO: stderr: "+ nc -v -z -w 2 10.9.140.106 32243\nConnection to 10.9.140.106 32243 port [tcp/*] succeeded!\n"
Jan 12 01:28:11.904: INFO: stdout: ""
Jan 12 01:28:11.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-1025 exec execpod-affinityxkd68 -- /bin/sh -x -c nc -v -z -w 2 10.9.40.106 32243'
Jan 12 01:28:12.099: INFO: stderr: "+ nc -v -z -w 2 10.9.40.106 32243\nConnection to 10.9.40.106 32243 port [tcp/*] succeeded!\n"
Jan 12 01:28:12.099: INFO: stdout: ""
Jan 12 01:28:12.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-1025 exec execpod-affinityxkd68 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.140.106:32243/ ; done'
Jan 12 01:28:12.387: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n"
Jan 12 01:28:12.387: INFO: stdout: "\naffinity-nodeport-transition-msv4s\naffinity-nodeport-transition-zl4pw\naffinity-nodeport-transition-zl4pw\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-msv4s\naffinity-nodeport-transition-msv4s\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-zl4pw\naffinity-nodeport-transition-msv4s\naffinity-nodeport-transition-zl4pw\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-zl4pw"
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-msv4s
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-zl4pw
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-zl4pw
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-msv4s
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-msv4s
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-zl4pw
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-msv4s
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-zl4pw
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-zl4pw
Jan 12 01:28:12.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-1025 exec execpod-affinityxkd68 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.140.106:32243/ ; done'
Jan 12 01:28:12.674: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n"
Jan 12 01:28:12.674: INFO: stdout: "\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl"
Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.675: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.675: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.675: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.675: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.675: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.675: INFO: Received response from host: affinity-nodeport-transition-mqmbl
Jan 12 01:28:12.675: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1025, will wait for the garbage collector to delete the pods 01/12/23 01:28:12.694
Jan 12 01:28:12.769: INFO: Deleting ReplicationController affinity-nodeport-transition took: 22.061226ms
Jan 12 01:28:12.869: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.497991ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 01:28:15.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1025" for this suite. 01/12/23 01:28:15.913
------------------------------
• [SLOW TEST] [24.910 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:27:51.019
    Jan 12 01:27:51.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 01:27:51.02
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:27:51.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:27:51.042
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-1025 01/12/23 01:27:51.05
    STEP: creating service affinity-nodeport-transition in namespace services-1025 01/12/23 01:27:51.05
    STEP: creating replication controller affinity-nodeport-transition in namespace services-1025 01/12/23 01:27:51.076
    I0112 01:27:51.096600      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1025, replica count: 3
    I0112 01:27:54.149132      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0112 01:27:57.150079      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0112 01:28:00.150675      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0112 01:28:03.151735      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0112 01:28:06.152398      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 01:28:06.161: INFO: Creating new exec pod
    Jan 12 01:28:06.267: INFO: Waiting up to 5m0s for pod "execpod-affinityxkd68" in namespace "services-1025" to be "running"
    Jan 12 01:28:06.270: INFO: Pod "execpod-affinityxkd68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.22002ms
    Jan 12 01:28:08.273: INFO: Pod "execpod-affinityxkd68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005468393s
    Jan 12 01:28:10.273: INFO: Pod "execpod-affinityxkd68": Phase="Running", Reason="", readiness=true. Elapsed: 4.00589839s
    Jan 12 01:28:10.273: INFO: Pod "execpod-affinityxkd68" satisfied condition "running"
    Jan 12 01:28:11.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-1025 exec execpod-affinityxkd68 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan 12 01:28:11.505: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 12 01:28:11.505: INFO: stdout: ""
    Jan 12 01:28:11.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-1025 exec execpod-affinityxkd68 -- /bin/sh -x -c nc -v -z -w 2 172.19.149.204 80'
    Jan 12 01:28:11.705: INFO: stderr: "+ nc -v -z -w 2 172.19.149.204 80\nConnection to 172.19.149.204 80 port [tcp/http] succeeded!\n"
    Jan 12 01:28:11.705: INFO: stdout: ""
    Jan 12 01:28:11.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-1025 exec execpod-affinityxkd68 -- /bin/sh -x -c nc -v -z -w 2 10.9.140.106 32243'
    Jan 12 01:28:11.904: INFO: stderr: "+ nc -v -z -w 2 10.9.140.106 32243\nConnection to 10.9.140.106 32243 port [tcp/*] succeeded!\n"
    Jan 12 01:28:11.904: INFO: stdout: ""
    Jan 12 01:28:11.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-1025 exec execpod-affinityxkd68 -- /bin/sh -x -c nc -v -z -w 2 10.9.40.106 32243'
    Jan 12 01:28:12.099: INFO: stderr: "+ nc -v -z -w 2 10.9.40.106 32243\nConnection to 10.9.40.106 32243 port [tcp/*] succeeded!\n"
    Jan 12 01:28:12.099: INFO: stdout: ""
    Jan 12 01:28:12.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-1025 exec execpod-affinityxkd68 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.140.106:32243/ ; done'
    Jan 12 01:28:12.387: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n"
    Jan 12 01:28:12.387: INFO: stdout: "\naffinity-nodeport-transition-msv4s\naffinity-nodeport-transition-zl4pw\naffinity-nodeport-transition-zl4pw\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-msv4s\naffinity-nodeport-transition-msv4s\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-zl4pw\naffinity-nodeport-transition-msv4s\naffinity-nodeport-transition-zl4pw\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-zl4pw"
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-msv4s
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-zl4pw
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-zl4pw
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-msv4s
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-msv4s
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-zl4pw
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-msv4s
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-zl4pw
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.387: INFO: Received response from host: affinity-nodeport-transition-zl4pw
    Jan 12 01:28:12.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-1025 exec execpod-affinityxkd68 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.140.106:32243/ ; done'
    Jan 12 01:28:12.674: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:32243/\n"
    Jan 12 01:28:12.674: INFO: stdout: "\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl\naffinity-nodeport-transition-mqmbl"
    Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.674: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.675: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.675: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.675: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.675: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.675: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.675: INFO: Received response from host: affinity-nodeport-transition-mqmbl
    Jan 12 01:28:12.675: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1025, will wait for the garbage collector to delete the pods 01/12/23 01:28:12.694
    Jan 12 01:28:12.769: INFO: Deleting ReplicationController affinity-nodeport-transition took: 22.061226ms
    Jan 12 01:28:12.869: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.497991ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:28:15.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1025" for this suite. 01/12/23 01:28:15.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:28:15.931
Jan 12 01:28:15.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename replication-controller 01/12/23 01:28:15.932
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:28:15.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:28:15.97
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 01/12/23 01:28:15.972
Jan 12 01:28:16.022: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8286" to be "running and ready"
Jan 12 01:28:16.024: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.339365ms
Jan 12 01:28:16.024: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:28:18.029: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.007111497s
Jan 12 01:28:18.029: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 12 01:28:18.029: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/12/23 01:28:18.031
STEP: Then the orphan pod is adopted 01/12/23 01:28:18.038
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 12 01:28:19.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8286" for this suite. 01/12/23 01:28:19.056
------------------------------
• [3.143 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:28:15.931
    Jan 12 01:28:15.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename replication-controller 01/12/23 01:28:15.932
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:28:15.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:28:15.97
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/12/23 01:28:15.972
    Jan 12 01:28:16.022: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8286" to be "running and ready"
    Jan 12 01:28:16.024: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.339365ms
    Jan 12 01:28:16.024: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:28:18.029: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.007111497s
    Jan 12 01:28:18.029: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 12 01:28:18.029: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/12/23 01:28:18.031
    STEP: Then the orphan pod is adopted 01/12/23 01:28:18.038
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:28:19.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8286" for this suite. 01/12/23 01:28:19.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:28:19.075
Jan 12 01:28:19.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename containers 01/12/23 01:28:19.076
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:28:19.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:28:19.1
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 01/12/23 01:28:19.102
Jan 12 01:28:19.133: INFO: Waiting up to 5m0s for pod "client-containers-da5351a9-5f9d-4c69-8771-01278622f67e" in namespace "containers-4676" to be "Succeeded or Failed"
Jan 12 01:28:19.136: INFO: Pod "client-containers-da5351a9-5f9d-4c69-8771-01278622f67e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.361852ms
Jan 12 01:28:21.148: INFO: Pod "client-containers-da5351a9-5f9d-4c69-8771-01278622f67e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01479048s
Jan 12 01:28:23.139: INFO: Pod "client-containers-da5351a9-5f9d-4c69-8771-01278622f67e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006081237s
Jan 12 01:28:25.149: INFO: Pod "client-containers-da5351a9-5f9d-4c69-8771-01278622f67e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015301679s
STEP: Saw pod success 01/12/23 01:28:25.149
Jan 12 01:28:25.149: INFO: Pod "client-containers-da5351a9-5f9d-4c69-8771-01278622f67e" satisfied condition "Succeeded or Failed"
Jan 12 01:28:25.151: INFO: Trying to get logs from node eqx04-flash06 pod client-containers-da5351a9-5f9d-4c69-8771-01278622f67e container agnhost-container: <nil>
STEP: delete the pod 01/12/23 01:28:25.16
Jan 12 01:28:25.180: INFO: Waiting for pod client-containers-da5351a9-5f9d-4c69-8771-01278622f67e to disappear
Jan 12 01:28:25.182: INFO: Pod client-containers-da5351a9-5f9d-4c69-8771-01278622f67e no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 12 01:28:25.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4676" for this suite. 01/12/23 01:28:25.186
------------------------------
• [SLOW TEST] [6.128 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:28:19.075
    Jan 12 01:28:19.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename containers 01/12/23 01:28:19.076
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:28:19.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:28:19.1
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 01/12/23 01:28:19.102
    Jan 12 01:28:19.133: INFO: Waiting up to 5m0s for pod "client-containers-da5351a9-5f9d-4c69-8771-01278622f67e" in namespace "containers-4676" to be "Succeeded or Failed"
    Jan 12 01:28:19.136: INFO: Pod "client-containers-da5351a9-5f9d-4c69-8771-01278622f67e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.361852ms
    Jan 12 01:28:21.148: INFO: Pod "client-containers-da5351a9-5f9d-4c69-8771-01278622f67e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01479048s
    Jan 12 01:28:23.139: INFO: Pod "client-containers-da5351a9-5f9d-4c69-8771-01278622f67e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006081237s
    Jan 12 01:28:25.149: INFO: Pod "client-containers-da5351a9-5f9d-4c69-8771-01278622f67e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015301679s
    STEP: Saw pod success 01/12/23 01:28:25.149
    Jan 12 01:28:25.149: INFO: Pod "client-containers-da5351a9-5f9d-4c69-8771-01278622f67e" satisfied condition "Succeeded or Failed"
    Jan 12 01:28:25.151: INFO: Trying to get logs from node eqx04-flash06 pod client-containers-da5351a9-5f9d-4c69-8771-01278622f67e container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 01:28:25.16
    Jan 12 01:28:25.180: INFO: Waiting for pod client-containers-da5351a9-5f9d-4c69-8771-01278622f67e to disappear
    Jan 12 01:28:25.182: INFO: Pod client-containers-da5351a9-5f9d-4c69-8771-01278622f67e no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:28:25.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4676" for this suite. 01/12/23 01:28:25.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:28:25.204
Jan 12 01:28:25.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pods 01/12/23 01:28:25.205
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:28:25.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:28:25.23
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 01/12/23 01:28:25.232
Jan 12 01:28:25.272: INFO: created test-pod-1
Jan 12 01:28:25.331: INFO: created test-pod-2
Jan 12 01:28:25.385: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/12/23 01:28:25.385
Jan 12 01:28:25.385: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1026' to be running and ready
Jan 12 01:28:25.392: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 01:28:25.392: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 01:28:25.392: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 01:28:25.392: INFO: 0 / 3 pods in namespace 'pods-1026' are running and ready (0 seconds elapsed)
Jan 12 01:28:25.392: INFO: expected 0 pod replicas in namespace 'pods-1026', 0 are Running and Ready.
Jan 12 01:28:25.392: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Jan 12 01:28:25.392: INFO: test-pod-1  eqx04-flash06  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  }]
Jan 12 01:28:25.392: INFO: test-pod-2                 Pending         []
Jan 12 01:28:25.392: INFO: test-pod-3                 Pending         []
Jan 12 01:28:25.392: INFO: 
Jan 12 01:28:27.416: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 01:28:27.416: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 01:28:27.416: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 01:28:27.416: INFO: 0 / 3 pods in namespace 'pods-1026' are running and ready (2 seconds elapsed)
Jan 12 01:28:27.416: INFO: expected 0 pod replicas in namespace 'pods-1026', 0 are Running and Ready.
Jan 12 01:28:27.416: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Jan 12 01:28:27.416: INFO: test-pod-1  eqx04-flash06  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  }]
Jan 12 01:28:27.416: INFO: test-pod-2  eqx04-flash06  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  }]
Jan 12 01:28:27.416: INFO: test-pod-3  eqx04-flash06  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  }]
Jan 12 01:28:27.416: INFO: 
Jan 12 01:28:29.400: INFO: 3 / 3 pods in namespace 'pods-1026' are running and ready (4 seconds elapsed)
Jan 12 01:28:29.400: INFO: expected 0 pod replicas in namespace 'pods-1026', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/12/23 01:28:29.469
Jan 12 01:28:29.472: INFO: Pod quantity 3 is different from expected quantity 0
Jan 12 01:28:30.476: INFO: Pod quantity 3 is different from expected quantity 0
Jan 12 01:28:31.476: INFO: Pod quantity 3 is different from expected quantity 0
Jan 12 01:28:32.483: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 01:28:33.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1026" for this suite. 01/12/23 01:28:33.479
------------------------------
• [SLOW TEST] [8.292 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:28:25.204
    Jan 12 01:28:25.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pods 01/12/23 01:28:25.205
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:28:25.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:28:25.23
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 01/12/23 01:28:25.232
    Jan 12 01:28:25.272: INFO: created test-pod-1
    Jan 12 01:28:25.331: INFO: created test-pod-2
    Jan 12 01:28:25.385: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/12/23 01:28:25.385
    Jan 12 01:28:25.385: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1026' to be running and ready
    Jan 12 01:28:25.392: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 01:28:25.392: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 01:28:25.392: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 01:28:25.392: INFO: 0 / 3 pods in namespace 'pods-1026' are running and ready (0 seconds elapsed)
    Jan 12 01:28:25.392: INFO: expected 0 pod replicas in namespace 'pods-1026', 0 are Running and Ready.
    Jan 12 01:28:25.392: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
    Jan 12 01:28:25.392: INFO: test-pod-1  eqx04-flash06  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  }]
    Jan 12 01:28:25.392: INFO: test-pod-2                 Pending         []
    Jan 12 01:28:25.392: INFO: test-pod-3                 Pending         []
    Jan 12 01:28:25.392: INFO: 
    Jan 12 01:28:27.416: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 01:28:27.416: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 01:28:27.416: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 01:28:27.416: INFO: 0 / 3 pods in namespace 'pods-1026' are running and ready (2 seconds elapsed)
    Jan 12 01:28:27.416: INFO: expected 0 pod replicas in namespace 'pods-1026', 0 are Running and Ready.
    Jan 12 01:28:27.416: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
    Jan 12 01:28:27.416: INFO: test-pod-1  eqx04-flash06  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  }]
    Jan 12 01:28:27.416: INFO: test-pod-2  eqx04-flash06  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  }]
    Jan 12 01:28:27.416: INFO: test-pod-3  eqx04-flash06  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 01:28:25 +0000 UTC  }]
    Jan 12 01:28:27.416: INFO: 
    Jan 12 01:28:29.400: INFO: 3 / 3 pods in namespace 'pods-1026' are running and ready (4 seconds elapsed)
    Jan 12 01:28:29.400: INFO: expected 0 pod replicas in namespace 'pods-1026', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/12/23 01:28:29.469
    Jan 12 01:28:29.472: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 12 01:28:30.476: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 12 01:28:31.476: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 12 01:28:32.483: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:28:33.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1026" for this suite. 01/12/23 01:28:33.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:28:33.499
Jan 12 01:28:33.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 01:28:33.499
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:28:33.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:28:33.531
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 01:28:33.551
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:28:33.758
STEP: Deploying the webhook pod 01/12/23 01:28:33.768
STEP: Wait for the deployment to be ready 01/12/23 01:28:33.819
Jan 12 01:28:33.862: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:28:35.869: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 01:28:37.866
STEP: Verifying the service has paired with the endpoint 01/12/23 01:28:37.89
Jan 12 01:28:38.891: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 01/12/23 01:28:38.895
STEP: create a pod that should be denied by the webhook 01/12/23 01:28:38.915
STEP: create a pod that causes the webhook to hang 01/12/23 01:28:38.947
STEP: create a configmap that should be denied by the webhook 01/12/23 01:28:48.953
STEP: create a configmap that should be admitted by the webhook 01/12/23 01:28:48.962
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/12/23 01:28:48.975
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/12/23 01:28:48.981
STEP: create a namespace that bypass the webhook 01/12/23 01:28:48.985
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/12/23 01:28:48.996
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:28:49.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5710" for this suite. 01/12/23 01:28:49.15
STEP: Destroying namespace "webhook-5710-markers" for this suite. 01/12/23 01:28:49.194
------------------------------
• [SLOW TEST] [15.740 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:28:33.499
    Jan 12 01:28:33.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 01:28:33.499
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:28:33.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:28:33.531
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 01:28:33.551
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:28:33.758
    STEP: Deploying the webhook pod 01/12/23 01:28:33.768
    STEP: Wait for the deployment to be ready 01/12/23 01:28:33.819
    Jan 12 01:28:33.862: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-865554f4d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:28:35.869: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 28, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 01:28:37.866
    STEP: Verifying the service has paired with the endpoint 01/12/23 01:28:37.89
    Jan 12 01:28:38.891: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 01/12/23 01:28:38.895
    STEP: create a pod that should be denied by the webhook 01/12/23 01:28:38.915
    STEP: create a pod that causes the webhook to hang 01/12/23 01:28:38.947
    STEP: create a configmap that should be denied by the webhook 01/12/23 01:28:48.953
    STEP: create a configmap that should be admitted by the webhook 01/12/23 01:28:48.962
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/12/23 01:28:48.975
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/12/23 01:28:48.981
    STEP: create a namespace that bypass the webhook 01/12/23 01:28:48.985
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/12/23 01:28:48.996
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:28:49.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5710" for this suite. 01/12/23 01:28:49.15
    STEP: Destroying namespace "webhook-5710-markers" for this suite. 01/12/23 01:28:49.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:28:49.26
Jan 12 01:28:49.260: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename podtemplate 01/12/23 01:28:49.261
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:28:49.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:28:49.288
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/12/23 01:28:49.29
Jan 12 01:28:49.304: INFO: created test-podtemplate-1
Jan 12 01:28:49.311: INFO: created test-podtemplate-2
Jan 12 01:28:49.319: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/12/23 01:28:49.319
STEP: delete collection of pod templates 01/12/23 01:28:49.322
Jan 12 01:28:49.322: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/12/23 01:28:49.342
Jan 12 01:28:49.342: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 12 01:28:49.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-451" for this suite. 01/12/23 01:28:49.348
------------------------------
• [0.109 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:28:49.26
    Jan 12 01:28:49.260: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename podtemplate 01/12/23 01:28:49.261
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:28:49.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:28:49.288
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/12/23 01:28:49.29
    Jan 12 01:28:49.304: INFO: created test-podtemplate-1
    Jan 12 01:28:49.311: INFO: created test-podtemplate-2
    Jan 12 01:28:49.319: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/12/23 01:28:49.319
    STEP: delete collection of pod templates 01/12/23 01:28:49.322
    Jan 12 01:28:49.322: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/12/23 01:28:49.342
    Jan 12 01:28:49.342: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:28:49.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-451" for this suite. 01/12/23 01:28:49.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:28:49.371
Jan 12 01:28:49.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename dns 01/12/23 01:28:49.372
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:28:49.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:28:49.398
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/12/23 01:28:49.4
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4982.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4982.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 55.226.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.226.55_udp@PTR;check="$$(dig +tcp +noall +answer +search 55.226.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.226.55_tcp@PTR;sleep 1; done
 01/12/23 01:28:49.441
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4982.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4982.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 55.226.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.226.55_udp@PTR;check="$$(dig +tcp +noall +answer +search 55.226.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.226.55_tcp@PTR;sleep 1; done
 01/12/23 01:28:49.442
STEP: creating a pod to probe DNS 01/12/23 01:28:49.442
STEP: submitting the pod to kubernetes 01/12/23 01:28:49.442
Jan 12 01:28:49.486: INFO: Waiting up to 15m0s for pod "dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de" in namespace "dns-4982" to be "running"
Jan 12 01:28:49.488: INFO: Pod "dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.508848ms
Jan 12 01:28:51.500: INFO: Pod "dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014141591s
Jan 12 01:28:53.493: INFO: Pod "dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de": Phase="Running", Reason="", readiness=true. Elapsed: 4.006642864s
Jan 12 01:28:53.493: INFO: Pod "dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de" satisfied condition "running"
STEP: retrieving the pod 01/12/23 01:28:53.493
STEP: looking for the results for each expected name from probers 01/12/23 01:28:53.495
Jan 12 01:28:53.498: INFO: Unable to read wheezy_udp@dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
Jan 12 01:28:53.501: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
Jan 12 01:28:53.503: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
Jan 12 01:28:53.506: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
Jan 12 01:28:53.519: INFO: Unable to read jessie_udp@dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
Jan 12 01:28:53.521: INFO: Unable to read jessie_tcp@dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
Jan 12 01:28:53.524: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
Jan 12 01:28:53.526: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
Jan 12 01:28:53.535: INFO: Lookups using dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de failed for: [wheezy_udp@dns-test-service.dns-4982.svc.cluster.local wheezy_tcp@dns-test-service.dns-4982.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local jessie_udp@dns-test-service.dns-4982.svc.cluster.local jessie_tcp@dns-test-service.dns-4982.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local]

Jan 12 01:28:58.602: INFO: DNS probes using dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de succeeded

STEP: deleting the pod 01/12/23 01:28:58.603
STEP: deleting the test service 01/12/23 01:28:58.669
STEP: deleting the test headless service 01/12/23 01:28:58.714
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 01:28:58.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4982" for this suite. 01/12/23 01:28:58.739
------------------------------
• [SLOW TEST] [9.386 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:28:49.371
    Jan 12 01:28:49.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename dns 01/12/23 01:28:49.372
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:28:49.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:28:49.398
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/12/23 01:28:49.4
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4982.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4982.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 55.226.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.226.55_udp@PTR;check="$$(dig +tcp +noall +answer +search 55.226.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.226.55_tcp@PTR;sleep 1; done
     01/12/23 01:28:49.441
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4982.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4982.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4982.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4982.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4982.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 55.226.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.226.55_udp@PTR;check="$$(dig +tcp +noall +answer +search 55.226.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.226.55_tcp@PTR;sleep 1; done
     01/12/23 01:28:49.442
    STEP: creating a pod to probe DNS 01/12/23 01:28:49.442
    STEP: submitting the pod to kubernetes 01/12/23 01:28:49.442
    Jan 12 01:28:49.486: INFO: Waiting up to 15m0s for pod "dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de" in namespace "dns-4982" to be "running"
    Jan 12 01:28:49.488: INFO: Pod "dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.508848ms
    Jan 12 01:28:51.500: INFO: Pod "dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014141591s
    Jan 12 01:28:53.493: INFO: Pod "dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de": Phase="Running", Reason="", readiness=true. Elapsed: 4.006642864s
    Jan 12 01:28:53.493: INFO: Pod "dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 01:28:53.493
    STEP: looking for the results for each expected name from probers 01/12/23 01:28:53.495
    Jan 12 01:28:53.498: INFO: Unable to read wheezy_udp@dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
    Jan 12 01:28:53.501: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
    Jan 12 01:28:53.503: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
    Jan 12 01:28:53.506: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
    Jan 12 01:28:53.519: INFO: Unable to read jessie_udp@dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
    Jan 12 01:28:53.521: INFO: Unable to read jessie_tcp@dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
    Jan 12 01:28:53.524: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
    Jan 12 01:28:53.526: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local from pod dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de: the server could not find the requested resource (get pods dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de)
    Jan 12 01:28:53.535: INFO: Lookups using dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de failed for: [wheezy_udp@dns-test-service.dns-4982.svc.cluster.local wheezy_tcp@dns-test-service.dns-4982.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local jessie_udp@dns-test-service.dns-4982.svc.cluster.local jessie_tcp@dns-test-service.dns-4982.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4982.svc.cluster.local]

    Jan 12 01:28:58.602: INFO: DNS probes using dns-4982/dns-test-fe90ea92-5d69-4585-9b5e-0f1fc7b6a5de succeeded

    STEP: deleting the pod 01/12/23 01:28:58.603
    STEP: deleting the test service 01/12/23 01:28:58.669
    STEP: deleting the test headless service 01/12/23 01:28:58.714
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:28:58.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4982" for this suite. 01/12/23 01:28:58.739
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:28:58.759
Jan 12 01:28:58.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename security-context-test 01/12/23 01:28:58.76
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:28:58.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:28:58.779
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Jan 12 01:28:58.808: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-f51c9454-af80-45d7-b6e1-389b0b69894f" in namespace "security-context-test-2388" to be "Succeeded or Failed"
Jan 12 01:28:58.811: INFO: Pod "alpine-nnp-false-f51c9454-af80-45d7-b6e1-389b0b69894f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.331216ms
Jan 12 01:29:00.815: INFO: Pod "alpine-nnp-false-f51c9454-af80-45d7-b6e1-389b0b69894f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006609775s
Jan 12 01:29:02.819: INFO: Pod "alpine-nnp-false-f51c9454-af80-45d7-b6e1-389b0b69894f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010654611s
Jan 12 01:29:04.815: INFO: Pod "alpine-nnp-false-f51c9454-af80-45d7-b6e1-389b0b69894f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006801888s
Jan 12 01:29:04.815: INFO: Pod "alpine-nnp-false-f51c9454-af80-45d7-b6e1-389b0b69894f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 12 01:29:04.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2388" for this suite. 01/12/23 01:29:04.828
------------------------------
• [SLOW TEST] [6.086 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:28:58.759
    Jan 12 01:28:58.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename security-context-test 01/12/23 01:28:58.76
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:28:58.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:28:58.779
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Jan 12 01:28:58.808: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-f51c9454-af80-45d7-b6e1-389b0b69894f" in namespace "security-context-test-2388" to be "Succeeded or Failed"
    Jan 12 01:28:58.811: INFO: Pod "alpine-nnp-false-f51c9454-af80-45d7-b6e1-389b0b69894f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.331216ms
    Jan 12 01:29:00.815: INFO: Pod "alpine-nnp-false-f51c9454-af80-45d7-b6e1-389b0b69894f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006609775s
    Jan 12 01:29:02.819: INFO: Pod "alpine-nnp-false-f51c9454-af80-45d7-b6e1-389b0b69894f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010654611s
    Jan 12 01:29:04.815: INFO: Pod "alpine-nnp-false-f51c9454-af80-45d7-b6e1-389b0b69894f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006801888s
    Jan 12 01:29:04.815: INFO: Pod "alpine-nnp-false-f51c9454-af80-45d7-b6e1-389b0b69894f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:29:04.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2388" for this suite. 01/12/23 01:29:04.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:29:04.846
Jan 12 01:29:04.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename daemonsets 01/12/23 01:29:04.847
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:29:04.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:29:04.865
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Jan 12 01:29:04.889: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 01:29:04.93
Jan 12 01:29:04.937: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:04.937: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:04.937: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:04.940: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:29:04.940: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:29:05.944: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:05.944: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:05.944: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:05.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:29:05.947: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:29:06.945: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:06.945: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:06.945: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:06.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:29:06.948: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:29:07.945: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:07.945: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:07.945: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:07.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 01:29:07.948: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 01/12/23 01:29:07.958
STEP: Check that daemon pods images are updated. 01/12/23 01:29:07.973
Jan 12 01:29:07.986: INFO: Wrong image for pod: daemon-set-m8g6v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 12 01:29:07.986: INFO: Wrong image for pod: daemon-set-r8ws2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 12 01:29:08.000: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:08.000: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:08.000: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:09.004: INFO: Wrong image for pod: daemon-set-r8ws2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 12 01:29:09.007: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:09.008: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:09.008: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:10.004: INFO: Wrong image for pod: daemon-set-r8ws2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 12 01:29:10.008: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:10.008: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:10.008: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:11.004: INFO: Pod daemon-set-r4629 is not available
Jan 12 01:29:11.004: INFO: Wrong image for pod: daemon-set-r8ws2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 12 01:29:11.007: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:11.007: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:11.007: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:12.004: INFO: Pod daemon-set-r4629 is not available
Jan 12 01:29:12.004: INFO: Wrong image for pod: daemon-set-r8ws2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 12 01:29:12.009: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:12.009: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:12.009: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:13.007: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:13.007: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:13.007: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:14.004: INFO: Pod daemon-set-wtsh9 is not available
Jan 12 01:29:14.007: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:14.007: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:14.008: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 01/12/23 01:29:14.008
Jan 12 01:29:14.011: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:14.011: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:14.011: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:14.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 01:29:14.013: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:29:15.017: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:15.017: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:15.017: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:15.020: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 01:29:15.020: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:29:16.017: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:16.017: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:16.017: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:29:16.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 01:29:16.019: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/12/23 01:29:16.03
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6123, will wait for the garbage collector to delete the pods 01/12/23 01:29:16.03
Jan 12 01:29:16.103: INFO: Deleting DaemonSet.extensions daemon-set took: 9.16411ms
Jan 12 01:29:16.204: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.966764ms
Jan 12 01:29:18.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:29:18.807: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 12 01:29:18.809: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20163137"},"items":null}

Jan 12 01:29:18.811: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20163137"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:29:18.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6123" for this suite. 01/12/23 01:29:18.847
------------------------------
• [SLOW TEST] [14.023 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:29:04.846
    Jan 12 01:29:04.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename daemonsets 01/12/23 01:29:04.847
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:29:04.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:29:04.865
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Jan 12 01:29:04.889: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 01:29:04.93
    Jan 12 01:29:04.937: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:04.937: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:04.937: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:04.940: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:29:04.940: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:29:05.944: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:05.944: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:05.944: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:05.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:29:05.947: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:29:06.945: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:06.945: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:06.945: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:06.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:29:06.948: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:29:07.945: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:07.945: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:07.945: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:07.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 01:29:07.948: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 01/12/23 01:29:07.958
    STEP: Check that daemon pods images are updated. 01/12/23 01:29:07.973
    Jan 12 01:29:07.986: INFO: Wrong image for pod: daemon-set-m8g6v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 12 01:29:07.986: INFO: Wrong image for pod: daemon-set-r8ws2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 12 01:29:08.000: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:08.000: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:08.000: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:09.004: INFO: Wrong image for pod: daemon-set-r8ws2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 12 01:29:09.007: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:09.008: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:09.008: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:10.004: INFO: Wrong image for pod: daemon-set-r8ws2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 12 01:29:10.008: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:10.008: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:10.008: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:11.004: INFO: Pod daemon-set-r4629 is not available
    Jan 12 01:29:11.004: INFO: Wrong image for pod: daemon-set-r8ws2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 12 01:29:11.007: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:11.007: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:11.007: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:12.004: INFO: Pod daemon-set-r4629 is not available
    Jan 12 01:29:12.004: INFO: Wrong image for pod: daemon-set-r8ws2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 12 01:29:12.009: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:12.009: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:12.009: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:13.007: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:13.007: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:13.007: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:14.004: INFO: Pod daemon-set-wtsh9 is not available
    Jan 12 01:29:14.007: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:14.007: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:14.008: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 01/12/23 01:29:14.008
    Jan 12 01:29:14.011: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:14.011: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:14.011: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:14.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 01:29:14.013: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:29:15.017: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:15.017: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:15.017: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:15.020: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 01:29:15.020: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:29:16.017: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:16.017: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:16.017: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:29:16.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 01:29:16.019: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/12/23 01:29:16.03
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6123, will wait for the garbage collector to delete the pods 01/12/23 01:29:16.03
    Jan 12 01:29:16.103: INFO: Deleting DaemonSet.extensions daemon-set took: 9.16411ms
    Jan 12 01:29:16.204: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.966764ms
    Jan 12 01:29:18.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:29:18.807: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 12 01:29:18.809: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20163137"},"items":null}

    Jan 12 01:29:18.811: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20163137"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:29:18.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6123" for this suite. 01/12/23 01:29:18.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:29:18.871
Jan 12 01:29:18.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename endpointslice 01/12/23 01:29:18.872
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:29:18.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:29:18.891
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 01/12/23 01:29:24.14
STEP: referencing matching pods with named port 01/12/23 01:29:29.146
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/12/23 01:29:34.154
STEP: recreating EndpointSlices after they've been deleted 01/12/23 01:29:39.171
Jan 12 01:29:39.190: INFO: EndpointSlice for Service endpointslice-5362/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 12 01:29:49.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5362" for this suite. 01/12/23 01:29:49.202
------------------------------
• [SLOW TEST] [30.351 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:29:18.871
    Jan 12 01:29:18.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename endpointslice 01/12/23 01:29:18.872
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:29:18.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:29:18.891
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 01/12/23 01:29:24.14
    STEP: referencing matching pods with named port 01/12/23 01:29:29.146
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/12/23 01:29:34.154
    STEP: recreating EndpointSlices after they've been deleted 01/12/23 01:29:39.171
    Jan 12 01:29:39.190: INFO: EndpointSlice for Service endpointslice-5362/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:29:49.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5362" for this suite. 01/12/23 01:29:49.202
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:29:49.223
Jan 12 01:29:49.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 01:29:49.224
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:29:49.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:29:49.256
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 01/12/23 01:29:49.259
Jan 12 01:29:49.318: INFO: Waiting up to 5m0s for pod "pod-8b19a155-096e-47e5-97c7-0761e65373de" in namespace "emptydir-7684" to be "Succeeded or Failed"
Jan 12 01:29:49.320: INFO: Pod "pod-8b19a155-096e-47e5-97c7-0761e65373de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.541394ms
Jan 12 01:29:51.323: INFO: Pod "pod-8b19a155-096e-47e5-97c7-0761e65373de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005459274s
Jan 12 01:29:53.323: INFO: Pod "pod-8b19a155-096e-47e5-97c7-0761e65373de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005523702s
Jan 12 01:29:55.324: INFO: Pod "pod-8b19a155-096e-47e5-97c7-0761e65373de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006375814s
STEP: Saw pod success 01/12/23 01:29:55.324
Jan 12 01:29:55.324: INFO: Pod "pod-8b19a155-096e-47e5-97c7-0761e65373de" satisfied condition "Succeeded or Failed"
Jan 12 01:29:55.327: INFO: Trying to get logs from node eqx04-flash06 pod pod-8b19a155-096e-47e5-97c7-0761e65373de container test-container: <nil>
STEP: delete the pod 01/12/23 01:29:55.334
Jan 12 01:29:55.364: INFO: Waiting for pod pod-8b19a155-096e-47e5-97c7-0761e65373de to disappear
Jan 12 01:29:55.366: INFO: Pod pod-8b19a155-096e-47e5-97c7-0761e65373de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:29:55.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7684" for this suite. 01/12/23 01:29:55.37
------------------------------
• [SLOW TEST] [6.165 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:29:49.223
    Jan 12 01:29:49.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 01:29:49.224
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:29:49.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:29:49.256
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 01/12/23 01:29:49.259
    Jan 12 01:29:49.318: INFO: Waiting up to 5m0s for pod "pod-8b19a155-096e-47e5-97c7-0761e65373de" in namespace "emptydir-7684" to be "Succeeded or Failed"
    Jan 12 01:29:49.320: INFO: Pod "pod-8b19a155-096e-47e5-97c7-0761e65373de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.541394ms
    Jan 12 01:29:51.323: INFO: Pod "pod-8b19a155-096e-47e5-97c7-0761e65373de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005459274s
    Jan 12 01:29:53.323: INFO: Pod "pod-8b19a155-096e-47e5-97c7-0761e65373de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005523702s
    Jan 12 01:29:55.324: INFO: Pod "pod-8b19a155-096e-47e5-97c7-0761e65373de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006375814s
    STEP: Saw pod success 01/12/23 01:29:55.324
    Jan 12 01:29:55.324: INFO: Pod "pod-8b19a155-096e-47e5-97c7-0761e65373de" satisfied condition "Succeeded or Failed"
    Jan 12 01:29:55.327: INFO: Trying to get logs from node eqx04-flash06 pod pod-8b19a155-096e-47e5-97c7-0761e65373de container test-container: <nil>
    STEP: delete the pod 01/12/23 01:29:55.334
    Jan 12 01:29:55.364: INFO: Waiting for pod pod-8b19a155-096e-47e5-97c7-0761e65373de to disappear
    Jan 12 01:29:55.366: INFO: Pod pod-8b19a155-096e-47e5-97c7-0761e65373de no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:29:55.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7684" for this suite. 01/12/23 01:29:55.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:29:55.388
Jan 12 01:29:55.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename endpointslice 01/12/23 01:29:55.389
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:29:55.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:29:55.42
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 12 01:29:57.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3751" for this suite. 01/12/23 01:29:57.51
------------------------------
• [2.144 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:29:55.388
    Jan 12 01:29:55.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename endpointslice 01/12/23 01:29:55.389
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:29:55.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:29:55.42
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:29:57.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3751" for this suite. 01/12/23 01:29:57.51
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:29:57.534
Jan 12 01:29:57.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pods 01/12/23 01:29:57.535
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:29:57.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:29:57.557
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jan 12 01:29:57.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: creating the pod 01/12/23 01:29:57.559
STEP: submitting the pod to kubernetes 01/12/23 01:29:57.559
Jan 12 01:29:57.605: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15" in namespace "pods-4238" to be "running and ready"
Jan 12 01:29:57.609: INFO: Pod "pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15": Phase="Pending", Reason="", readiness=false. Elapsed: 3.889804ms
Jan 12 01:29:57.609: INFO: The phase of Pod pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:29:59.612: INFO: Pod "pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007205533s
Jan 12 01:29:59.612: INFO: The phase of Pod pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:30:01.613: INFO: Pod "pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15": Phase="Running", Reason="", readiness=true. Elapsed: 4.008490604s
Jan 12 01:30:01.613: INFO: The phase of Pod pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15 is Running (Ready = true)
Jan 12 01:30:01.613: INFO: Pod "pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 01:30:01.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4238" for this suite. 01/12/23 01:30:01.787
------------------------------
• [4.273 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:29:57.534
    Jan 12 01:29:57.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pods 01/12/23 01:29:57.535
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:29:57.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:29:57.557
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jan 12 01:29:57.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: creating the pod 01/12/23 01:29:57.559
    STEP: submitting the pod to kubernetes 01/12/23 01:29:57.559
    Jan 12 01:29:57.605: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15" in namespace "pods-4238" to be "running and ready"
    Jan 12 01:29:57.609: INFO: Pod "pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15": Phase="Pending", Reason="", readiness=false. Elapsed: 3.889804ms
    Jan 12 01:29:57.609: INFO: The phase of Pod pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:29:59.612: INFO: Pod "pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007205533s
    Jan 12 01:29:59.612: INFO: The phase of Pod pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:30:01.613: INFO: Pod "pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15": Phase="Running", Reason="", readiness=true. Elapsed: 4.008490604s
    Jan 12 01:30:01.613: INFO: The phase of Pod pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15 is Running (Ready = true)
    Jan 12 01:30:01.613: INFO: Pod "pod-exec-websocket-51598513-ea42-4a39-8a1e-7dba6620fe15" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:30:01.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4238" for this suite. 01/12/23 01:30:01.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:30:01.808
Jan 12 01:30:01.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename resourcequota 01/12/23 01:30:01.81
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:01.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:01.833
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 01/12/23 01:30:01.835
STEP: Creating a ResourceQuota 01/12/23 01:30:06.842
STEP: Ensuring resource quota status is calculated 01/12/23 01:30:06.85
STEP: Creating a ReplicationController 01/12/23 01:30:08.853
STEP: Ensuring resource quota status captures replication controller creation 01/12/23 01:30:08.887
STEP: Deleting a ReplicationController 01/12/23 01:30:10.894
STEP: Ensuring resource quota status released usage 01/12/23 01:30:10.908
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 01:30:12.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6214" for this suite. 01/12/23 01:30:12.915
------------------------------
• [SLOW TEST] [11.124 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:30:01.808
    Jan 12 01:30:01.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename resourcequota 01/12/23 01:30:01.81
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:01.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:01.833
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 01/12/23 01:30:01.835
    STEP: Creating a ResourceQuota 01/12/23 01:30:06.842
    STEP: Ensuring resource quota status is calculated 01/12/23 01:30:06.85
    STEP: Creating a ReplicationController 01/12/23 01:30:08.853
    STEP: Ensuring resource quota status captures replication controller creation 01/12/23 01:30:08.887
    STEP: Deleting a ReplicationController 01/12/23 01:30:10.894
    STEP: Ensuring resource quota status released usage 01/12/23 01:30:10.908
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:30:12.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6214" for this suite. 01/12/23 01:30:12.915
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:30:12.933
Jan 12 01:30:12.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 01:30:12.934
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:12.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:12.964
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 01/12/23 01:30:12.967
Jan 12 01:30:12.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4059 create -f -'
Jan 12 01:30:14.346: INFO: stderr: ""
Jan 12 01:30:14.346: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/12/23 01:30:14.346
Jan 12 01:30:15.350: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 01:30:15.350: INFO: Found 0 / 1
Jan 12 01:30:16.350: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 01:30:16.350: INFO: Found 1 / 1
Jan 12 01:30:16.350: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/12/23 01:30:16.35
Jan 12 01:30:16.352: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 01:30:16.353: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 12 01:30:16.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4059 patch pod agnhost-primary-nnqjz -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 12 01:30:16.431: INFO: stderr: ""
Jan 12 01:30:16.431: INFO: stdout: "pod/agnhost-primary-nnqjz patched\n"
STEP: checking annotations 01/12/23 01:30:16.431
Jan 12 01:30:16.439: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 01:30:16.439: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 01:30:16.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4059" for this suite. 01/12/23 01:30:16.443
------------------------------
• [3.531 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:30:12.933
    Jan 12 01:30:12.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 01:30:12.934
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:12.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:12.964
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 01/12/23 01:30:12.967
    Jan 12 01:30:12.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4059 create -f -'
    Jan 12 01:30:14.346: INFO: stderr: ""
    Jan 12 01:30:14.346: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/12/23 01:30:14.346
    Jan 12 01:30:15.350: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 01:30:15.350: INFO: Found 0 / 1
    Jan 12 01:30:16.350: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 01:30:16.350: INFO: Found 1 / 1
    Jan 12 01:30:16.350: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/12/23 01:30:16.35
    Jan 12 01:30:16.352: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 01:30:16.353: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 12 01:30:16.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4059 patch pod agnhost-primary-nnqjz -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 12 01:30:16.431: INFO: stderr: ""
    Jan 12 01:30:16.431: INFO: stdout: "pod/agnhost-primary-nnqjz patched\n"
    STEP: checking annotations 01/12/23 01:30:16.431
    Jan 12 01:30:16.439: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 01:30:16.439: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:30:16.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4059" for this suite. 01/12/23 01:30:16.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:30:16.466
Jan 12 01:30:16.466: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:30:16.467
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:16.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:16.488
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-d6c99f70-ffa5-48be-83d2-979c4622faf0 01/12/23 01:30:16.49
STEP: Creating a pod to test consume configMaps 01/12/23 01:30:16.503
Jan 12 01:30:16.551: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081" in namespace "projected-2522" to be "Succeeded or Failed"
Jan 12 01:30:16.553: INFO: Pod "pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081": Phase="Pending", Reason="", readiness=false. Elapsed: 2.331988ms
Jan 12 01:30:18.561: INFO: Pod "pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009862589s
Jan 12 01:30:20.560: INFO: Pod "pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009417544s
Jan 12 01:30:22.556: INFO: Pod "pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005153502s
STEP: Saw pod success 01/12/23 01:30:22.556
Jan 12 01:30:22.556: INFO: Pod "pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081" satisfied condition "Succeeded or Failed"
Jan 12 01:30:22.568: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/12/23 01:30:22.575
Jan 12 01:30:22.605: INFO: Waiting for pod pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081 to disappear
Jan 12 01:30:22.608: INFO: Pod pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:30:22.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2522" for this suite. 01/12/23 01:30:22.611
------------------------------
• [SLOW TEST] [6.164 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:30:16.466
    Jan 12 01:30:16.466: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:30:16.467
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:16.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:16.488
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-d6c99f70-ffa5-48be-83d2-979c4622faf0 01/12/23 01:30:16.49
    STEP: Creating a pod to test consume configMaps 01/12/23 01:30:16.503
    Jan 12 01:30:16.551: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081" in namespace "projected-2522" to be "Succeeded or Failed"
    Jan 12 01:30:16.553: INFO: Pod "pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081": Phase="Pending", Reason="", readiness=false. Elapsed: 2.331988ms
    Jan 12 01:30:18.561: INFO: Pod "pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009862589s
    Jan 12 01:30:20.560: INFO: Pod "pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009417544s
    Jan 12 01:30:22.556: INFO: Pod "pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005153502s
    STEP: Saw pod success 01/12/23 01:30:22.556
    Jan 12 01:30:22.556: INFO: Pod "pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081" satisfied condition "Succeeded or Failed"
    Jan 12 01:30:22.568: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/12/23 01:30:22.575
    Jan 12 01:30:22.605: INFO: Waiting for pod pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081 to disappear
    Jan 12 01:30:22.608: INFO: Pod pod-projected-configmaps-619e6158-8e79-4c5e-863e-f1020b118081 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:30:22.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2522" for this suite. 01/12/23 01:30:22.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:30:22.631
Jan 12 01:30:22.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 01:30:22.632
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:22.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:22.65
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 01/12/23 01:30:22.658
Jan 12 01:30:22.719: INFO: Waiting up to 5m0s for pod "labelsupdatecd681d6f-513f-404a-89b6-d43dac6ff6c7" in namespace "downward-api-3133" to be "running and ready"
Jan 12 01:30:22.721: INFO: Pod "labelsupdatecd681d6f-513f-404a-89b6-d43dac6ff6c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407253ms
Jan 12 01:30:22.721: INFO: The phase of Pod labelsupdatecd681d6f-513f-404a-89b6-d43dac6ff6c7 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:30:24.724: INFO: Pod "labelsupdatecd681d6f-513f-404a-89b6-d43dac6ff6c7": Phase="Running", Reason="", readiness=true. Elapsed: 2.005758551s
Jan 12 01:30:24.724: INFO: The phase of Pod labelsupdatecd681d6f-513f-404a-89b6-d43dac6ff6c7 is Running (Ready = true)
Jan 12 01:30:24.724: INFO: Pod "labelsupdatecd681d6f-513f-404a-89b6-d43dac6ff6c7" satisfied condition "running and ready"
Jan 12 01:30:25.252: INFO: Successfully updated pod "labelsupdatecd681d6f-513f-404a-89b6-d43dac6ff6c7"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 01:30:27.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3133" for this suite. 01/12/23 01:30:27.274
------------------------------
• [4.663 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:30:22.631
    Jan 12 01:30:22.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 01:30:22.632
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:22.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:22.65
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 01/12/23 01:30:22.658
    Jan 12 01:30:22.719: INFO: Waiting up to 5m0s for pod "labelsupdatecd681d6f-513f-404a-89b6-d43dac6ff6c7" in namespace "downward-api-3133" to be "running and ready"
    Jan 12 01:30:22.721: INFO: Pod "labelsupdatecd681d6f-513f-404a-89b6-d43dac6ff6c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407253ms
    Jan 12 01:30:22.721: INFO: The phase of Pod labelsupdatecd681d6f-513f-404a-89b6-d43dac6ff6c7 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:30:24.724: INFO: Pod "labelsupdatecd681d6f-513f-404a-89b6-d43dac6ff6c7": Phase="Running", Reason="", readiness=true. Elapsed: 2.005758551s
    Jan 12 01:30:24.724: INFO: The phase of Pod labelsupdatecd681d6f-513f-404a-89b6-d43dac6ff6c7 is Running (Ready = true)
    Jan 12 01:30:24.724: INFO: Pod "labelsupdatecd681d6f-513f-404a-89b6-d43dac6ff6c7" satisfied condition "running and ready"
    Jan 12 01:30:25.252: INFO: Successfully updated pod "labelsupdatecd681d6f-513f-404a-89b6-d43dac6ff6c7"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:30:27.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3133" for this suite. 01/12/23 01:30:27.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:30:27.295
Jan 12 01:30:27.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 01:30:27.296
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:27.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:27.322
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 01/12/23 01:30:27.324
Jan 12 01:30:27.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-9562 cluster-info'
Jan 12 01:30:27.398: INFO: stderr: ""
Jan 12 01:30:27.398: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.19.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 01:30:27.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9562" for this suite. 01/12/23 01:30:27.403
------------------------------
• [0.132 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:30:27.295
    Jan 12 01:30:27.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 01:30:27.296
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:27.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:27.322
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 01/12/23 01:30:27.324
    Jan 12 01:30:27.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-9562 cluster-info'
    Jan 12 01:30:27.398: INFO: stderr: ""
    Jan 12 01:30:27.398: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.19.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:30:27.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9562" for this suite. 01/12/23 01:30:27.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:30:27.428
Jan 12 01:30:27.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 01:30:27.429
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:27.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:27.453
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jan 12 01:30:27.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/12/23 01:30:29.818
Jan 12 01:30:29.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-1432 --namespace=crd-publish-openapi-1432 create -f -'
Jan 12 01:30:30.674: INFO: stderr: ""
Jan 12 01:30:30.674: INFO: stdout: "e2e-test-crd-publish-openapi-671-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 12 01:30:30.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-1432 --namespace=crd-publish-openapi-1432 delete e2e-test-crd-publish-openapi-671-crds test-cr'
Jan 12 01:30:30.745: INFO: stderr: ""
Jan 12 01:30:30.745: INFO: stdout: "e2e-test-crd-publish-openapi-671-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 12 01:30:30.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-1432 --namespace=crd-publish-openapi-1432 apply -f -'
Jan 12 01:30:30.928: INFO: stderr: ""
Jan 12 01:30:30.928: INFO: stdout: "e2e-test-crd-publish-openapi-671-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 12 01:30:30.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-1432 --namespace=crd-publish-openapi-1432 delete e2e-test-crd-publish-openapi-671-crds test-cr'
Jan 12 01:30:30.996: INFO: stderr: ""
Jan 12 01:30:30.996: INFO: stdout: "e2e-test-crd-publish-openapi-671-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/12/23 01:30:30.996
Jan 12 01:30:30.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-1432 explain e2e-test-crd-publish-openapi-671-crds'
Jan 12 01:30:31.515: INFO: stderr: ""
Jan 12 01:30:31.515: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-671-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:30:33.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1432" for this suite. 01/12/23 01:30:33.928
------------------------------
• [SLOW TEST] [6.530 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:30:27.428
    Jan 12 01:30:27.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 01:30:27.429
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:27.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:27.453
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jan 12 01:30:27.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/12/23 01:30:29.818
    Jan 12 01:30:29.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-1432 --namespace=crd-publish-openapi-1432 create -f -'
    Jan 12 01:30:30.674: INFO: stderr: ""
    Jan 12 01:30:30.674: INFO: stdout: "e2e-test-crd-publish-openapi-671-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 12 01:30:30.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-1432 --namespace=crd-publish-openapi-1432 delete e2e-test-crd-publish-openapi-671-crds test-cr'
    Jan 12 01:30:30.745: INFO: stderr: ""
    Jan 12 01:30:30.745: INFO: stdout: "e2e-test-crd-publish-openapi-671-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 12 01:30:30.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-1432 --namespace=crd-publish-openapi-1432 apply -f -'
    Jan 12 01:30:30.928: INFO: stderr: ""
    Jan 12 01:30:30.928: INFO: stdout: "e2e-test-crd-publish-openapi-671-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 12 01:30:30.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-1432 --namespace=crd-publish-openapi-1432 delete e2e-test-crd-publish-openapi-671-crds test-cr'
    Jan 12 01:30:30.996: INFO: stderr: ""
    Jan 12 01:30:30.996: INFO: stdout: "e2e-test-crd-publish-openapi-671-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/12/23 01:30:30.996
    Jan 12 01:30:30.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-1432 explain e2e-test-crd-publish-openapi-671-crds'
    Jan 12 01:30:31.515: INFO: stderr: ""
    Jan 12 01:30:31.515: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-671-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:30:33.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1432" for this suite. 01/12/23 01:30:33.928
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:30:33.959
Jan 12 01:30:33.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 01:30:33.96
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:33.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:33.983
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 01/12/23 01:30:33.985
Jan 12 01:30:33.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6387 api-versions'
Jan 12 01:30:34.062: INFO: stderr: ""
Jan 12 01:30:34.062: INFO: stdout: "acid.zalan.do/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nk8s.cni.cncf.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nrobin.io/v1alpha1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 01:30:34.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6387" for this suite. 01/12/23 01:30:34.067
------------------------------
• [0.135 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:30:33.959
    Jan 12 01:30:33.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 01:30:33.96
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:33.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:33.983
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 01/12/23 01:30:33.985
    Jan 12 01:30:33.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6387 api-versions'
    Jan 12 01:30:34.062: INFO: stderr: ""
    Jan 12 01:30:34.062: INFO: stdout: "acid.zalan.do/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nk8s.cni.cncf.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nrobin.io/v1alpha1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:30:34.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6387" for this suite. 01/12/23 01:30:34.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:30:34.096
Jan 12 01:30:34.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename csiinlinevolumes 01/12/23 01:30:34.097
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:34.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:34.119
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 01/12/23 01:30:34.122
STEP: getting 01/12/23 01:30:34.148
STEP: listing 01/12/23 01:30:34.153
STEP: deleting 01/12/23 01:30:34.156
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:30:34.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-6600" for this suite. 01/12/23 01:30:34.178
------------------------------
• [0.099 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:30:34.096
    Jan 12 01:30:34.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename csiinlinevolumes 01/12/23 01:30:34.097
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:34.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:34.119
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 01/12/23 01:30:34.122
    STEP: getting 01/12/23 01:30:34.148
    STEP: listing 01/12/23 01:30:34.153
    STEP: deleting 01/12/23 01:30:34.156
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:30:34.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-6600" for this suite. 01/12/23 01:30:34.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:30:34.196
Jan 12 01:30:34.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 01:30:34.197
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:34.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:34.222
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 01:30:34.238
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:30:34.727
STEP: Deploying the webhook pod 01/12/23 01:30:34.738
STEP: Wait for the deployment to be ready 01/12/23 01:30:34.794
Jan 12 01:30:34.802: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 12 01:30:36.813: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 01:30:38.818
STEP: Verifying the service has paired with the endpoint 01/12/23 01:30:38.834
Jan 12 01:30:39.834: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jan 12 01:30:39.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3015-crds.webhook.example.com via the AdmissionRegistration API 01/12/23 01:30:40.35
STEP: Creating a custom resource that should be mutated by the webhook 01/12/23 01:30:40.369
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:30:42.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8792" for this suite. 01/12/23 01:30:43.027
STEP: Destroying namespace "webhook-8792-markers" for this suite. 01/12/23 01:30:43.087
------------------------------
• [SLOW TEST] [8.934 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:30:34.196
    Jan 12 01:30:34.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 01:30:34.197
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:34.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:34.222
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 01:30:34.238
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:30:34.727
    STEP: Deploying the webhook pod 01/12/23 01:30:34.738
    STEP: Wait for the deployment to be ready 01/12/23 01:30:34.794
    Jan 12 01:30:34.802: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 12 01:30:36.813: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 01:30:38.818
    STEP: Verifying the service has paired with the endpoint 01/12/23 01:30:38.834
    Jan 12 01:30:39.834: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jan 12 01:30:39.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3015-crds.webhook.example.com via the AdmissionRegistration API 01/12/23 01:30:40.35
    STEP: Creating a custom resource that should be mutated by the webhook 01/12/23 01:30:40.369
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:30:42.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8792" for this suite. 01/12/23 01:30:43.027
    STEP: Destroying namespace "webhook-8792-markers" for this suite. 01/12/23 01:30:43.087
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:30:43.132
Jan 12 01:30:43.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename deployment 01/12/23 01:30:43.132
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:43.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:43.152
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 12 01:30:43.164: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 12 01:30:48.168: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/12/23 01:30:48.168
Jan 12 01:30:48.168: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 12 01:30:50.173: INFO: Creating deployment "test-rollover-deployment"
Jan 12 01:30:50.228: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 12 01:30:52.235: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 12 01:30:52.241: INFO: Ensure that both replica sets have 1 created replica
Jan 12 01:30:52.247: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 12 01:30:52.258: INFO: Updating deployment test-rollover-deployment
Jan 12 01:30:52.258: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 12 01:30:54.265: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 12 01:30:54.271: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 12 01:30:54.277: INFO: all replica sets need to contain the pod-template-hash label
Jan 12 01:30:54.277: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:30:56.287: INFO: all replica sets need to contain the pod-template-hash label
Jan 12 01:30:56.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:30:58.286: INFO: all replica sets need to contain the pod-template-hash label
Jan 12 01:30:58.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:31:00.287: INFO: all replica sets need to contain the pod-template-hash label
Jan 12 01:31:00.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:31:02.284: INFO: all replica sets need to contain the pod-template-hash label
Jan 12 01:31:02.284: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:31:04.287: INFO: all replica sets need to contain the pod-template-hash label
Jan 12 01:31:04.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:31:06.286: INFO: 
Jan 12 01:31:06.286: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 01:31:06.294: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8091  48d6cad2-efa3-4407-9296-cd6bbbb77bbe 20164061 2 2023-01-12 01:30:50 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-12 01:30:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:31:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058dcc98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-12 01:30:50 +0000 UTC,LastTransitionTime:2023-01-12 01:30:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-12 01:31:04 +0000 UTC,LastTransitionTime:2023-01-12 01:30:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 12 01:31:06.297: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8091  635c9d17-c4ec-4eaf-a036-d949fb109083 20164052 2 2023-01-12 01:30:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 48d6cad2-efa3-4407-9296-cd6bbbb77bbe 0xc005044af7 0xc005044af8}] [] [{kube-controller-manager Update apps/v1 2023-01-12 01:30:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48d6cad2-efa3-4407-9296-cd6bbbb77bbe\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:31:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005044ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 12 01:31:06.297: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 12 01:31:06.298: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8091  b1cebcc4-ef45-43f4-b58f-5a1822f0777a 20164060 2 2023-01-12 01:30:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 48d6cad2-efa3-4407-9296-cd6bbbb77bbe 0xc005044397 0xc005044398}] [] [{e2e.test Update apps/v1 2023-01-12 01:30:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:31:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48d6cad2-efa3-4407-9296-cd6bbbb77bbe\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:31:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005044a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 12 01:31:06.298: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8091  d1fdca47-e258-4a86-97d8-b367891143be 20163958 2 2023-01-12 01:30:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 48d6cad2-efa3-4407-9296-cd6bbbb77bbe 0xc005044d67 0xc005044d68}] [] [{kube-controller-manager Update apps/v1 2023-01-12 01:30:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48d6cad2-efa3-4407-9296-cd6bbbb77bbe\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:30:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005044e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 12 01:31:06.301: INFO: Pod "test-rollover-deployment-6c6df9974f-hbhxw" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-hbhxw test-rollover-deployment-6c6df9974f- deployment-8091  a21cd12e-a8ed-4c4a-89d6-954d14badf46 20163989 0 2023-01-12 01:30:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:26cce8d454c5e24ae0d8eaf014f71c634f67d2fcb015227ba4308256caddb70a cni.projectcalico.org/podIP:172.21.88.161/32 cni.projectcalico.org/podIPs:172.21.88.161/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.161"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.161"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 635c9d17-c4ec-4eaf-a036-d949fb109083 0xc005045817 0xc005045818}] [] [{kube-controller-manager Update v1 2023-01-12 01:30:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"635c9d17-c4ec-4eaf-a036-d949fb109083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:30:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:30:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:30:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ts68b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ts68b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:30:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:30:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:30:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:30:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.161,StartTime:2023-01-12 01:30:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:30:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:robin://9f1343d0ce8caff6e31159f1de1c4b9e98c654f54b5466a23b0ffbd85c8e0688,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 01:31:06.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8091" for this suite. 01/12/23 01:31:06.306
------------------------------
• [SLOW TEST] [23.193 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:30:43.132
    Jan 12 01:30:43.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename deployment 01/12/23 01:30:43.132
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:30:43.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:30:43.152
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 12 01:30:43.164: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 12 01:30:48.168: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/12/23 01:30:48.168
    Jan 12 01:30:48.168: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 12 01:30:50.173: INFO: Creating deployment "test-rollover-deployment"
    Jan 12 01:30:50.228: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 12 01:30:52.235: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 12 01:30:52.241: INFO: Ensure that both replica sets have 1 created replica
    Jan 12 01:30:52.247: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 12 01:30:52.258: INFO: Updating deployment test-rollover-deployment
    Jan 12 01:30:52.258: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 12 01:30:54.265: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 12 01:30:54.271: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 12 01:30:54.277: INFO: all replica sets need to contain the pod-template-hash label
    Jan 12 01:30:54.277: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:30:56.287: INFO: all replica sets need to contain the pod-template-hash label
    Jan 12 01:30:56.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:30:58.286: INFO: all replica sets need to contain the pod-template-hash label
    Jan 12 01:30:58.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:31:00.287: INFO: all replica sets need to contain the pod-template-hash label
    Jan 12 01:31:00.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:31:02.284: INFO: all replica sets need to contain the pod-template-hash label
    Jan 12 01:31:02.284: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:31:04.287: INFO: all replica sets need to contain the pod-template-hash label
    Jan 12 01:31:04.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 30, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 30, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:31:06.286: INFO: 
    Jan 12 01:31:06.286: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 01:31:06.294: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-8091  48d6cad2-efa3-4407-9296-cd6bbbb77bbe 20164061 2 2023-01-12 01:30:50 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-12 01:30:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:31:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058dcc98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-12 01:30:50 +0000 UTC,LastTransitionTime:2023-01-12 01:30:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-12 01:31:04 +0000 UTC,LastTransitionTime:2023-01-12 01:30:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 12 01:31:06.297: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8091  635c9d17-c4ec-4eaf-a036-d949fb109083 20164052 2 2023-01-12 01:30:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 48d6cad2-efa3-4407-9296-cd6bbbb77bbe 0xc005044af7 0xc005044af8}] [] [{kube-controller-manager Update apps/v1 2023-01-12 01:30:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48d6cad2-efa3-4407-9296-cd6bbbb77bbe\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:31:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005044ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 01:31:06.297: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 12 01:31:06.298: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8091  b1cebcc4-ef45-43f4-b58f-5a1822f0777a 20164060 2 2023-01-12 01:30:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 48d6cad2-efa3-4407-9296-cd6bbbb77bbe 0xc005044397 0xc005044398}] [] [{e2e.test Update apps/v1 2023-01-12 01:30:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:31:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48d6cad2-efa3-4407-9296-cd6bbbb77bbe\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:31:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005044a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 01:31:06.298: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8091  d1fdca47-e258-4a86-97d8-b367891143be 20163958 2 2023-01-12 01:30:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 48d6cad2-efa3-4407-9296-cd6bbbb77bbe 0xc005044d67 0xc005044d68}] [] [{kube-controller-manager Update apps/v1 2023-01-12 01:30:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48d6cad2-efa3-4407-9296-cd6bbbb77bbe\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:30:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005044e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 01:31:06.301: INFO: Pod "test-rollover-deployment-6c6df9974f-hbhxw" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-hbhxw test-rollover-deployment-6c6df9974f- deployment-8091  a21cd12e-a8ed-4c4a-89d6-954d14badf46 20163989 0 2023-01-12 01:30:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:26cce8d454c5e24ae0d8eaf014f71c634f67d2fcb015227ba4308256caddb70a cni.projectcalico.org/podIP:172.21.88.161/32 cni.projectcalico.org/podIPs:172.21.88.161/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.161"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.161"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 635c9d17-c4ec-4eaf-a036-d949fb109083 0xc005045817 0xc005045818}] [] [{kube-controller-manager Update v1 2023-01-12 01:30:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"635c9d17-c4ec-4eaf-a036-d949fb109083\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:30:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:30:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:30:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ts68b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ts68b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:30:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:30:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:30:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:30:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.161,StartTime:2023-01-12 01:30:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:30:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:robin://9f1343d0ce8caff6e31159f1de1c4b9e98c654f54b5466a23b0ffbd85c8e0688,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:31:06.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8091" for this suite. 01/12/23 01:31:06.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:31:06.327
Jan 12 01:31:06.327: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename var-expansion 01/12/23 01:31:06.328
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:31:06.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:31:06.357
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Jan 12 01:31:06.431: INFO: Waiting up to 2m0s for pod "var-expansion-f54ed9ab-7b0b-4d5b-a776-70271fb7fcf3" in namespace "var-expansion-3227" to be "container 0 failed with reason CreateContainerConfigError"
Jan 12 01:31:06.434: INFO: Pod "var-expansion-f54ed9ab-7b0b-4d5b-a776-70271fb7fcf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.970419ms
Jan 12 01:31:08.440: INFO: Pod "var-expansion-f54ed9ab-7b0b-4d5b-a776-70271fb7fcf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00856886s
Jan 12 01:31:10.439: INFO: Pod "var-expansion-f54ed9ab-7b0b-4d5b-a776-70271fb7fcf3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007182108s
Jan 12 01:31:10.439: INFO: Pod "var-expansion-f54ed9ab-7b0b-4d5b-a776-70271fb7fcf3" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 12 01:31:10.439: INFO: Deleting pod "var-expansion-f54ed9ab-7b0b-4d5b-a776-70271fb7fcf3" in namespace "var-expansion-3227"
Jan 12 01:31:10.447: INFO: Wait up to 5m0s for pod "var-expansion-f54ed9ab-7b0b-4d5b-a776-70271fb7fcf3" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 01:31:12.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3227" for this suite. 01/12/23 01:31:12.459
------------------------------
• [SLOW TEST] [6.192 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:31:06.327
    Jan 12 01:31:06.327: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename var-expansion 01/12/23 01:31:06.328
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:31:06.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:31:06.357
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Jan 12 01:31:06.431: INFO: Waiting up to 2m0s for pod "var-expansion-f54ed9ab-7b0b-4d5b-a776-70271fb7fcf3" in namespace "var-expansion-3227" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 12 01:31:06.434: INFO: Pod "var-expansion-f54ed9ab-7b0b-4d5b-a776-70271fb7fcf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.970419ms
    Jan 12 01:31:08.440: INFO: Pod "var-expansion-f54ed9ab-7b0b-4d5b-a776-70271fb7fcf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00856886s
    Jan 12 01:31:10.439: INFO: Pod "var-expansion-f54ed9ab-7b0b-4d5b-a776-70271fb7fcf3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007182108s
    Jan 12 01:31:10.439: INFO: Pod "var-expansion-f54ed9ab-7b0b-4d5b-a776-70271fb7fcf3" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 12 01:31:10.439: INFO: Deleting pod "var-expansion-f54ed9ab-7b0b-4d5b-a776-70271fb7fcf3" in namespace "var-expansion-3227"
    Jan 12 01:31:10.447: INFO: Wait up to 5m0s for pod "var-expansion-f54ed9ab-7b0b-4d5b-a776-70271fb7fcf3" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:31:12.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3227" for this suite. 01/12/23 01:31:12.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:31:12.524
Jan 12 01:31:12.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename secrets 01/12/23 01:31:12.525
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:31:12.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:31:12.544
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-9a8d4416-7c76-42aa-b08c-108f6a8a9666 01/12/23 01:31:12.567
STEP: Creating a pod to test consume secrets 01/12/23 01:31:12.573
Jan 12 01:31:12.605: INFO: Waiting up to 5m0s for pod "pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054" in namespace "secrets-7424" to be "Succeeded or Failed"
Jan 12 01:31:12.611: INFO: Pod "pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054": Phase="Pending", Reason="", readiness=false. Elapsed: 6.159714ms
Jan 12 01:31:14.615: INFO: Pod "pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010593616s
Jan 12 01:31:16.616: INFO: Pod "pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010723647s
Jan 12 01:31:18.616: INFO: Pod "pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011523927s
STEP: Saw pod success 01/12/23 01:31:18.616
Jan 12 01:31:18.617: INFO: Pod "pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054" satisfied condition "Succeeded or Failed"
Jan 12 01:31:18.619: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054 container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 01:31:18.64
Jan 12 01:31:18.655: INFO: Waiting for pod pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054 to disappear
Jan 12 01:31:18.658: INFO: Pod pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 01:31:18.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7424" for this suite. 01/12/23 01:31:18.663
STEP: Destroying namespace "secret-namespace-7896" for this suite. 01/12/23 01:31:18.693
------------------------------
• [SLOW TEST] [6.198 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:31:12.524
    Jan 12 01:31:12.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename secrets 01/12/23 01:31:12.525
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:31:12.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:31:12.544
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-9a8d4416-7c76-42aa-b08c-108f6a8a9666 01/12/23 01:31:12.567
    STEP: Creating a pod to test consume secrets 01/12/23 01:31:12.573
    Jan 12 01:31:12.605: INFO: Waiting up to 5m0s for pod "pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054" in namespace "secrets-7424" to be "Succeeded or Failed"
    Jan 12 01:31:12.611: INFO: Pod "pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054": Phase="Pending", Reason="", readiness=false. Elapsed: 6.159714ms
    Jan 12 01:31:14.615: INFO: Pod "pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010593616s
    Jan 12 01:31:16.616: INFO: Pod "pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010723647s
    Jan 12 01:31:18.616: INFO: Pod "pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011523927s
    STEP: Saw pod success 01/12/23 01:31:18.616
    Jan 12 01:31:18.617: INFO: Pod "pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054" satisfied condition "Succeeded or Failed"
    Jan 12 01:31:18.619: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054 container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 01:31:18.64
    Jan 12 01:31:18.655: INFO: Waiting for pod pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054 to disappear
    Jan 12 01:31:18.658: INFO: Pod pod-secrets-4adcf3b8-7a10-4c43-9391-8840e2632054 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:31:18.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7424" for this suite. 01/12/23 01:31:18.663
    STEP: Destroying namespace "secret-namespace-7896" for this suite. 01/12/23 01:31:18.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:31:18.724
Jan 12 01:31:18.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pods 01/12/23 01:31:18.725
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:31:18.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:31:18.743
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jan 12 01:31:18.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: creating the pod 01/12/23 01:31:18.746
STEP: submitting the pod to kubernetes 01/12/23 01:31:18.746
Jan 12 01:31:18.809: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9" in namespace "pods-6464" to be "running and ready"
Jan 12 01:31:18.812: INFO: Pod "pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.762893ms
Jan 12 01:31:18.812: INFO: The phase of Pod pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:31:20.817: INFO: Pod "pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008372626s
Jan 12 01:31:20.817: INFO: The phase of Pod pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:31:22.817: INFO: Pod "pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9": Phase="Running", Reason="", readiness=true. Elapsed: 4.00759803s
Jan 12 01:31:22.817: INFO: The phase of Pod pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9 is Running (Ready = true)
Jan 12 01:31:22.817: INFO: Pod "pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 01:31:22.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6464" for this suite. 01/12/23 01:31:22.848
------------------------------
• [4.141 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:31:18.724
    Jan 12 01:31:18.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pods 01/12/23 01:31:18.725
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:31:18.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:31:18.743
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jan 12 01:31:18.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: creating the pod 01/12/23 01:31:18.746
    STEP: submitting the pod to kubernetes 01/12/23 01:31:18.746
    Jan 12 01:31:18.809: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9" in namespace "pods-6464" to be "running and ready"
    Jan 12 01:31:18.812: INFO: Pod "pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.762893ms
    Jan 12 01:31:18.812: INFO: The phase of Pod pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:31:20.817: INFO: Pod "pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008372626s
    Jan 12 01:31:20.817: INFO: The phase of Pod pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:31:22.817: INFO: Pod "pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9": Phase="Running", Reason="", readiness=true. Elapsed: 4.00759803s
    Jan 12 01:31:22.817: INFO: The phase of Pod pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9 is Running (Ready = true)
    Jan 12 01:31:22.817: INFO: Pod "pod-logs-websocket-d07c85eb-cec1-4373-9e4d-54f0cda65bc9" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:31:22.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6464" for this suite. 01/12/23 01:31:22.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:31:22.868
Jan 12 01:31:22.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 01:31:22.869
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:31:22.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:31:22.888
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 01/12/23 01:31:22.89
Jan 12 01:31:22.924: INFO: Waiting up to 5m0s for pod "annotationupdate463f86a5-b829-4521-ad0e-acb07b3a5c4f" in namespace "downward-api-5544" to be "running and ready"
Jan 12 01:31:22.928: INFO: Pod "annotationupdate463f86a5-b829-4521-ad0e-acb07b3a5c4f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.635209ms
Jan 12 01:31:22.928: INFO: The phase of Pod annotationupdate463f86a5-b829-4521-ad0e-acb07b3a5c4f is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:31:24.931: INFO: Pod "annotationupdate463f86a5-b829-4521-ad0e-acb07b3a5c4f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007800104s
Jan 12 01:31:24.932: INFO: The phase of Pod annotationupdate463f86a5-b829-4521-ad0e-acb07b3a5c4f is Running (Ready = true)
Jan 12 01:31:24.932: INFO: Pod "annotationupdate463f86a5-b829-4521-ad0e-acb07b3a5c4f" satisfied condition "running and ready"
Jan 12 01:31:25.459: INFO: Successfully updated pod "annotationupdate463f86a5-b829-4521-ad0e-acb07b3a5c4f"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 01:31:27.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5544" for this suite. 01/12/23 01:31:27.486
------------------------------
• [4.635 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:31:22.868
    Jan 12 01:31:22.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 01:31:22.869
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:31:22.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:31:22.888
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 01/12/23 01:31:22.89
    Jan 12 01:31:22.924: INFO: Waiting up to 5m0s for pod "annotationupdate463f86a5-b829-4521-ad0e-acb07b3a5c4f" in namespace "downward-api-5544" to be "running and ready"
    Jan 12 01:31:22.928: INFO: Pod "annotationupdate463f86a5-b829-4521-ad0e-acb07b3a5c4f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.635209ms
    Jan 12 01:31:22.928: INFO: The phase of Pod annotationupdate463f86a5-b829-4521-ad0e-acb07b3a5c4f is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:31:24.931: INFO: Pod "annotationupdate463f86a5-b829-4521-ad0e-acb07b3a5c4f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007800104s
    Jan 12 01:31:24.932: INFO: The phase of Pod annotationupdate463f86a5-b829-4521-ad0e-acb07b3a5c4f is Running (Ready = true)
    Jan 12 01:31:24.932: INFO: Pod "annotationupdate463f86a5-b829-4521-ad0e-acb07b3a5c4f" satisfied condition "running and ready"
    Jan 12 01:31:25.459: INFO: Successfully updated pod "annotationupdate463f86a5-b829-4521-ad0e-acb07b3a5c4f"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:31:27.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5544" for this suite. 01/12/23 01:31:27.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:31:27.52
Jan 12 01:31:27.520: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename subpath 01/12/23 01:31:27.523
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:31:27.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:31:27.544
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/12/23 01:31:27.547
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-ggq5 01/12/23 01:31:27.556
STEP: Creating a pod to test atomic-volume-subpath 01/12/23 01:31:27.556
Jan 12 01:31:27.595: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ggq5" in namespace "subpath-2195" to be "Succeeded or Failed"
Jan 12 01:31:27.599: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.86921ms
Jan 12 01:31:29.604: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008835171s
Jan 12 01:31:31.605: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 4.009418259s
Jan 12 01:31:33.604: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 6.00854877s
Jan 12 01:31:35.605: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 8.00936047s
Jan 12 01:31:37.604: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 10.008691781s
Jan 12 01:31:39.604: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 12.008277815s
Jan 12 01:31:41.605: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 14.009755135s
Jan 12 01:31:43.604: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 16.008373744s
Jan 12 01:31:45.604: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 18.008806495s
Jan 12 01:31:47.604: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 20.008705101s
Jan 12 01:31:49.605: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 22.009562294s
Jan 12 01:31:51.605: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=false. Elapsed: 24.009289181s
Jan 12 01:31:53.605: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.01024028s
STEP: Saw pod success 01/12/23 01:31:53.606
Jan 12 01:31:53.606: INFO: Pod "pod-subpath-test-downwardapi-ggq5" satisfied condition "Succeeded or Failed"
Jan 12 01:31:53.609: INFO: Trying to get logs from node eqx04-flash06 pod pod-subpath-test-downwardapi-ggq5 container test-container-subpath-downwardapi-ggq5: <nil>
STEP: delete the pod 01/12/23 01:31:53.619
Jan 12 01:31:53.634: INFO: Waiting for pod pod-subpath-test-downwardapi-ggq5 to disappear
Jan 12 01:31:53.638: INFO: Pod pod-subpath-test-downwardapi-ggq5 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-ggq5 01/12/23 01:31:53.638
Jan 12 01:31:53.638: INFO: Deleting pod "pod-subpath-test-downwardapi-ggq5" in namespace "subpath-2195"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 12 01:31:53.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2195" for this suite. 01/12/23 01:31:53.645
------------------------------
• [SLOW TEST] [26.142 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:31:27.52
    Jan 12 01:31:27.520: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename subpath 01/12/23 01:31:27.523
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:31:27.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:31:27.544
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/12/23 01:31:27.547
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-ggq5 01/12/23 01:31:27.556
    STEP: Creating a pod to test atomic-volume-subpath 01/12/23 01:31:27.556
    Jan 12 01:31:27.595: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ggq5" in namespace "subpath-2195" to be "Succeeded or Failed"
    Jan 12 01:31:27.599: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.86921ms
    Jan 12 01:31:29.604: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008835171s
    Jan 12 01:31:31.605: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 4.009418259s
    Jan 12 01:31:33.604: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 6.00854877s
    Jan 12 01:31:35.605: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 8.00936047s
    Jan 12 01:31:37.604: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 10.008691781s
    Jan 12 01:31:39.604: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 12.008277815s
    Jan 12 01:31:41.605: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 14.009755135s
    Jan 12 01:31:43.604: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 16.008373744s
    Jan 12 01:31:45.604: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 18.008806495s
    Jan 12 01:31:47.604: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 20.008705101s
    Jan 12 01:31:49.605: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=true. Elapsed: 22.009562294s
    Jan 12 01:31:51.605: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Running", Reason="", readiness=false. Elapsed: 24.009289181s
    Jan 12 01:31:53.605: INFO: Pod "pod-subpath-test-downwardapi-ggq5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.01024028s
    STEP: Saw pod success 01/12/23 01:31:53.606
    Jan 12 01:31:53.606: INFO: Pod "pod-subpath-test-downwardapi-ggq5" satisfied condition "Succeeded or Failed"
    Jan 12 01:31:53.609: INFO: Trying to get logs from node eqx04-flash06 pod pod-subpath-test-downwardapi-ggq5 container test-container-subpath-downwardapi-ggq5: <nil>
    STEP: delete the pod 01/12/23 01:31:53.619
    Jan 12 01:31:53.634: INFO: Waiting for pod pod-subpath-test-downwardapi-ggq5 to disappear
    Jan 12 01:31:53.638: INFO: Pod pod-subpath-test-downwardapi-ggq5 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-ggq5 01/12/23 01:31:53.638
    Jan 12 01:31:53.638: INFO: Deleting pod "pod-subpath-test-downwardapi-ggq5" in namespace "subpath-2195"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:31:53.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2195" for this suite. 01/12/23 01:31:53.645
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:31:53.663
Jan 12 01:31:53.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename svcaccounts 01/12/23 01:31:53.664
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:31:53.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:31:53.683
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Jan 12 01:31:53.751: INFO: Waiting up to 5m0s for pod "pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6" in namespace "svcaccounts-9975" to be "running"
Jan 12 01:31:53.754: INFO: Pod "pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.893584ms
Jan 12 01:31:55.760: INFO: Pod "pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00905159s
Jan 12 01:31:57.759: INFO: Pod "pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6": Phase="Running", Reason="", readiness=true. Elapsed: 4.007576603s
Jan 12 01:31:57.759: INFO: Pod "pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6" satisfied condition "running"
STEP: reading a file in the container 01/12/23 01:31:57.759
Jan 12 01:31:57.759: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9975 pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/12/23 01:31:57.962
Jan 12 01:31:57.962: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9975 pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/12/23 01:31:58.159
Jan 12 01:31:58.159: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9975 pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 12 01:31:58.375: INFO: Got root ca configmap in namespace "svcaccounts-9975"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 12 01:31:58.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9975" for this suite. 01/12/23 01:31:58.384
------------------------------
• [4.786 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:31:53.663
    Jan 12 01:31:53.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename svcaccounts 01/12/23 01:31:53.664
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:31:53.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:31:53.683
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Jan 12 01:31:53.751: INFO: Waiting up to 5m0s for pod "pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6" in namespace "svcaccounts-9975" to be "running"
    Jan 12 01:31:53.754: INFO: Pod "pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.893584ms
    Jan 12 01:31:55.760: INFO: Pod "pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00905159s
    Jan 12 01:31:57.759: INFO: Pod "pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6": Phase="Running", Reason="", readiness=true. Elapsed: 4.007576603s
    Jan 12 01:31:57.759: INFO: Pod "pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6" satisfied condition "running"
    STEP: reading a file in the container 01/12/23 01:31:57.759
    Jan 12 01:31:57.759: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9975 pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/12/23 01:31:57.962
    Jan 12 01:31:57.962: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9975 pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/12/23 01:31:58.159
    Jan 12 01:31:58.159: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9975 pod-service-account-0b31f667-224c-4dcc-b3ca-7f81df2368c6 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 12 01:31:58.375: INFO: Got root ca configmap in namespace "svcaccounts-9975"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:31:58.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9975" for this suite. 01/12/23 01:31:58.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:31:58.451
Jan 12 01:31:58.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename secrets 01/12/23 01:31:58.452
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:31:58.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:31:58.47
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-f750258b-d5df-409d-8254-e6b103aea082 01/12/23 01:31:58.473
STEP: Creating a pod to test consume secrets 01/12/23 01:31:58.478
Jan 12 01:31:58.612: INFO: Waiting up to 5m0s for pod "pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257" in namespace "secrets-8559" to be "Succeeded or Failed"
Jan 12 01:31:58.616: INFO: Pod "pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257": Phase="Pending", Reason="", readiness=false. Elapsed: 3.385748ms
Jan 12 01:32:00.620: INFO: Pod "pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00822407s
Jan 12 01:32:02.620: INFO: Pod "pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008229387s
Jan 12 01:32:04.620: INFO: Pod "pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0072812s
STEP: Saw pod success 01/12/23 01:32:04.62
Jan 12 01:32:04.620: INFO: Pod "pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257" satisfied condition "Succeeded or Failed"
Jan 12 01:32:04.623: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257 container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 01:32:04.632
Jan 12 01:32:04.643: INFO: Waiting for pod pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257 to disappear
Jan 12 01:32:04.646: INFO: Pod pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 01:32:04.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8559" for this suite. 01/12/23 01:32:04.652
------------------------------
• [SLOW TEST] [6.222 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:31:58.451
    Jan 12 01:31:58.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename secrets 01/12/23 01:31:58.452
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:31:58.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:31:58.47
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-f750258b-d5df-409d-8254-e6b103aea082 01/12/23 01:31:58.473
    STEP: Creating a pod to test consume secrets 01/12/23 01:31:58.478
    Jan 12 01:31:58.612: INFO: Waiting up to 5m0s for pod "pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257" in namespace "secrets-8559" to be "Succeeded or Failed"
    Jan 12 01:31:58.616: INFO: Pod "pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257": Phase="Pending", Reason="", readiness=false. Elapsed: 3.385748ms
    Jan 12 01:32:00.620: INFO: Pod "pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00822407s
    Jan 12 01:32:02.620: INFO: Pod "pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008229387s
    Jan 12 01:32:04.620: INFO: Pod "pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0072812s
    STEP: Saw pod success 01/12/23 01:32:04.62
    Jan 12 01:32:04.620: INFO: Pod "pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257" satisfied condition "Succeeded or Failed"
    Jan 12 01:32:04.623: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257 container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 01:32:04.632
    Jan 12 01:32:04.643: INFO: Waiting for pod pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257 to disappear
    Jan 12 01:32:04.646: INFO: Pod pod-secrets-fb2a268b-756b-4b78-ab19-67cbbf109257 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:32:04.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8559" for this suite. 01/12/23 01:32:04.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:32:04.674
Jan 12 01:32:04.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename aggregator 01/12/23 01:32:04.675
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:32:04.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:32:04.693
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 12 01:32:04.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/12/23 01:32:04.695
Jan 12 01:32:05.249: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 12 01:32:07.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:32:09.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:32:11.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:32:13.295: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:32:15.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:32:17.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:32:19.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:32:21.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:32:23.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:32:25.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:32:27.426: INFO: Waited 121.138131ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/12/23 01:32:27.478
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/12/23 01:32:27.481
STEP: List APIServices 01/12/23 01:32:27.489
Jan 12 01:32:27.496: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jan 12 01:32:27.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-8906" for this suite. 01/12/23 01:32:27.865
------------------------------
• [SLOW TEST] [23.258 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:32:04.674
    Jan 12 01:32:04.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename aggregator 01/12/23 01:32:04.675
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:32:04.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:32:04.693
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 12 01:32:04.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/12/23 01:32:04.695
    Jan 12 01:32:05.249: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan 12 01:32:07.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:32:09.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:32:11.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:32:13.295: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:32:15.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:32:17.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:32:19.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:32:21.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:32:23.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:32:25.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 32, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:32:27.426: INFO: Waited 121.138131ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/12/23 01:32:27.478
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/12/23 01:32:27.481
    STEP: List APIServices 01/12/23 01:32:27.489
    Jan 12 01:32:27.496: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:32:27.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-8906" for this suite. 01/12/23 01:32:27.865
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:32:27.932
Jan 12 01:32:27.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename deployment 01/12/23 01:32:27.933
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:32:27.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:32:27.953
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 12 01:32:27.956: INFO: Creating deployment "webserver-deployment"
Jan 12 01:32:28.024: INFO: Waiting for observed generation 1
Jan 12 01:32:30.036: INFO: Waiting for all required pods to come up
Jan 12 01:32:30.042: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/12/23 01:32:30.042
Jan 12 01:32:30.042: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-z8rx2" in namespace "deployment-4836" to be "running"
Jan 12 01:32:30.042: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-24mjr" in namespace "deployment-4836" to be "running"
Jan 12 01:32:30.042: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-44642" in namespace "deployment-4836" to be "running"
Jan 12 01:32:30.042: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4742n" in namespace "deployment-4836" to be "running"
Jan 12 01:32:30.043: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dnz2m" in namespace "deployment-4836" to be "running"
Jan 12 01:32:30.043: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-xtdvf" in namespace "deployment-4836" to be "running"
Jan 12 01:32:30.043: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-m244q" in namespace "deployment-4836" to be "running"
Jan 12 01:32:30.043: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hd9lz" in namespace "deployment-4836" to be "running"
Jan 12 01:32:30.043: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-tkzkd" in namespace "deployment-4836" to be "running"
Jan 12 01:32:30.043: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-j5jrq" in namespace "deployment-4836" to be "running"
Jan 12 01:32:30.046: INFO: Pod "webserver-deployment-7f5969cbc7-z8rx2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.378673ms
Jan 12 01:32:30.048: INFO: Pod "webserver-deployment-7f5969cbc7-xtdvf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.275936ms
Jan 12 01:32:30.048: INFO: Pod "webserver-deployment-7f5969cbc7-hd9lz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.417857ms
Jan 12 01:32:30.048: INFO: Pod "webserver-deployment-7f5969cbc7-j5jrq": Phase="Pending", Reason="", readiness=false. Elapsed: 5.022203ms
Jan 12 01:32:30.048: INFO: Pod "webserver-deployment-7f5969cbc7-4742n": Phase="Pending", Reason="", readiness=false. Elapsed: 5.903946ms
Jan 12 01:32:30.049: INFO: Pod "webserver-deployment-7f5969cbc7-tkzkd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.424304ms
Jan 12 01:32:30.049: INFO: Pod "webserver-deployment-7f5969cbc7-m244q": Phase="Pending", Reason="", readiness=false. Elapsed: 5.873262ms
Jan 12 01:32:30.049: INFO: Pod "webserver-deployment-7f5969cbc7-24mjr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.558588ms
Jan 12 01:32:30.049: INFO: Pod "webserver-deployment-7f5969cbc7-44642": Phase="Pending", Reason="", readiness=false. Elapsed: 6.411042ms
Jan 12 01:32:30.049: INFO: Pod "webserver-deployment-7f5969cbc7-dnz2m": Phase="Pending", Reason="", readiness=false. Elapsed: 6.096527ms
Jan 12 01:32:32.051: INFO: Pod "webserver-deployment-7f5969cbc7-z8rx2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009246094s
Jan 12 01:32:32.051: INFO: Pod "webserver-deployment-7f5969cbc7-z8rx2" satisfied condition "running"
Jan 12 01:32:32.052: INFO: Pod "webserver-deployment-7f5969cbc7-xtdvf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008875046s
Jan 12 01:32:32.052: INFO: Pod "webserver-deployment-7f5969cbc7-j5jrq": Phase="Running", Reason="", readiness=true. Elapsed: 2.008406588s
Jan 12 01:32:32.052: INFO: Pod "webserver-deployment-7f5969cbc7-j5jrq" satisfied condition "running"
Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-hd9lz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009932736s
Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-dnz2m": Phase="Running", Reason="", readiness=true. Elapsed: 2.010138129s
Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-dnz2m" satisfied condition "running"
Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-4742n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010344182s
Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-44642": Phase="Running", Reason="", readiness=true. Elapsed: 2.010590325s
Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-44642" satisfied condition "running"
Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-24mjr": Phase="Running", Reason="", readiness=true. Elapsed: 2.01110975s
Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-24mjr" satisfied condition "running"
Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-tkzkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010238744s
Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-tkzkd" satisfied condition "running"
Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-m244q": Phase="Running", Reason="", readiness=true. Elapsed: 2.010616902s
Jan 12 01:32:32.054: INFO: Pod "webserver-deployment-7f5969cbc7-m244q" satisfied condition "running"
Jan 12 01:32:34.052: INFO: Pod "webserver-deployment-7f5969cbc7-xtdvf": Phase="Running", Reason="", readiness=true. Elapsed: 4.009801082s
Jan 12 01:32:34.053: INFO: Pod "webserver-deployment-7f5969cbc7-xtdvf" satisfied condition "running"
Jan 12 01:32:34.052: INFO: Pod "webserver-deployment-7f5969cbc7-4742n": Phase="Running", Reason="", readiness=true. Elapsed: 4.009959557s
Jan 12 01:32:34.053: INFO: Pod "webserver-deployment-7f5969cbc7-4742n" satisfied condition "running"
Jan 12 01:32:34.053: INFO: Pod "webserver-deployment-7f5969cbc7-hd9lz": Phase="Running", Reason="", readiness=true. Elapsed: 4.009832588s
Jan 12 01:32:34.053: INFO: Pod "webserver-deployment-7f5969cbc7-hd9lz" satisfied condition "running"
Jan 12 01:32:34.053: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 12 01:32:34.059: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 12 01:32:34.068: INFO: Updating deployment webserver-deployment
Jan 12 01:32:34.068: INFO: Waiting for observed generation 2
Jan 12 01:32:36.075: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 12 01:32:36.078: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 12 01:32:36.081: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 12 01:32:36.088: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 12 01:32:36.089: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 12 01:32:36.091: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 12 01:32:36.097: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 12 01:32:36.097: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 12 01:32:36.106: INFO: Updating deployment webserver-deployment
Jan 12 01:32:36.106: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 12 01:32:36.113: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 12 01:32:36.116: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 01:32:36.127: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4836  9835b0d7-2d01-4415-b32a-671b05e48f16 20164998 3 2023-01-12 01:32:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-12 01:32:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:32:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002886658 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-12 01:32:34 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-12 01:32:36 +0000 UTC,LastTransitionTime:2023-01-12 01:32:36 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 12 01:32:36.132: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-4836  c02ce747-2873-41e8-9475-5058b5847f47 20164996 3 2023-01-12 01:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 9835b0d7-2d01-4415-b32a-671b05e48f16 0xc002886b37 0xc002886b38}] [] [{kube-controller-manager Update apps/v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-12 01:32:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9835b0d7-2d01-4415-b32a-671b05e48f16\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002886bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 12 01:32:36.132: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 12 01:32:36.132: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-4836  72b75ddb-2b64-4198-a8b0-bb9fd3494f07 20164994 3 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 9835b0d7-2d01-4415-b32a-671b05e48f16 0xc002886a47 0xc002886a48}] [] [{kube-controller-manager Update apps/v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-12 01:32:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9835b0d7-2d01-4415-b32a-671b05e48f16\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002886ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 12 01:32:36.137: INFO: Pod "webserver-deployment-7f5969cbc7-24mjr" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-24mjr webserver-deployment-7f5969cbc7- deployment-4836  724f65ec-f02e-46b0-a44e-3b83e6e23b18 20164844 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3dc16f830118dff91f8fae97d362755c7deb95453024953612a0622b908d922d cni.projectcalico.org/podIP:172.21.88.188/32 cni.projectcalico.org/podIPs:172.21.88.188/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.188"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.188"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc002887287 0xc002887288}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fbd42,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fbd42,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.188,StartTime:2023-01-12 01:32:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://19cd10d27163a8a374e6540d5b0a14345e65c45f38dcf60bbbce8adb7f8d2565,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 01:32:36.137: INFO: Pod "webserver-deployment-7f5969cbc7-44642" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-44642 webserver-deployment-7f5969cbc7- deployment-4836  befa79f8-3c93-4c0e-8f90-144927a6ec59 20164856 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:67fa844ac408eaa92eb3a7ca2b384be633972c14350e1ca899f5c0b4c4a827f5 cni.projectcalico.org/podIP:172.21.88.186/32 cni.projectcalico.org/podIPs:172.21.88.186/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.186"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.186"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc0028874e7 0xc0028874e8}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mslh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mslh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.186,StartTime:2023-01-12 01:32:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://a2d0d69135b7303767e16e5dbd5c00271588734500896baf504c8b880e2b6f28,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 01:32:36.137: INFO: Pod "webserver-deployment-7f5969cbc7-dnz2m" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dnz2m webserver-deployment-7f5969cbc7- deployment-4836  3d70e9a7-6693-4e76-8c1c-dae83e76c60f 20164814 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7d902163320d755a171e1bbdaa793df77e825134d5a324385086052179174637 cni.projectcalico.org/podIP:172.21.117.132/32 cni.projectcalico.org/podIPs:172.21.117.132/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.117.132"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.117.132"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc002887717 0xc002887718}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.117.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-phl2p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-phl2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.117.132,StartTime:2023-01-12 01:32:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://06d34972999d30efee8a5596a6745fb0ae01690fe4a959cc95b4342ed26e6188,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.117.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 01:32:36.138: INFO: Pod "webserver-deployment-7f5969cbc7-hd9lz" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hd9lz webserver-deployment-7f5969cbc7- deployment-4836  4d779eda-2810-4867-8492-ddf850a15fb4 20164878 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:834a341db91eff6da7641b76357dc8bd5847918f10ab83a44307480196b6cb39 cni.projectcalico.org/podIP:172.21.88.174/32 cni.projectcalico.org/podIPs:172.21.88.174/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.174"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.174"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc002887967 0xc002887968}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttjfn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttjfn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.174,StartTime:2023-01-12 01:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://60aaff3ce6eaea43f0bc19dc4d12f7b7e62aeb36c61840085610189ab57fc66c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 01:32:36.138: INFO: Pod "webserver-deployment-7f5969cbc7-j5jrq" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-j5jrq webserver-deployment-7f5969cbc7- deployment-4836  f9b41a41-ff32-4e47-81e1-ded7f1e4335e 20164819 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4c43dcdd9463a35eeefcdce1d0771b004c9d4fd073ee5efb30768c56a9f35d84 cni.projectcalico.org/podIP:172.21.117.138/32 cni.projectcalico.org/podIPs:172.21.117.138/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.117.138"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.117.138"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc002887b97 0xc002887b98}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.117.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4z2ms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4z2ms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.117.138,StartTime:2023-01-12 01:32:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://1a7b79df108bfaa986c2f669acc31447a05796955f949e8d0373fc9e24df5c64,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.117.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 01:32:36.138: INFO: Pod "webserver-deployment-7f5969cbc7-m244q" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m244q webserver-deployment-7f5969cbc7- deployment-4836  af767f2b-c8da-409a-9cea-451d261da892 20164859 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ac4cb7456bdcba9fc5dfbdaa7c68c3758ab76a0cde87e56a078395a43180ddff cni.projectcalico.org/podIP:172.21.88.185/32 cni.projectcalico.org/podIPs:172.21.88.185/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.185"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.185"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc002887de7 0xc002887de8}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqmxk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqmxk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.185,StartTime:2023-01-12 01:32:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://f4a4c4723e04061c40655c3e20d55e6b5968fbda21c05dcc58997ef3d3bce4d3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 01:32:36.138: INFO: Pod "webserver-deployment-7f5969cbc7-tkzkd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tkzkd webserver-deployment-7f5969cbc7- deployment-4836  7893af1f-c0ea-4f9a-8fa9-c8eaa4727d01 20164870 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c945cea01cc335d7c728ec0b83847611b3c334fc4a172137e94a0116ad83eb19 cni.projectcalico.org/podIP:172.21.117.134/32 cni.projectcalico.org/podIPs:172.21.117.134/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.117.134"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.117.134"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc002112017 0xc002112018}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.117.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwpw4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwpw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.117.134,StartTime:2023-01-12 01:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://87fee791d553e9d06cd2386e687e06253c50293d4e8f8300b6afa114cfb32333,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.117.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 01:32:36.138: INFO: Pod "webserver-deployment-7f5969cbc7-z8rx2" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-z8rx2 webserver-deployment-7f5969cbc7- deployment-4836  3a04be1b-78de-4d3b-ae58-d36d89507fad 20164873 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:42ff6f8428d24b2dabf2096128a5187dd2fe119921d0c39442695f3040f92c77 cni.projectcalico.org/podIP:172.21.117.170/32 cni.projectcalico.org/podIPs:172.21.117.170/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.117.170"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.117.170"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc002112927 0xc002112928}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.117.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mfwhc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mfwhc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.117.170,StartTime:2023-01-12 01:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://03614df3efdcb4f5351fcbfc028744ab4179c6c5383c4eb6ba994acbe813494b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.117.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 01:32:36.139: INFO: Pod "webserver-deployment-d9f79cb5-fw22p" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fw22p webserver-deployment-d9f79cb5- deployment-4836  67995af7-f62f-4272-a7fc-187a70c586f8 20164990 0 2023-01-12 01:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:35179479ce1bde9d4e25a135f79db07cc985148df77de31112441967e788e2f9 cni.projectcalico.org/podIP:172.21.88.180/32 cni.projectcalico.org/podIPs:172.21.88.180/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.180"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.180"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c02ce747-2873-41e8-9475-5058b5847f47 0xc002113057 0xc002113058}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c02ce747-2873-41e8-9475-5058b5847f47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jsvg5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jsvg5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:,StartTime:2023-01-12 01:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 01:32:36.139: INFO: Pod "webserver-deployment-d9f79cb5-qjv5m" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qjv5m webserver-deployment-d9f79cb5- deployment-4836  319b9eee-2b39-4dc7-87a6-6ad9a2fe6ccf 20164978 0 2023-01-12 01:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f7bcaf6ef24fb8b752891abbeac170c24bc42049017accd8cff1f6b1ffb97657 cni.projectcalico.org/podIP:172.21.88.157/32 cni.projectcalico.org/podIPs:172.21.88.157/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.157"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.157"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c02ce747-2873-41e8-9475-5058b5847f47 0xc002113337 0xc002113338}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c02ce747-2873-41e8-9475-5058b5847f47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5tkdw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5tkdw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:,StartTime:2023-01-12 01:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 01:32:36.139: INFO: Pod "webserver-deployment-d9f79cb5-rxbsn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rxbsn webserver-deployment-d9f79cb5- deployment-4836  6212f23a-64d7-4773-92b4-986ab2c99487 20164973 0 2023-01-12 01:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2f0324212a53e7489a35f3e595ef6a8592e30206f88671c7a5f61d854948792e cni.projectcalico.org/podIP:172.21.117.154/32 cni.projectcalico.org/podIPs:172.21.117.154/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.117.154"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.117.154"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c02ce747-2873-41e8-9475-5058b5847f47 0xc002113577 0xc002113578}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c02ce747-2873-41e8-9475-5058b5847f47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n5c5b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n5c5b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:,StartTime:2023-01-12 01:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 01:32:36.139: INFO: Pod "webserver-deployment-d9f79cb5-w9w98" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w9w98 webserver-deployment-d9f79cb5- deployment-4836  301e26c5-9604-4ffc-b029-332fecf2b773 20164984 0 2023-01-12 01:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:c8021c3fb3cd224c9afb4ad442bec13a544054866d04fc3ba1a1a5de250ed4a0 cni.projectcalico.org/podIP:172.21.117.155/32 cni.projectcalico.org/podIPs:172.21.117.155/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.117.155"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.117.155"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c02ce747-2873-41e8-9475-5058b5847f47 0xc0021137b7 0xc0021137b8}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c02ce747-2873-41e8-9475-5058b5847f47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5rpgq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5rpgq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:,StartTime:2023-01-12 01:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 01:32:36.139: INFO: Pod "webserver-deployment-d9f79cb5-x4tnb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-x4tnb webserver-deployment-d9f79cb5- deployment-4836  0a4dab4f-56ee-49bb-9af3-f92f6b2362a1 20164959 0 2023-01-12 01:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c02ce747-2873-41e8-9475-5058b5847f47 0xc002113a07 0xc002113a08}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c02ce747-2873-41e8-9475-5058b5847f47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2jtt2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2jtt2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:,StartTime:2023-01-12 01:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 01:32:36.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4836" for this suite. 01/12/23 01:32:36.144
------------------------------
• [SLOW TEST] [8.295 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:32:27.932
    Jan 12 01:32:27.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename deployment 01/12/23 01:32:27.933
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:32:27.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:32:27.953
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 12 01:32:27.956: INFO: Creating deployment "webserver-deployment"
    Jan 12 01:32:28.024: INFO: Waiting for observed generation 1
    Jan 12 01:32:30.036: INFO: Waiting for all required pods to come up
    Jan 12 01:32:30.042: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/12/23 01:32:30.042
    Jan 12 01:32:30.042: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-z8rx2" in namespace "deployment-4836" to be "running"
    Jan 12 01:32:30.042: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-24mjr" in namespace "deployment-4836" to be "running"
    Jan 12 01:32:30.042: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-44642" in namespace "deployment-4836" to be "running"
    Jan 12 01:32:30.042: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4742n" in namespace "deployment-4836" to be "running"
    Jan 12 01:32:30.043: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dnz2m" in namespace "deployment-4836" to be "running"
    Jan 12 01:32:30.043: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-xtdvf" in namespace "deployment-4836" to be "running"
    Jan 12 01:32:30.043: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-m244q" in namespace "deployment-4836" to be "running"
    Jan 12 01:32:30.043: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hd9lz" in namespace "deployment-4836" to be "running"
    Jan 12 01:32:30.043: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-tkzkd" in namespace "deployment-4836" to be "running"
    Jan 12 01:32:30.043: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-j5jrq" in namespace "deployment-4836" to be "running"
    Jan 12 01:32:30.046: INFO: Pod "webserver-deployment-7f5969cbc7-z8rx2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.378673ms
    Jan 12 01:32:30.048: INFO: Pod "webserver-deployment-7f5969cbc7-xtdvf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.275936ms
    Jan 12 01:32:30.048: INFO: Pod "webserver-deployment-7f5969cbc7-hd9lz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.417857ms
    Jan 12 01:32:30.048: INFO: Pod "webserver-deployment-7f5969cbc7-j5jrq": Phase="Pending", Reason="", readiness=false. Elapsed: 5.022203ms
    Jan 12 01:32:30.048: INFO: Pod "webserver-deployment-7f5969cbc7-4742n": Phase="Pending", Reason="", readiness=false. Elapsed: 5.903946ms
    Jan 12 01:32:30.049: INFO: Pod "webserver-deployment-7f5969cbc7-tkzkd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.424304ms
    Jan 12 01:32:30.049: INFO: Pod "webserver-deployment-7f5969cbc7-m244q": Phase="Pending", Reason="", readiness=false. Elapsed: 5.873262ms
    Jan 12 01:32:30.049: INFO: Pod "webserver-deployment-7f5969cbc7-24mjr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.558588ms
    Jan 12 01:32:30.049: INFO: Pod "webserver-deployment-7f5969cbc7-44642": Phase="Pending", Reason="", readiness=false. Elapsed: 6.411042ms
    Jan 12 01:32:30.049: INFO: Pod "webserver-deployment-7f5969cbc7-dnz2m": Phase="Pending", Reason="", readiness=false. Elapsed: 6.096527ms
    Jan 12 01:32:32.051: INFO: Pod "webserver-deployment-7f5969cbc7-z8rx2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009246094s
    Jan 12 01:32:32.051: INFO: Pod "webserver-deployment-7f5969cbc7-z8rx2" satisfied condition "running"
    Jan 12 01:32:32.052: INFO: Pod "webserver-deployment-7f5969cbc7-xtdvf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008875046s
    Jan 12 01:32:32.052: INFO: Pod "webserver-deployment-7f5969cbc7-j5jrq": Phase="Running", Reason="", readiness=true. Elapsed: 2.008406588s
    Jan 12 01:32:32.052: INFO: Pod "webserver-deployment-7f5969cbc7-j5jrq" satisfied condition "running"
    Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-hd9lz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009932736s
    Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-dnz2m": Phase="Running", Reason="", readiness=true. Elapsed: 2.010138129s
    Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-dnz2m" satisfied condition "running"
    Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-4742n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010344182s
    Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-44642": Phase="Running", Reason="", readiness=true. Elapsed: 2.010590325s
    Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-44642" satisfied condition "running"
    Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-24mjr": Phase="Running", Reason="", readiness=true. Elapsed: 2.01110975s
    Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-24mjr" satisfied condition "running"
    Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-tkzkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010238744s
    Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-tkzkd" satisfied condition "running"
    Jan 12 01:32:32.053: INFO: Pod "webserver-deployment-7f5969cbc7-m244q": Phase="Running", Reason="", readiness=true. Elapsed: 2.010616902s
    Jan 12 01:32:32.054: INFO: Pod "webserver-deployment-7f5969cbc7-m244q" satisfied condition "running"
    Jan 12 01:32:34.052: INFO: Pod "webserver-deployment-7f5969cbc7-xtdvf": Phase="Running", Reason="", readiness=true. Elapsed: 4.009801082s
    Jan 12 01:32:34.053: INFO: Pod "webserver-deployment-7f5969cbc7-xtdvf" satisfied condition "running"
    Jan 12 01:32:34.052: INFO: Pod "webserver-deployment-7f5969cbc7-4742n": Phase="Running", Reason="", readiness=true. Elapsed: 4.009959557s
    Jan 12 01:32:34.053: INFO: Pod "webserver-deployment-7f5969cbc7-4742n" satisfied condition "running"
    Jan 12 01:32:34.053: INFO: Pod "webserver-deployment-7f5969cbc7-hd9lz": Phase="Running", Reason="", readiness=true. Elapsed: 4.009832588s
    Jan 12 01:32:34.053: INFO: Pod "webserver-deployment-7f5969cbc7-hd9lz" satisfied condition "running"
    Jan 12 01:32:34.053: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 12 01:32:34.059: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 12 01:32:34.068: INFO: Updating deployment webserver-deployment
    Jan 12 01:32:34.068: INFO: Waiting for observed generation 2
    Jan 12 01:32:36.075: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 12 01:32:36.078: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 12 01:32:36.081: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 12 01:32:36.088: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 12 01:32:36.089: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 12 01:32:36.091: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 12 01:32:36.097: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 12 01:32:36.097: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 12 01:32:36.106: INFO: Updating deployment webserver-deployment
    Jan 12 01:32:36.106: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 12 01:32:36.113: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 12 01:32:36.116: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 01:32:36.127: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-4836  9835b0d7-2d01-4415-b32a-671b05e48f16 20164998 3 2023-01-12 01:32:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-12 01:32:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:32:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002886658 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-12 01:32:34 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-12 01:32:36 +0000 UTC,LastTransitionTime:2023-01-12 01:32:36 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 12 01:32:36.132: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-4836  c02ce747-2873-41e8-9475-5058b5847f47 20164996 3 2023-01-12 01:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 9835b0d7-2d01-4415-b32a-671b05e48f16 0xc002886b37 0xc002886b38}] [] [{kube-controller-manager Update apps/v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-12 01:32:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9835b0d7-2d01-4415-b32a-671b05e48f16\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002886bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 01:32:36.132: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 12 01:32:36.132: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-4836  72b75ddb-2b64-4198-a8b0-bb9fd3494f07 20164994 3 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 9835b0d7-2d01-4415-b32a-671b05e48f16 0xc002886a47 0xc002886a48}] [] [{kube-controller-manager Update apps/v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-12 01:32:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9835b0d7-2d01-4415-b32a-671b05e48f16\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002886ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 01:32:36.137: INFO: Pod "webserver-deployment-7f5969cbc7-24mjr" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-24mjr webserver-deployment-7f5969cbc7- deployment-4836  724f65ec-f02e-46b0-a44e-3b83e6e23b18 20164844 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3dc16f830118dff91f8fae97d362755c7deb95453024953612a0622b908d922d cni.projectcalico.org/podIP:172.21.88.188/32 cni.projectcalico.org/podIPs:172.21.88.188/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.188"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.188"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc002887287 0xc002887288}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fbd42,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fbd42,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.188,StartTime:2023-01-12 01:32:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://19cd10d27163a8a374e6540d5b0a14345e65c45f38dcf60bbbce8adb7f8d2565,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 01:32:36.137: INFO: Pod "webserver-deployment-7f5969cbc7-44642" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-44642 webserver-deployment-7f5969cbc7- deployment-4836  befa79f8-3c93-4c0e-8f90-144927a6ec59 20164856 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:67fa844ac408eaa92eb3a7ca2b384be633972c14350e1ca899f5c0b4c4a827f5 cni.projectcalico.org/podIP:172.21.88.186/32 cni.projectcalico.org/podIPs:172.21.88.186/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.186"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.186"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc0028874e7 0xc0028874e8}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mslh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mslh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.186,StartTime:2023-01-12 01:32:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://a2d0d69135b7303767e16e5dbd5c00271588734500896baf504c8b880e2b6f28,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 01:32:36.137: INFO: Pod "webserver-deployment-7f5969cbc7-dnz2m" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dnz2m webserver-deployment-7f5969cbc7- deployment-4836  3d70e9a7-6693-4e76-8c1c-dae83e76c60f 20164814 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7d902163320d755a171e1bbdaa793df77e825134d5a324385086052179174637 cni.projectcalico.org/podIP:172.21.117.132/32 cni.projectcalico.org/podIPs:172.21.117.132/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.117.132"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.117.132"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc002887717 0xc002887718}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.117.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-phl2p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-phl2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.117.132,StartTime:2023-01-12 01:32:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://06d34972999d30efee8a5596a6745fb0ae01690fe4a959cc95b4342ed26e6188,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.117.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 01:32:36.138: INFO: Pod "webserver-deployment-7f5969cbc7-hd9lz" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hd9lz webserver-deployment-7f5969cbc7- deployment-4836  4d779eda-2810-4867-8492-ddf850a15fb4 20164878 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:834a341db91eff6da7641b76357dc8bd5847918f10ab83a44307480196b6cb39 cni.projectcalico.org/podIP:172.21.88.174/32 cni.projectcalico.org/podIPs:172.21.88.174/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.174"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.174"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc002887967 0xc002887968}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttjfn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttjfn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.174,StartTime:2023-01-12 01:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://60aaff3ce6eaea43f0bc19dc4d12f7b7e62aeb36c61840085610189ab57fc66c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 01:32:36.138: INFO: Pod "webserver-deployment-7f5969cbc7-j5jrq" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-j5jrq webserver-deployment-7f5969cbc7- deployment-4836  f9b41a41-ff32-4e47-81e1-ded7f1e4335e 20164819 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4c43dcdd9463a35eeefcdce1d0771b004c9d4fd073ee5efb30768c56a9f35d84 cni.projectcalico.org/podIP:172.21.117.138/32 cni.projectcalico.org/podIPs:172.21.117.138/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.117.138"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.117.138"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc002887b97 0xc002887b98}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.117.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4z2ms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4z2ms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.117.138,StartTime:2023-01-12 01:32:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://1a7b79df108bfaa986c2f669acc31447a05796955f949e8d0373fc9e24df5c64,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.117.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 01:32:36.138: INFO: Pod "webserver-deployment-7f5969cbc7-m244q" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m244q webserver-deployment-7f5969cbc7- deployment-4836  af767f2b-c8da-409a-9cea-451d261da892 20164859 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ac4cb7456bdcba9fc5dfbdaa7c68c3758ab76a0cde87e56a078395a43180ddff cni.projectcalico.org/podIP:172.21.88.185/32 cni.projectcalico.org/podIPs:172.21.88.185/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.185"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.185"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc002887de7 0xc002887de8}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqmxk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqmxk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.185,StartTime:2023-01-12 01:32:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://f4a4c4723e04061c40655c3e20d55e6b5968fbda21c05dcc58997ef3d3bce4d3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 01:32:36.138: INFO: Pod "webserver-deployment-7f5969cbc7-tkzkd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tkzkd webserver-deployment-7f5969cbc7- deployment-4836  7893af1f-c0ea-4f9a-8fa9-c8eaa4727d01 20164870 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c945cea01cc335d7c728ec0b83847611b3c334fc4a172137e94a0116ad83eb19 cni.projectcalico.org/podIP:172.21.117.134/32 cni.projectcalico.org/podIPs:172.21.117.134/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.117.134"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.117.134"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc002112017 0xc002112018}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.117.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwpw4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwpw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.117.134,StartTime:2023-01-12 01:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://87fee791d553e9d06cd2386e687e06253c50293d4e8f8300b6afa114cfb32333,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.117.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 01:32:36.138: INFO: Pod "webserver-deployment-7f5969cbc7-z8rx2" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-z8rx2 webserver-deployment-7f5969cbc7- deployment-4836  3a04be1b-78de-4d3b-ae58-d36d89507fad 20164873 0 2023-01-12 01:32:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:42ff6f8428d24b2dabf2096128a5187dd2fe119921d0c39442695f3040f92c77 cni.projectcalico.org/podIP:172.21.117.170/32 cni.projectcalico.org/podIPs:172.21.117.170/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.117.170"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.117.170"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 72b75ddb-2b64-4198-a8b0-bb9fd3494f07 0xc002112927 0xc002112928}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"72b75ddb-2b64-4198-a8b0-bb9fd3494f07\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.117.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mfwhc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mfwhc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:172.21.117.170,StartTime:2023-01-12 01:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:32:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://03614df3efdcb4f5351fcbfc028744ab4179c6c5383c4eb6ba994acbe813494b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.117.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 01:32:36.139: INFO: Pod "webserver-deployment-d9f79cb5-fw22p" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fw22p webserver-deployment-d9f79cb5- deployment-4836  67995af7-f62f-4272-a7fc-187a70c586f8 20164990 0 2023-01-12 01:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:35179479ce1bde9d4e25a135f79db07cc985148df77de31112441967e788e2f9 cni.projectcalico.org/podIP:172.21.88.180/32 cni.projectcalico.org/podIPs:172.21.88.180/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.180"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.180"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c02ce747-2873-41e8-9475-5058b5847f47 0xc002113057 0xc002113058}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c02ce747-2873-41e8-9475-5058b5847f47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jsvg5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jsvg5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:,StartTime:2023-01-12 01:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 01:32:36.139: INFO: Pod "webserver-deployment-d9f79cb5-qjv5m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qjv5m webserver-deployment-d9f79cb5- deployment-4836  319b9eee-2b39-4dc7-87a6-6ad9a2fe6ccf 20164978 0 2023-01-12 01:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f7bcaf6ef24fb8b752891abbeac170c24bc42049017accd8cff1f6b1ffb97657 cni.projectcalico.org/podIP:172.21.88.157/32 cni.projectcalico.org/podIPs:172.21.88.157/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.157"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.157"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c02ce747-2873-41e8-9475-5058b5847f47 0xc002113337 0xc002113338}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c02ce747-2873-41e8-9475-5058b5847f47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5tkdw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5tkdw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:,StartTime:2023-01-12 01:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 01:32:36.139: INFO: Pod "webserver-deployment-d9f79cb5-rxbsn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rxbsn webserver-deployment-d9f79cb5- deployment-4836  6212f23a-64d7-4773-92b4-986ab2c99487 20164973 0 2023-01-12 01:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2f0324212a53e7489a35f3e595ef6a8592e30206f88671c7a5f61d854948792e cni.projectcalico.org/podIP:172.21.117.154/32 cni.projectcalico.org/podIPs:172.21.117.154/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.117.154"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.117.154"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c02ce747-2873-41e8-9475-5058b5847f47 0xc002113577 0xc002113578}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c02ce747-2873-41e8-9475-5058b5847f47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n5c5b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n5c5b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:,StartTime:2023-01-12 01:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 01:32:36.139: INFO: Pod "webserver-deployment-d9f79cb5-w9w98" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w9w98 webserver-deployment-d9f79cb5- deployment-4836  301e26c5-9604-4ffc-b029-332fecf2b773 20164984 0 2023-01-12 01:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:c8021c3fb3cd224c9afb4ad442bec13a544054866d04fc3ba1a1a5de250ed4a0 cni.projectcalico.org/podIP:172.21.117.155/32 cni.projectcalico.org/podIPs:172.21.117.155/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.117.155"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.117.155"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c02ce747-2873-41e8-9475-5058b5847f47 0xc0021137b7 0xc0021137b8}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c02ce747-2873-41e8-9475-5058b5847f47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5rpgq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5rpgq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx03-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.140.106,PodIP:,StartTime:2023-01-12 01:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 01:32:36.139: INFO: Pod "webserver-deployment-d9f79cb5-x4tnb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-x4tnb webserver-deployment-d9f79cb5- deployment-4836  0a4dab4f-56ee-49bb-9af3-f92f6b2362a1 20164959 0 2023-01-12 01:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c02ce747-2873-41e8-9475-5058b5847f47 0xc002113a07 0xc002113a08}] [] [{kube-controller-manager Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c02ce747-2873-41e8-9475-5058b5847f47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 01:32:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2jtt2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2jtt2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:,StartTime:2023-01-12 01:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:32:36.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4836" for this suite. 01/12/23 01:32:36.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:32:36.229
Jan 12 01:32:36.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename var-expansion 01/12/23 01:32:36.23
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:32:36.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:32:36.25
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 01/12/23 01:32:36.252
Jan 12 01:32:36.455: INFO: Waiting up to 5m0s for pod "var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59" in namespace "var-expansion-7380" to be "Succeeded or Failed"
Jan 12 01:32:36.459: INFO: Pod "var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.49388ms
Jan 12 01:32:38.464: INFO: Pod "var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009079454s
Jan 12 01:32:40.464: INFO: Pod "var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008709439s
Jan 12 01:32:42.468: INFO: Pod "var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013605991s
STEP: Saw pod success 01/12/23 01:32:42.468
Jan 12 01:32:42.469: INFO: Pod "var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59" satisfied condition "Succeeded or Failed"
Jan 12 01:32:42.472: INFO: Trying to get logs from node eqx04-flash06 pod var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59 container dapi-container: <nil>
STEP: delete the pod 01/12/23 01:32:42.491
Jan 12 01:32:42.516: INFO: Waiting for pod var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59 to disappear
Jan 12 01:32:42.526: INFO: Pod var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 01:32:42.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7380" for this suite. 01/12/23 01:32:42.531
------------------------------
• [SLOW TEST] [6.396 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:32:36.229
    Jan 12 01:32:36.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename var-expansion 01/12/23 01:32:36.23
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:32:36.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:32:36.25
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 01/12/23 01:32:36.252
    Jan 12 01:32:36.455: INFO: Waiting up to 5m0s for pod "var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59" in namespace "var-expansion-7380" to be "Succeeded or Failed"
    Jan 12 01:32:36.459: INFO: Pod "var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.49388ms
    Jan 12 01:32:38.464: INFO: Pod "var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009079454s
    Jan 12 01:32:40.464: INFO: Pod "var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008709439s
    Jan 12 01:32:42.468: INFO: Pod "var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013605991s
    STEP: Saw pod success 01/12/23 01:32:42.468
    Jan 12 01:32:42.469: INFO: Pod "var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59" satisfied condition "Succeeded or Failed"
    Jan 12 01:32:42.472: INFO: Trying to get logs from node eqx04-flash06 pod var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59 container dapi-container: <nil>
    STEP: delete the pod 01/12/23 01:32:42.491
    Jan 12 01:32:42.516: INFO: Waiting for pod var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59 to disappear
    Jan 12 01:32:42.526: INFO: Pod var-expansion-b1417881-d350-42d2-bc16-b2649ca35d59 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:32:42.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7380" for this suite. 01/12/23 01:32:42.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:32:42.625
Jan 12 01:32:42.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename subpath 01/12/23 01:32:42.626
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:32:42.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:32:42.65
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/12/23 01:32:42.654
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-6865 01/12/23 01:32:42.664
STEP: Creating a pod to test atomic-volume-subpath 01/12/23 01:32:42.664
Jan 12 01:32:42.747: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6865" in namespace "subpath-2231" to be "Succeeded or Failed"
Jan 12 01:32:42.752: INFO: Pod "pod-subpath-test-projected-6865": Phase="Pending", Reason="", readiness=false. Elapsed: 5.055913ms
Jan 12 01:32:44.756: INFO: Pod "pod-subpath-test-projected-6865": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009412961s
Jan 12 01:32:46.758: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 4.010965501s
Jan 12 01:32:48.755: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 6.008560699s
Jan 12 01:32:50.756: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 8.009181016s
Jan 12 01:32:52.757: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 10.010303418s
Jan 12 01:32:54.757: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 12.01022578s
Jan 12 01:32:56.756: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 14.009767234s
Jan 12 01:32:58.758: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 16.011056657s
Jan 12 01:33:00.757: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 18.010638658s
Jan 12 01:33:02.757: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 20.010016827s
Jan 12 01:33:04.757: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 22.010107106s
Jan 12 01:33:06.757: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=false. Elapsed: 24.009859202s
Jan 12 01:33:08.757: INFO: Pod "pod-subpath-test-projected-6865": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.010053686s
STEP: Saw pod success 01/12/23 01:33:08.757
Jan 12 01:33:08.757: INFO: Pod "pod-subpath-test-projected-6865" satisfied condition "Succeeded or Failed"
Jan 12 01:33:08.762: INFO: Trying to get logs from node eqx04-flash06 pod pod-subpath-test-projected-6865 container test-container-subpath-projected-6865: <nil>
STEP: delete the pod 01/12/23 01:33:08.772
Jan 12 01:33:08.788: INFO: Waiting for pod pod-subpath-test-projected-6865 to disappear
Jan 12 01:33:08.790: INFO: Pod pod-subpath-test-projected-6865 no longer exists
STEP: Deleting pod pod-subpath-test-projected-6865 01/12/23 01:33:08.79
Jan 12 01:33:08.791: INFO: Deleting pod "pod-subpath-test-projected-6865" in namespace "subpath-2231"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 12 01:33:08.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2231" for this suite. 01/12/23 01:33:08.798
------------------------------
• [SLOW TEST] [26.251 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:32:42.625
    Jan 12 01:32:42.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename subpath 01/12/23 01:32:42.626
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:32:42.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:32:42.65
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/12/23 01:32:42.654
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-6865 01/12/23 01:32:42.664
    STEP: Creating a pod to test atomic-volume-subpath 01/12/23 01:32:42.664
    Jan 12 01:32:42.747: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6865" in namespace "subpath-2231" to be "Succeeded or Failed"
    Jan 12 01:32:42.752: INFO: Pod "pod-subpath-test-projected-6865": Phase="Pending", Reason="", readiness=false. Elapsed: 5.055913ms
    Jan 12 01:32:44.756: INFO: Pod "pod-subpath-test-projected-6865": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009412961s
    Jan 12 01:32:46.758: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 4.010965501s
    Jan 12 01:32:48.755: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 6.008560699s
    Jan 12 01:32:50.756: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 8.009181016s
    Jan 12 01:32:52.757: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 10.010303418s
    Jan 12 01:32:54.757: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 12.01022578s
    Jan 12 01:32:56.756: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 14.009767234s
    Jan 12 01:32:58.758: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 16.011056657s
    Jan 12 01:33:00.757: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 18.010638658s
    Jan 12 01:33:02.757: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 20.010016827s
    Jan 12 01:33:04.757: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=true. Elapsed: 22.010107106s
    Jan 12 01:33:06.757: INFO: Pod "pod-subpath-test-projected-6865": Phase="Running", Reason="", readiness=false. Elapsed: 24.009859202s
    Jan 12 01:33:08.757: INFO: Pod "pod-subpath-test-projected-6865": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.010053686s
    STEP: Saw pod success 01/12/23 01:33:08.757
    Jan 12 01:33:08.757: INFO: Pod "pod-subpath-test-projected-6865" satisfied condition "Succeeded or Failed"
    Jan 12 01:33:08.762: INFO: Trying to get logs from node eqx04-flash06 pod pod-subpath-test-projected-6865 container test-container-subpath-projected-6865: <nil>
    STEP: delete the pod 01/12/23 01:33:08.772
    Jan 12 01:33:08.788: INFO: Waiting for pod pod-subpath-test-projected-6865 to disappear
    Jan 12 01:33:08.790: INFO: Pod pod-subpath-test-projected-6865 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-6865 01/12/23 01:33:08.79
    Jan 12 01:33:08.791: INFO: Deleting pod "pod-subpath-test-projected-6865" in namespace "subpath-2231"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:33:08.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2231" for this suite. 01/12/23 01:33:08.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:33:08.878
Jan 12 01:33:08.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 01:33:08.879
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:33:08.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:33:08.899
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-9356 01/12/23 01:33:08.902
STEP: creating replication controller nodeport-test in namespace services-9356 01/12/23 01:33:08.918
I0112 01:33:08.927390      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9356, replica count: 2
I0112 01:33:11.978789      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 01:33:11.978: INFO: Creating new exec pod
Jan 12 01:33:12.014: INFO: Waiting up to 5m0s for pod "execpod8n72p" in namespace "services-9356" to be "running"
Jan 12 01:33:12.017: INFO: Pod "execpod8n72p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.862228ms
Jan 12 01:33:14.020: INFO: Pod "execpod8n72p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006169044s
Jan 12 01:33:16.022: INFO: Pod "execpod8n72p": Phase="Running", Reason="", readiness=true. Elapsed: 4.008203676s
Jan 12 01:33:16.022: INFO: Pod "execpod8n72p" satisfied condition "running"
Jan 12 01:33:17.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9356 exec execpod8n72p -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jan 12 01:33:17.231: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 12 01:33:17.231: INFO: stdout: ""
Jan 12 01:33:17.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9356 exec execpod8n72p -- /bin/sh -x -c nc -v -z -w 2 172.19.220.153 80'
Jan 12 01:33:17.449: INFO: stderr: "+ nc -v -z -w 2 172.19.220.153 80\nConnection to 172.19.220.153 80 port [tcp/http] succeeded!\n"
Jan 12 01:33:17.449: INFO: stdout: ""
Jan 12 01:33:17.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9356 exec execpod8n72p -- /bin/sh -x -c nc -v -z -w 2 10.9.140.106 31185'
Jan 12 01:33:17.657: INFO: stderr: "+ nc -v -z -w 2 10.9.140.106 31185\nConnection to 10.9.140.106 31185 port [tcp/*] succeeded!\n"
Jan 12 01:33:17.657: INFO: stdout: ""
Jan 12 01:33:17.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9356 exec execpod8n72p -- /bin/sh -x -c nc -v -z -w 2 10.9.40.106 31185'
Jan 12 01:33:17.856: INFO: stderr: "+ nc -v -z -w 2 10.9.40.106 31185\nConnection to 10.9.40.106 31185 port [tcp/*] succeeded!\n"
Jan 12 01:33:17.856: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 01:33:17.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9356" for this suite. 01/12/23 01:33:17.861
------------------------------
• [SLOW TEST] [9.001 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:33:08.878
    Jan 12 01:33:08.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 01:33:08.879
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:33:08.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:33:08.899
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-9356 01/12/23 01:33:08.902
    STEP: creating replication controller nodeport-test in namespace services-9356 01/12/23 01:33:08.918
    I0112 01:33:08.927390      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9356, replica count: 2
    I0112 01:33:11.978789      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 01:33:11.978: INFO: Creating new exec pod
    Jan 12 01:33:12.014: INFO: Waiting up to 5m0s for pod "execpod8n72p" in namespace "services-9356" to be "running"
    Jan 12 01:33:12.017: INFO: Pod "execpod8n72p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.862228ms
    Jan 12 01:33:14.020: INFO: Pod "execpod8n72p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006169044s
    Jan 12 01:33:16.022: INFO: Pod "execpod8n72p": Phase="Running", Reason="", readiness=true. Elapsed: 4.008203676s
    Jan 12 01:33:16.022: INFO: Pod "execpod8n72p" satisfied condition "running"
    Jan 12 01:33:17.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9356 exec execpod8n72p -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jan 12 01:33:17.231: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 12 01:33:17.231: INFO: stdout: ""
    Jan 12 01:33:17.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9356 exec execpod8n72p -- /bin/sh -x -c nc -v -z -w 2 172.19.220.153 80'
    Jan 12 01:33:17.449: INFO: stderr: "+ nc -v -z -w 2 172.19.220.153 80\nConnection to 172.19.220.153 80 port [tcp/http] succeeded!\n"
    Jan 12 01:33:17.449: INFO: stdout: ""
    Jan 12 01:33:17.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9356 exec execpod8n72p -- /bin/sh -x -c nc -v -z -w 2 10.9.140.106 31185'
    Jan 12 01:33:17.657: INFO: stderr: "+ nc -v -z -w 2 10.9.140.106 31185\nConnection to 10.9.140.106 31185 port [tcp/*] succeeded!\n"
    Jan 12 01:33:17.657: INFO: stdout: ""
    Jan 12 01:33:17.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9356 exec execpod8n72p -- /bin/sh -x -c nc -v -z -w 2 10.9.40.106 31185'
    Jan 12 01:33:17.856: INFO: stderr: "+ nc -v -z -w 2 10.9.40.106 31185\nConnection to 10.9.40.106 31185 port [tcp/*] succeeded!\n"
    Jan 12 01:33:17.856: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:33:17.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9356" for this suite. 01/12/23 01:33:17.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:33:17.881
Jan 12 01:33:17.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 01:33:17.881
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:33:17.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:33:17.902
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 12 01:33:17.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:33:21.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4658" for this suite. 01/12/23 01:33:21.095
------------------------------
• [3.288 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:33:17.881
    Jan 12 01:33:17.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 01:33:17.881
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:33:17.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:33:17.902
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 12 01:33:17.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:33:21.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4658" for this suite. 01/12/23 01:33:21.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:33:21.17
Jan 12 01:33:21.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:33:21.171
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:33:21.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:33:21.189
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-550488b0-4e27-4a09-a9dc-461d5f7eec0d 01/12/23 01:33:21.192
STEP: Creating a pod to test consume configMaps 01/12/23 01:33:21.198
Jan 12 01:33:21.232: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f" in namespace "projected-3519" to be "Succeeded or Failed"
Jan 12 01:33:21.235: INFO: Pod "pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.272103ms
Jan 12 01:33:23.240: INFO: Pod "pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007799892s
Jan 12 01:33:25.239: INFO: Pod "pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007196828s
Jan 12 01:33:27.239: INFO: Pod "pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007606707s
STEP: Saw pod success 01/12/23 01:33:27.239
Jan 12 01:33:27.240: INFO: Pod "pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f" satisfied condition "Succeeded or Failed"
Jan 12 01:33:27.242: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f container agnhost-container: <nil>
STEP: delete the pod 01/12/23 01:33:27.252
Jan 12 01:33:27.267: INFO: Waiting for pod pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f to disappear
Jan 12 01:33:27.269: INFO: Pod pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:33:27.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3519" for this suite. 01/12/23 01:33:27.274
------------------------------
• [SLOW TEST] [6.219 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:33:21.17
    Jan 12 01:33:21.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:33:21.171
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:33:21.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:33:21.189
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-550488b0-4e27-4a09-a9dc-461d5f7eec0d 01/12/23 01:33:21.192
    STEP: Creating a pod to test consume configMaps 01/12/23 01:33:21.198
    Jan 12 01:33:21.232: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f" in namespace "projected-3519" to be "Succeeded or Failed"
    Jan 12 01:33:21.235: INFO: Pod "pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.272103ms
    Jan 12 01:33:23.240: INFO: Pod "pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007799892s
    Jan 12 01:33:25.239: INFO: Pod "pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007196828s
    Jan 12 01:33:27.239: INFO: Pod "pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007606707s
    STEP: Saw pod success 01/12/23 01:33:27.239
    Jan 12 01:33:27.240: INFO: Pod "pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f" satisfied condition "Succeeded or Failed"
    Jan 12 01:33:27.242: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 01:33:27.252
    Jan 12 01:33:27.267: INFO: Waiting for pod pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f to disappear
    Jan 12 01:33:27.269: INFO: Pod pod-projected-configmaps-49c2df34-8275-4437-843d-a984fb1cca0f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:33:27.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3519" for this suite. 01/12/23 01:33:27.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:33:27.39
Jan 12 01:33:27.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename proxy 01/12/23 01:33:27.391
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:33:27.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:33:27.409
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 12 01:33:27.412: INFO: Creating pod...
Jan 12 01:33:27.533: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3634" to be "running"
Jan 12 01:33:27.536: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.815156ms
Jan 12 01:33:29.540: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00753987s
Jan 12 01:33:31.541: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.008142009s
Jan 12 01:33:31.541: INFO: Pod "agnhost" satisfied condition "running"
Jan 12 01:33:31.541: INFO: Creating service...
Jan 12 01:33:31.554: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/pods/agnhost/proxy/some/path/with/DELETE
Jan 12 01:33:31.559: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 12 01:33:31.559: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/pods/agnhost/proxy/some/path/with/GET
Jan 12 01:33:31.564: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 12 01:33:31.564: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/pods/agnhost/proxy/some/path/with/HEAD
Jan 12 01:33:31.567: INFO: http.Client request:HEAD | StatusCode:200
Jan 12 01:33:31.567: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 12 01:33:31.570: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 12 01:33:31.570: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/pods/agnhost/proxy/some/path/with/PATCH
Jan 12 01:33:31.573: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 12 01:33:31.573: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/pods/agnhost/proxy/some/path/with/POST
Jan 12 01:33:31.576: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 12 01:33:31.576: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/pods/agnhost/proxy/some/path/with/PUT
Jan 12 01:33:31.581: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 12 01:33:31.581: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/services/test-service/proxy/some/path/with/DELETE
Jan 12 01:33:31.585: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 12 01:33:31.585: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/services/test-service/proxy/some/path/with/GET
Jan 12 01:33:31.590: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 12 01:33:31.590: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/services/test-service/proxy/some/path/with/HEAD
Jan 12 01:33:31.594: INFO: http.Client request:HEAD | StatusCode:200
Jan 12 01:33:31.594: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/services/test-service/proxy/some/path/with/OPTIONS
Jan 12 01:33:31.599: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 12 01:33:31.599: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/services/test-service/proxy/some/path/with/PATCH
Jan 12 01:33:31.604: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 12 01:33:31.604: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/services/test-service/proxy/some/path/with/POST
Jan 12 01:33:31.608: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 12 01:33:31.608: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/services/test-service/proxy/some/path/with/PUT
Jan 12 01:33:31.613: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 12 01:33:31.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3634" for this suite. 01/12/23 01:33:31.618
------------------------------
• [4.243 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:33:27.39
    Jan 12 01:33:27.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename proxy 01/12/23 01:33:27.391
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:33:27.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:33:27.409
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 12 01:33:27.412: INFO: Creating pod...
    Jan 12 01:33:27.533: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3634" to be "running"
    Jan 12 01:33:27.536: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.815156ms
    Jan 12 01:33:29.540: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00753987s
    Jan 12 01:33:31.541: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.008142009s
    Jan 12 01:33:31.541: INFO: Pod "agnhost" satisfied condition "running"
    Jan 12 01:33:31.541: INFO: Creating service...
    Jan 12 01:33:31.554: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/pods/agnhost/proxy/some/path/with/DELETE
    Jan 12 01:33:31.559: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 12 01:33:31.559: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/pods/agnhost/proxy/some/path/with/GET
    Jan 12 01:33:31.564: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 12 01:33:31.564: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/pods/agnhost/proxy/some/path/with/HEAD
    Jan 12 01:33:31.567: INFO: http.Client request:HEAD | StatusCode:200
    Jan 12 01:33:31.567: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 12 01:33:31.570: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 12 01:33:31.570: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/pods/agnhost/proxy/some/path/with/PATCH
    Jan 12 01:33:31.573: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 12 01:33:31.573: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/pods/agnhost/proxy/some/path/with/POST
    Jan 12 01:33:31.576: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 12 01:33:31.576: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/pods/agnhost/proxy/some/path/with/PUT
    Jan 12 01:33:31.581: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 12 01:33:31.581: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/services/test-service/proxy/some/path/with/DELETE
    Jan 12 01:33:31.585: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 12 01:33:31.585: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/services/test-service/proxy/some/path/with/GET
    Jan 12 01:33:31.590: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 12 01:33:31.590: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/services/test-service/proxy/some/path/with/HEAD
    Jan 12 01:33:31.594: INFO: http.Client request:HEAD | StatusCode:200
    Jan 12 01:33:31.594: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/services/test-service/proxy/some/path/with/OPTIONS
    Jan 12 01:33:31.599: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 12 01:33:31.599: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/services/test-service/proxy/some/path/with/PATCH
    Jan 12 01:33:31.604: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 12 01:33:31.604: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/services/test-service/proxy/some/path/with/POST
    Jan 12 01:33:31.608: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 12 01:33:31.608: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-3634/services/test-service/proxy/some/path/with/PUT
    Jan 12 01:33:31.613: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:33:31.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3634" for this suite. 01/12/23 01:33:31.618
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:33:31.634
Jan 12 01:33:31.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 01:33:31.635
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:33:31.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:33:31.654
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 01/12/23 01:33:31.657
Jan 12 01:33:31.688: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac" in namespace "downward-api-6278" to be "Succeeded or Failed"
Jan 12 01:33:31.691: INFO: Pod "downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.896933ms
Jan 12 01:33:33.695: INFO: Pod "downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007534756s
Jan 12 01:33:35.695: INFO: Pod "downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007069365s
Jan 12 01:33:37.696: INFO: Pod "downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007750963s
STEP: Saw pod success 01/12/23 01:33:37.696
Jan 12 01:33:37.696: INFO: Pod "downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac" satisfied condition "Succeeded or Failed"
Jan 12 01:33:37.699: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac container client-container: <nil>
STEP: delete the pod 01/12/23 01:33:37.707
Jan 12 01:33:37.721: INFO: Waiting for pod downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac to disappear
Jan 12 01:33:37.724: INFO: Pod downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 01:33:37.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6278" for this suite. 01/12/23 01:33:37.728
------------------------------
• [SLOW TEST] [6.112 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:33:31.634
    Jan 12 01:33:31.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 01:33:31.635
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:33:31.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:33:31.654
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 01/12/23 01:33:31.657
    Jan 12 01:33:31.688: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac" in namespace "downward-api-6278" to be "Succeeded or Failed"
    Jan 12 01:33:31.691: INFO: Pod "downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.896933ms
    Jan 12 01:33:33.695: INFO: Pod "downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007534756s
    Jan 12 01:33:35.695: INFO: Pod "downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007069365s
    Jan 12 01:33:37.696: INFO: Pod "downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007750963s
    STEP: Saw pod success 01/12/23 01:33:37.696
    Jan 12 01:33:37.696: INFO: Pod "downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac" satisfied condition "Succeeded or Failed"
    Jan 12 01:33:37.699: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac container client-container: <nil>
    STEP: delete the pod 01/12/23 01:33:37.707
    Jan 12 01:33:37.721: INFO: Waiting for pod downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac to disappear
    Jan 12 01:33:37.724: INFO: Pod downwardapi-volume-1d49bc96-2ca8-41bb-9f80-2029586666ac no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:33:37.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6278" for this suite. 01/12/23 01:33:37.728
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:33:37.746
Jan 12 01:33:37.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 01:33:37.747
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:33:37.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:33:37.765
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-5509/configmap-test-8729348f-4416-48fd-b373-c4449ee08991 01/12/23 01:33:37.767
STEP: Creating a pod to test consume configMaps 01/12/23 01:33:37.773
Jan 12 01:33:37.821: INFO: Waiting up to 5m0s for pod "pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d" in namespace "configmap-5509" to be "Succeeded or Failed"
Jan 12 01:33:37.824: INFO: Pod "pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.997877ms
Jan 12 01:33:39.832: INFO: Pod "pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010953097s
Jan 12 01:33:41.829: INFO: Pod "pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008289928s
Jan 12 01:33:43.829: INFO: Pod "pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008181204s
STEP: Saw pod success 01/12/23 01:33:43.829
Jan 12 01:33:43.829: INFO: Pod "pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d" satisfied condition "Succeeded or Failed"
Jan 12 01:33:43.832: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d container env-test: <nil>
STEP: delete the pod 01/12/23 01:33:43.842
Jan 12 01:33:43.857: INFO: Waiting for pod pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d to disappear
Jan 12 01:33:43.860: INFO: Pod pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:33:43.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5509" for this suite. 01/12/23 01:33:43.864
------------------------------
• [SLOW TEST] [6.134 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:33:37.746
    Jan 12 01:33:37.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 01:33:37.747
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:33:37.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:33:37.765
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-5509/configmap-test-8729348f-4416-48fd-b373-c4449ee08991 01/12/23 01:33:37.767
    STEP: Creating a pod to test consume configMaps 01/12/23 01:33:37.773
    Jan 12 01:33:37.821: INFO: Waiting up to 5m0s for pod "pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d" in namespace "configmap-5509" to be "Succeeded or Failed"
    Jan 12 01:33:37.824: INFO: Pod "pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.997877ms
    Jan 12 01:33:39.832: INFO: Pod "pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010953097s
    Jan 12 01:33:41.829: INFO: Pod "pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008289928s
    Jan 12 01:33:43.829: INFO: Pod "pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008181204s
    STEP: Saw pod success 01/12/23 01:33:43.829
    Jan 12 01:33:43.829: INFO: Pod "pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d" satisfied condition "Succeeded or Failed"
    Jan 12 01:33:43.832: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d container env-test: <nil>
    STEP: delete the pod 01/12/23 01:33:43.842
    Jan 12 01:33:43.857: INFO: Waiting for pod pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d to disappear
    Jan 12 01:33:43.860: INFO: Pod pod-configmaps-0c32d3ad-27e5-465c-b20d-056147ddb62d no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:33:43.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5509" for this suite. 01/12/23 01:33:43.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:33:43.881
Jan 12 01:33:43.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename job 01/12/23 01:33:43.882
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:33:43.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:33:43.903
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 01/12/23 01:33:43.906
STEP: Ensuring active pods == parallelism 01/12/23 01:33:43.912
STEP: delete a job 01/12/23 01:33:47.917
STEP: deleting Job.batch foo in namespace job-1281, will wait for the garbage collector to delete the pods 01/12/23 01:33:47.918
Jan 12 01:33:47.978: INFO: Deleting Job.batch foo took: 6.129722ms
Jan 12 01:33:48.078: INFO: Terminating Job.batch foo pods took: 100.646922ms
STEP: Ensuring job was deleted 01/12/23 01:34:20.079
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 12 01:34:20.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1281" for this suite. 01/12/23 01:34:20.087
------------------------------
• [SLOW TEST] [36.240 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:33:43.881
    Jan 12 01:33:43.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename job 01/12/23 01:33:43.882
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:33:43.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:33:43.903
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 01/12/23 01:33:43.906
    STEP: Ensuring active pods == parallelism 01/12/23 01:33:43.912
    STEP: delete a job 01/12/23 01:33:47.917
    STEP: deleting Job.batch foo in namespace job-1281, will wait for the garbage collector to delete the pods 01/12/23 01:33:47.918
    Jan 12 01:33:47.978: INFO: Deleting Job.batch foo took: 6.129722ms
    Jan 12 01:33:48.078: INFO: Terminating Job.batch foo pods took: 100.646922ms
    STEP: Ensuring job was deleted 01/12/23 01:34:20.079
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:34:20.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1281" for this suite. 01/12/23 01:34:20.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:34:20.126
Jan 12 01:34:20.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename runtimeclass 01/12/23 01:34:20.127
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:20.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:20.145
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 12 01:34:20.234: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8913 to be scheduled
Jan 12 01:34:20.236: INFO: 1 pods are not scheduled: [runtimeclass-8913/test-runtimeclass-runtimeclass-8913-preconfigured-handler-z99vl(3c60e71d-ab3c-4ace-a189-73de80bfbdbb)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 12 01:34:22.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8913" for this suite. 01/12/23 01:34:22.251
------------------------------
• [2.202 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:34:20.126
    Jan 12 01:34:20.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename runtimeclass 01/12/23 01:34:20.127
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:20.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:20.145
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 12 01:34:20.234: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8913 to be scheduled
    Jan 12 01:34:20.236: INFO: 1 pods are not scheduled: [runtimeclass-8913/test-runtimeclass-runtimeclass-8913-preconfigured-handler-z99vl(3c60e71d-ab3c-4ace-a189-73de80bfbdbb)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:34:22.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8913" for this suite. 01/12/23 01:34:22.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:34:22.33
Jan 12 01:34:22.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename replicaset 01/12/23 01:34:22.331
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:22.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:22.35
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/12/23 01:34:22.353
Jan 12 01:34:22.469: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6430" to be "running and ready"
Jan 12 01:34:22.472: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.81125ms
Jan 12 01:34:22.472: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:34:24.481: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.012759073s
Jan 12 01:34:24.482: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 12 01:34:24.482: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/12/23 01:34:24.491
STEP: Then the orphan pod is adopted 01/12/23 01:34:24.498
STEP: When the matched label of one of its pods change 01/12/23 01:34:25.506
Jan 12 01:34:25.509: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/12/23 01:34:25.522
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 12 01:34:26.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6430" for this suite. 01/12/23 01:34:26.535
------------------------------
• [4.225 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:34:22.33
    Jan 12 01:34:22.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename replicaset 01/12/23 01:34:22.331
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:22.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:22.35
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/12/23 01:34:22.353
    Jan 12 01:34:22.469: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6430" to be "running and ready"
    Jan 12 01:34:22.472: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.81125ms
    Jan 12 01:34:22.472: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:34:24.481: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.012759073s
    Jan 12 01:34:24.482: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 12 01:34:24.482: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/12/23 01:34:24.491
    STEP: Then the orphan pod is adopted 01/12/23 01:34:24.498
    STEP: When the matched label of one of its pods change 01/12/23 01:34:25.506
    Jan 12 01:34:25.509: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/12/23 01:34:25.522
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:34:26.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6430" for this suite. 01/12/23 01:34:26.535
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:34:26.556
Jan 12 01:34:26.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 01:34:26.557
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:26.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:26.577
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-9722 01/12/23 01:34:26.581
STEP: creating service affinity-clusterip-transition in namespace services-9722 01/12/23 01:34:26.581
STEP: creating replication controller affinity-clusterip-transition in namespace services-9722 01/12/23 01:34:26.593
I0112 01:34:26.601283      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9722, replica count: 3
I0112 01:34:29.652463      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 01:34:29.658: INFO: Creating new exec pod
Jan 12 01:34:29.689: INFO: Waiting up to 5m0s for pod "execpod-affinity5wjv6" in namespace "services-9722" to be "running"
Jan 12 01:34:29.692: INFO: Pod "execpod-affinity5wjv6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.929786ms
Jan 12 01:34:31.698: INFO: Pod "execpod-affinity5wjv6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008843398s
Jan 12 01:34:31.698: INFO: Pod "execpod-affinity5wjv6" satisfied condition "running"
Jan 12 01:34:32.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9722 exec execpod-affinity5wjv6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan 12 01:34:32.937: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 12 01:34:32.937: INFO: stdout: ""
Jan 12 01:34:32.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9722 exec execpod-affinity5wjv6 -- /bin/sh -x -c nc -v -z -w 2 172.19.158.76 80'
Jan 12 01:34:33.139: INFO: stderr: "+ nc -v -z -w 2 172.19.158.76 80\nConnection to 172.19.158.76 80 port [tcp/http] succeeded!\n"
Jan 12 01:34:33.139: INFO: stdout: ""
Jan 12 01:34:33.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9722 exec execpod-affinity5wjv6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.158.76:80/ ; done'
Jan 12 01:34:33.424: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n"
Jan 12 01:34:33.424: INFO: stdout: "\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-f4xfj\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-f4xfj\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-r9bww"
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-f4xfj
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-f4xfj
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
Jan 12 01:34:33.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9722 exec execpod-affinity5wjv6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.158.76:80/ ; done'
Jan 12 01:34:33.701: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n"
Jan 12 01:34:33.702: INFO: stdout: "\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq"
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
Jan 12 01:34:33.702: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9722, will wait for the garbage collector to delete the pods 01/12/23 01:34:33.716
Jan 12 01:34:33.776: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.204175ms
Jan 12 01:34:33.876: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.144694ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 01:34:36.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9722" for this suite. 01/12/23 01:34:36.204
------------------------------
• [SLOW TEST] [9.756 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:34:26.556
    Jan 12 01:34:26.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 01:34:26.557
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:26.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:26.577
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-9722 01/12/23 01:34:26.581
    STEP: creating service affinity-clusterip-transition in namespace services-9722 01/12/23 01:34:26.581
    STEP: creating replication controller affinity-clusterip-transition in namespace services-9722 01/12/23 01:34:26.593
    I0112 01:34:26.601283      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9722, replica count: 3
    I0112 01:34:29.652463      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 01:34:29.658: INFO: Creating new exec pod
    Jan 12 01:34:29.689: INFO: Waiting up to 5m0s for pod "execpod-affinity5wjv6" in namespace "services-9722" to be "running"
    Jan 12 01:34:29.692: INFO: Pod "execpod-affinity5wjv6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.929786ms
    Jan 12 01:34:31.698: INFO: Pod "execpod-affinity5wjv6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008843398s
    Jan 12 01:34:31.698: INFO: Pod "execpod-affinity5wjv6" satisfied condition "running"
    Jan 12 01:34:32.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9722 exec execpod-affinity5wjv6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan 12 01:34:32.937: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 12 01:34:32.937: INFO: stdout: ""
    Jan 12 01:34:32.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9722 exec execpod-affinity5wjv6 -- /bin/sh -x -c nc -v -z -w 2 172.19.158.76 80'
    Jan 12 01:34:33.139: INFO: stderr: "+ nc -v -z -w 2 172.19.158.76 80\nConnection to 172.19.158.76 80 port [tcp/http] succeeded!\n"
    Jan 12 01:34:33.139: INFO: stdout: ""
    Jan 12 01:34:33.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9722 exec execpod-affinity5wjv6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.158.76:80/ ; done'
    Jan 12 01:34:33.424: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n"
    Jan 12 01:34:33.424: INFO: stdout: "\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-f4xfj\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-f4xfj\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-r9bww\naffinity-clusterip-transition-r9bww"
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-f4xfj
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-f4xfj
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
    Jan 12 01:34:33.424: INFO: Received response from host: affinity-clusterip-transition-r9bww
    Jan 12 01:34:33.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9722 exec execpod-affinity5wjv6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.158.76:80/ ; done'
    Jan 12 01:34:33.701: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.158.76:80/\n"
    Jan 12 01:34:33.702: INFO: stdout: "\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq\naffinity-clusterip-transition-fncqq"
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Received response from host: affinity-clusterip-transition-fncqq
    Jan 12 01:34:33.702: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9722, will wait for the garbage collector to delete the pods 01/12/23 01:34:33.716
    Jan 12 01:34:33.776: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.204175ms
    Jan 12 01:34:33.876: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.144694ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:34:36.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9722" for this suite. 01/12/23 01:34:36.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:34:36.314
Jan 12 01:34:36.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:34:36.315
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:36.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:36.333
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-2d1cde47-84a1-409f-b188-a821bac3e872 01/12/23 01:34:36.34
STEP: Creating configMap with name cm-test-opt-upd-17faa64f-9a20-45ec-8aae-ebdf05c4fed6 01/12/23 01:34:36.344
STEP: Creating the pod 01/12/23 01:34:36.349
Jan 12 01:34:36.467: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168" in namespace "projected-5891" to be "running and ready"
Jan 12 01:34:36.470: INFO: Pod "pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168": Phase="Pending", Reason="", readiness=false. Elapsed: 2.740829ms
Jan 12 01:34:36.470: INFO: The phase of Pod pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:34:38.475: INFO: Pod "pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007590259s
Jan 12 01:34:38.475: INFO: The phase of Pod pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:34:40.476: INFO: Pod "pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168": Phase="Running", Reason="", readiness=true. Elapsed: 4.008908836s
Jan 12 01:34:40.476: INFO: The phase of Pod pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168 is Running (Ready = true)
Jan 12 01:34:40.476: INFO: Pod "pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-2d1cde47-84a1-409f-b188-a821bac3e872 01/12/23 01:34:40.507
STEP: Updating configmap cm-test-opt-upd-17faa64f-9a20-45ec-8aae-ebdf05c4fed6 01/12/23 01:34:40.514
STEP: Creating configMap with name cm-test-opt-create-81a61889-415b-4050-b9db-102e2f0f589f 01/12/23 01:34:40.519
STEP: waiting to observe update in volume 01/12/23 01:34:40.525
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:34:44.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5891" for this suite. 01/12/23 01:34:44.586
------------------------------
• [SLOW TEST] [8.288 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:34:36.314
    Jan 12 01:34:36.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:34:36.315
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:36.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:36.333
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-2d1cde47-84a1-409f-b188-a821bac3e872 01/12/23 01:34:36.34
    STEP: Creating configMap with name cm-test-opt-upd-17faa64f-9a20-45ec-8aae-ebdf05c4fed6 01/12/23 01:34:36.344
    STEP: Creating the pod 01/12/23 01:34:36.349
    Jan 12 01:34:36.467: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168" in namespace "projected-5891" to be "running and ready"
    Jan 12 01:34:36.470: INFO: Pod "pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168": Phase="Pending", Reason="", readiness=false. Elapsed: 2.740829ms
    Jan 12 01:34:36.470: INFO: The phase of Pod pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:34:38.475: INFO: Pod "pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007590259s
    Jan 12 01:34:38.475: INFO: The phase of Pod pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:34:40.476: INFO: Pod "pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168": Phase="Running", Reason="", readiness=true. Elapsed: 4.008908836s
    Jan 12 01:34:40.476: INFO: The phase of Pod pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168 is Running (Ready = true)
    Jan 12 01:34:40.476: INFO: Pod "pod-projected-configmaps-9067d43b-bf63-4a73-be36-2ed7a0e5a168" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-2d1cde47-84a1-409f-b188-a821bac3e872 01/12/23 01:34:40.507
    STEP: Updating configmap cm-test-opt-upd-17faa64f-9a20-45ec-8aae-ebdf05c4fed6 01/12/23 01:34:40.514
    STEP: Creating configMap with name cm-test-opt-create-81a61889-415b-4050-b9db-102e2f0f589f 01/12/23 01:34:40.519
    STEP: waiting to observe update in volume 01/12/23 01:34:40.525
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:34:44.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5891" for this suite. 01/12/23 01:34:44.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:34:44.604
Jan 12 01:34:44.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename security-context 01/12/23 01:34:44.605
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:44.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:44.629
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/12/23 01:34:44.631
Jan 12 01:34:44.664: INFO: Waiting up to 5m0s for pod "security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220" in namespace "security-context-9403" to be "Succeeded or Failed"
Jan 12 01:34:44.666: INFO: Pod "security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220": Phase="Pending", Reason="", readiness=false. Elapsed: 2.86481ms
Jan 12 01:34:46.671: INFO: Pod "security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007531756s
Jan 12 01:34:48.672: INFO: Pod "security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007909831s
STEP: Saw pod success 01/12/23 01:34:48.672
Jan 12 01:34:48.672: INFO: Pod "security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220" satisfied condition "Succeeded or Failed"
Jan 12 01:34:48.675: INFO: Trying to get logs from node eqx04-flash06 pod security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220 container test-container: <nil>
STEP: delete the pod 01/12/23 01:34:48.683
Jan 12 01:34:48.698: INFO: Waiting for pod security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220 to disappear
Jan 12 01:34:48.701: INFO: Pod security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 12 01:34:48.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-9403" for this suite. 01/12/23 01:34:48.708
------------------------------
• [4.189 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:34:44.604
    Jan 12 01:34:44.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename security-context 01/12/23 01:34:44.605
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:44.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:44.629
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/12/23 01:34:44.631
    Jan 12 01:34:44.664: INFO: Waiting up to 5m0s for pod "security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220" in namespace "security-context-9403" to be "Succeeded or Failed"
    Jan 12 01:34:44.666: INFO: Pod "security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220": Phase="Pending", Reason="", readiness=false. Elapsed: 2.86481ms
    Jan 12 01:34:46.671: INFO: Pod "security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007531756s
    Jan 12 01:34:48.672: INFO: Pod "security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007909831s
    STEP: Saw pod success 01/12/23 01:34:48.672
    Jan 12 01:34:48.672: INFO: Pod "security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220" satisfied condition "Succeeded or Failed"
    Jan 12 01:34:48.675: INFO: Trying to get logs from node eqx04-flash06 pod security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220 container test-container: <nil>
    STEP: delete the pod 01/12/23 01:34:48.683
    Jan 12 01:34:48.698: INFO: Waiting for pod security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220 to disappear
    Jan 12 01:34:48.701: INFO: Pod security-context-b62e179b-aba4-4e4c-b2ac-e86bbd763220 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:34:48.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-9403" for this suite. 01/12/23 01:34:48.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:34:48.794
Jan 12 01:34:48.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename security-context 01/12/23 01:34:48.795
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:48.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:48.814
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/12/23 01:34:48.816
Jan 12 01:34:48.937: INFO: Waiting up to 5m0s for pod "security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e" in namespace "security-context-7156" to be "Succeeded or Failed"
Jan 12 01:34:48.940: INFO: Pod "security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.933589ms
Jan 12 01:34:50.944: INFO: Pod "security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006718835s
Jan 12 01:34:52.945: INFO: Pod "security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00765173s
Jan 12 01:34:54.946: INFO: Pod "security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008230383s
STEP: Saw pod success 01/12/23 01:34:54.946
Jan 12 01:34:54.946: INFO: Pod "security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e" satisfied condition "Succeeded or Failed"
Jan 12 01:34:54.949: INFO: Trying to get logs from node eqx04-flash06 pod security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e container test-container: <nil>
STEP: delete the pod 01/12/23 01:34:54.958
Jan 12 01:34:54.974: INFO: Waiting for pod security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e to disappear
Jan 12 01:34:54.977: INFO: Pod security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 12 01:34:54.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-7156" for this suite. 01/12/23 01:34:54.981
------------------------------
• [SLOW TEST] [6.202 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:34:48.794
    Jan 12 01:34:48.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename security-context 01/12/23 01:34:48.795
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:48.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:48.814
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/12/23 01:34:48.816
    Jan 12 01:34:48.937: INFO: Waiting up to 5m0s for pod "security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e" in namespace "security-context-7156" to be "Succeeded or Failed"
    Jan 12 01:34:48.940: INFO: Pod "security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.933589ms
    Jan 12 01:34:50.944: INFO: Pod "security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006718835s
    Jan 12 01:34:52.945: INFO: Pod "security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00765173s
    Jan 12 01:34:54.946: INFO: Pod "security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008230383s
    STEP: Saw pod success 01/12/23 01:34:54.946
    Jan 12 01:34:54.946: INFO: Pod "security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e" satisfied condition "Succeeded or Failed"
    Jan 12 01:34:54.949: INFO: Trying to get logs from node eqx04-flash06 pod security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e container test-container: <nil>
    STEP: delete the pod 01/12/23 01:34:54.958
    Jan 12 01:34:54.974: INFO: Waiting for pod security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e to disappear
    Jan 12 01:34:54.977: INFO: Pod security-context-a450bc33-919f-4a73-9751-1e9cdea0ea9e no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:34:54.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-7156" for this suite. 01/12/23 01:34:54.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:34:54.999
Jan 12 01:34:54.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename gc 01/12/23 01:34:55
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:55.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:55.018
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/12/23 01:34:55.02
STEP: Wait for the Deployment to create new ReplicaSet 01/12/23 01:34:55.055
STEP: delete the deployment 01/12/23 01:34:55.561
STEP: wait for all rs to be garbage collected 01/12/23 01:34:55.568
STEP: expected 0 rs, got 1 rs 01/12/23 01:34:55.571
STEP: expected 0 pods, got 2 pods 01/12/23 01:34:55.574
STEP: Gathering metrics 01/12/23 01:34:56.085
Jan 12 01:34:56.118: INFO: Waiting up to 5m0s for pod "kube-controller-manager-eqx04-flash04" in namespace "kube-system" to be "running and ready"
Jan 12 01:34:56.121: INFO: Pod "kube-controller-manager-eqx04-flash04": Phase="Running", Reason="", readiness=true. Elapsed: 3.015355ms
Jan 12 01:34:56.121: INFO: The phase of Pod kube-controller-manager-eqx04-flash04 is Running (Ready = true)
Jan 12 01:34:56.121: INFO: Pod "kube-controller-manager-eqx04-flash04" satisfied condition "running and ready"
Jan 12 01:34:56.180: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 12 01:34:56.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1957" for this suite. 01/12/23 01:34:56.184
------------------------------
• [1.222 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:34:54.999
    Jan 12 01:34:54.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename gc 01/12/23 01:34:55
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:55.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:55.018
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/12/23 01:34:55.02
    STEP: Wait for the Deployment to create new ReplicaSet 01/12/23 01:34:55.055
    STEP: delete the deployment 01/12/23 01:34:55.561
    STEP: wait for all rs to be garbage collected 01/12/23 01:34:55.568
    STEP: expected 0 rs, got 1 rs 01/12/23 01:34:55.571
    STEP: expected 0 pods, got 2 pods 01/12/23 01:34:55.574
    STEP: Gathering metrics 01/12/23 01:34:56.085
    Jan 12 01:34:56.118: INFO: Waiting up to 5m0s for pod "kube-controller-manager-eqx04-flash04" in namespace "kube-system" to be "running and ready"
    Jan 12 01:34:56.121: INFO: Pod "kube-controller-manager-eqx04-flash04": Phase="Running", Reason="", readiness=true. Elapsed: 3.015355ms
    Jan 12 01:34:56.121: INFO: The phase of Pod kube-controller-manager-eqx04-flash04 is Running (Ready = true)
    Jan 12 01:34:56.121: INFO: Pod "kube-controller-manager-eqx04-flash04" satisfied condition "running and ready"
    Jan 12 01:34:56.180: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:34:56.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1957" for this suite. 01/12/23 01:34:56.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:34:56.227
Jan 12 01:34:56.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename watch 01/12/23 01:34:56.228
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:56.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:56.247
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/12/23 01:34:56.25
STEP: creating a new configmap 01/12/23 01:34:56.251
STEP: modifying the configmap once 01/12/23 01:34:56.256
STEP: closing the watch once it receives two notifications 01/12/23 01:34:56.264
Jan 12 01:34:56.264: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2065  e87a0780-a1f4-42f4-9da2-0594151d8ed9 20166489 0 2023-01-12 01:34:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 01:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 01:34:56.264: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2065  e87a0780-a1f4-42f4-9da2-0594151d8ed9 20166490 0 2023-01-12 01:34:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 01:34:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/12/23 01:34:56.264
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/12/23 01:34:56.271
STEP: deleting the configmap 01/12/23 01:34:56.273
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/12/23 01:34:56.279
Jan 12 01:34:56.279: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2065  e87a0780-a1f4-42f4-9da2-0594151d8ed9 20166491 0 2023-01-12 01:34:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 01:34:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 01:34:56.279: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2065  e87a0780-a1f4-42f4-9da2-0594151d8ed9 20166492 0 2023-01-12 01:34:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 01:34:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 12 01:34:56.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2065" for this suite. 01/12/23 01:34:56.285
------------------------------
• [0.094 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:34:56.227
    Jan 12 01:34:56.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename watch 01/12/23 01:34:56.228
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:56.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:56.247
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/12/23 01:34:56.25
    STEP: creating a new configmap 01/12/23 01:34:56.251
    STEP: modifying the configmap once 01/12/23 01:34:56.256
    STEP: closing the watch once it receives two notifications 01/12/23 01:34:56.264
    Jan 12 01:34:56.264: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2065  e87a0780-a1f4-42f4-9da2-0594151d8ed9 20166489 0 2023-01-12 01:34:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 01:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 01:34:56.264: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2065  e87a0780-a1f4-42f4-9da2-0594151d8ed9 20166490 0 2023-01-12 01:34:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 01:34:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/12/23 01:34:56.264
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/12/23 01:34:56.271
    STEP: deleting the configmap 01/12/23 01:34:56.273
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/12/23 01:34:56.279
    Jan 12 01:34:56.279: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2065  e87a0780-a1f4-42f4-9da2-0594151d8ed9 20166491 0 2023-01-12 01:34:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 01:34:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 01:34:56.279: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2065  e87a0780-a1f4-42f4-9da2-0594151d8ed9 20166492 0 2023-01-12 01:34:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 01:34:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:34:56.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2065" for this suite. 01/12/23 01:34:56.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:34:56.322
Jan 12 01:34:56.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 01:34:56.323
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:56.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:56.34
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-9f363fd8-c719-4968-b11b-86cc7e6655fb 01/12/23 01:34:56.343
STEP: Creating a pod to test consume configMaps 01/12/23 01:34:56.347
Jan 12 01:34:56.463: INFO: Waiting up to 5m0s for pod "pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af" in namespace "configmap-7082" to be "Succeeded or Failed"
Jan 12 01:34:56.466: INFO: Pod "pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.877504ms
Jan 12 01:34:58.469: INFO: Pod "pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006601456s
Jan 12 01:35:00.470: INFO: Pod "pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007084573s
Jan 12 01:35:02.470: INFO: Pod "pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006993472s
STEP: Saw pod success 01/12/23 01:35:02.47
Jan 12 01:35:02.470: INFO: Pod "pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af" satisfied condition "Succeeded or Failed"
Jan 12 01:35:02.473: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af container agnhost-container: <nil>
STEP: delete the pod 01/12/23 01:35:02.483
Jan 12 01:35:02.503: INFO: Waiting for pod pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af to disappear
Jan 12 01:35:02.506: INFO: Pod pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:35:02.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7082" for this suite. 01/12/23 01:35:02.511
------------------------------
• [SLOW TEST] [6.209 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:34:56.322
    Jan 12 01:34:56.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 01:34:56.323
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:34:56.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:34:56.34
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-9f363fd8-c719-4968-b11b-86cc7e6655fb 01/12/23 01:34:56.343
    STEP: Creating a pod to test consume configMaps 01/12/23 01:34:56.347
    Jan 12 01:34:56.463: INFO: Waiting up to 5m0s for pod "pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af" in namespace "configmap-7082" to be "Succeeded or Failed"
    Jan 12 01:34:56.466: INFO: Pod "pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.877504ms
    Jan 12 01:34:58.469: INFO: Pod "pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006601456s
    Jan 12 01:35:00.470: INFO: Pod "pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007084573s
    Jan 12 01:35:02.470: INFO: Pod "pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006993472s
    STEP: Saw pod success 01/12/23 01:35:02.47
    Jan 12 01:35:02.470: INFO: Pod "pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af" satisfied condition "Succeeded or Failed"
    Jan 12 01:35:02.473: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 01:35:02.483
    Jan 12 01:35:02.503: INFO: Waiting for pod pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af to disappear
    Jan 12 01:35:02.506: INFO: Pod pod-configmaps-a35ce218-8ba8-4b8c-a2e1-3ce335adf6af no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:35:02.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7082" for this suite. 01/12/23 01:35:02.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:35:02.535
Jan 12 01:35:02.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename sched-preemption 01/12/23 01:35:02.536
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:35:02.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:35:02.562
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 12 01:35:02.580: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 12 01:36:02.687: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
STEP: Create pods that use 4/5 of node resources. 01/12/23 01:36:02.691
Jan 12 01:36:02.843: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 12 01:36:03.017: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 12 01:36:03.120: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 12 01:36:03.261: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/12/23 01:36:03.262
Jan 12 01:36:03.262: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-228" to be "running"
Jan 12 01:36:03.265: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.958746ms
Jan 12 01:36:05.269: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007068894s
Jan 12 01:36:05.269: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 12 01:36:05.269: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-228" to be "running"
Jan 12 01:36:05.272: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.83165ms
Jan 12 01:36:07.276: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.006793891s
Jan 12 01:36:07.276: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 12 01:36:07.276: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-228" to be "running"
Jan 12 01:36:07.279: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.932958ms
Jan 12 01:36:07.279: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 12 01:36:07.279: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-228" to be "running"
Jan 12 01:36:07.281: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.839996ms
Jan 12 01:36:07.281: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/12/23 01:36:07.281
Jan 12 01:36:07.319: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-228" to be "running"
Jan 12 01:36:07.322: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.84335ms
Jan 12 01:36:09.328: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008789433s
Jan 12 01:36:11.327: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007815509s
Jan 12 01:36:11.327: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:36:11.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-228" for this suite. 01/12/23 01:36:11.408
------------------------------
• [SLOW TEST] [68.929 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:35:02.535
    Jan 12 01:35:02.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename sched-preemption 01/12/23 01:35:02.536
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:35:02.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:35:02.562
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 12 01:35:02.580: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 12 01:36:02.687: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:129
    STEP: Create pods that use 4/5 of node resources. 01/12/23 01:36:02.691
    Jan 12 01:36:02.843: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 12 01:36:03.017: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 12 01:36:03.120: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 12 01:36:03.261: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/12/23 01:36:03.262
    Jan 12 01:36:03.262: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-228" to be "running"
    Jan 12 01:36:03.265: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.958746ms
    Jan 12 01:36:05.269: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007068894s
    Jan 12 01:36:05.269: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 12 01:36:05.269: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-228" to be "running"
    Jan 12 01:36:05.272: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.83165ms
    Jan 12 01:36:07.276: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.006793891s
    Jan 12 01:36:07.276: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 12 01:36:07.276: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-228" to be "running"
    Jan 12 01:36:07.279: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.932958ms
    Jan 12 01:36:07.279: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 12 01:36:07.279: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-228" to be "running"
    Jan 12 01:36:07.281: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.839996ms
    Jan 12 01:36:07.281: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/12/23 01:36:07.281
    Jan 12 01:36:07.319: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-228" to be "running"
    Jan 12 01:36:07.322: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.84335ms
    Jan 12 01:36:09.328: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008789433s
    Jan 12 01:36:11.327: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007815509s
    Jan 12 01:36:11.327: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:36:11.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-228" for this suite. 01/12/23 01:36:11.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:36:11.469
Jan 12 01:36:11.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 01:36:11.47
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:36:11.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:36:11.494
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/12/23 01:36:11.497
Jan 12 01:36:11.552: INFO: Waiting up to 5m0s for pod "pod-631335d1-83a4-4141-9f35-2c83a675a886" in namespace "emptydir-2226" to be "Succeeded or Failed"
Jan 12 01:36:11.554: INFO: Pod "pod-631335d1-83a4-4141-9f35-2c83a675a886": Phase="Pending", Reason="", readiness=false. Elapsed: 2.876088ms
Jan 12 01:36:13.559: INFO: Pod "pod-631335d1-83a4-4141-9f35-2c83a675a886": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007888935s
Jan 12 01:36:15.558: INFO: Pod "pod-631335d1-83a4-4141-9f35-2c83a675a886": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006757179s
Jan 12 01:36:17.558: INFO: Pod "pod-631335d1-83a4-4141-9f35-2c83a675a886": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00653769s
STEP: Saw pod success 01/12/23 01:36:17.558
Jan 12 01:36:17.558: INFO: Pod "pod-631335d1-83a4-4141-9f35-2c83a675a886" satisfied condition "Succeeded or Failed"
Jan 12 01:36:17.561: INFO: Trying to get logs from node eqx04-flash06 pod pod-631335d1-83a4-4141-9f35-2c83a675a886 container test-container: <nil>
STEP: delete the pod 01/12/23 01:36:17.57
Jan 12 01:36:17.585: INFO: Waiting for pod pod-631335d1-83a4-4141-9f35-2c83a675a886 to disappear
Jan 12 01:36:17.588: INFO: Pod pod-631335d1-83a4-4141-9f35-2c83a675a886 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:36:17.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2226" for this suite. 01/12/23 01:36:17.599
------------------------------
• [SLOW TEST] [6.203 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:36:11.469
    Jan 12 01:36:11.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 01:36:11.47
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:36:11.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:36:11.494
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/12/23 01:36:11.497
    Jan 12 01:36:11.552: INFO: Waiting up to 5m0s for pod "pod-631335d1-83a4-4141-9f35-2c83a675a886" in namespace "emptydir-2226" to be "Succeeded or Failed"
    Jan 12 01:36:11.554: INFO: Pod "pod-631335d1-83a4-4141-9f35-2c83a675a886": Phase="Pending", Reason="", readiness=false. Elapsed: 2.876088ms
    Jan 12 01:36:13.559: INFO: Pod "pod-631335d1-83a4-4141-9f35-2c83a675a886": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007888935s
    Jan 12 01:36:15.558: INFO: Pod "pod-631335d1-83a4-4141-9f35-2c83a675a886": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006757179s
    Jan 12 01:36:17.558: INFO: Pod "pod-631335d1-83a4-4141-9f35-2c83a675a886": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00653769s
    STEP: Saw pod success 01/12/23 01:36:17.558
    Jan 12 01:36:17.558: INFO: Pod "pod-631335d1-83a4-4141-9f35-2c83a675a886" satisfied condition "Succeeded or Failed"
    Jan 12 01:36:17.561: INFO: Trying to get logs from node eqx04-flash06 pod pod-631335d1-83a4-4141-9f35-2c83a675a886 container test-container: <nil>
    STEP: delete the pod 01/12/23 01:36:17.57
    Jan 12 01:36:17.585: INFO: Waiting for pod pod-631335d1-83a4-4141-9f35-2c83a675a886 to disappear
    Jan 12 01:36:17.588: INFO: Pod pod-631335d1-83a4-4141-9f35-2c83a675a886 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:36:17.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2226" for this suite. 01/12/23 01:36:17.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:36:17.676
Jan 12 01:36:17.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename svcaccounts 01/12/23 01:36:17.678
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:36:17.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:36:17.705
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  01/12/23 01:36:17.708
Jan 12 01:36:17.831: INFO: Waiting up to 5m0s for pod "test-pod-02761377-ef68-469d-b639-eb2a74544869" in namespace "svcaccounts-8031" to be "Succeeded or Failed"
Jan 12 01:36:17.834: INFO: Pod "test-pod-02761377-ef68-469d-b639-eb2a74544869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.638785ms
Jan 12 01:36:19.839: INFO: Pod "test-pod-02761377-ef68-469d-b639-eb2a74544869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007647137s
Jan 12 01:36:21.837: INFO: Pod "test-pod-02761377-ef68-469d-b639-eb2a74544869": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006093283s
Jan 12 01:36:23.838: INFO: Pod "test-pod-02761377-ef68-469d-b639-eb2a74544869": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006398306s
STEP: Saw pod success 01/12/23 01:36:23.838
Jan 12 01:36:23.838: INFO: Pod "test-pod-02761377-ef68-469d-b639-eb2a74544869" satisfied condition "Succeeded or Failed"
Jan 12 01:36:23.840: INFO: Trying to get logs from node eqx04-flash06 pod test-pod-02761377-ef68-469d-b639-eb2a74544869 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 01:36:23.853
Jan 12 01:36:23.868: INFO: Waiting for pod test-pod-02761377-ef68-469d-b639-eb2a74544869 to disappear
Jan 12 01:36:23.871: INFO: Pod test-pod-02761377-ef68-469d-b639-eb2a74544869 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 12 01:36:23.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8031" for this suite. 01/12/23 01:36:23.876
------------------------------
• [SLOW TEST] [6.244 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:36:17.676
    Jan 12 01:36:17.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename svcaccounts 01/12/23 01:36:17.678
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:36:17.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:36:17.705
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  01/12/23 01:36:17.708
    Jan 12 01:36:17.831: INFO: Waiting up to 5m0s for pod "test-pod-02761377-ef68-469d-b639-eb2a74544869" in namespace "svcaccounts-8031" to be "Succeeded or Failed"
    Jan 12 01:36:17.834: INFO: Pod "test-pod-02761377-ef68-469d-b639-eb2a74544869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.638785ms
    Jan 12 01:36:19.839: INFO: Pod "test-pod-02761377-ef68-469d-b639-eb2a74544869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007647137s
    Jan 12 01:36:21.837: INFO: Pod "test-pod-02761377-ef68-469d-b639-eb2a74544869": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006093283s
    Jan 12 01:36:23.838: INFO: Pod "test-pod-02761377-ef68-469d-b639-eb2a74544869": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006398306s
    STEP: Saw pod success 01/12/23 01:36:23.838
    Jan 12 01:36:23.838: INFO: Pod "test-pod-02761377-ef68-469d-b639-eb2a74544869" satisfied condition "Succeeded or Failed"
    Jan 12 01:36:23.840: INFO: Trying to get logs from node eqx04-flash06 pod test-pod-02761377-ef68-469d-b639-eb2a74544869 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 01:36:23.853
    Jan 12 01:36:23.868: INFO: Waiting for pod test-pod-02761377-ef68-469d-b639-eb2a74544869 to disappear
    Jan 12 01:36:23.871: INFO: Pod test-pod-02761377-ef68-469d-b639-eb2a74544869 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:36:23.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8031" for this suite. 01/12/23 01:36:23.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:36:23.926
Jan 12 01:36:23.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename replicaset 01/12/23 01:36:23.926
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:36:23.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:36:23.945
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/12/23 01:36:23.947
STEP: Verify that the required pods have come up 01/12/23 01:36:23.958
Jan 12 01:36:23.961: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 12 01:36:28.966: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/12/23 01:36:28.966
Jan 12 01:36:28.972: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/12/23 01:36:28.972
STEP: DeleteCollection of the ReplicaSets 01/12/23 01:36:28.981
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/12/23 01:36:28.996
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 12 01:36:29.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1942" for this suite. 01/12/23 01:36:29.015
------------------------------
• [SLOW TEST] [5.140 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:36:23.926
    Jan 12 01:36:23.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename replicaset 01/12/23 01:36:23.926
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:36:23.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:36:23.945
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/12/23 01:36:23.947
    STEP: Verify that the required pods have come up 01/12/23 01:36:23.958
    Jan 12 01:36:23.961: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 12 01:36:28.966: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/12/23 01:36:28.966
    Jan 12 01:36:28.972: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/12/23 01:36:28.972
    STEP: DeleteCollection of the ReplicaSets 01/12/23 01:36:28.981
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/12/23 01:36:28.996
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:36:29.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1942" for this suite. 01/12/23 01:36:29.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:36:29.066
Jan 12 01:36:29.066: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename var-expansion 01/12/23 01:36:29.069
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:36:29.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:36:29.091
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 01/12/23 01:36:29.096
Jan 12 01:36:29.165: INFO: Waiting up to 5m0s for pod "var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5" in namespace "var-expansion-7690" to be "Succeeded or Failed"
Jan 12 01:36:29.168: INFO: Pod "var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.963564ms
Jan 12 01:36:31.172: INFO: Pod "var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007680327s
Jan 12 01:36:33.172: INFO: Pod "var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007356919s
Jan 12 01:36:35.173: INFO: Pod "var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008089134s
STEP: Saw pod success 01/12/23 01:36:35.173
Jan 12 01:36:35.173: INFO: Pod "var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5" satisfied condition "Succeeded or Failed"
Jan 12 01:36:35.176: INFO: Trying to get logs from node eqx04-flash06 pod var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5 container dapi-container: <nil>
STEP: delete the pod 01/12/23 01:36:35.186
Jan 12 01:36:35.202: INFO: Waiting for pod var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5 to disappear
Jan 12 01:36:35.205: INFO: Pod var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 01:36:35.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7690" for this suite. 01/12/23 01:36:35.21
------------------------------
• [SLOW TEST] [6.164 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:36:29.066
    Jan 12 01:36:29.066: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename var-expansion 01/12/23 01:36:29.069
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:36:29.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:36:29.091
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 01/12/23 01:36:29.096
    Jan 12 01:36:29.165: INFO: Waiting up to 5m0s for pod "var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5" in namespace "var-expansion-7690" to be "Succeeded or Failed"
    Jan 12 01:36:29.168: INFO: Pod "var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.963564ms
    Jan 12 01:36:31.172: INFO: Pod "var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007680327s
    Jan 12 01:36:33.172: INFO: Pod "var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007356919s
    Jan 12 01:36:35.173: INFO: Pod "var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008089134s
    STEP: Saw pod success 01/12/23 01:36:35.173
    Jan 12 01:36:35.173: INFO: Pod "var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5" satisfied condition "Succeeded or Failed"
    Jan 12 01:36:35.176: INFO: Trying to get logs from node eqx04-flash06 pod var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5 container dapi-container: <nil>
    STEP: delete the pod 01/12/23 01:36:35.186
    Jan 12 01:36:35.202: INFO: Waiting for pod var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5 to disappear
    Jan 12 01:36:35.205: INFO: Pod var-expansion-5b215081-67e1-4daf-ba2b-0125fe1088a5 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:36:35.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7690" for this suite. 01/12/23 01:36:35.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:36:35.232
Jan 12 01:36:35.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename certificates 01/12/23 01:36:35.233
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:36:35.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:36:35.254
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/12/23 01:36:35.837
STEP: getting /apis/certificates.k8s.io 01/12/23 01:36:35.841
STEP: getting /apis/certificates.k8s.io/v1 01/12/23 01:36:35.842
STEP: creating 01/12/23 01:36:35.843
STEP: getting 01/12/23 01:36:35.875
STEP: listing 01/12/23 01:36:35.878
STEP: watching 01/12/23 01:36:35.881
Jan 12 01:36:35.881: INFO: starting watch
STEP: patching 01/12/23 01:36:35.882
STEP: updating 01/12/23 01:36:35.891
Jan 12 01:36:35.896: INFO: waiting for watch events with expected annotations
Jan 12 01:36:35.896: INFO: saw patched and updated annotations
STEP: getting /approval 01/12/23 01:36:35.897
STEP: patching /approval 01/12/23 01:36:35.899
STEP: updating /approval 01/12/23 01:36:35.907
STEP: getting /status 01/12/23 01:36:35.917
STEP: patching /status 01/12/23 01:36:35.92
STEP: updating /status 01/12/23 01:36:35.934
STEP: deleting 01/12/23 01:36:35.942
STEP: deleting a collection 01/12/23 01:36:35.96
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:36:35.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-5646" for this suite. 01/12/23 01:36:35.986
------------------------------
• [0.772 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:36:35.232
    Jan 12 01:36:35.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename certificates 01/12/23 01:36:35.233
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:36:35.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:36:35.254
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/12/23 01:36:35.837
    STEP: getting /apis/certificates.k8s.io 01/12/23 01:36:35.841
    STEP: getting /apis/certificates.k8s.io/v1 01/12/23 01:36:35.842
    STEP: creating 01/12/23 01:36:35.843
    STEP: getting 01/12/23 01:36:35.875
    STEP: listing 01/12/23 01:36:35.878
    STEP: watching 01/12/23 01:36:35.881
    Jan 12 01:36:35.881: INFO: starting watch
    STEP: patching 01/12/23 01:36:35.882
    STEP: updating 01/12/23 01:36:35.891
    Jan 12 01:36:35.896: INFO: waiting for watch events with expected annotations
    Jan 12 01:36:35.896: INFO: saw patched and updated annotations
    STEP: getting /approval 01/12/23 01:36:35.897
    STEP: patching /approval 01/12/23 01:36:35.899
    STEP: updating /approval 01/12/23 01:36:35.907
    STEP: getting /status 01/12/23 01:36:35.917
    STEP: patching /status 01/12/23 01:36:35.92
    STEP: updating /status 01/12/23 01:36:35.934
    STEP: deleting 01/12/23 01:36:35.942
    STEP: deleting a collection 01/12/23 01:36:35.96
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:36:35.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-5646" for this suite. 01/12/23 01:36:35.986
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:36:36.006
Jan 12 01:36:36.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename statefulset 01/12/23 01:36:36.007
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:36:36.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:36:36.033
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9851 01/12/23 01:36:36.036
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-9851 01/12/23 01:36:36.054
Jan 12 01:36:36.100: INFO: Found 0 stateful pods, waiting for 1
Jan 12 01:36:46.106: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/12/23 01:36:46.112
STEP: Getting /status 01/12/23 01:36:46.12
Jan 12 01:36:46.124: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/12/23 01:36:46.124
Jan 12 01:36:46.133: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/12/23 01:36:46.133
Jan 12 01:36:46.135: INFO: Observed &StatefulSet event: ADDED
Jan 12 01:36:46.135: INFO: Found Statefulset ss in namespace statefulset-9851 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 12 01:36:46.135: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/12/23 01:36:46.135
Jan 12 01:36:46.135: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 12 01:36:46.144: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/12/23 01:36:46.144
Jan 12 01:36:46.145: INFO: Observed &StatefulSet event: ADDED
Jan 12 01:36:46.145: INFO: Observed Statefulset ss in namespace statefulset-9851 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 12 01:36:46.145: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 01:36:46.146: INFO: Deleting all statefulset in ns statefulset-9851
Jan 12 01:36:46.148: INFO: Scaling statefulset ss to 0
Jan 12 01:36:56.167: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 01:36:56.170: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 01:36:56.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9851" for this suite. 01/12/23 01:36:56.19
------------------------------
• [SLOW TEST] [20.250 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:36:36.006
    Jan 12 01:36:36.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename statefulset 01/12/23 01:36:36.007
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:36:36.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:36:36.033
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9851 01/12/23 01:36:36.036
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-9851 01/12/23 01:36:36.054
    Jan 12 01:36:36.100: INFO: Found 0 stateful pods, waiting for 1
    Jan 12 01:36:46.106: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/12/23 01:36:46.112
    STEP: Getting /status 01/12/23 01:36:46.12
    Jan 12 01:36:46.124: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/12/23 01:36:46.124
    Jan 12 01:36:46.133: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/12/23 01:36:46.133
    Jan 12 01:36:46.135: INFO: Observed &StatefulSet event: ADDED
    Jan 12 01:36:46.135: INFO: Found Statefulset ss in namespace statefulset-9851 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 12 01:36:46.135: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/12/23 01:36:46.135
    Jan 12 01:36:46.135: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 12 01:36:46.144: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/12/23 01:36:46.144
    Jan 12 01:36:46.145: INFO: Observed &StatefulSet event: ADDED
    Jan 12 01:36:46.145: INFO: Observed Statefulset ss in namespace statefulset-9851 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 12 01:36:46.145: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 01:36:46.146: INFO: Deleting all statefulset in ns statefulset-9851
    Jan 12 01:36:46.148: INFO: Scaling statefulset ss to 0
    Jan 12 01:36:56.167: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 01:36:56.170: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:36:56.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9851" for this suite. 01/12/23 01:36:56.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:36:56.258
Jan 12 01:36:56.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename secrets 01/12/23 01:36:56.259
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:36:56.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:36:56.277
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-6766/secret-test-3726ba0e-fbba-4cbd-b07e-d2c808393f9a 01/12/23 01:36:56.28
STEP: Creating a pod to test consume secrets 01/12/23 01:36:56.285
Jan 12 01:36:56.387: INFO: Waiting up to 5m0s for pod "pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5" in namespace "secrets-6766" to be "Succeeded or Failed"
Jan 12 01:36:56.390: INFO: Pod "pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.957881ms
Jan 12 01:36:58.395: INFO: Pod "pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007553209s
Jan 12 01:37:00.395: INFO: Pod "pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008091595s
Jan 12 01:37:02.394: INFO: Pod "pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006882824s
STEP: Saw pod success 01/12/23 01:37:02.394
Jan 12 01:37:02.394: INFO: Pod "pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5" satisfied condition "Succeeded or Failed"
Jan 12 01:37:02.397: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5 container env-test: <nil>
STEP: delete the pod 01/12/23 01:37:02.408
Jan 12 01:37:02.422: INFO: Waiting for pod pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5 to disappear
Jan 12 01:37:02.425: INFO: Pod pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 01:37:02.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6766" for this suite. 01/12/23 01:37:02.43
------------------------------
• [SLOW TEST] [6.200 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:36:56.258
    Jan 12 01:36:56.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename secrets 01/12/23 01:36:56.259
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:36:56.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:36:56.277
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-6766/secret-test-3726ba0e-fbba-4cbd-b07e-d2c808393f9a 01/12/23 01:36:56.28
    STEP: Creating a pod to test consume secrets 01/12/23 01:36:56.285
    Jan 12 01:36:56.387: INFO: Waiting up to 5m0s for pod "pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5" in namespace "secrets-6766" to be "Succeeded or Failed"
    Jan 12 01:36:56.390: INFO: Pod "pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.957881ms
    Jan 12 01:36:58.395: INFO: Pod "pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007553209s
    Jan 12 01:37:00.395: INFO: Pod "pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008091595s
    Jan 12 01:37:02.394: INFO: Pod "pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006882824s
    STEP: Saw pod success 01/12/23 01:37:02.394
    Jan 12 01:37:02.394: INFO: Pod "pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5" satisfied condition "Succeeded or Failed"
    Jan 12 01:37:02.397: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5 container env-test: <nil>
    STEP: delete the pod 01/12/23 01:37:02.408
    Jan 12 01:37:02.422: INFO: Waiting for pod pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5 to disappear
    Jan 12 01:37:02.425: INFO: Pod pod-configmaps-42e0e091-f649-45b3-b8f3-fd8ed0c436c5 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:37:02.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6766" for this suite. 01/12/23 01:37:02.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:37:02.459
Jan 12 01:37:02.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 01:37:02.46
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:02.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:02.479
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jan 12 01:37:02.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/12/23 01:37:04.867
Jan 12 01:37:04.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3736 --namespace=crd-publish-openapi-3736 create -f -'
Jan 12 01:37:05.720: INFO: stderr: ""
Jan 12 01:37:05.720: INFO: stdout: "e2e-test-crd-publish-openapi-3350-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 12 01:37:05.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3736 --namespace=crd-publish-openapi-3736 delete e2e-test-crd-publish-openapi-3350-crds test-cr'
Jan 12 01:37:05.826: INFO: stderr: ""
Jan 12 01:37:05.826: INFO: stdout: "e2e-test-crd-publish-openapi-3350-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 12 01:37:05.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3736 --namespace=crd-publish-openapi-3736 apply -f -'
Jan 12 01:37:06.380: INFO: stderr: ""
Jan 12 01:37:06.380: INFO: stdout: "e2e-test-crd-publish-openapi-3350-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 12 01:37:06.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3736 --namespace=crd-publish-openapi-3736 delete e2e-test-crd-publish-openapi-3350-crds test-cr'
Jan 12 01:37:06.493: INFO: stderr: ""
Jan 12 01:37:06.493: INFO: stdout: "e2e-test-crd-publish-openapi-3350-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/12/23 01:37:06.493
Jan 12 01:37:06.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3736 explain e2e-test-crd-publish-openapi-3350-crds'
Jan 12 01:37:06.770: INFO: stderr: ""
Jan 12 01:37:06.770: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3350-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:37:09.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3736" for this suite. 01/12/23 01:37:09.072
------------------------------
• [SLOW TEST] [6.631 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:37:02.459
    Jan 12 01:37:02.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 01:37:02.46
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:02.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:02.479
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jan 12 01:37:02.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/12/23 01:37:04.867
    Jan 12 01:37:04.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3736 --namespace=crd-publish-openapi-3736 create -f -'
    Jan 12 01:37:05.720: INFO: stderr: ""
    Jan 12 01:37:05.720: INFO: stdout: "e2e-test-crd-publish-openapi-3350-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 12 01:37:05.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3736 --namespace=crd-publish-openapi-3736 delete e2e-test-crd-publish-openapi-3350-crds test-cr'
    Jan 12 01:37:05.826: INFO: stderr: ""
    Jan 12 01:37:05.826: INFO: stdout: "e2e-test-crd-publish-openapi-3350-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 12 01:37:05.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3736 --namespace=crd-publish-openapi-3736 apply -f -'
    Jan 12 01:37:06.380: INFO: stderr: ""
    Jan 12 01:37:06.380: INFO: stdout: "e2e-test-crd-publish-openapi-3350-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 12 01:37:06.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3736 --namespace=crd-publish-openapi-3736 delete e2e-test-crd-publish-openapi-3350-crds test-cr'
    Jan 12 01:37:06.493: INFO: stderr: ""
    Jan 12 01:37:06.493: INFO: stdout: "e2e-test-crd-publish-openapi-3350-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/12/23 01:37:06.493
    Jan 12 01:37:06.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-3736 explain e2e-test-crd-publish-openapi-3350-crds'
    Jan 12 01:37:06.770: INFO: stderr: ""
    Jan 12 01:37:06.770: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3350-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:37:09.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3736" for this suite. 01/12/23 01:37:09.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:37:09.091
Jan 12 01:37:09.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 01:37:09.092
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:09.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:09.111
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-623a28aa-e9a8-4e38-8fe6-46fff9afea66 01/12/23 01:37:09.114
STEP: Creating a pod to test consume configMaps 01/12/23 01:37:09.119
Jan 12 01:37:09.152: INFO: Waiting up to 5m0s for pod "pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb" in namespace "configmap-3532" to be "Succeeded or Failed"
Jan 12 01:37:09.155: INFO: Pod "pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.780462ms
Jan 12 01:37:11.159: INFO: Pod "pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007385019s
Jan 12 01:37:13.159: INFO: Pod "pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007079348s
Jan 12 01:37:15.160: INFO: Pod "pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008165152s
STEP: Saw pod success 01/12/23 01:37:15.16
Jan 12 01:37:15.160: INFO: Pod "pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb" satisfied condition "Succeeded or Failed"
Jan 12 01:37:15.163: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb container agnhost-container: <nil>
STEP: delete the pod 01/12/23 01:37:15.173
Jan 12 01:37:15.188: INFO: Waiting for pod pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb to disappear
Jan 12 01:37:15.191: INFO: Pod pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:37:15.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3532" for this suite. 01/12/23 01:37:15.196
------------------------------
• [SLOW TEST] [6.122 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:37:09.091
    Jan 12 01:37:09.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 01:37:09.092
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:09.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:09.111
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-623a28aa-e9a8-4e38-8fe6-46fff9afea66 01/12/23 01:37:09.114
    STEP: Creating a pod to test consume configMaps 01/12/23 01:37:09.119
    Jan 12 01:37:09.152: INFO: Waiting up to 5m0s for pod "pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb" in namespace "configmap-3532" to be "Succeeded or Failed"
    Jan 12 01:37:09.155: INFO: Pod "pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.780462ms
    Jan 12 01:37:11.159: INFO: Pod "pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007385019s
    Jan 12 01:37:13.159: INFO: Pod "pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007079348s
    Jan 12 01:37:15.160: INFO: Pod "pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008165152s
    STEP: Saw pod success 01/12/23 01:37:15.16
    Jan 12 01:37:15.160: INFO: Pod "pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb" satisfied condition "Succeeded or Failed"
    Jan 12 01:37:15.163: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 01:37:15.173
    Jan 12 01:37:15.188: INFO: Waiting for pod pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb to disappear
    Jan 12 01:37:15.191: INFO: Pod pod-configmaps-39c689e0-a5e8-4721-894f-f286ba3d46fb no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:37:15.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3532" for this suite. 01/12/23 01:37:15.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:37:15.216
Jan 12 01:37:15.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename replicaset 01/12/23 01:37:15.217
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:15.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:15.235
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 12 01:37:15.238: INFO: Creating ReplicaSet my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198
Jan 12 01:37:15.246: INFO: Pod name my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198: Found 0 pods out of 1
Jan 12 01:37:20.252: INFO: Pod name my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198: Found 1 pods out of 1
Jan 12 01:37:20.252: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198" is running
Jan 12 01:37:20.252: INFO: Waiting up to 5m0s for pod "my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198-l2kwv" in namespace "replicaset-3165" to be "running"
Jan 12 01:37:20.256: INFO: Pod "my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198-l2kwv": Phase="Running", Reason="", readiness=true. Elapsed: 3.671069ms
Jan 12 01:37:20.256: INFO: Pod "my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198-l2kwv" satisfied condition "running"
Jan 12 01:37:20.256: INFO: Pod "my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198-l2kwv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:37:15 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:37:17 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:37:17 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:37:15 +0000 UTC Reason: Message:}])
Jan 12 01:37:20.256: INFO: Trying to dial the pod
Jan 12 01:37:25.267: INFO: Controller my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198: Got expected result from replica 1 [my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198-l2kwv]: "my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198-l2kwv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 12 01:37:25.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3165" for this suite. 01/12/23 01:37:25.271
------------------------------
• [SLOW TEST] [10.080 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:37:15.216
    Jan 12 01:37:15.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename replicaset 01/12/23 01:37:15.217
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:15.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:15.235
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 12 01:37:15.238: INFO: Creating ReplicaSet my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198
    Jan 12 01:37:15.246: INFO: Pod name my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198: Found 0 pods out of 1
    Jan 12 01:37:20.252: INFO: Pod name my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198: Found 1 pods out of 1
    Jan 12 01:37:20.252: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198" is running
    Jan 12 01:37:20.252: INFO: Waiting up to 5m0s for pod "my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198-l2kwv" in namespace "replicaset-3165" to be "running"
    Jan 12 01:37:20.256: INFO: Pod "my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198-l2kwv": Phase="Running", Reason="", readiness=true. Elapsed: 3.671069ms
    Jan 12 01:37:20.256: INFO: Pod "my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198-l2kwv" satisfied condition "running"
    Jan 12 01:37:20.256: INFO: Pod "my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198-l2kwv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:37:15 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:37:17 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:37:17 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 01:37:15 +0000 UTC Reason: Message:}])
    Jan 12 01:37:20.256: INFO: Trying to dial the pod
    Jan 12 01:37:25.267: INFO: Controller my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198: Got expected result from replica 1 [my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198-l2kwv]: "my-hostname-basic-77a2d13e-4c4f-4876-b053-c799cc0ba198-l2kwv", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:37:25.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3165" for this suite. 01/12/23 01:37:25.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:37:25.298
Jan 12 01:37:25.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename secrets 01/12/23 01:37:25.299
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:25.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:25.318
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-e6e79dc7-d117-46af-8012-3a82b9995807 01/12/23 01:37:25.321
STEP: Creating a pod to test consume secrets 01/12/23 01:37:25.326
Jan 12 01:37:25.368: INFO: Waiting up to 5m0s for pod "pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e" in namespace "secrets-6401" to be "Succeeded or Failed"
Jan 12 01:37:25.373: INFO: Pod "pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.995985ms
Jan 12 01:37:27.377: INFO: Pod "pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008739142s
Jan 12 01:37:29.378: INFO: Pod "pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009421529s
Jan 12 01:37:31.378: INFO: Pod "pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010109656s
STEP: Saw pod success 01/12/23 01:37:31.378
Jan 12 01:37:31.378: INFO: Pod "pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e" satisfied condition "Succeeded or Failed"
Jan 12 01:37:31.382: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 01:37:31.393
Jan 12 01:37:31.408: INFO: Waiting for pod pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e to disappear
Jan 12 01:37:31.411: INFO: Pod pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 01:37:31.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6401" for this suite. 01/12/23 01:37:31.416
------------------------------
• [SLOW TEST] [6.185 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:37:25.298
    Jan 12 01:37:25.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename secrets 01/12/23 01:37:25.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:25.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:25.318
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-e6e79dc7-d117-46af-8012-3a82b9995807 01/12/23 01:37:25.321
    STEP: Creating a pod to test consume secrets 01/12/23 01:37:25.326
    Jan 12 01:37:25.368: INFO: Waiting up to 5m0s for pod "pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e" in namespace "secrets-6401" to be "Succeeded or Failed"
    Jan 12 01:37:25.373: INFO: Pod "pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.995985ms
    Jan 12 01:37:27.377: INFO: Pod "pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008739142s
    Jan 12 01:37:29.378: INFO: Pod "pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009421529s
    Jan 12 01:37:31.378: INFO: Pod "pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010109656s
    STEP: Saw pod success 01/12/23 01:37:31.378
    Jan 12 01:37:31.378: INFO: Pod "pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e" satisfied condition "Succeeded or Failed"
    Jan 12 01:37:31.382: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 01:37:31.393
    Jan 12 01:37:31.408: INFO: Waiting for pod pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e to disappear
    Jan 12 01:37:31.411: INFO: Pod pod-secrets-b651255c-249f-4d6c-aba9-ba0ad970114e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:37:31.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6401" for this suite. 01/12/23 01:37:31.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:37:31.483
Jan 12 01:37:31.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 01:37:31.484
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:31.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:31.504
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 01/12/23 01:37:31.507
Jan 12 01:37:31.539: INFO: Waiting up to 5m0s for pod "pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5" in namespace "emptydir-2889" to be "Succeeded or Failed"
Jan 12 01:37:31.542: INFO: Pod "pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.951735ms
Jan 12 01:37:33.548: INFO: Pod "pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008093932s
Jan 12 01:37:35.546: INFO: Pod "pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006801928s
Jan 12 01:37:37.546: INFO: Pod "pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006876571s
STEP: Saw pod success 01/12/23 01:37:37.546
Jan 12 01:37:37.547: INFO: Pod "pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5" satisfied condition "Succeeded or Failed"
Jan 12 01:37:37.550: INFO: Trying to get logs from node eqx04-flash06 pod pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5 container test-container: <nil>
STEP: delete the pod 01/12/23 01:37:37.562
Jan 12 01:37:37.576: INFO: Waiting for pod pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5 to disappear
Jan 12 01:37:37.579: INFO: Pod pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:37:37.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2889" for this suite. 01/12/23 01:37:37.584
------------------------------
• [SLOW TEST] [6.151 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:37:31.483
    Jan 12 01:37:31.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 01:37:31.484
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:31.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:31.504
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/12/23 01:37:31.507
    Jan 12 01:37:31.539: INFO: Waiting up to 5m0s for pod "pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5" in namespace "emptydir-2889" to be "Succeeded or Failed"
    Jan 12 01:37:31.542: INFO: Pod "pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.951735ms
    Jan 12 01:37:33.548: INFO: Pod "pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008093932s
    Jan 12 01:37:35.546: INFO: Pod "pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006801928s
    Jan 12 01:37:37.546: INFO: Pod "pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006876571s
    STEP: Saw pod success 01/12/23 01:37:37.546
    Jan 12 01:37:37.547: INFO: Pod "pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5" satisfied condition "Succeeded or Failed"
    Jan 12 01:37:37.550: INFO: Trying to get logs from node eqx04-flash06 pod pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5 container test-container: <nil>
    STEP: delete the pod 01/12/23 01:37:37.562
    Jan 12 01:37:37.576: INFO: Waiting for pod pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5 to disappear
    Jan 12 01:37:37.579: INFO: Pod pod-acd63ce0-737e-4f09-8fdd-bfdfcc06c7b5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:37:37.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2889" for this suite. 01/12/23 01:37:37.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:37:37.635
Jan 12 01:37:37.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 01:37:37.636
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:37.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:37.655
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 01:37:37.657
Jan 12 01:37:37.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-188 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Jan 12 01:37:37.792: INFO: stderr: ""
Jan 12 01:37:37.792: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/12/23 01:37:37.792
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jan 12 01:37:37.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-188 delete pods e2e-test-httpd-pod'
Jan 12 01:37:37.873: INFO: stderr: ""
Jan 12 01:37:37.873: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 01:37:37.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-188" for this suite. 01/12/23 01:37:37.877
------------------------------
• [0.284 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:37:37.635
    Jan 12 01:37:37.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 01:37:37.636
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:37.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:37.655
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 01:37:37.657
    Jan 12 01:37:37.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-188 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Jan 12 01:37:37.792: INFO: stderr: ""
    Jan 12 01:37:37.792: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/12/23 01:37:37.792
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jan 12 01:37:37.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-188 delete pods e2e-test-httpd-pod'
    Jan 12 01:37:37.873: INFO: stderr: ""
    Jan 12 01:37:37.873: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:37:37.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-188" for this suite. 01/12/23 01:37:37.877
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:37:37.92
Jan 12 01:37:37.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 01:37:37.921
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:37.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:37.938
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/12/23 01:37:37.941
Jan 12 01:37:37.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/12/23 01:37:45.752
Jan 12 01:37:45.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:37:48.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:37:58.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9275" for this suite. 01/12/23 01:37:58.303
------------------------------
• [SLOW TEST] [20.414 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:37:37.92
    Jan 12 01:37:37.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 01:37:37.921
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:37.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:37.938
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/12/23 01:37:37.941
    Jan 12 01:37:37.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/12/23 01:37:45.752
    Jan 12 01:37:45.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:37:48.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:37:58.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9275" for this suite. 01/12/23 01:37:58.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:37:58.336
Jan 12 01:37:58.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename sysctl 01/12/23 01:37:58.337
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:58.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:58.35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/12/23 01:37:58.352
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:37:58.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-4850" for this suite. 01/12/23 01:37:58.362
------------------------------
• [0.044 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:37:58.336
    Jan 12 01:37:58.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename sysctl 01/12/23 01:37:58.337
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:58.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:58.35
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/12/23 01:37:58.352
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:37:58.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-4850" for this suite. 01/12/23 01:37:58.362
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:37:58.383
Jan 12 01:37:58.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename statefulset 01/12/23 01:37:58.383
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:58.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:58.396
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1525 01/12/23 01:37:58.398
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Jan 12 01:37:58.450: INFO: Found 0 stateful pods, waiting for 1
Jan 12 01:38:08.455: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/12/23 01:38:08.46
W0112 01:38:08.476380      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 12 01:38:08.485: INFO: Found 1 stateful pods, waiting for 2
Jan 12 01:38:18.491: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 01:38:18.491: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/12/23 01:38:18.496
STEP: Delete all of the StatefulSets 01/12/23 01:38:18.499
STEP: Verify that StatefulSets have been deleted 01/12/23 01:38:18.506
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 01:38:18.515: INFO: Deleting all statefulset in ns statefulset-1525
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 01:38:18.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1525" for this suite. 01/12/23 01:38:18.563
------------------------------
• [SLOW TEST] [20.227 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:37:58.383
    Jan 12 01:37:58.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename statefulset 01/12/23 01:37:58.383
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:37:58.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:37:58.396
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1525 01/12/23 01:37:58.398
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Jan 12 01:37:58.450: INFO: Found 0 stateful pods, waiting for 1
    Jan 12 01:38:08.455: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/12/23 01:38:08.46
    W0112 01:38:08.476380      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 12 01:38:08.485: INFO: Found 1 stateful pods, waiting for 2
    Jan 12 01:38:18.491: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 01:38:18.491: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/12/23 01:38:18.496
    STEP: Delete all of the StatefulSets 01/12/23 01:38:18.499
    STEP: Verify that StatefulSets have been deleted 01/12/23 01:38:18.506
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 01:38:18.515: INFO: Deleting all statefulset in ns statefulset-1525
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:38:18.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1525" for this suite. 01/12/23 01:38:18.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:38:18.621
Jan 12 01:38:18.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename limitrange 01/12/23 01:38:18.629
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:18.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:18.643
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 01/12/23 01:38:18.646
STEP: Setting up watch 01/12/23 01:38:18.646
STEP: Submitting a LimitRange 01/12/23 01:38:18.75
STEP: Verifying LimitRange creation was observed 01/12/23 01:38:18.754
STEP: Fetching the LimitRange to ensure it has proper values 01/12/23 01:38:18.754
Jan 12 01:38:18.757: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 12 01:38:18.757: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/12/23 01:38:18.757
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/12/23 01:38:18.832
Jan 12 01:38:18.834: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 12 01:38:18.834: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/12/23 01:38:18.834
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/12/23 01:38:18.958
Jan 12 01:38:18.962: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 12 01:38:18.962: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/12/23 01:38:18.962
STEP: Failing to create a Pod with more than max resources 01/12/23 01:38:18.966
STEP: Updating a LimitRange 01/12/23 01:38:18.972
STEP: Verifying LimitRange updating is effective 01/12/23 01:38:18.979
STEP: Creating a Pod with less than former min resources 01/12/23 01:38:20.983
STEP: Failing to create a Pod with more than max resources 01/12/23 01:38:21.015
STEP: Deleting a LimitRange 01/12/23 01:38:21.018
STEP: Verifying the LimitRange was deleted 01/12/23 01:38:21.023
Jan 12 01:38:26.028: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/12/23 01:38:26.028
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 12 01:38:26.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-4702" for this suite. 01/12/23 01:38:26.067
------------------------------
• [SLOW TEST] [7.480 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:38:18.621
    Jan 12 01:38:18.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename limitrange 01/12/23 01:38:18.629
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:18.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:18.643
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 01/12/23 01:38:18.646
    STEP: Setting up watch 01/12/23 01:38:18.646
    STEP: Submitting a LimitRange 01/12/23 01:38:18.75
    STEP: Verifying LimitRange creation was observed 01/12/23 01:38:18.754
    STEP: Fetching the LimitRange to ensure it has proper values 01/12/23 01:38:18.754
    Jan 12 01:38:18.757: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 12 01:38:18.757: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/12/23 01:38:18.757
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/12/23 01:38:18.832
    Jan 12 01:38:18.834: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 12 01:38:18.834: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/12/23 01:38:18.834
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/12/23 01:38:18.958
    Jan 12 01:38:18.962: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 12 01:38:18.962: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/12/23 01:38:18.962
    STEP: Failing to create a Pod with more than max resources 01/12/23 01:38:18.966
    STEP: Updating a LimitRange 01/12/23 01:38:18.972
    STEP: Verifying LimitRange updating is effective 01/12/23 01:38:18.979
    STEP: Creating a Pod with less than former min resources 01/12/23 01:38:20.983
    STEP: Failing to create a Pod with more than max resources 01/12/23 01:38:21.015
    STEP: Deleting a LimitRange 01/12/23 01:38:21.018
    STEP: Verifying the LimitRange was deleted 01/12/23 01:38:21.023
    Jan 12 01:38:26.028: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/12/23 01:38:26.028
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:38:26.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-4702" for this suite. 01/12/23 01:38:26.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:38:26.103
Jan 12 01:38:26.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename controllerrevisions 01/12/23 01:38:26.104
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:26.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:26.116
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-g9nrk-daemon-set" 01/12/23 01:38:26.133
STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 01:38:26.161
Jan 12 01:38:26.166: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:38:26.166: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:38:26.166: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:38:26.168: INFO: Number of nodes with available pods controlled by daemonset e2e-g9nrk-daemon-set: 0
Jan 12 01:38:26.168: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:38:27.172: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:38:27.173: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:38:27.173: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:38:27.175: INFO: Number of nodes with available pods controlled by daemonset e2e-g9nrk-daemon-set: 0
Jan 12 01:38:27.175: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:38:28.172: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:38:28.172: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:38:28.172: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:38:28.175: INFO: Number of nodes with available pods controlled by daemonset e2e-g9nrk-daemon-set: 0
Jan 12 01:38:28.175: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:38:29.174: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:38:29.174: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:38:29.174: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:38:29.176: INFO: Number of nodes with available pods controlled by daemonset e2e-g9nrk-daemon-set: 2
Jan 12 01:38:29.176: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-g9nrk-daemon-set
STEP: Confirm DaemonSet "e2e-g9nrk-daemon-set" successfully created with "daemonset-name=e2e-g9nrk-daemon-set" label 01/12/23 01:38:29.179
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-g9nrk-daemon-set" 01/12/23 01:38:29.185
Jan 12 01:38:29.189: INFO: Located ControllerRevision: "e2e-g9nrk-daemon-set-5dd4fb7db6"
STEP: Patching ControllerRevision "e2e-g9nrk-daemon-set-5dd4fb7db6" 01/12/23 01:38:29.191
Jan 12 01:38:29.197: INFO: e2e-g9nrk-daemon-set-5dd4fb7db6 has been patched
STEP: Create a new ControllerRevision 01/12/23 01:38:29.197
Jan 12 01:38:29.201: INFO: Created ControllerRevision: e2e-g9nrk-daemon-set-5699664cb
STEP: Confirm that there are two ControllerRevisions 01/12/23 01:38:29.201
Jan 12 01:38:29.202: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 12 01:38:29.204: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-g9nrk-daemon-set-5dd4fb7db6" 01/12/23 01:38:29.204
STEP: Confirm that there is only one ControllerRevision 01/12/23 01:38:29.21
Jan 12 01:38:29.210: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 12 01:38:29.212: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-g9nrk-daemon-set-5699664cb" 01/12/23 01:38:29.214
Jan 12 01:38:29.220: INFO: e2e-g9nrk-daemon-set-5699664cb has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/12/23 01:38:29.221
W0112 01:38:29.231341      21 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/12/23 01:38:29.231
Jan 12 01:38:29.231: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 12 01:38:30.233: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 12 01:38:30.237: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-g9nrk-daemon-set-5699664cb=updated" 01/12/23 01:38:30.237
STEP: Confirm that there is only one ControllerRevision 01/12/23 01:38:30.242
Jan 12 01:38:30.242: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 12 01:38:30.244: INFO: Found 1 ControllerRevisions
Jan 12 01:38:30.246: INFO: ControllerRevision "e2e-g9nrk-daemon-set-5757b8b7fc" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-g9nrk-daemon-set" 01/12/23 01:38:30.248
STEP: deleting DaemonSet.extensions e2e-g9nrk-daemon-set in namespace controllerrevisions-3111, will wait for the garbage collector to delete the pods 01/12/23 01:38:30.248
Jan 12 01:38:30.307: INFO: Deleting DaemonSet.extensions e2e-g9nrk-daemon-set took: 5.911669ms
Jan 12 01:38:30.407: INFO: Terminating DaemonSet.extensions e2e-g9nrk-daemon-set pods took: 100.5256ms
Jan 12 01:38:32.110: INFO: Number of nodes with available pods controlled by daemonset e2e-g9nrk-daemon-set: 0
Jan 12 01:38:32.110: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-g9nrk-daemon-set
Jan 12 01:38:32.112: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20168273"},"items":null}

Jan 12 01:38:32.114: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20168273"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:38:32.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-3111" for this suite. 01/12/23 01:38:32.124
------------------------------
• [SLOW TEST] [6.040 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:38:26.103
    Jan 12 01:38:26.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename controllerrevisions 01/12/23 01:38:26.104
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:26.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:26.116
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-g9nrk-daemon-set" 01/12/23 01:38:26.133
    STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 01:38:26.161
    Jan 12 01:38:26.166: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:38:26.166: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:38:26.166: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:38:26.168: INFO: Number of nodes with available pods controlled by daemonset e2e-g9nrk-daemon-set: 0
    Jan 12 01:38:26.168: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:38:27.172: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:38:27.173: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:38:27.173: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:38:27.175: INFO: Number of nodes with available pods controlled by daemonset e2e-g9nrk-daemon-set: 0
    Jan 12 01:38:27.175: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:38:28.172: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:38:28.172: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:38:28.172: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:38:28.175: INFO: Number of nodes with available pods controlled by daemonset e2e-g9nrk-daemon-set: 0
    Jan 12 01:38:28.175: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:38:29.174: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:38:29.174: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:38:29.174: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:38:29.176: INFO: Number of nodes with available pods controlled by daemonset e2e-g9nrk-daemon-set: 2
    Jan 12 01:38:29.176: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-g9nrk-daemon-set
    STEP: Confirm DaemonSet "e2e-g9nrk-daemon-set" successfully created with "daemonset-name=e2e-g9nrk-daemon-set" label 01/12/23 01:38:29.179
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-g9nrk-daemon-set" 01/12/23 01:38:29.185
    Jan 12 01:38:29.189: INFO: Located ControllerRevision: "e2e-g9nrk-daemon-set-5dd4fb7db6"
    STEP: Patching ControllerRevision "e2e-g9nrk-daemon-set-5dd4fb7db6" 01/12/23 01:38:29.191
    Jan 12 01:38:29.197: INFO: e2e-g9nrk-daemon-set-5dd4fb7db6 has been patched
    STEP: Create a new ControllerRevision 01/12/23 01:38:29.197
    Jan 12 01:38:29.201: INFO: Created ControllerRevision: e2e-g9nrk-daemon-set-5699664cb
    STEP: Confirm that there are two ControllerRevisions 01/12/23 01:38:29.201
    Jan 12 01:38:29.202: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 12 01:38:29.204: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-g9nrk-daemon-set-5dd4fb7db6" 01/12/23 01:38:29.204
    STEP: Confirm that there is only one ControllerRevision 01/12/23 01:38:29.21
    Jan 12 01:38:29.210: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 12 01:38:29.212: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-g9nrk-daemon-set-5699664cb" 01/12/23 01:38:29.214
    Jan 12 01:38:29.220: INFO: e2e-g9nrk-daemon-set-5699664cb has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/12/23 01:38:29.221
    W0112 01:38:29.231341      21 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/12/23 01:38:29.231
    Jan 12 01:38:29.231: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 12 01:38:30.233: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 12 01:38:30.237: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-g9nrk-daemon-set-5699664cb=updated" 01/12/23 01:38:30.237
    STEP: Confirm that there is only one ControllerRevision 01/12/23 01:38:30.242
    Jan 12 01:38:30.242: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 12 01:38:30.244: INFO: Found 1 ControllerRevisions
    Jan 12 01:38:30.246: INFO: ControllerRevision "e2e-g9nrk-daemon-set-5757b8b7fc" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-g9nrk-daemon-set" 01/12/23 01:38:30.248
    STEP: deleting DaemonSet.extensions e2e-g9nrk-daemon-set in namespace controllerrevisions-3111, will wait for the garbage collector to delete the pods 01/12/23 01:38:30.248
    Jan 12 01:38:30.307: INFO: Deleting DaemonSet.extensions e2e-g9nrk-daemon-set took: 5.911669ms
    Jan 12 01:38:30.407: INFO: Terminating DaemonSet.extensions e2e-g9nrk-daemon-set pods took: 100.5256ms
    Jan 12 01:38:32.110: INFO: Number of nodes with available pods controlled by daemonset e2e-g9nrk-daemon-set: 0
    Jan 12 01:38:32.110: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-g9nrk-daemon-set
    Jan 12 01:38:32.112: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20168273"},"items":null}

    Jan 12 01:38:32.114: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20168273"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:38:32.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-3111" for this suite. 01/12/23 01:38:32.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:38:32.147
Jan 12 01:38:32.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-runtime 01/12/23 01:38:32.147
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:32.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:32.163
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 01/12/23 01:38:32.165
STEP: wait for the container to reach Failed 01/12/23 01:38:32.192
STEP: get the container status 01/12/23 01:38:37.211
STEP: the container should be terminated 01/12/23 01:38:37.213
STEP: the termination message should be set 01/12/23 01:38:37.213
Jan 12 01:38:37.213: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/12/23 01:38:37.213
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 12 01:38:37.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1089" for this suite. 01/12/23 01:38:37.23
------------------------------
• [SLOW TEST] [5.096 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:38:32.147
    Jan 12 01:38:32.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-runtime 01/12/23 01:38:32.147
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:32.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:32.163
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 01/12/23 01:38:32.165
    STEP: wait for the container to reach Failed 01/12/23 01:38:32.192
    STEP: get the container status 01/12/23 01:38:37.211
    STEP: the container should be terminated 01/12/23 01:38:37.213
    STEP: the termination message should be set 01/12/23 01:38:37.213
    Jan 12 01:38:37.213: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/12/23 01:38:37.213
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:38:37.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1089" for this suite. 01/12/23 01:38:37.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:38:37.245
Jan 12 01:38:37.245: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename svcaccounts 01/12/23 01:38:37.246
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:37.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:37.258
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jan 12 01:38:37.262: INFO: Got root ca configmap in namespace "svcaccounts-1730"
Jan 12 01:38:37.266: INFO: Deleted root ca configmap in namespace "svcaccounts-1730"
STEP: waiting for a new root ca configmap created 01/12/23 01:38:37.767
Jan 12 01:38:37.770: INFO: Recreated root ca configmap in namespace "svcaccounts-1730"
Jan 12 01:38:37.777: INFO: Updated root ca configmap in namespace "svcaccounts-1730"
STEP: waiting for the root ca configmap reconciled 01/12/23 01:38:38.277
Jan 12 01:38:38.280: INFO: Reconciled root ca configmap in namespace "svcaccounts-1730"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 12 01:38:38.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1730" for this suite. 01/12/23 01:38:38.284
------------------------------
• [1.051 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:38:37.245
    Jan 12 01:38:37.245: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename svcaccounts 01/12/23 01:38:37.246
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:37.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:37.258
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jan 12 01:38:37.262: INFO: Got root ca configmap in namespace "svcaccounts-1730"
    Jan 12 01:38:37.266: INFO: Deleted root ca configmap in namespace "svcaccounts-1730"
    STEP: waiting for a new root ca configmap created 01/12/23 01:38:37.767
    Jan 12 01:38:37.770: INFO: Recreated root ca configmap in namespace "svcaccounts-1730"
    Jan 12 01:38:37.777: INFO: Updated root ca configmap in namespace "svcaccounts-1730"
    STEP: waiting for the root ca configmap reconciled 01/12/23 01:38:38.277
    Jan 12 01:38:38.280: INFO: Reconciled root ca configmap in namespace "svcaccounts-1730"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:38:38.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1730" for this suite. 01/12/23 01:38:38.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:38:38.299
Jan 12 01:38:38.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename deployment 01/12/23 01:38:38.3
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:38.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:38.315
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 12 01:38:38.324: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 12 01:38:43.328: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/12/23 01:38:43.328
Jan 12 01:38:43.328: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/12/23 01:38:43.43
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 01:38:43.436: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2414  267fff7d-f3b6-4309-91f6-5452eaf346ad 20168398 1 2023-01-12 01:38:43 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-12 01:38:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005023a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 12 01:38:43.439: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 12 01:38:43.439: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 12 01:38:43.439: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2414  8b99ffca-bb64-433a-a296-f4da61ba1f5b 20168399 1 2023-01-12 01:38:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 267fff7d-f3b6-4309-91f6-5452eaf346ad 0xc005023d97 0xc005023d98}] [] [{e2e.test Update apps/v1 2023-01-12 01:38:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:38:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-12 01:38:43 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"267fff7d-f3b6-4309-91f6-5452eaf346ad\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005023e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 12 01:38:43.441: INFO: Pod "test-cleanup-controller-phr6f" is available:
&Pod{ObjectMeta:{test-cleanup-controller-phr6f test-cleanup-controller- deployment-2414  6be9f9e4-459c-452b-b591-a36b5eb3a147 20168377 0 2023-01-12 01:38:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:0934d6133cb7173d717b7b3a22d72639562d5ae47146bacae0b6cf1e00da24e7 cni.projectcalico.org/podIP:172.21.88.181/32 cni.projectcalico.org/podIPs:172.21.88.181/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.181"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.181"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-cleanup-controller 8b99ffca-bb64-433a-a296-f4da61ba1f5b 0xc00475a287 0xc00475a288}] [] [{kube-controller-manager Update v1 2023-01-12 01:38:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b99ffca-bb64-433a-a296-f4da61ba1f5b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:38:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:38:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:38:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dnxh9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dnxh9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:38:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:38:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:38:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:38:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.181,StartTime:2023-01-12 01:38:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:38:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://51fb564a83c31619363e9aa3f3197990c2c13023df5a1002ab21fe22278a3887,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 01:38:43.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2414" for this suite. 01/12/23 01:38:43.445
------------------------------
• [SLOW TEST] [5.166 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:38:38.299
    Jan 12 01:38:38.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename deployment 01/12/23 01:38:38.3
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:38.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:38.315
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 12 01:38:38.324: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 12 01:38:43.328: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/12/23 01:38:43.328
    Jan 12 01:38:43.328: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/12/23 01:38:43.43
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 01:38:43.436: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2414  267fff7d-f3b6-4309-91f6-5452eaf346ad 20168398 1 2023-01-12 01:38:43 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-12 01:38:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005023a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 12 01:38:43.439: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Jan 12 01:38:43.439: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan 12 01:38:43.439: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2414  8b99ffca-bb64-433a-a296-f4da61ba1f5b 20168399 1 2023-01-12 01:38:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 267fff7d-f3b6-4309-91f6-5452eaf346ad 0xc005023d97 0xc005023d98}] [] [{e2e.test Update apps/v1 2023-01-12 01:38:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:38:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-12 01:38:43 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"267fff7d-f3b6-4309-91f6-5452eaf346ad\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005023e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 01:38:43.441: INFO: Pod "test-cleanup-controller-phr6f" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-phr6f test-cleanup-controller- deployment-2414  6be9f9e4-459c-452b-b591-a36b5eb3a147 20168377 0 2023-01-12 01:38:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:0934d6133cb7173d717b7b3a22d72639562d5ae47146bacae0b6cf1e00da24e7 cni.projectcalico.org/podIP:172.21.88.181/32 cni.projectcalico.org/podIPs:172.21.88.181/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.181"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.181"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-cleanup-controller 8b99ffca-bb64-433a-a296-f4da61ba1f5b 0xc00475a287 0xc00475a288}] [] [{kube-controller-manager Update v1 2023-01-12 01:38:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b99ffca-bb64-433a-a296-f4da61ba1f5b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:38:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:38:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:38:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dnxh9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dnxh9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:38:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:38:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:38:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:38:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.181,StartTime:2023-01-12 01:38:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:38:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:robin://51fb564a83c31619363e9aa3f3197990c2c13023df5a1002ab21fe22278a3887,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:38:43.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2414" for this suite. 01/12/23 01:38:43.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:38:43.467
Jan 12 01:38:43.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 01:38:43.468
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:43.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:43.482
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 01/12/23 01:38:43.485
Jan 12 01:38:43.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 create -f -'
Jan 12 01:38:44.061: INFO: stderr: ""
Jan 12 01:38:44.061: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 01:38:44.061
Jan 12 01:38:44.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 12 01:38:44.122: INFO: stderr: ""
Jan 12 01:38:44.122: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0 01/12/23 01:38:44.122
Jan 12 01:38:49.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 12 01:38:49.199: INFO: stderr: ""
Jan 12 01:38:49.199: INFO: stdout: "update-demo-nautilus-gtbkt update-demo-nautilus-v8zqx "
Jan 12 01:38:49.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get pods update-demo-nautilus-gtbkt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 01:38:49.278: INFO: stderr: ""
Jan 12 01:38:49.278: INFO: stdout: "true"
Jan 12 01:38:49.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get pods update-demo-nautilus-gtbkt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 12 01:38:49.342: INFO: stderr: ""
Jan 12 01:38:49.342: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 12 01:38:49.342: INFO: validating pod update-demo-nautilus-gtbkt
Jan 12 01:38:49.347: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 12 01:38:49.347: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 12 01:38:49.347: INFO: update-demo-nautilus-gtbkt is verified up and running
Jan 12 01:38:49.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get pods update-demo-nautilus-v8zqx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 01:38:49.412: INFO: stderr: ""
Jan 12 01:38:49.412: INFO: stdout: "true"
Jan 12 01:38:49.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get pods update-demo-nautilus-v8zqx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 12 01:38:49.492: INFO: stderr: ""
Jan 12 01:38:49.492: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 12 01:38:49.492: INFO: validating pod update-demo-nautilus-v8zqx
Jan 12 01:38:49.496: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 12 01:38:49.496: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 12 01:38:49.496: INFO: update-demo-nautilus-v8zqx is verified up and running
STEP: using delete to clean up resources 01/12/23 01:38:49.496
Jan 12 01:38:49.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 delete --grace-period=0 --force -f -'
Jan 12 01:38:49.637: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 01:38:49.637: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 12 01:38:49.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get rc,svc -l name=update-demo --no-headers'
Jan 12 01:38:49.740: INFO: stderr: "No resources found in kubectl-6087 namespace.\n"
Jan 12 01:38:49.740: INFO: stdout: ""
Jan 12 01:38:49.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 12 01:38:49.826: INFO: stderr: ""
Jan 12 01:38:49.826: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 01:38:49.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6087" for this suite. 01/12/23 01:38:49.829
------------------------------
• [SLOW TEST] [6.376 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:38:43.467
    Jan 12 01:38:43.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 01:38:43.468
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:43.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:43.482
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 01/12/23 01:38:43.485
    Jan 12 01:38:43.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 create -f -'
    Jan 12 01:38:44.061: INFO: stderr: ""
    Jan 12 01:38:44.061: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 01:38:44.061
    Jan 12 01:38:44.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 12 01:38:44.122: INFO: stderr: ""
    Jan 12 01:38:44.122: INFO: stdout: ""
    STEP: Replicas for name=update-demo: expected=2 actual=0 01/12/23 01:38:44.122
    Jan 12 01:38:49.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 12 01:38:49.199: INFO: stderr: ""
    Jan 12 01:38:49.199: INFO: stdout: "update-demo-nautilus-gtbkt update-demo-nautilus-v8zqx "
    Jan 12 01:38:49.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get pods update-demo-nautilus-gtbkt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 01:38:49.278: INFO: stderr: ""
    Jan 12 01:38:49.278: INFO: stdout: "true"
    Jan 12 01:38:49.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get pods update-demo-nautilus-gtbkt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 12 01:38:49.342: INFO: stderr: ""
    Jan 12 01:38:49.342: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 12 01:38:49.342: INFO: validating pod update-demo-nautilus-gtbkt
    Jan 12 01:38:49.347: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 12 01:38:49.347: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 12 01:38:49.347: INFO: update-demo-nautilus-gtbkt is verified up and running
    Jan 12 01:38:49.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get pods update-demo-nautilus-v8zqx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 01:38:49.412: INFO: stderr: ""
    Jan 12 01:38:49.412: INFO: stdout: "true"
    Jan 12 01:38:49.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get pods update-demo-nautilus-v8zqx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 12 01:38:49.492: INFO: stderr: ""
    Jan 12 01:38:49.492: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 12 01:38:49.492: INFO: validating pod update-demo-nautilus-v8zqx
    Jan 12 01:38:49.496: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 12 01:38:49.496: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 12 01:38:49.496: INFO: update-demo-nautilus-v8zqx is verified up and running
    STEP: using delete to clean up resources 01/12/23 01:38:49.496
    Jan 12 01:38:49.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 delete --grace-period=0 --force -f -'
    Jan 12 01:38:49.637: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 01:38:49.637: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 12 01:38:49.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get rc,svc -l name=update-demo --no-headers'
    Jan 12 01:38:49.740: INFO: stderr: "No resources found in kubectl-6087 namespace.\n"
    Jan 12 01:38:49.740: INFO: stdout: ""
    Jan 12 01:38:49.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-6087 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 12 01:38:49.826: INFO: stderr: ""
    Jan 12 01:38:49.826: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:38:49.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6087" for this suite. 01/12/23 01:38:49.829
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:38:49.844
Jan 12 01:38:49.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename cronjob 01/12/23 01:38:49.844
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:49.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:49.857
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/12/23 01:38:49.861
STEP: creating 01/12/23 01:38:49.861
STEP: getting 01/12/23 01:38:49.865
STEP: listing 01/12/23 01:38:49.867
STEP: watching 01/12/23 01:38:49.869
Jan 12 01:38:49.869: INFO: starting watch
STEP: cluster-wide listing 01/12/23 01:38:49.87
STEP: cluster-wide watching 01/12/23 01:38:49.872
Jan 12 01:38:49.872: INFO: starting watch
STEP: patching 01/12/23 01:38:49.873
STEP: updating 01/12/23 01:38:49.879
Jan 12 01:38:49.885: INFO: waiting for watch events with expected annotations
Jan 12 01:38:49.885: INFO: saw patched and updated annotations
STEP: patching /status 01/12/23 01:38:49.885
STEP: updating /status 01/12/23 01:38:49.89
STEP: get /status 01/12/23 01:38:49.895
STEP: deleting 01/12/23 01:38:49.897
STEP: deleting a collection 01/12/23 01:38:49.912
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 12 01:38:49.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8499" for this suite. 01/12/23 01:38:49.923
------------------------------
• [0.091 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:38:49.844
    Jan 12 01:38:49.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename cronjob 01/12/23 01:38:49.844
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:49.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:49.857
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/12/23 01:38:49.861
    STEP: creating 01/12/23 01:38:49.861
    STEP: getting 01/12/23 01:38:49.865
    STEP: listing 01/12/23 01:38:49.867
    STEP: watching 01/12/23 01:38:49.869
    Jan 12 01:38:49.869: INFO: starting watch
    STEP: cluster-wide listing 01/12/23 01:38:49.87
    STEP: cluster-wide watching 01/12/23 01:38:49.872
    Jan 12 01:38:49.872: INFO: starting watch
    STEP: patching 01/12/23 01:38:49.873
    STEP: updating 01/12/23 01:38:49.879
    Jan 12 01:38:49.885: INFO: waiting for watch events with expected annotations
    Jan 12 01:38:49.885: INFO: saw patched and updated annotations
    STEP: patching /status 01/12/23 01:38:49.885
    STEP: updating /status 01/12/23 01:38:49.89
    STEP: get /status 01/12/23 01:38:49.895
    STEP: deleting 01/12/23 01:38:49.897
    STEP: deleting a collection 01/12/23 01:38:49.912
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:38:49.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8499" for this suite. 01/12/23 01:38:49.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:38:49.935
Jan 12 01:38:49.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:38:49.936
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:49.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:49.953
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-06ab0ed5-9ec1-4fec-861b-70aaff4ad5cd 01/12/23 01:38:49.956
STEP: Creating a pod to test consume secrets 01/12/23 01:38:49.959
Jan 12 01:38:49.987: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1" in namespace "projected-3142" to be "Succeeded or Failed"
Jan 12 01:38:49.994: INFO: Pod "pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.509447ms
Jan 12 01:38:51.996: INFO: Pod "pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008927516s
Jan 12 01:38:53.998: INFO: Pod "pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010747429s
Jan 12 01:38:55.997: INFO: Pod "pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00994528s
STEP: Saw pod success 01/12/23 01:38:55.997
Jan 12 01:38:55.997: INFO: Pod "pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1" satisfied condition "Succeeded or Failed"
Jan 12 01:38:55.999: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1 container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 01:38:56.017
Jan 12 01:38:56.030: INFO: Waiting for pod pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1 to disappear
Jan 12 01:38:56.032: INFO: Pod pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 12 01:38:56.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3142" for this suite. 01/12/23 01:38:56.036
------------------------------
• [SLOW TEST] [6.175 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:38:49.935
    Jan 12 01:38:49.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:38:49.936
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:49.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:49.953
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-06ab0ed5-9ec1-4fec-861b-70aaff4ad5cd 01/12/23 01:38:49.956
    STEP: Creating a pod to test consume secrets 01/12/23 01:38:49.959
    Jan 12 01:38:49.987: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1" in namespace "projected-3142" to be "Succeeded or Failed"
    Jan 12 01:38:49.994: INFO: Pod "pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.509447ms
    Jan 12 01:38:51.996: INFO: Pod "pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008927516s
    Jan 12 01:38:53.998: INFO: Pod "pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010747429s
    Jan 12 01:38:55.997: INFO: Pod "pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00994528s
    STEP: Saw pod success 01/12/23 01:38:55.997
    Jan 12 01:38:55.997: INFO: Pod "pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1" satisfied condition "Succeeded or Failed"
    Jan 12 01:38:55.999: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1 container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 01:38:56.017
    Jan 12 01:38:56.030: INFO: Waiting for pod pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1 to disappear
    Jan 12 01:38:56.032: INFO: Pod pod-projected-secrets-9d7becbd-30e3-49f4-b165-958ce04a47f1 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:38:56.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3142" for this suite. 01/12/23 01:38:56.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:38:56.112
Jan 12 01:38:56.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:38:56.113
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:56.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:56.129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 01/12/23 01:38:56.131
Jan 12 01:38:56.503: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c" in namespace "projected-9262" to be "Succeeded or Failed"
Jan 12 01:38:56.506: INFO: Pod "downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.618631ms
Jan 12 01:38:58.510: INFO: Pod "downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c": Phase="Running", Reason="", readiness=true. Elapsed: 2.0065435s
Jan 12 01:39:00.511: INFO: Pod "downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c": Phase="Running", Reason="", readiness=false. Elapsed: 4.007710311s
Jan 12 01:39:02.510: INFO: Pod "downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007252769s
STEP: Saw pod success 01/12/23 01:39:02.51
Jan 12 01:39:02.511: INFO: Pod "downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c" satisfied condition "Succeeded or Failed"
Jan 12 01:39:02.513: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c container client-container: <nil>
STEP: delete the pod 01/12/23 01:39:02.534
Jan 12 01:39:02.547: INFO: Waiting for pod downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c to disappear
Jan 12 01:39:02.555: INFO: Pod downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 01:39:02.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9262" for this suite. 01/12/23 01:39:02.573
------------------------------
• [SLOW TEST] [6.475 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:38:56.112
    Jan 12 01:38:56.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:38:56.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:38:56.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:38:56.129
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 01/12/23 01:38:56.131
    Jan 12 01:38:56.503: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c" in namespace "projected-9262" to be "Succeeded or Failed"
    Jan 12 01:38:56.506: INFO: Pod "downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.618631ms
    Jan 12 01:38:58.510: INFO: Pod "downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c": Phase="Running", Reason="", readiness=true. Elapsed: 2.0065435s
    Jan 12 01:39:00.511: INFO: Pod "downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c": Phase="Running", Reason="", readiness=false. Elapsed: 4.007710311s
    Jan 12 01:39:02.510: INFO: Pod "downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007252769s
    STEP: Saw pod success 01/12/23 01:39:02.51
    Jan 12 01:39:02.511: INFO: Pod "downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c" satisfied condition "Succeeded or Failed"
    Jan 12 01:39:02.513: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c container client-container: <nil>
    STEP: delete the pod 01/12/23 01:39:02.534
    Jan 12 01:39:02.547: INFO: Waiting for pod downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c to disappear
    Jan 12 01:39:02.555: INFO: Pod downwardapi-volume-e8997cfb-4d17-4970-a9a1-1f285c5f821c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:39:02.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9262" for this suite. 01/12/23 01:39:02.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:39:02.59
Jan 12 01:39:02.590: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:39:02.591
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:02.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:02.603
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-11261b18-85f0-46c4-8219-c353284c8e42 01/12/23 01:39:02.605
STEP: Creating a pod to test consume configMaps 01/12/23 01:39:02.609
Jan 12 01:39:02.639: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591" in namespace "projected-9821" to be "Succeeded or Failed"
Jan 12 01:39:02.641: INFO: Pod "pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591": Phase="Pending", Reason="", readiness=false. Elapsed: 2.212524ms
Jan 12 01:39:04.645: INFO: Pod "pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005708667s
Jan 12 01:39:06.644: INFO: Pod "pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005038062s
STEP: Saw pod success 01/12/23 01:39:06.644
Jan 12 01:39:06.644: INFO: Pod "pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591" satisfied condition "Succeeded or Failed"
Jan 12 01:39:06.646: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 01:39:06.665
Jan 12 01:39:06.674: INFO: Waiting for pod pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591 to disappear
Jan 12 01:39:06.676: INFO: Pod pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:39:06.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9821" for this suite. 01/12/23 01:39:06.679
------------------------------
• [4.181 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:39:02.59
    Jan 12 01:39:02.590: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:39:02.591
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:02.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:02.603
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-11261b18-85f0-46c4-8219-c353284c8e42 01/12/23 01:39:02.605
    STEP: Creating a pod to test consume configMaps 01/12/23 01:39:02.609
    Jan 12 01:39:02.639: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591" in namespace "projected-9821" to be "Succeeded or Failed"
    Jan 12 01:39:02.641: INFO: Pod "pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591": Phase="Pending", Reason="", readiness=false. Elapsed: 2.212524ms
    Jan 12 01:39:04.645: INFO: Pod "pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005708667s
    Jan 12 01:39:06.644: INFO: Pod "pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005038062s
    STEP: Saw pod success 01/12/23 01:39:06.644
    Jan 12 01:39:06.644: INFO: Pod "pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591" satisfied condition "Succeeded or Failed"
    Jan 12 01:39:06.646: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 01:39:06.665
    Jan 12 01:39:06.674: INFO: Waiting for pod pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591 to disappear
    Jan 12 01:39:06.676: INFO: Pod pod-projected-configmaps-fd6547f5-3fa0-4a50-b0d1-992c299a6591 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:39:06.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9821" for this suite. 01/12/23 01:39:06.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:39:06.772
Jan 12 01:39:06.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename proxy 01/12/23 01:39:06.773
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:06.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:06.786
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/12/23 01:39:06.799
STEP: creating replication controller proxy-service-rjpj9 in namespace proxy-7153 01/12/23 01:39:06.799
I0112 01:39:06.805977      21 runners.go:193] Created replication controller with name: proxy-service-rjpj9, namespace: proxy-7153, replica count: 1
I0112 01:39:07.857255      21 runners.go:193] proxy-service-rjpj9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0112 01:39:08.857922      21 runners.go:193] proxy-service-rjpj9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0112 01:39:09.857984      21 runners.go:193] proxy-service-rjpj9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 01:39:09.860: INFO: setup took 3.0710044s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/12/23 01:39:09.86
Jan 12 01:39:09.864: INFO: (0) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.612379ms)
Jan 12 01:39:09.865: INFO: (0) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 4.56273ms)
Jan 12 01:39:09.866: INFO: (0) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 5.62714ms)
Jan 12 01:39:09.866: INFO: (0) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 5.384945ms)
Jan 12 01:39:09.866: INFO: (0) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 5.387084ms)
Jan 12 01:39:09.866: INFO: (0) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 5.389559ms)
Jan 12 01:39:09.866: INFO: (0) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 5.361902ms)
Jan 12 01:39:09.867: INFO: (0) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 6.802701ms)
Jan 12 01:39:09.868: INFO: (0) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 7.325891ms)
Jan 12 01:39:09.868: INFO: (0) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 7.507254ms)
Jan 12 01:39:09.874: INFO: (0) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 13.548153ms)
Jan 12 01:39:09.876: INFO: (0) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 15.516064ms)
Jan 12 01:39:09.876: INFO: (0) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 15.849234ms)
Jan 12 01:39:09.876: INFO: (0) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 15.933877ms)
Jan 12 01:39:09.876: INFO: (0) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 15.679503ms)
Jan 12 01:39:09.876: INFO: (0) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 15.825355ms)
Jan 12 01:39:09.881: INFO: (1) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.056682ms)
Jan 12 01:39:09.881: INFO: (1) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 3.433093ms)
Jan 12 01:39:09.881: INFO: (1) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.173218ms)
Jan 12 01:39:09.881: INFO: (1) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.829706ms)
Jan 12 01:39:09.881: INFO: (1) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.143646ms)
Jan 12 01:39:09.882: INFO: (1) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.907894ms)
Jan 12 01:39:09.882: INFO: (1) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 3.854844ms)
Jan 12 01:39:09.882: INFO: (1) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.141237ms)
Jan 12 01:39:09.883: INFO: (1) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.410429ms)
Jan 12 01:39:09.885: INFO: (1) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 6.813693ms)
Jan 12 01:39:09.885: INFO: (1) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 6.775828ms)
Jan 12 01:39:09.885: INFO: (1) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 5.842456ms)
Jan 12 01:39:09.885: INFO: (1) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 6.143292ms)
Jan 12 01:39:09.885: INFO: (1) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 6.2085ms)
Jan 12 01:39:09.885: INFO: (1) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 6.806616ms)
Jan 12 01:39:09.886: INFO: (1) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 7.875306ms)
Jan 12 01:39:09.889: INFO: (2) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.365744ms)
Jan 12 01:39:09.889: INFO: (2) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.693308ms)
Jan 12 01:39:09.890: INFO: (2) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 3.188703ms)
Jan 12 01:39:09.890: INFO: (2) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.045359ms)
Jan 12 01:39:09.890: INFO: (2) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 2.958811ms)
Jan 12 01:39:09.891: INFO: (2) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.735766ms)
Jan 12 01:39:09.892: INFO: (2) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.657027ms)
Jan 12 01:39:09.892: INFO: (2) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 5.838668ms)
Jan 12 01:39:09.893: INFO: (2) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 5.052865ms)
Jan 12 01:39:09.892: INFO: (2) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.757786ms)
Jan 12 01:39:09.893: INFO: (2) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 5.248383ms)
Jan 12 01:39:09.893: INFO: (2) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 5.551224ms)
Jan 12 01:39:09.893: INFO: (2) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 5.071405ms)
Jan 12 01:39:09.893: INFO: (2) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 5.854486ms)
Jan 12 01:39:09.893: INFO: (2) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 5.745993ms)
Jan 12 01:39:09.893: INFO: (2) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 6.716454ms)
Jan 12 01:39:09.896: INFO: (3) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 2.719492ms)
Jan 12 01:39:09.897: INFO: (3) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.94049ms)
Jan 12 01:39:09.897: INFO: (3) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.049677ms)
Jan 12 01:39:09.897: INFO: (3) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 4.086475ms)
Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.16144ms)
Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.288645ms)
Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 4.174389ms)
Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 4.344498ms)
Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.140039ms)
Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.350913ms)
Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 4.165856ms)
Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.34385ms)
Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.430958ms)
Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.431516ms)
Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.751135ms)
Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.591117ms)
Jan 12 01:39:09.901: INFO: (4) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.176575ms)
Jan 12 01:39:09.901: INFO: (4) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.223084ms)
Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.687437ms)
Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.927302ms)
Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.88324ms)
Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.162324ms)
Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.091945ms)
Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.088967ms)
Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 4.135524ms)
Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.149875ms)
Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 4.251359ms)
Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 4.289514ms)
Jan 12 01:39:09.903: INFO: (4) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.331271ms)
Jan 12 01:39:09.903: INFO: (4) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.486322ms)
Jan 12 01:39:09.903: INFO: (4) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.633511ms)
Jan 12 01:39:09.903: INFO: (4) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.681813ms)
Jan 12 01:39:09.906: INFO: (5) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 2.821492ms)
Jan 12 01:39:09.906: INFO: (5) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.062278ms)
Jan 12 01:39:09.907: INFO: (5) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.284219ms)
Jan 12 01:39:09.907: INFO: (5) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 3.50604ms)
Jan 12 01:39:09.907: INFO: (5) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.803317ms)
Jan 12 01:39:09.907: INFO: (5) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.827869ms)
Jan 12 01:39:09.907: INFO: (5) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 4.328675ms)
Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.114548ms)
Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 4.2145ms)
Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.701971ms)
Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.669447ms)
Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.726559ms)
Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.506456ms)
Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.238836ms)
Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.786988ms)
Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.92175ms)
Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.272682ms)
Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 3.353957ms)
Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.814165ms)
Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.469187ms)
Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.093287ms)
Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 3.931971ms)
Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 4.023687ms)
Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.012787ms)
Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.802837ms)
Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.474501ms)
Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.654665ms)
Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.309011ms)
Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 3.813029ms)
Jan 12 01:39:09.914: INFO: (6) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.380096ms)
Jan 12 01:39:09.914: INFO: (6) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.331687ms)
Jan 12 01:39:09.914: INFO: (6) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.618244ms)
Jan 12 01:39:09.917: INFO: (7) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 2.674918ms)
Jan 12 01:39:09.917: INFO: (7) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.795467ms)
Jan 12 01:39:09.917: INFO: (7) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 2.903241ms)
Jan 12 01:39:09.917: INFO: (7) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.036699ms)
Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.596978ms)
Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.940147ms)
Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.15102ms)
Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 4.273396ms)
Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.223142ms)
Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.266168ms)
Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.127271ms)
Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 4.211808ms)
Jan 12 01:39:09.919: INFO: (7) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 4.384616ms)
Jan 12 01:39:09.919: INFO: (7) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.624844ms)
Jan 12 01:39:09.919: INFO: (7) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.489749ms)
Jan 12 01:39:09.919: INFO: (7) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.945344ms)
Jan 12 01:39:09.922: INFO: (8) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 2.457496ms)
Jan 12 01:39:09.922: INFO: (8) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.584362ms)
Jan 12 01:39:09.922: INFO: (8) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.685317ms)
Jan 12 01:39:09.922: INFO: (8) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 2.569121ms)
Jan 12 01:39:09.923: INFO: (8) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.407388ms)
Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.81746ms)
Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.217157ms)
Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 4.136702ms)
Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 3.606666ms)
Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 3.673101ms)
Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.106664ms)
Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.810477ms)
Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.765478ms)
Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.948523ms)
Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 3.740973ms)
Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 3.920067ms)
Jan 12 01:39:09.927: INFO: (9) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 2.508362ms)
Jan 12 01:39:09.928: INFO: (9) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.043432ms)
Jan 12 01:39:09.928: INFO: (9) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 2.89291ms)
Jan 12 01:39:09.928: INFO: (9) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.038551ms)
Jan 12 01:39:09.928: INFO: (9) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.476513ms)
Jan 12 01:39:09.928: INFO: (9) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.1351ms)
Jan 12 01:39:09.928: INFO: (9) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.105167ms)
Jan 12 01:39:09.928: INFO: (9) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.36463ms)
Jan 12 01:39:09.929: INFO: (9) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 3.84918ms)
Jan 12 01:39:09.929: INFO: (9) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.450913ms)
Jan 12 01:39:09.929: INFO: (9) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.38806ms)
Jan 12 01:39:09.929: INFO: (9) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.128756ms)
Jan 12 01:39:09.930: INFO: (9) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 4.741821ms)
Jan 12 01:39:09.930: INFO: (9) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 4.689262ms)
Jan 12 01:39:09.930: INFO: (9) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 5.270577ms)
Jan 12 01:39:09.930: INFO: (9) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 5.429046ms)
Jan 12 01:39:09.932: INFO: (10) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 2.610455ms)
Jan 12 01:39:09.933: INFO: (10) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.102625ms)
Jan 12 01:39:09.933: INFO: (10) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.277386ms)
Jan 12 01:39:09.933: INFO: (10) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.346175ms)
Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.865626ms)
Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.140417ms)
Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 4.110137ms)
Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.133091ms)
Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.38908ms)
Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.302824ms)
Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 4.342431ms)
Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.328473ms)
Jan 12 01:39:09.935: INFO: (10) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.671065ms)
Jan 12 01:39:09.935: INFO: (10) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.740112ms)
Jan 12 01:39:09.935: INFO: (10) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.761466ms)
Jan 12 01:39:09.935: INFO: (10) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.92644ms)
Jan 12 01:39:09.937: INFO: (11) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 2.444328ms)
Jan 12 01:39:09.937: INFO: (11) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.546891ms)
Jan 12 01:39:09.937: INFO: (11) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 2.703459ms)
Jan 12 01:39:09.938: INFO: (11) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 2.897759ms)
Jan 12 01:39:09.938: INFO: (11) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 2.909936ms)
Jan 12 01:39:09.939: INFO: (11) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.261237ms)
Jan 12 01:39:09.939: INFO: (11) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.121445ms)
Jan 12 01:39:09.939: INFO: (11) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.106052ms)
Jan 12 01:39:09.939: INFO: (11) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.155475ms)
Jan 12 01:39:09.939: INFO: (11) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.061502ms)
Jan 12 01:39:09.939: INFO: (11) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.264307ms)
Jan 12 01:39:09.940: INFO: (11) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.416197ms)
Jan 12 01:39:09.940: INFO: (11) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 4.057791ms)
Jan 12 01:39:09.940: INFO: (11) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.631652ms)
Jan 12 01:39:09.940: INFO: (11) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.282702ms)
Jan 12 01:39:09.940: INFO: (11) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 4.093727ms)
Jan 12 01:39:09.942: INFO: (12) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 2.559203ms)
Jan 12 01:39:09.942: INFO: (12) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.483013ms)
Jan 12 01:39:09.943: INFO: (12) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.007435ms)
Jan 12 01:39:09.943: INFO: (12) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 2.693069ms)
Jan 12 01:39:09.943: INFO: (12) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.116803ms)
Jan 12 01:39:09.943: INFO: (12) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.428139ms)
Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 3.444123ms)
Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 3.730459ms)
Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 3.958317ms)
Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.356367ms)
Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.187832ms)
Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.865476ms)
Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.49794ms)
Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.564408ms)
Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.373203ms)
Jan 12 01:39:09.945: INFO: (12) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.020821ms)
Jan 12 01:39:09.947: INFO: (13) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 2.440578ms)
Jan 12 01:39:09.948: INFO: (13) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.898187ms)
Jan 12 01:39:09.948: INFO: (13) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 2.916533ms)
Jan 12 01:39:09.948: INFO: (13) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.089654ms)
Jan 12 01:39:09.948: INFO: (13) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.257297ms)
Jan 12 01:39:09.948: INFO: (13) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.114846ms)
Jan 12 01:39:09.948: INFO: (13) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.200724ms)
Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.567292ms)
Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 4.007554ms)
Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.069894ms)
Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.113029ms)
Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 4.034053ms)
Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.149177ms)
Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.232284ms)
Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.155202ms)
Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.305825ms)
Jan 12 01:39:09.952: INFO: (14) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 2.46027ms)
Jan 12 01:39:09.952: INFO: (14) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 2.734824ms)
Jan 12 01:39:09.952: INFO: (14) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.683832ms)
Jan 12 01:39:09.952: INFO: (14) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.829291ms)
Jan 12 01:39:09.952: INFO: (14) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.003364ms)
Jan 12 01:39:09.952: INFO: (14) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.980728ms)
Jan 12 01:39:09.953: INFO: (14) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.532962ms)
Jan 12 01:39:09.953: INFO: (14) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.534032ms)
Jan 12 01:39:09.953: INFO: (14) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.759749ms)
Jan 12 01:39:09.953: INFO: (14) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.021372ms)
Jan 12 01:39:09.953: INFO: (14) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 3.301924ms)
Jan 12 01:39:09.953: INFO: (14) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.023851ms)
Jan 12 01:39:09.954: INFO: (14) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.52464ms)
Jan 12 01:39:09.954: INFO: (14) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.411567ms)
Jan 12 01:39:09.954: INFO: (14) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.737008ms)
Jan 12 01:39:09.954: INFO: (14) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.619493ms)
Jan 12 01:39:09.957: INFO: (15) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 2.75433ms)
Jan 12 01:39:09.957: INFO: (15) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 2.884492ms)
Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.460062ms)
Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.522238ms)
Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.926191ms)
Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.918844ms)
Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.449426ms)
Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.162486ms)
Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 4.026151ms)
Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.091659ms)
Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.129158ms)
Jan 12 01:39:09.959: INFO: (15) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.347382ms)
Jan 12 01:39:09.959: INFO: (15) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.383303ms)
Jan 12 01:39:09.959: INFO: (15) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.430569ms)
Jan 12 01:39:09.959: INFO: (15) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.557425ms)
Jan 12 01:39:09.959: INFO: (15) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.661632ms)
Jan 12 01:39:09.961: INFO: (16) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.009162ms)
Jan 12 01:39:09.962: INFO: (16) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.678416ms)
Jan 12 01:39:09.962: INFO: (16) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 2.931508ms)
Jan 12 01:39:09.962: INFO: (16) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.050918ms)
Jan 12 01:39:09.962: INFO: (16) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.081234ms)
Jan 12 01:39:09.962: INFO: (16) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.285136ms)
Jan 12 01:39:09.962: INFO: (16) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 3.347549ms)
Jan 12 01:39:09.962: INFO: (16) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.369327ms)
Jan 12 01:39:09.963: INFO: (16) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 3.752238ms)
Jan 12 01:39:09.963: INFO: (16) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.769967ms)
Jan 12 01:39:09.963: INFO: (16) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 3.903065ms)
Jan 12 01:39:09.963: INFO: (16) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 3.993806ms)
Jan 12 01:39:09.963: INFO: (16) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.068833ms)
Jan 12 01:39:09.964: INFO: (16) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.717704ms)
Jan 12 01:39:09.964: INFO: (16) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.684826ms)
Jan 12 01:39:09.964: INFO: (16) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.749272ms)
Jan 12 01:39:09.966: INFO: (17) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 2.45428ms)
Jan 12 01:39:09.966: INFO: (17) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.516174ms)
Jan 12 01:39:09.967: INFO: (17) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 2.71621ms)
Jan 12 01:39:09.967: INFO: (17) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.953225ms)
Jan 12 01:39:09.967: INFO: (17) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.298776ms)
Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.478972ms)
Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.465831ms)
Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.562772ms)
Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 3.863312ms)
Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 3.815888ms)
Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.733035ms)
Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 3.969183ms)
Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.041116ms)
Jan 12 01:39:09.969: INFO: (17) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.416562ms)
Jan 12 01:39:09.969: INFO: (17) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.538358ms)
Jan 12 01:39:09.969: INFO: (17) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.546431ms)
Jan 12 01:39:09.971: INFO: (18) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 2.442337ms)
Jan 12 01:39:09.971: INFO: (18) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.390833ms)
Jan 12 01:39:09.972: INFO: (18) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 3.158899ms)
Jan 12 01:39:09.972: INFO: (18) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.40674ms)
Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.75596ms)
Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 4.370769ms)
Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.246995ms)
Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.42167ms)
Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 4.30334ms)
Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.322841ms)
Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.271775ms)
Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 4.263775ms)
Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.503083ms)
Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.458524ms)
Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.409562ms)
Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.75166ms)
Jan 12 01:39:09.976: INFO: (19) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 2.561399ms)
Jan 12 01:39:09.976: INFO: (19) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 2.809131ms)
Jan 12 01:39:09.976: INFO: (19) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.645797ms)
Jan 12 01:39:09.976: INFO: (19) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 2.754206ms)
Jan 12 01:39:09.976: INFO: (19) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 2.658657ms)
Jan 12 01:39:09.977: INFO: (19) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 2.510764ms)
Jan 12 01:39:09.977: INFO: (19) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.92908ms)
Jan 12 01:39:09.977: INFO: (19) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.109372ms)
Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 3.552085ms)
Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.115174ms)
Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 3.88103ms)
Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 3.387197ms)
Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.03706ms)
Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 3.689599ms)
Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 3.94899ms)
Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 3.588552ms)
STEP: deleting ReplicationController proxy-service-rjpj9 in namespace proxy-7153, will wait for the garbage collector to delete the pods 01/12/23 01:39:09.978
Jan 12 01:39:10.036: INFO: Deleting ReplicationController proxy-service-rjpj9 took: 4.7505ms
Jan 12 01:39:10.137: INFO: Terminating ReplicationController proxy-service-rjpj9 pods took: 100.872213ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 12 01:39:13.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7153" for this suite. 01/12/23 01:39:13.042
------------------------------
• [SLOW TEST] [6.292 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:39:06.772
    Jan 12 01:39:06.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename proxy 01/12/23 01:39:06.773
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:06.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:06.786
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/12/23 01:39:06.799
    STEP: creating replication controller proxy-service-rjpj9 in namespace proxy-7153 01/12/23 01:39:06.799
    I0112 01:39:06.805977      21 runners.go:193] Created replication controller with name: proxy-service-rjpj9, namespace: proxy-7153, replica count: 1
    I0112 01:39:07.857255      21 runners.go:193] proxy-service-rjpj9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0112 01:39:08.857922      21 runners.go:193] proxy-service-rjpj9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0112 01:39:09.857984      21 runners.go:193] proxy-service-rjpj9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 01:39:09.860: INFO: setup took 3.0710044s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/12/23 01:39:09.86
    Jan 12 01:39:09.864: INFO: (0) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.612379ms)
    Jan 12 01:39:09.865: INFO: (0) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 4.56273ms)
    Jan 12 01:39:09.866: INFO: (0) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 5.62714ms)
    Jan 12 01:39:09.866: INFO: (0) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 5.384945ms)
    Jan 12 01:39:09.866: INFO: (0) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 5.387084ms)
    Jan 12 01:39:09.866: INFO: (0) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 5.389559ms)
    Jan 12 01:39:09.866: INFO: (0) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 5.361902ms)
    Jan 12 01:39:09.867: INFO: (0) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 6.802701ms)
    Jan 12 01:39:09.868: INFO: (0) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 7.325891ms)
    Jan 12 01:39:09.868: INFO: (0) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 7.507254ms)
    Jan 12 01:39:09.874: INFO: (0) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 13.548153ms)
    Jan 12 01:39:09.876: INFO: (0) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 15.516064ms)
    Jan 12 01:39:09.876: INFO: (0) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 15.849234ms)
    Jan 12 01:39:09.876: INFO: (0) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 15.933877ms)
    Jan 12 01:39:09.876: INFO: (0) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 15.679503ms)
    Jan 12 01:39:09.876: INFO: (0) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 15.825355ms)
    Jan 12 01:39:09.881: INFO: (1) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.056682ms)
    Jan 12 01:39:09.881: INFO: (1) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 3.433093ms)
    Jan 12 01:39:09.881: INFO: (1) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.173218ms)
    Jan 12 01:39:09.881: INFO: (1) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.829706ms)
    Jan 12 01:39:09.881: INFO: (1) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.143646ms)
    Jan 12 01:39:09.882: INFO: (1) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.907894ms)
    Jan 12 01:39:09.882: INFO: (1) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 3.854844ms)
    Jan 12 01:39:09.882: INFO: (1) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.141237ms)
    Jan 12 01:39:09.883: INFO: (1) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.410429ms)
    Jan 12 01:39:09.885: INFO: (1) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 6.813693ms)
    Jan 12 01:39:09.885: INFO: (1) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 6.775828ms)
    Jan 12 01:39:09.885: INFO: (1) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 5.842456ms)
    Jan 12 01:39:09.885: INFO: (1) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 6.143292ms)
    Jan 12 01:39:09.885: INFO: (1) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 6.2085ms)
    Jan 12 01:39:09.885: INFO: (1) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 6.806616ms)
    Jan 12 01:39:09.886: INFO: (1) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 7.875306ms)
    Jan 12 01:39:09.889: INFO: (2) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.365744ms)
    Jan 12 01:39:09.889: INFO: (2) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.693308ms)
    Jan 12 01:39:09.890: INFO: (2) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 3.188703ms)
    Jan 12 01:39:09.890: INFO: (2) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.045359ms)
    Jan 12 01:39:09.890: INFO: (2) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 2.958811ms)
    Jan 12 01:39:09.891: INFO: (2) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.735766ms)
    Jan 12 01:39:09.892: INFO: (2) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.657027ms)
    Jan 12 01:39:09.892: INFO: (2) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 5.838668ms)
    Jan 12 01:39:09.893: INFO: (2) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 5.052865ms)
    Jan 12 01:39:09.892: INFO: (2) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.757786ms)
    Jan 12 01:39:09.893: INFO: (2) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 5.248383ms)
    Jan 12 01:39:09.893: INFO: (2) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 5.551224ms)
    Jan 12 01:39:09.893: INFO: (2) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 5.071405ms)
    Jan 12 01:39:09.893: INFO: (2) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 5.854486ms)
    Jan 12 01:39:09.893: INFO: (2) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 5.745993ms)
    Jan 12 01:39:09.893: INFO: (2) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 6.716454ms)
    Jan 12 01:39:09.896: INFO: (3) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 2.719492ms)
    Jan 12 01:39:09.897: INFO: (3) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.94049ms)
    Jan 12 01:39:09.897: INFO: (3) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.049677ms)
    Jan 12 01:39:09.897: INFO: (3) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 4.086475ms)
    Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.16144ms)
    Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.288645ms)
    Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 4.174389ms)
    Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 4.344498ms)
    Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.140039ms)
    Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.350913ms)
    Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 4.165856ms)
    Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.34385ms)
    Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.430958ms)
    Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.431516ms)
    Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.751135ms)
    Jan 12 01:39:09.898: INFO: (3) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.591117ms)
    Jan 12 01:39:09.901: INFO: (4) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.176575ms)
    Jan 12 01:39:09.901: INFO: (4) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.223084ms)
    Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.687437ms)
    Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.927302ms)
    Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.88324ms)
    Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.162324ms)
    Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.091945ms)
    Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.088967ms)
    Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 4.135524ms)
    Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.149875ms)
    Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 4.251359ms)
    Jan 12 01:39:09.902: INFO: (4) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 4.289514ms)
    Jan 12 01:39:09.903: INFO: (4) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.331271ms)
    Jan 12 01:39:09.903: INFO: (4) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.486322ms)
    Jan 12 01:39:09.903: INFO: (4) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.633511ms)
    Jan 12 01:39:09.903: INFO: (4) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.681813ms)
    Jan 12 01:39:09.906: INFO: (5) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 2.821492ms)
    Jan 12 01:39:09.906: INFO: (5) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.062278ms)
    Jan 12 01:39:09.907: INFO: (5) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.284219ms)
    Jan 12 01:39:09.907: INFO: (5) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 3.50604ms)
    Jan 12 01:39:09.907: INFO: (5) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.803317ms)
    Jan 12 01:39:09.907: INFO: (5) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.827869ms)
    Jan 12 01:39:09.907: INFO: (5) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 4.328675ms)
    Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.114548ms)
    Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 4.2145ms)
    Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.701971ms)
    Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.669447ms)
    Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.726559ms)
    Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.506456ms)
    Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.238836ms)
    Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.786988ms)
    Jan 12 01:39:09.908: INFO: (5) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.92175ms)
    Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.272682ms)
    Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 3.353957ms)
    Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.814165ms)
    Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.469187ms)
    Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.093287ms)
    Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 3.931971ms)
    Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 4.023687ms)
    Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.012787ms)
    Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.802837ms)
    Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.474501ms)
    Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.654665ms)
    Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.309011ms)
    Jan 12 01:39:09.913: INFO: (6) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 3.813029ms)
    Jan 12 01:39:09.914: INFO: (6) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.380096ms)
    Jan 12 01:39:09.914: INFO: (6) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.331687ms)
    Jan 12 01:39:09.914: INFO: (6) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.618244ms)
    Jan 12 01:39:09.917: INFO: (7) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 2.674918ms)
    Jan 12 01:39:09.917: INFO: (7) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.795467ms)
    Jan 12 01:39:09.917: INFO: (7) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 2.903241ms)
    Jan 12 01:39:09.917: INFO: (7) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.036699ms)
    Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.596978ms)
    Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.940147ms)
    Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.15102ms)
    Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 4.273396ms)
    Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.223142ms)
    Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.266168ms)
    Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.127271ms)
    Jan 12 01:39:09.918: INFO: (7) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 4.211808ms)
    Jan 12 01:39:09.919: INFO: (7) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 4.384616ms)
    Jan 12 01:39:09.919: INFO: (7) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.624844ms)
    Jan 12 01:39:09.919: INFO: (7) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.489749ms)
    Jan 12 01:39:09.919: INFO: (7) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.945344ms)
    Jan 12 01:39:09.922: INFO: (8) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 2.457496ms)
    Jan 12 01:39:09.922: INFO: (8) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.584362ms)
    Jan 12 01:39:09.922: INFO: (8) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.685317ms)
    Jan 12 01:39:09.922: INFO: (8) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 2.569121ms)
    Jan 12 01:39:09.923: INFO: (8) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.407388ms)
    Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.81746ms)
    Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.217157ms)
    Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 4.136702ms)
    Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 3.606666ms)
    Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 3.673101ms)
    Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.106664ms)
    Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.810477ms)
    Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.765478ms)
    Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.948523ms)
    Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 3.740973ms)
    Jan 12 01:39:09.924: INFO: (8) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 3.920067ms)
    Jan 12 01:39:09.927: INFO: (9) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 2.508362ms)
    Jan 12 01:39:09.928: INFO: (9) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.043432ms)
    Jan 12 01:39:09.928: INFO: (9) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 2.89291ms)
    Jan 12 01:39:09.928: INFO: (9) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.038551ms)
    Jan 12 01:39:09.928: INFO: (9) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.476513ms)
    Jan 12 01:39:09.928: INFO: (9) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.1351ms)
    Jan 12 01:39:09.928: INFO: (9) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.105167ms)
    Jan 12 01:39:09.928: INFO: (9) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.36463ms)
    Jan 12 01:39:09.929: INFO: (9) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 3.84918ms)
    Jan 12 01:39:09.929: INFO: (9) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.450913ms)
    Jan 12 01:39:09.929: INFO: (9) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.38806ms)
    Jan 12 01:39:09.929: INFO: (9) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.128756ms)
    Jan 12 01:39:09.930: INFO: (9) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 4.741821ms)
    Jan 12 01:39:09.930: INFO: (9) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 4.689262ms)
    Jan 12 01:39:09.930: INFO: (9) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 5.270577ms)
    Jan 12 01:39:09.930: INFO: (9) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 5.429046ms)
    Jan 12 01:39:09.932: INFO: (10) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 2.610455ms)
    Jan 12 01:39:09.933: INFO: (10) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.102625ms)
    Jan 12 01:39:09.933: INFO: (10) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.277386ms)
    Jan 12 01:39:09.933: INFO: (10) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.346175ms)
    Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.865626ms)
    Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.140417ms)
    Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 4.110137ms)
    Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.133091ms)
    Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.38908ms)
    Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.302824ms)
    Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 4.342431ms)
    Jan 12 01:39:09.934: INFO: (10) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.328473ms)
    Jan 12 01:39:09.935: INFO: (10) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.671065ms)
    Jan 12 01:39:09.935: INFO: (10) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.740112ms)
    Jan 12 01:39:09.935: INFO: (10) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.761466ms)
    Jan 12 01:39:09.935: INFO: (10) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.92644ms)
    Jan 12 01:39:09.937: INFO: (11) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 2.444328ms)
    Jan 12 01:39:09.937: INFO: (11) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.546891ms)
    Jan 12 01:39:09.937: INFO: (11) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 2.703459ms)
    Jan 12 01:39:09.938: INFO: (11) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 2.897759ms)
    Jan 12 01:39:09.938: INFO: (11) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 2.909936ms)
    Jan 12 01:39:09.939: INFO: (11) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.261237ms)
    Jan 12 01:39:09.939: INFO: (11) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.121445ms)
    Jan 12 01:39:09.939: INFO: (11) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.106052ms)
    Jan 12 01:39:09.939: INFO: (11) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.155475ms)
    Jan 12 01:39:09.939: INFO: (11) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.061502ms)
    Jan 12 01:39:09.939: INFO: (11) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.264307ms)
    Jan 12 01:39:09.940: INFO: (11) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.416197ms)
    Jan 12 01:39:09.940: INFO: (11) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 4.057791ms)
    Jan 12 01:39:09.940: INFO: (11) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.631652ms)
    Jan 12 01:39:09.940: INFO: (11) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.282702ms)
    Jan 12 01:39:09.940: INFO: (11) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 4.093727ms)
    Jan 12 01:39:09.942: INFO: (12) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 2.559203ms)
    Jan 12 01:39:09.942: INFO: (12) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.483013ms)
    Jan 12 01:39:09.943: INFO: (12) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.007435ms)
    Jan 12 01:39:09.943: INFO: (12) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 2.693069ms)
    Jan 12 01:39:09.943: INFO: (12) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.116803ms)
    Jan 12 01:39:09.943: INFO: (12) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.428139ms)
    Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 3.444123ms)
    Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 3.730459ms)
    Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 3.958317ms)
    Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.356367ms)
    Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.187832ms)
    Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.865476ms)
    Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.49794ms)
    Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.564408ms)
    Jan 12 01:39:09.944: INFO: (12) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.373203ms)
    Jan 12 01:39:09.945: INFO: (12) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.020821ms)
    Jan 12 01:39:09.947: INFO: (13) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 2.440578ms)
    Jan 12 01:39:09.948: INFO: (13) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.898187ms)
    Jan 12 01:39:09.948: INFO: (13) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 2.916533ms)
    Jan 12 01:39:09.948: INFO: (13) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.089654ms)
    Jan 12 01:39:09.948: INFO: (13) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.257297ms)
    Jan 12 01:39:09.948: INFO: (13) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.114846ms)
    Jan 12 01:39:09.948: INFO: (13) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.200724ms)
    Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.567292ms)
    Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 4.007554ms)
    Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.069894ms)
    Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.113029ms)
    Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 4.034053ms)
    Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.149177ms)
    Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.232284ms)
    Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.155202ms)
    Jan 12 01:39:09.949: INFO: (13) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.305825ms)
    Jan 12 01:39:09.952: INFO: (14) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 2.46027ms)
    Jan 12 01:39:09.952: INFO: (14) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 2.734824ms)
    Jan 12 01:39:09.952: INFO: (14) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.683832ms)
    Jan 12 01:39:09.952: INFO: (14) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.829291ms)
    Jan 12 01:39:09.952: INFO: (14) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.003364ms)
    Jan 12 01:39:09.952: INFO: (14) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.980728ms)
    Jan 12 01:39:09.953: INFO: (14) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.532962ms)
    Jan 12 01:39:09.953: INFO: (14) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.534032ms)
    Jan 12 01:39:09.953: INFO: (14) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.759749ms)
    Jan 12 01:39:09.953: INFO: (14) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.021372ms)
    Jan 12 01:39:09.953: INFO: (14) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 3.301924ms)
    Jan 12 01:39:09.953: INFO: (14) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.023851ms)
    Jan 12 01:39:09.954: INFO: (14) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.52464ms)
    Jan 12 01:39:09.954: INFO: (14) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.411567ms)
    Jan 12 01:39:09.954: INFO: (14) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.737008ms)
    Jan 12 01:39:09.954: INFO: (14) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.619493ms)
    Jan 12 01:39:09.957: INFO: (15) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 2.75433ms)
    Jan 12 01:39:09.957: INFO: (15) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 2.884492ms)
    Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.460062ms)
    Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.522238ms)
    Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.926191ms)
    Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.918844ms)
    Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.449426ms)
    Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.162486ms)
    Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 4.026151ms)
    Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 4.091659ms)
    Jan 12 01:39:09.958: INFO: (15) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.129158ms)
    Jan 12 01:39:09.959: INFO: (15) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.347382ms)
    Jan 12 01:39:09.959: INFO: (15) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.383303ms)
    Jan 12 01:39:09.959: INFO: (15) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.430569ms)
    Jan 12 01:39:09.959: INFO: (15) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.557425ms)
    Jan 12 01:39:09.959: INFO: (15) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.661632ms)
    Jan 12 01:39:09.961: INFO: (16) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.009162ms)
    Jan 12 01:39:09.962: INFO: (16) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.678416ms)
    Jan 12 01:39:09.962: INFO: (16) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 2.931508ms)
    Jan 12 01:39:09.962: INFO: (16) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.050918ms)
    Jan 12 01:39:09.962: INFO: (16) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.081234ms)
    Jan 12 01:39:09.962: INFO: (16) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.285136ms)
    Jan 12 01:39:09.962: INFO: (16) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 3.347549ms)
    Jan 12 01:39:09.962: INFO: (16) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.369327ms)
    Jan 12 01:39:09.963: INFO: (16) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 3.752238ms)
    Jan 12 01:39:09.963: INFO: (16) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 3.769967ms)
    Jan 12 01:39:09.963: INFO: (16) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 3.903065ms)
    Jan 12 01:39:09.963: INFO: (16) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 3.993806ms)
    Jan 12 01:39:09.963: INFO: (16) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.068833ms)
    Jan 12 01:39:09.964: INFO: (16) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.717704ms)
    Jan 12 01:39:09.964: INFO: (16) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.684826ms)
    Jan 12 01:39:09.964: INFO: (16) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.749272ms)
    Jan 12 01:39:09.966: INFO: (17) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 2.45428ms)
    Jan 12 01:39:09.966: INFO: (17) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.516174ms)
    Jan 12 01:39:09.967: INFO: (17) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 2.71621ms)
    Jan 12 01:39:09.967: INFO: (17) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.953225ms)
    Jan 12 01:39:09.967: INFO: (17) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.298776ms)
    Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.478972ms)
    Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 3.465831ms)
    Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.562772ms)
    Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 3.863312ms)
    Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 3.815888ms)
    Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 3.733035ms)
    Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 3.969183ms)
    Jan 12 01:39:09.968: INFO: (17) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.041116ms)
    Jan 12 01:39:09.969: INFO: (17) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.416562ms)
    Jan 12 01:39:09.969: INFO: (17) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.538358ms)
    Jan 12 01:39:09.969: INFO: (17) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.546431ms)
    Jan 12 01:39:09.971: INFO: (18) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 2.442337ms)
    Jan 12 01:39:09.971: INFO: (18) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.390833ms)
    Jan 12 01:39:09.972: INFO: (18) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 3.158899ms)
    Jan 12 01:39:09.972: INFO: (18) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 3.40674ms)
    Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.75596ms)
    Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 4.370769ms)
    Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.246995ms)
    Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 4.42167ms)
    Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 4.30334ms)
    Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 4.322841ms)
    Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.271775ms)
    Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 4.263775ms)
    Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 4.503083ms)
    Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 4.458524ms)
    Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 4.409562ms)
    Jan 12 01:39:09.973: INFO: (18) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 4.75166ms)
    Jan 12 01:39:09.976: INFO: (19) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:462/proxy/: tls qux (200; 2.561399ms)
    Jan 12 01:39:09.976: INFO: (19) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">test<... (200; 2.809131ms)
    Jan 12 01:39:09.976: INFO: (19) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 2.645797ms)
    Jan 12 01:39:09.976: INFO: (19) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:1080/proxy/rewriteme">... (200; 2.754206ms)
    Jan 12 01:39:09.976: INFO: (19) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:460/proxy/: tls baz (200; 2.658657ms)
    Jan 12 01:39:09.977: INFO: (19) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl/proxy/rewriteme">test</a> (200; 2.510764ms)
    Jan 12 01:39:09.977: INFO: (19) /api/v1/namespaces/proxy-7153/pods/http:proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 2.92908ms)
    Jan 12 01:39:09.977: INFO: (19) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:162/proxy/: bar (200; 3.109372ms)
    Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname2/proxy/: tls qux (200; 3.552085ms)
    Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/pods/proxy-service-rjpj9-thvbl:160/proxy/: foo (200; 3.115174ms)
    Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname1/proxy/: foo (200; 3.88103ms)
    Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/services/https:proxy-service-rjpj9:tlsportname1/proxy/: tls baz (200; 3.387197ms)
    Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname1/proxy/: foo (200; 4.03706ms)
    Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/services/proxy-service-rjpj9:portname2/proxy/: bar (200; 3.689599ms)
    Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/services/http:proxy-service-rjpj9:portname2/proxy/: bar (200; 3.94899ms)
    Jan 12 01:39:09.978: INFO: (19) /api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/: <a href="/api/v1/namespaces/proxy-7153/pods/https:proxy-service-rjpj9-thvbl:443/proxy/tlsrewritem... (200; 3.588552ms)
    STEP: deleting ReplicationController proxy-service-rjpj9 in namespace proxy-7153, will wait for the garbage collector to delete the pods 01/12/23 01:39:09.978
    Jan 12 01:39:10.036: INFO: Deleting ReplicationController proxy-service-rjpj9 took: 4.7505ms
    Jan 12 01:39:10.137: INFO: Terminating ReplicationController proxy-service-rjpj9 pods took: 100.872213ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:39:13.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7153" for this suite. 01/12/23 01:39:13.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:39:13.067
Jan 12 01:39:13.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 01:39:13.067
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:13.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:13.082
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/12/23 01:39:13.085
Jan 12 01:39:13.120: INFO: Waiting up to 5m0s for pod "pod-638e6381-5e82-4dce-bb9e-86cee3eab63f" in namespace "emptydir-5770" to be "Succeeded or Failed"
Jan 12 01:39:13.122: INFO: Pod "pod-638e6381-5e82-4dce-bb9e-86cee3eab63f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.166243ms
Jan 12 01:39:15.125: INFO: Pod "pod-638e6381-5e82-4dce-bb9e-86cee3eab63f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005188262s
Jan 12 01:39:17.126: INFO: Pod "pod-638e6381-5e82-4dce-bb9e-86cee3eab63f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006381602s
Jan 12 01:39:19.125: INFO: Pod "pod-638e6381-5e82-4dce-bb9e-86cee3eab63f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005042865s
STEP: Saw pod success 01/12/23 01:39:19.125
Jan 12 01:39:19.125: INFO: Pod "pod-638e6381-5e82-4dce-bb9e-86cee3eab63f" satisfied condition "Succeeded or Failed"
Jan 12 01:39:19.127: INFO: Trying to get logs from node eqx04-flash06 pod pod-638e6381-5e82-4dce-bb9e-86cee3eab63f container test-container: <nil>
STEP: delete the pod 01/12/23 01:39:19.135
Jan 12 01:39:19.147: INFO: Waiting for pod pod-638e6381-5e82-4dce-bb9e-86cee3eab63f to disappear
Jan 12 01:39:19.149: INFO: Pod pod-638e6381-5e82-4dce-bb9e-86cee3eab63f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:39:19.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5770" for this suite. 01/12/23 01:39:19.152
------------------------------
• [SLOW TEST] [6.104 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:39:13.067
    Jan 12 01:39:13.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 01:39:13.067
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:13.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:13.082
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/12/23 01:39:13.085
    Jan 12 01:39:13.120: INFO: Waiting up to 5m0s for pod "pod-638e6381-5e82-4dce-bb9e-86cee3eab63f" in namespace "emptydir-5770" to be "Succeeded or Failed"
    Jan 12 01:39:13.122: INFO: Pod "pod-638e6381-5e82-4dce-bb9e-86cee3eab63f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.166243ms
    Jan 12 01:39:15.125: INFO: Pod "pod-638e6381-5e82-4dce-bb9e-86cee3eab63f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005188262s
    Jan 12 01:39:17.126: INFO: Pod "pod-638e6381-5e82-4dce-bb9e-86cee3eab63f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006381602s
    Jan 12 01:39:19.125: INFO: Pod "pod-638e6381-5e82-4dce-bb9e-86cee3eab63f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005042865s
    STEP: Saw pod success 01/12/23 01:39:19.125
    Jan 12 01:39:19.125: INFO: Pod "pod-638e6381-5e82-4dce-bb9e-86cee3eab63f" satisfied condition "Succeeded or Failed"
    Jan 12 01:39:19.127: INFO: Trying to get logs from node eqx04-flash06 pod pod-638e6381-5e82-4dce-bb9e-86cee3eab63f container test-container: <nil>
    STEP: delete the pod 01/12/23 01:39:19.135
    Jan 12 01:39:19.147: INFO: Waiting for pod pod-638e6381-5e82-4dce-bb9e-86cee3eab63f to disappear
    Jan 12 01:39:19.149: INFO: Pod pod-638e6381-5e82-4dce-bb9e-86cee3eab63f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:39:19.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5770" for this suite. 01/12/23 01:39:19.152
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:39:19.171
Jan 12 01:39:19.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir-wrapper 01/12/23 01:39:19.172
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:19.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:19.185
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 12 01:39:19.237: INFO: Waiting up to 5m0s for pod "pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b" in namespace "emptydir-wrapper-9858" to be "running and ready"
Jan 12 01:39:19.239: INFO: Pod "pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.51775ms
Jan 12 01:39:19.239: INFO: The phase of Pod pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:39:21.243: INFO: Pod "pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006009863s
Jan 12 01:39:21.243: INFO: The phase of Pod pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:39:23.243: INFO: Pod "pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b": Phase="Running", Reason="", readiness=true. Elapsed: 4.006311537s
Jan 12 01:39:23.243: INFO: The phase of Pod pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b is Running (Ready = true)
Jan 12 01:39:23.243: INFO: Pod "pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/12/23 01:39:23.245
STEP: Cleaning up the configmap 01/12/23 01:39:23.251
STEP: Cleaning up the pod 01/12/23 01:39:23.256
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:39:23.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-9858" for this suite. 01/12/23 01:39:23.269
------------------------------
• [4.112 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:39:19.171
    Jan 12 01:39:19.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir-wrapper 01/12/23 01:39:19.172
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:19.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:19.185
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 12 01:39:19.237: INFO: Waiting up to 5m0s for pod "pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b" in namespace "emptydir-wrapper-9858" to be "running and ready"
    Jan 12 01:39:19.239: INFO: Pod "pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.51775ms
    Jan 12 01:39:19.239: INFO: The phase of Pod pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:39:21.243: INFO: Pod "pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006009863s
    Jan 12 01:39:21.243: INFO: The phase of Pod pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:39:23.243: INFO: Pod "pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b": Phase="Running", Reason="", readiness=true. Elapsed: 4.006311537s
    Jan 12 01:39:23.243: INFO: The phase of Pod pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b is Running (Ready = true)
    Jan 12 01:39:23.243: INFO: Pod "pod-secrets-0b03a61c-6c31-4298-a542-8bac2eb7dc6b" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/12/23 01:39:23.245
    STEP: Cleaning up the configmap 01/12/23 01:39:23.251
    STEP: Cleaning up the pod 01/12/23 01:39:23.256
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:39:23.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-9858" for this suite. 01/12/23 01:39:23.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:39:23.296
Jan 12 01:39:23.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:39:23.297
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:23.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:23.309
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-53303c90-b5c5-44b4-afaf-d56372cf6aa8 01/12/23 01:39:23.311
STEP: Creating a pod to test consume configMaps 01/12/23 01:39:23.314
Jan 12 01:39:23.344: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9" in namespace "projected-4360" to be "Succeeded or Failed"
Jan 12 01:39:23.346: INFO: Pod "pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.24994ms
Jan 12 01:39:25.349: INFO: Pod "pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005282637s
Jan 12 01:39:27.350: INFO: Pod "pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00601571s
Jan 12 01:39:29.350: INFO: Pod "pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006027608s
STEP: Saw pod success 01/12/23 01:39:29.35
Jan 12 01:39:29.350: INFO: Pod "pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9" satisfied condition "Succeeded or Failed"
Jan 12 01:39:29.352: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 01:39:29.365
Jan 12 01:39:29.378: INFO: Waiting for pod pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9 to disappear
Jan 12 01:39:29.381: INFO: Pod pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:39:29.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4360" for this suite. 01/12/23 01:39:29.384
------------------------------
• [SLOW TEST] [6.658 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:39:23.296
    Jan 12 01:39:23.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:39:23.297
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:23.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:23.309
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-53303c90-b5c5-44b4-afaf-d56372cf6aa8 01/12/23 01:39:23.311
    STEP: Creating a pod to test consume configMaps 01/12/23 01:39:23.314
    Jan 12 01:39:23.344: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9" in namespace "projected-4360" to be "Succeeded or Failed"
    Jan 12 01:39:23.346: INFO: Pod "pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.24994ms
    Jan 12 01:39:25.349: INFO: Pod "pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005282637s
    Jan 12 01:39:27.350: INFO: Pod "pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00601571s
    Jan 12 01:39:29.350: INFO: Pod "pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006027608s
    STEP: Saw pod success 01/12/23 01:39:29.35
    Jan 12 01:39:29.350: INFO: Pod "pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9" satisfied condition "Succeeded or Failed"
    Jan 12 01:39:29.352: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 01:39:29.365
    Jan 12 01:39:29.378: INFO: Waiting for pod pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9 to disappear
    Jan 12 01:39:29.381: INFO: Pod pod-projected-configmaps-8899eaf2-2178-4edd-b996-90c499af49d9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:39:29.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4360" for this suite. 01/12/23 01:39:29.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:39:29.956
Jan 12 01:39:29.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubelet-test 01/12/23 01:39:29.957
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:29.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:29.972
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/12/23 01:39:30.097
Jan 12 01:39:30.097: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasescc15dc79-7a42-4a32-ad19-490a26741e41" in namespace "kubelet-test-1468" to be "completed"
Jan 12 01:39:30.100: INFO: Pod "agnhost-host-aliasescc15dc79-7a42-4a32-ad19-490a26741e41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.403133ms
Jan 12 01:39:32.103: INFO: Pod "agnhost-host-aliasescc15dc79-7a42-4a32-ad19-490a26741e41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006128864s
Jan 12 01:39:34.104: INFO: Pod "agnhost-host-aliasescc15dc79-7a42-4a32-ad19-490a26741e41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006868756s
Jan 12 01:39:36.103: INFO: Pod "agnhost-host-aliasescc15dc79-7a42-4a32-ad19-490a26741e41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005742103s
Jan 12 01:39:36.103: INFO: Pod "agnhost-host-aliasescc15dc79-7a42-4a32-ad19-490a26741e41" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 12 01:39:36.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1468" for this suite. 01/12/23 01:39:36.114
------------------------------
• [SLOW TEST] [6.173 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:39:29.956
    Jan 12 01:39:29.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubelet-test 01/12/23 01:39:29.957
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:29.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:29.972
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/12/23 01:39:30.097
    Jan 12 01:39:30.097: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasescc15dc79-7a42-4a32-ad19-490a26741e41" in namespace "kubelet-test-1468" to be "completed"
    Jan 12 01:39:30.100: INFO: Pod "agnhost-host-aliasescc15dc79-7a42-4a32-ad19-490a26741e41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.403133ms
    Jan 12 01:39:32.103: INFO: Pod "agnhost-host-aliasescc15dc79-7a42-4a32-ad19-490a26741e41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006128864s
    Jan 12 01:39:34.104: INFO: Pod "agnhost-host-aliasescc15dc79-7a42-4a32-ad19-490a26741e41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006868756s
    Jan 12 01:39:36.103: INFO: Pod "agnhost-host-aliasescc15dc79-7a42-4a32-ad19-490a26741e41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005742103s
    Jan 12 01:39:36.103: INFO: Pod "agnhost-host-aliasescc15dc79-7a42-4a32-ad19-490a26741e41" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:39:36.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1468" for this suite. 01/12/23 01:39:36.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:39:36.131
Jan 12 01:39:36.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 01:39:36.131
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:36.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:36.144
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/12/23 01:39:36.146
Jan 12 01:39:36.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:39:38.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:39:47.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8088" for this suite. 01/12/23 01:39:47.111
------------------------------
• [SLOW TEST] [10.995 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:39:36.131
    Jan 12 01:39:36.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 01:39:36.131
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:36.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:36.144
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/12/23 01:39:36.146
    Jan 12 01:39:36.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:39:38.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:39:47.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8088" for this suite. 01/12/23 01:39:47.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:39:47.128
Jan 12 01:39:47.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 01:39:47.129
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:47.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:47.142
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-05937392-5dbe-4d91-9ca6-e6dd57dbfc4e 01/12/23 01:39:47.144
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:39:47.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6767" for this suite. 01/12/23 01:39:47.149
------------------------------
• [0.034 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:39:47.128
    Jan 12 01:39:47.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 01:39:47.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:47.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:47.142
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-05937392-5dbe-4d91-9ca6-e6dd57dbfc4e 01/12/23 01:39:47.144
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:39:47.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6767" for this suite. 01/12/23 01:39:47.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:39:47.163
Jan 12 01:39:47.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-runtime 01/12/23 01:39:47.164
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:47.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:47.175
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 01/12/23 01:39:47.177
STEP: wait for the container to reach Succeeded 01/12/23 01:39:47.206
STEP: get the container status 01/12/23 01:39:52.225
STEP: the container should be terminated 01/12/23 01:39:52.227
STEP: the termination message should be set 01/12/23 01:39:52.227
Jan 12 01:39:52.227: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/12/23 01:39:52.227
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 12 01:39:52.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2681" for this suite. 01/12/23 01:39:52.248
------------------------------
• [SLOW TEST] [5.100 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:39:47.163
    Jan 12 01:39:47.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-runtime 01/12/23 01:39:47.164
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:47.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:47.175
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 01/12/23 01:39:47.177
    STEP: wait for the container to reach Succeeded 01/12/23 01:39:47.206
    STEP: get the container status 01/12/23 01:39:52.225
    STEP: the container should be terminated 01/12/23 01:39:52.227
    STEP: the termination message should be set 01/12/23 01:39:52.227
    Jan 12 01:39:52.227: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/12/23 01:39:52.227
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:39:52.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2681" for this suite. 01/12/23 01:39:52.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:39:52.265
Jan 12 01:39:52.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 01:39:52.266
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:52.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:52.283
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-008e73df-f767-4bbf-93a6-fb8c8577e279 01/12/23 01:39:52.289
STEP: Creating the pod 01/12/23 01:39:52.293
Jan 12 01:39:52.325: INFO: Waiting up to 5m0s for pod "pod-configmaps-f62e7cee-d8d9-4f4b-9f5b-eebd67a77e8d" in namespace "configmap-1837" to be "running and ready"
Jan 12 01:39:52.328: INFO: Pod "pod-configmaps-f62e7cee-d8d9-4f4b-9f5b-eebd67a77e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.844279ms
Jan 12 01:39:52.328: INFO: The phase of Pod pod-configmaps-f62e7cee-d8d9-4f4b-9f5b-eebd67a77e8d is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:39:54.332: INFO: Pod "pod-configmaps-f62e7cee-d8d9-4f4b-9f5b-eebd67a77e8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006363461s
Jan 12 01:39:54.332: INFO: The phase of Pod pod-configmaps-f62e7cee-d8d9-4f4b-9f5b-eebd67a77e8d is Running (Ready = true)
Jan 12 01:39:54.332: INFO: Pod "pod-configmaps-f62e7cee-d8d9-4f4b-9f5b-eebd67a77e8d" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-008e73df-f767-4bbf-93a6-fb8c8577e279 01/12/23 01:39:54.342
STEP: waiting to observe update in volume 01/12/23 01:39:54.347
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:39:56.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1837" for this suite. 01/12/23 01:39:56.367
------------------------------
• [4.174 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:39:52.265
    Jan 12 01:39:52.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 01:39:52.266
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:52.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:52.283
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-008e73df-f767-4bbf-93a6-fb8c8577e279 01/12/23 01:39:52.289
    STEP: Creating the pod 01/12/23 01:39:52.293
    Jan 12 01:39:52.325: INFO: Waiting up to 5m0s for pod "pod-configmaps-f62e7cee-d8d9-4f4b-9f5b-eebd67a77e8d" in namespace "configmap-1837" to be "running and ready"
    Jan 12 01:39:52.328: INFO: Pod "pod-configmaps-f62e7cee-d8d9-4f4b-9f5b-eebd67a77e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.844279ms
    Jan 12 01:39:52.328: INFO: The phase of Pod pod-configmaps-f62e7cee-d8d9-4f4b-9f5b-eebd67a77e8d is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:39:54.332: INFO: Pod "pod-configmaps-f62e7cee-d8d9-4f4b-9f5b-eebd67a77e8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006363461s
    Jan 12 01:39:54.332: INFO: The phase of Pod pod-configmaps-f62e7cee-d8d9-4f4b-9f5b-eebd67a77e8d is Running (Ready = true)
    Jan 12 01:39:54.332: INFO: Pod "pod-configmaps-f62e7cee-d8d9-4f4b-9f5b-eebd67a77e8d" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-008e73df-f767-4bbf-93a6-fb8c8577e279 01/12/23 01:39:54.342
    STEP: waiting to observe update in volume 01/12/23 01:39:54.347
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:39:56.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1837" for this suite. 01/12/23 01:39:56.367
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:39:56.439
Jan 12 01:39:56.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 01:39:56.44
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:56.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:56.454
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 01/12/23 01:39:56.456
Jan 12 01:39:56.592: INFO: Waiting up to 5m0s for pod "pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7" in namespace "emptydir-403" to be "Succeeded or Failed"
Jan 12 01:39:56.595: INFO: Pod "pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.000812ms
Jan 12 01:39:58.599: INFO: Pod "pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007665034s
Jan 12 01:40:00.599: INFO: Pod "pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007124832s
STEP: Saw pod success 01/12/23 01:40:00.599
Jan 12 01:40:00.599: INFO: Pod "pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7" satisfied condition "Succeeded or Failed"
Jan 12 01:40:00.601: INFO: Trying to get logs from node eqx04-flash06 pod pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7 container test-container: <nil>
STEP: delete the pod 01/12/23 01:40:00.609
Jan 12 01:40:00.624: INFO: Waiting for pod pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7 to disappear
Jan 12 01:40:00.626: INFO: Pod pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:40:00.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-403" for this suite. 01/12/23 01:40:00.63
------------------------------
• [4.212 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:39:56.439
    Jan 12 01:39:56.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 01:39:56.44
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:39:56.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:39:56.454
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/12/23 01:39:56.456
    Jan 12 01:39:56.592: INFO: Waiting up to 5m0s for pod "pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7" in namespace "emptydir-403" to be "Succeeded or Failed"
    Jan 12 01:39:56.595: INFO: Pod "pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.000812ms
    Jan 12 01:39:58.599: INFO: Pod "pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007665034s
    Jan 12 01:40:00.599: INFO: Pod "pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007124832s
    STEP: Saw pod success 01/12/23 01:40:00.599
    Jan 12 01:40:00.599: INFO: Pod "pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7" satisfied condition "Succeeded or Failed"
    Jan 12 01:40:00.601: INFO: Trying to get logs from node eqx04-flash06 pod pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7 container test-container: <nil>
    STEP: delete the pod 01/12/23 01:40:00.609
    Jan 12 01:40:00.624: INFO: Waiting for pod pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7 to disappear
    Jan 12 01:40:00.626: INFO: Pod pod-bb0340ef-1fe7-4381-a734-0db0ba33c0a7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:40:00.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-403" for this suite. 01/12/23 01:40:00.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:40:00.655
Jan 12 01:40:00.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename daemonsets 01/12/23 01:40:00.656
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:40:00.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:40:00.669
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 01/12/23 01:40:00.684
STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 01:40:00.729
Jan 12 01:40:00.733: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:00.733: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:00.733: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:00.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:40:00.735: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:40:01.739: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:01.739: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:01.739: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:01.742: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:40:01.742: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:40:02.739: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:02.739: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:02.739: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:02.743: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:40:02.743: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:40:03.755: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:03.755: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:03.755: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:03.758: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 01:40:03.758: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/12/23 01:40:03.76
Jan 12 01:40:03.773: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:03.773: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:03.773: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:03.775: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 01:40:03.775: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:40:04.780: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:04.780: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:04.781: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:04.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 01:40:04.783: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:40:05.780: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:05.780: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:05.780: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:05.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 01:40:05.782: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:40:06.780: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:06.780: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:06.780: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:06.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 01:40:06.782: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:40:07.779: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:07.779: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:07.779: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:07.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 01:40:07.782: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:40:08.781: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:08.781: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:08.781: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:08.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 01:40:08.783: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:40:09.781: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:09.781: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:09.781: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:40:09.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 01:40:09.783: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/12/23 01:40:09.785
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5781, will wait for the garbage collector to delete the pods 01/12/23 01:40:09.785
Jan 12 01:40:09.844: INFO: Deleting DaemonSet.extensions daemon-set took: 5.878921ms
Jan 12 01:40:09.945: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.781553ms
Jan 12 01:40:12.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:40:12.247: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 12 01:40:12.250: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20169330"},"items":null}

Jan 12 01:40:12.252: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20169330"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:40:12.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5781" for this suite. 01/12/23 01:40:12.263
------------------------------
• [SLOW TEST] [11.622 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:40:00.655
    Jan 12 01:40:00.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename daemonsets 01/12/23 01:40:00.656
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:40:00.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:40:00.669
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 01/12/23 01:40:00.684
    STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 01:40:00.729
    Jan 12 01:40:00.733: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:00.733: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:00.733: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:00.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:40:00.735: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:40:01.739: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:01.739: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:01.739: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:01.742: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:40:01.742: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:40:02.739: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:02.739: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:02.739: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:02.743: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:40:02.743: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:40:03.755: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:03.755: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:03.755: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:03.758: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 01:40:03.758: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/12/23 01:40:03.76
    Jan 12 01:40:03.773: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:03.773: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:03.773: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:03.775: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 01:40:03.775: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:40:04.780: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:04.780: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:04.781: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:04.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 01:40:04.783: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:40:05.780: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:05.780: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:05.780: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:05.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 01:40:05.782: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:40:06.780: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:06.780: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:06.780: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:06.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 01:40:06.782: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:40:07.779: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:07.779: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:07.779: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:07.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 01:40:07.782: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:40:08.781: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:08.781: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:08.781: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:08.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 01:40:08.783: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:40:09.781: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:09.781: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:09.781: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:40:09.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 01:40:09.783: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/12/23 01:40:09.785
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5781, will wait for the garbage collector to delete the pods 01/12/23 01:40:09.785
    Jan 12 01:40:09.844: INFO: Deleting DaemonSet.extensions daemon-set took: 5.878921ms
    Jan 12 01:40:09.945: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.781553ms
    Jan 12 01:40:12.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:40:12.247: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 12 01:40:12.250: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20169330"},"items":null}

    Jan 12 01:40:12.252: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20169330"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:40:12.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5781" for this suite. 01/12/23 01:40:12.263
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:40:12.278
Jan 12 01:40:12.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename var-expansion 01/12/23 01:40:12.279
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:40:12.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:40:12.293
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 01/12/23 01:40:12.295
Jan 12 01:40:12.349: INFO: Waiting up to 5m0s for pod "var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8" in namespace "var-expansion-1555" to be "Succeeded or Failed"
Jan 12 01:40:12.351: INFO: Pod "var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.158453ms
Jan 12 01:40:14.355: INFO: Pod "var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006449377s
Jan 12 01:40:16.354: INFO: Pod "var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005310975s
STEP: Saw pod success 01/12/23 01:40:16.354
Jan 12 01:40:16.354: INFO: Pod "var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8" satisfied condition "Succeeded or Failed"
Jan 12 01:40:16.357: INFO: Trying to get logs from node eqx04-flash06 pod var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8 container dapi-container: <nil>
STEP: delete the pod 01/12/23 01:40:16.37
Jan 12 01:40:16.383: INFO: Waiting for pod var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8 to disappear
Jan 12 01:40:16.385: INFO: Pod var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 01:40:16.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1555" for this suite. 01/12/23 01:40:16.388
------------------------------
• [4.123 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:40:12.278
    Jan 12 01:40:12.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename var-expansion 01/12/23 01:40:12.279
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:40:12.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:40:12.293
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 01/12/23 01:40:12.295
    Jan 12 01:40:12.349: INFO: Waiting up to 5m0s for pod "var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8" in namespace "var-expansion-1555" to be "Succeeded or Failed"
    Jan 12 01:40:12.351: INFO: Pod "var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.158453ms
    Jan 12 01:40:14.355: INFO: Pod "var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006449377s
    Jan 12 01:40:16.354: INFO: Pod "var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005310975s
    STEP: Saw pod success 01/12/23 01:40:16.354
    Jan 12 01:40:16.354: INFO: Pod "var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8" satisfied condition "Succeeded or Failed"
    Jan 12 01:40:16.357: INFO: Trying to get logs from node eqx04-flash06 pod var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8 container dapi-container: <nil>
    STEP: delete the pod 01/12/23 01:40:16.37
    Jan 12 01:40:16.383: INFO: Waiting for pod var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8 to disappear
    Jan 12 01:40:16.385: INFO: Pod var-expansion-12123beb-0787-4c7d-9b1c-ee5f1bd38ad8 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:40:16.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1555" for this suite. 01/12/23 01:40:16.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:40:16.405
Jan 12 01:40:16.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 01:40:16.406
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:40:16.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:40:16.419
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-5378 01/12/23 01:40:16.421
STEP: creating service affinity-nodeport in namespace services-5378 01/12/23 01:40:16.421
STEP: creating replication controller affinity-nodeport in namespace services-5378 01/12/23 01:40:16.435
I0112 01:40:16.441711      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5378, replica count: 3
I0112 01:40:19.493277      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 01:40:19.502: INFO: Creating new exec pod
Jan 12 01:40:19.534: INFO: Waiting up to 5m0s for pod "execpod-affinityq9x6x" in namespace "services-5378" to be "running"
Jan 12 01:40:19.537: INFO: Pod "execpod-affinityq9x6x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.346135ms
Jan 12 01:40:21.540: INFO: Pod "execpod-affinityq9x6x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005514478s
Jan 12 01:40:23.540: INFO: Pod "execpod-affinityq9x6x": Phase="Running", Reason="", readiness=true. Elapsed: 4.005369071s
Jan 12 01:40:23.540: INFO: Pod "execpod-affinityq9x6x" satisfied condition "running"
Jan 12 01:40:24.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5378 exec execpod-affinityq9x6x -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jan 12 01:40:24.753: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 12 01:40:24.753: INFO: stdout: ""
Jan 12 01:40:24.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5378 exec execpod-affinityq9x6x -- /bin/sh -x -c nc -v -z -w 2 172.19.2.117 80'
Jan 12 01:40:24.942: INFO: stderr: "+ nc -v -z -w 2 172.19.2.117 80\nConnection to 172.19.2.117 80 port [tcp/http] succeeded!\n"
Jan 12 01:40:24.942: INFO: stdout: ""
Jan 12 01:40:24.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5378 exec execpod-affinityq9x6x -- /bin/sh -x -c nc -v -z -w 2 10.9.140.106 31908'
Jan 12 01:40:25.153: INFO: stderr: "+ nc -v -z -w 2 10.9.140.106 31908\nConnection to 10.9.140.106 31908 port [tcp/*] succeeded!\n"
Jan 12 01:40:25.153: INFO: stdout: ""
Jan 12 01:40:25.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5378 exec execpod-affinityq9x6x -- /bin/sh -x -c nc -v -z -w 2 10.9.40.106 31908'
Jan 12 01:40:25.346: INFO: stderr: "+ nc -v -z -w 2 10.9.40.106 31908\nConnection to 10.9.40.106 31908 port [tcp/*] succeeded!\n"
Jan 12 01:40:25.346: INFO: stdout: ""
Jan 12 01:40:25.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5378 exec execpod-affinityq9x6x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.140.106:31908/ ; done'
Jan 12 01:40:25.610: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n"
Jan 12 01:40:25.611: INFO: stdout: "\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2"
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
Jan 12 01:40:25.611: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5378, will wait for the garbage collector to delete the pods 01/12/23 01:40:25.626
Jan 12 01:40:25.685: INFO: Deleting ReplicationController affinity-nodeport took: 6.697443ms
Jan 12 01:40:25.785: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.2274ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 01:40:28.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5378" for this suite. 01/12/23 01:40:28.62
------------------------------
• [SLOW TEST] [12.234 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:40:16.405
    Jan 12 01:40:16.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 01:40:16.406
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:40:16.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:40:16.419
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-5378 01/12/23 01:40:16.421
    STEP: creating service affinity-nodeport in namespace services-5378 01/12/23 01:40:16.421
    STEP: creating replication controller affinity-nodeport in namespace services-5378 01/12/23 01:40:16.435
    I0112 01:40:16.441711      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5378, replica count: 3
    I0112 01:40:19.493277      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 01:40:19.502: INFO: Creating new exec pod
    Jan 12 01:40:19.534: INFO: Waiting up to 5m0s for pod "execpod-affinityq9x6x" in namespace "services-5378" to be "running"
    Jan 12 01:40:19.537: INFO: Pod "execpod-affinityq9x6x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.346135ms
    Jan 12 01:40:21.540: INFO: Pod "execpod-affinityq9x6x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005514478s
    Jan 12 01:40:23.540: INFO: Pod "execpod-affinityq9x6x": Phase="Running", Reason="", readiness=true. Elapsed: 4.005369071s
    Jan 12 01:40:23.540: INFO: Pod "execpod-affinityq9x6x" satisfied condition "running"
    Jan 12 01:40:24.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5378 exec execpod-affinityq9x6x -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jan 12 01:40:24.753: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 12 01:40:24.753: INFO: stdout: ""
    Jan 12 01:40:24.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5378 exec execpod-affinityq9x6x -- /bin/sh -x -c nc -v -z -w 2 172.19.2.117 80'
    Jan 12 01:40:24.942: INFO: stderr: "+ nc -v -z -w 2 172.19.2.117 80\nConnection to 172.19.2.117 80 port [tcp/http] succeeded!\n"
    Jan 12 01:40:24.942: INFO: stdout: ""
    Jan 12 01:40:24.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5378 exec execpod-affinityq9x6x -- /bin/sh -x -c nc -v -z -w 2 10.9.140.106 31908'
    Jan 12 01:40:25.153: INFO: stderr: "+ nc -v -z -w 2 10.9.140.106 31908\nConnection to 10.9.140.106 31908 port [tcp/*] succeeded!\n"
    Jan 12 01:40:25.153: INFO: stdout: ""
    Jan 12 01:40:25.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5378 exec execpod-affinityq9x6x -- /bin/sh -x -c nc -v -z -w 2 10.9.40.106 31908'
    Jan 12 01:40:25.346: INFO: stderr: "+ nc -v -z -w 2 10.9.40.106 31908\nConnection to 10.9.40.106 31908 port [tcp/*] succeeded!\n"
    Jan 12 01:40:25.346: INFO: stdout: ""
    Jan 12 01:40:25.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5378 exec execpod-affinityq9x6x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.9.140.106:31908/ ; done'
    Jan 12 01:40:25.610: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.9.140.106:31908/\n"
    Jan 12 01:40:25.611: INFO: stdout: "\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2\naffinity-nodeport-rkgs2"
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Received response from host: affinity-nodeport-rkgs2
    Jan 12 01:40:25.611: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-5378, will wait for the garbage collector to delete the pods 01/12/23 01:40:25.626
    Jan 12 01:40:25.685: INFO: Deleting ReplicationController affinity-nodeport took: 6.697443ms
    Jan 12 01:40:25.785: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.2274ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:40:28.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5378" for this suite. 01/12/23 01:40:28.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:40:28.641
Jan 12 01:40:28.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:40:28.644
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:40:28.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:40:28.658
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 01/12/23 01:40:28.662
Jan 12 01:40:28.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7" in namespace "projected-5295" to be "Succeeded or Failed"
Jan 12 01:40:28.737: INFO: Pod "downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.946ms
Jan 12 01:40:30.741: INFO: Pod "downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008090384s
Jan 12 01:40:32.740: INFO: Pod "downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007111952s
Jan 12 01:40:34.740: INFO: Pod "downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007563988s
STEP: Saw pod success 01/12/23 01:40:34.74
Jan 12 01:40:34.740: INFO: Pod "downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7" satisfied condition "Succeeded or Failed"
Jan 12 01:40:34.742: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7 container client-container: <nil>
STEP: delete the pod 01/12/23 01:40:34.75
Jan 12 01:40:34.770: INFO: Waiting for pod downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7 to disappear
Jan 12 01:40:34.772: INFO: Pod downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 01:40:34.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5295" for this suite. 01/12/23 01:40:34.775
------------------------------
• [SLOW TEST] [6.151 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:40:28.641
    Jan 12 01:40:28.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:40:28.644
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:40:28.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:40:28.658
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 01/12/23 01:40:28.662
    Jan 12 01:40:28.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7" in namespace "projected-5295" to be "Succeeded or Failed"
    Jan 12 01:40:28.737: INFO: Pod "downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.946ms
    Jan 12 01:40:30.741: INFO: Pod "downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008090384s
    Jan 12 01:40:32.740: INFO: Pod "downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007111952s
    Jan 12 01:40:34.740: INFO: Pod "downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007563988s
    STEP: Saw pod success 01/12/23 01:40:34.74
    Jan 12 01:40:34.740: INFO: Pod "downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7" satisfied condition "Succeeded or Failed"
    Jan 12 01:40:34.742: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7 container client-container: <nil>
    STEP: delete the pod 01/12/23 01:40:34.75
    Jan 12 01:40:34.770: INFO: Waiting for pod downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7 to disappear
    Jan 12 01:40:34.772: INFO: Pod downwardapi-volume-04a33055-1c1a-445d-a41e-c6e5417193b7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:40:34.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5295" for this suite. 01/12/23 01:40:34.775
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:40:34.793
Jan 12 01:40:34.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename watch 01/12/23 01:40:34.794
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:40:34.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:40:34.81
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/12/23 01:40:34.813
STEP: creating a new configmap 01/12/23 01:40:34.814
STEP: modifying the configmap once 01/12/23 01:40:34.822
STEP: changing the label value of the configmap 01/12/23 01:40:34.828
STEP: Expecting to observe a delete notification for the watched object 01/12/23 01:40:34.834
Jan 12 01:40:34.834: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7776  bbe92f0e-172b-4041-b1da-fa637d428377 20169632 0 2023-01-12 01:40:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 01:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 01:40:34.834: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7776  bbe92f0e-172b-4041-b1da-fa637d428377 20169633 0 2023-01-12 01:40:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 01:40:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 01:40:34.834: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7776  bbe92f0e-172b-4041-b1da-fa637d428377 20169634 0 2023-01-12 01:40:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 01:40:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/12/23 01:40:34.835
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/12/23 01:40:34.84
STEP: changing the label value of the configmap back 01/12/23 01:40:44.841
STEP: modifying the configmap a third time 01/12/23 01:40:44.851
STEP: deleting the configmap 01/12/23 01:40:44.858
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/12/23 01:40:44.865
Jan 12 01:40:44.866: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7776  bbe92f0e-172b-4041-b1da-fa637d428377 20169674 0 2023-01-12 01:40:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 01:40:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 01:40:44.866: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7776  bbe92f0e-172b-4041-b1da-fa637d428377 20169675 0 2023-01-12 01:40:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 01:40:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 01:40:44.866: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7776  bbe92f0e-172b-4041-b1da-fa637d428377 20169676 0 2023-01-12 01:40:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 01:40:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 12 01:40:44.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7776" for this suite. 01/12/23 01:40:44.87
------------------------------
• [SLOW TEST] [10.094 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:40:34.793
    Jan 12 01:40:34.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename watch 01/12/23 01:40:34.794
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:40:34.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:40:34.81
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/12/23 01:40:34.813
    STEP: creating a new configmap 01/12/23 01:40:34.814
    STEP: modifying the configmap once 01/12/23 01:40:34.822
    STEP: changing the label value of the configmap 01/12/23 01:40:34.828
    STEP: Expecting to observe a delete notification for the watched object 01/12/23 01:40:34.834
    Jan 12 01:40:34.834: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7776  bbe92f0e-172b-4041-b1da-fa637d428377 20169632 0 2023-01-12 01:40:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 01:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 01:40:34.834: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7776  bbe92f0e-172b-4041-b1da-fa637d428377 20169633 0 2023-01-12 01:40:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 01:40:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 01:40:34.834: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7776  bbe92f0e-172b-4041-b1da-fa637d428377 20169634 0 2023-01-12 01:40:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 01:40:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/12/23 01:40:34.835
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/12/23 01:40:34.84
    STEP: changing the label value of the configmap back 01/12/23 01:40:44.841
    STEP: modifying the configmap a third time 01/12/23 01:40:44.851
    STEP: deleting the configmap 01/12/23 01:40:44.858
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/12/23 01:40:44.865
    Jan 12 01:40:44.866: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7776  bbe92f0e-172b-4041-b1da-fa637d428377 20169674 0 2023-01-12 01:40:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 01:40:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 01:40:44.866: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7776  bbe92f0e-172b-4041-b1da-fa637d428377 20169675 0 2023-01-12 01:40:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 01:40:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 01:40:44.866: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7776  bbe92f0e-172b-4041-b1da-fa637d428377 20169676 0 2023-01-12 01:40:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 01:40:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:40:44.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7776" for this suite. 01/12/23 01:40:44.87
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:40:44.888
Jan 12 01:40:44.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename server-version 01/12/23 01:40:44.889
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:40:44.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:40:44.906
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/12/23 01:40:44.908
STEP: Confirm major version 01/12/23 01:40:44.909
Jan 12 01:40:44.909: INFO: Major version: 1
STEP: Confirm minor version 01/12/23 01:40:44.909
Jan 12 01:40:44.909: INFO: cleanMinorVersion: 26
Jan 12 01:40:44.909: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jan 12 01:40:44.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-1719" for this suite. 01/12/23 01:40:44.913
------------------------------
• [0.042 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:40:44.888
    Jan 12 01:40:44.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename server-version 01/12/23 01:40:44.889
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:40:44.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:40:44.906
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/12/23 01:40:44.908
    STEP: Confirm major version 01/12/23 01:40:44.909
    Jan 12 01:40:44.909: INFO: Major version: 1
    STEP: Confirm minor version 01/12/23 01:40:44.909
    Jan 12 01:40:44.909: INFO: cleanMinorVersion: 26
    Jan 12 01:40:44.909: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:40:44.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-1719" for this suite. 01/12/23 01:40:44.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:40:44.933
Jan 12 01:40:44.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-probe 01/12/23 01:40:44.934
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:40:44.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:40:44.949
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-59d1b51e-7626-4be8-894d-dc0078b4077a in namespace container-probe-2006 01/12/23 01:40:44.951
Jan 12 01:40:44.983: INFO: Waiting up to 5m0s for pod "liveness-59d1b51e-7626-4be8-894d-dc0078b4077a" in namespace "container-probe-2006" to be "not pending"
Jan 12 01:40:44.986: INFO: Pod "liveness-59d1b51e-7626-4be8-894d-dc0078b4077a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.673036ms
Jan 12 01:40:46.990: INFO: Pod "liveness-59d1b51e-7626-4be8-894d-dc0078b4077a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006596236s
Jan 12 01:40:48.990: INFO: Pod "liveness-59d1b51e-7626-4be8-894d-dc0078b4077a": Phase="Running", Reason="", readiness=true. Elapsed: 4.007277455s
Jan 12 01:40:48.991: INFO: Pod "liveness-59d1b51e-7626-4be8-894d-dc0078b4077a" satisfied condition "not pending"
Jan 12 01:40:48.991: INFO: Started pod liveness-59d1b51e-7626-4be8-894d-dc0078b4077a in namespace container-probe-2006
STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 01:40:48.991
Jan 12 01:40:48.993: INFO: Initial restart count of pod liveness-59d1b51e-7626-4be8-894d-dc0078b4077a is 0
STEP: deleting the pod 01/12/23 01:44:49.502
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 01:44:49.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2006" for this suite. 01/12/23 01:44:49.536
------------------------------
• [SLOW TEST] [244.645 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:40:44.933
    Jan 12 01:40:44.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-probe 01/12/23 01:40:44.934
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:40:44.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:40:44.949
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-59d1b51e-7626-4be8-894d-dc0078b4077a in namespace container-probe-2006 01/12/23 01:40:44.951
    Jan 12 01:40:44.983: INFO: Waiting up to 5m0s for pod "liveness-59d1b51e-7626-4be8-894d-dc0078b4077a" in namespace "container-probe-2006" to be "not pending"
    Jan 12 01:40:44.986: INFO: Pod "liveness-59d1b51e-7626-4be8-894d-dc0078b4077a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.673036ms
    Jan 12 01:40:46.990: INFO: Pod "liveness-59d1b51e-7626-4be8-894d-dc0078b4077a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006596236s
    Jan 12 01:40:48.990: INFO: Pod "liveness-59d1b51e-7626-4be8-894d-dc0078b4077a": Phase="Running", Reason="", readiness=true. Elapsed: 4.007277455s
    Jan 12 01:40:48.991: INFO: Pod "liveness-59d1b51e-7626-4be8-894d-dc0078b4077a" satisfied condition "not pending"
    Jan 12 01:40:48.991: INFO: Started pod liveness-59d1b51e-7626-4be8-894d-dc0078b4077a in namespace container-probe-2006
    STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 01:40:48.991
    Jan 12 01:40:48.993: INFO: Initial restart count of pod liveness-59d1b51e-7626-4be8-894d-dc0078b4077a is 0
    STEP: deleting the pod 01/12/23 01:44:49.502
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:44:49.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2006" for this suite. 01/12/23 01:44:49.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:44:49.58
Jan 12 01:44:49.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename resourcequota 01/12/23 01:44:49.581
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:44:49.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:44:49.595
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 01/12/23 01:44:49.598
STEP: Getting a ResourceQuota 01/12/23 01:44:49.602
STEP: Updating a ResourceQuota 01/12/23 01:44:49.604
STEP: Verifying a ResourceQuota was modified 01/12/23 01:44:49.613
STEP: Deleting a ResourceQuota 01/12/23 01:44:49.617
STEP: Verifying the deleted ResourceQuota 01/12/23 01:44:49.626
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 01:44:49.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3827" for this suite. 01/12/23 01:44:49.632
------------------------------
• [0.086 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:44:49.58
    Jan 12 01:44:49.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename resourcequota 01/12/23 01:44:49.581
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:44:49.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:44:49.595
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 01/12/23 01:44:49.598
    STEP: Getting a ResourceQuota 01/12/23 01:44:49.602
    STEP: Updating a ResourceQuota 01/12/23 01:44:49.604
    STEP: Verifying a ResourceQuota was modified 01/12/23 01:44:49.613
    STEP: Deleting a ResourceQuota 01/12/23 01:44:49.617
    STEP: Verifying the deleted ResourceQuota 01/12/23 01:44:49.626
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:44:49.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3827" for this suite. 01/12/23 01:44:49.632
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:44:49.668
Jan 12 01:44:49.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 01:44:49.669
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:44:49.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:44:49.681
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 01/12/23 01:44:49.684
Jan 12 01:44:49.751: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746" in namespace "downward-api-2539" to be "Succeeded or Failed"
Jan 12 01:44:49.754: INFO: Pod "downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746": Phase="Pending", Reason="", readiness=false. Elapsed: 2.560323ms
Jan 12 01:44:51.758: INFO: Pod "downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006902599s
Jan 12 01:44:53.758: INFO: Pod "downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006758625s
Jan 12 01:44:55.758: INFO: Pod "downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00648229s
STEP: Saw pod success 01/12/23 01:44:55.758
Jan 12 01:44:55.758: INFO: Pod "downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746" satisfied condition "Succeeded or Failed"
Jan 12 01:44:55.760: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746 container client-container: <nil>
STEP: delete the pod 01/12/23 01:44:55.778
Jan 12 01:44:55.791: INFO: Waiting for pod downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746 to disappear
Jan 12 01:44:55.793: INFO: Pod downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 01:44:55.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2539" for this suite. 01/12/23 01:44:55.797
------------------------------
• [SLOW TEST] [6.163 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:44:49.668
    Jan 12 01:44:49.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 01:44:49.669
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:44:49.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:44:49.681
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 01/12/23 01:44:49.684
    Jan 12 01:44:49.751: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746" in namespace "downward-api-2539" to be "Succeeded or Failed"
    Jan 12 01:44:49.754: INFO: Pod "downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746": Phase="Pending", Reason="", readiness=false. Elapsed: 2.560323ms
    Jan 12 01:44:51.758: INFO: Pod "downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006902599s
    Jan 12 01:44:53.758: INFO: Pod "downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006758625s
    Jan 12 01:44:55.758: INFO: Pod "downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00648229s
    STEP: Saw pod success 01/12/23 01:44:55.758
    Jan 12 01:44:55.758: INFO: Pod "downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746" satisfied condition "Succeeded or Failed"
    Jan 12 01:44:55.760: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746 container client-container: <nil>
    STEP: delete the pod 01/12/23 01:44:55.778
    Jan 12 01:44:55.791: INFO: Waiting for pod downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746 to disappear
    Jan 12 01:44:55.793: INFO: Pod downwardapi-volume-9297bbb7-9edd-455a-bd4d-7bbe4d588746 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:44:55.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2539" for this suite. 01/12/23 01:44:55.797
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:44:55.833
Jan 12 01:44:55.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename disruption 01/12/23 01:44:55.833
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:44:55.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:44:55.847
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 01/12/23 01:44:55.853
STEP: Updating PodDisruptionBudget status 01/12/23 01:44:57.858
STEP: Waiting for all pods to be running 01/12/23 01:44:57.904
Jan 12 01:44:57.906: INFO: running pods: 0 < 1
Jan 12 01:44:59.911: INFO: running pods: 0 < 1
STEP: locating a running pod 01/12/23 01:45:01.91
STEP: Waiting for the pdb to be processed 01/12/23 01:45:01.919
STEP: Patching PodDisruptionBudget status 01/12/23 01:45:01.925
STEP: Waiting for the pdb to be processed 01/12/23 01:45:01.932
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 12 01:45:01.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3245" for this suite. 01/12/23 01:45:01.939
------------------------------
• [SLOW TEST] [6.172 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:44:55.833
    Jan 12 01:44:55.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename disruption 01/12/23 01:44:55.833
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:44:55.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:44:55.847
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 01/12/23 01:44:55.853
    STEP: Updating PodDisruptionBudget status 01/12/23 01:44:57.858
    STEP: Waiting for all pods to be running 01/12/23 01:44:57.904
    Jan 12 01:44:57.906: INFO: running pods: 0 < 1
    Jan 12 01:44:59.911: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/12/23 01:45:01.91
    STEP: Waiting for the pdb to be processed 01/12/23 01:45:01.919
    STEP: Patching PodDisruptionBudget status 01/12/23 01:45:01.925
    STEP: Waiting for the pdb to be processed 01/12/23 01:45:01.932
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:45:01.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3245" for this suite. 01/12/23 01:45:01.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:45:02.008
Jan 12 01:45:02.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename gc 01/12/23 01:45:02.009
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:45:02.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:45:02.023
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/12/23 01:45:02.026
STEP: delete the rc 01/12/23 01:45:07.034
STEP: wait for all pods to be garbage collected 01/12/23 01:45:07.039
STEP: Gathering metrics 01/12/23 01:45:12.045
Jan 12 01:45:12.073: INFO: Waiting up to 5m0s for pod "kube-controller-manager-eqx04-flash04" in namespace "kube-system" to be "running and ready"
Jan 12 01:45:12.076: INFO: Pod "kube-controller-manager-eqx04-flash04": Phase="Running", Reason="", readiness=true. Elapsed: 2.303009ms
Jan 12 01:45:12.076: INFO: The phase of Pod kube-controller-manager-eqx04-flash04 is Running (Ready = true)
Jan 12 01:45:12.076: INFO: Pod "kube-controller-manager-eqx04-flash04" satisfied condition "running and ready"
Jan 12 01:45:12.137: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 12 01:45:12.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1578" for this suite. 01/12/23 01:45:12.141
------------------------------
• [SLOW TEST] [10.148 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:45:02.008
    Jan 12 01:45:02.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename gc 01/12/23 01:45:02.009
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:45:02.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:45:02.023
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/12/23 01:45:02.026
    STEP: delete the rc 01/12/23 01:45:07.034
    STEP: wait for all pods to be garbage collected 01/12/23 01:45:07.039
    STEP: Gathering metrics 01/12/23 01:45:12.045
    Jan 12 01:45:12.073: INFO: Waiting up to 5m0s for pod "kube-controller-manager-eqx04-flash04" in namespace "kube-system" to be "running and ready"
    Jan 12 01:45:12.076: INFO: Pod "kube-controller-manager-eqx04-flash04": Phase="Running", Reason="", readiness=true. Elapsed: 2.303009ms
    Jan 12 01:45:12.076: INFO: The phase of Pod kube-controller-manager-eqx04-flash04 is Running (Ready = true)
    Jan 12 01:45:12.076: INFO: Pod "kube-controller-manager-eqx04-flash04" satisfied condition "running and ready"
    Jan 12 01:45:12.137: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:45:12.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1578" for this suite. 01/12/23 01:45:12.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:45:12.158
Jan 12 01:45:12.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:45:12.158
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:45:12.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:45:12.172
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-dc3e1412-4f9e-47ca-96a0-6ed0979cffbd 01/12/23 01:45:12.176
STEP: Creating secret with name secret-projected-all-test-volume-9a766588-9a39-4358-8a13-6fd52d884e0e 01/12/23 01:45:12.18
STEP: Creating a pod to test Check all projections for projected volume plugin 01/12/23 01:45:12.184
Jan 12 01:45:12.213: INFO: Waiting up to 5m0s for pod "projected-volume-046b3868-0df0-471c-8a71-87738b99816d" in namespace "projected-414" to be "Succeeded or Failed"
Jan 12 01:45:12.215: INFO: Pod "projected-volume-046b3868-0df0-471c-8a71-87738b99816d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.328271ms
Jan 12 01:45:14.219: INFO: Pod "projected-volume-046b3868-0df0-471c-8a71-87738b99816d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005663503s
Jan 12 01:45:16.220: INFO: Pod "projected-volume-046b3868-0df0-471c-8a71-87738b99816d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006441142s
Jan 12 01:45:18.219: INFO: Pod "projected-volume-046b3868-0df0-471c-8a71-87738b99816d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00633676s
STEP: Saw pod success 01/12/23 01:45:18.22
Jan 12 01:45:18.220: INFO: Pod "projected-volume-046b3868-0df0-471c-8a71-87738b99816d" satisfied condition "Succeeded or Failed"
Jan 12 01:45:18.222: INFO: Trying to get logs from node eqx04-flash06 pod projected-volume-046b3868-0df0-471c-8a71-87738b99816d container projected-all-volume-test: <nil>
STEP: delete the pod 01/12/23 01:45:18.231
Jan 12 01:45:18.242: INFO: Waiting for pod projected-volume-046b3868-0df0-471c-8a71-87738b99816d to disappear
Jan 12 01:45:18.244: INFO: Pod projected-volume-046b3868-0df0-471c-8a71-87738b99816d no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jan 12 01:45:18.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-414" for this suite. 01/12/23 01:45:18.247
------------------------------
• [SLOW TEST] [6.116 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:45:12.158
    Jan 12 01:45:12.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:45:12.158
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:45:12.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:45:12.172
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-dc3e1412-4f9e-47ca-96a0-6ed0979cffbd 01/12/23 01:45:12.176
    STEP: Creating secret with name secret-projected-all-test-volume-9a766588-9a39-4358-8a13-6fd52d884e0e 01/12/23 01:45:12.18
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/12/23 01:45:12.184
    Jan 12 01:45:12.213: INFO: Waiting up to 5m0s for pod "projected-volume-046b3868-0df0-471c-8a71-87738b99816d" in namespace "projected-414" to be "Succeeded or Failed"
    Jan 12 01:45:12.215: INFO: Pod "projected-volume-046b3868-0df0-471c-8a71-87738b99816d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.328271ms
    Jan 12 01:45:14.219: INFO: Pod "projected-volume-046b3868-0df0-471c-8a71-87738b99816d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005663503s
    Jan 12 01:45:16.220: INFO: Pod "projected-volume-046b3868-0df0-471c-8a71-87738b99816d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006441142s
    Jan 12 01:45:18.219: INFO: Pod "projected-volume-046b3868-0df0-471c-8a71-87738b99816d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00633676s
    STEP: Saw pod success 01/12/23 01:45:18.22
    Jan 12 01:45:18.220: INFO: Pod "projected-volume-046b3868-0df0-471c-8a71-87738b99816d" satisfied condition "Succeeded or Failed"
    Jan 12 01:45:18.222: INFO: Trying to get logs from node eqx04-flash06 pod projected-volume-046b3868-0df0-471c-8a71-87738b99816d container projected-all-volume-test: <nil>
    STEP: delete the pod 01/12/23 01:45:18.231
    Jan 12 01:45:18.242: INFO: Waiting for pod projected-volume-046b3868-0df0-471c-8a71-87738b99816d to disappear
    Jan 12 01:45:18.244: INFO: Pod projected-volume-046b3868-0df0-471c-8a71-87738b99816d no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:45:18.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-414" for this suite. 01/12/23 01:45:18.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:45:18.276
Jan 12 01:45:18.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename conformance-tests 01/12/23 01:45:18.277
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:45:18.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:45:18.289
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/12/23 01:45:18.292
Jan 12 01:45:18.292: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jan 12 01:45:18.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-7533" for this suite. 01/12/23 01:45:18.3
------------------------------
• [0.055 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:45:18.276
    Jan 12 01:45:18.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename conformance-tests 01/12/23 01:45:18.277
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:45:18.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:45:18.289
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/12/23 01:45:18.292
    Jan 12 01:45:18.292: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:45:18.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-7533" for this suite. 01/12/23 01:45:18.3
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:45:18.332
Jan 12 01:45:18.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename gc 01/12/23 01:45:18.333
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:45:18.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:45:18.348
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/12/23 01:45:18.353
STEP: delete the rc 01/12/23 01:45:23.363
STEP: wait for the rc to be deleted 01/12/23 01:45:23.368
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/12/23 01:45:28.373
STEP: Gathering metrics 01/12/23 01:45:58.392
Jan 12 01:45:58.412: INFO: Waiting up to 5m0s for pod "kube-controller-manager-eqx04-flash04" in namespace "kube-system" to be "running and ready"
Jan 12 01:45:58.414: INFO: Pod "kube-controller-manager-eqx04-flash04": Phase="Running", Reason="", readiness=true. Elapsed: 2.36877ms
Jan 12 01:45:58.414: INFO: The phase of Pod kube-controller-manager-eqx04-flash04 is Running (Ready = true)
Jan 12 01:45:58.414: INFO: Pod "kube-controller-manager-eqx04-flash04" satisfied condition "running and ready"
Jan 12 01:45:58.468: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 12 01:45:58.468: INFO: Deleting pod "simpletest.rc-2sf6b" in namespace "gc-9587"
Jan 12 01:45:58.477: INFO: Deleting pod "simpletest.rc-2ttmj" in namespace "gc-9587"
Jan 12 01:45:58.489: INFO: Deleting pod "simpletest.rc-42bcx" in namespace "gc-9587"
Jan 12 01:45:58.500: INFO: Deleting pod "simpletest.rc-458bc" in namespace "gc-9587"
Jan 12 01:45:58.527: INFO: Deleting pod "simpletest.rc-4bgxk" in namespace "gc-9587"
Jan 12 01:45:58.546: INFO: Deleting pod "simpletest.rc-4mprp" in namespace "gc-9587"
Jan 12 01:45:58.572: INFO: Deleting pod "simpletest.rc-4n2k2" in namespace "gc-9587"
Jan 12 01:45:58.602: INFO: Deleting pod "simpletest.rc-4v5p6" in namespace "gc-9587"
Jan 12 01:45:58.614: INFO: Deleting pod "simpletest.rc-5585w" in namespace "gc-9587"
Jan 12 01:45:58.622: INFO: Deleting pod "simpletest.rc-5c46m" in namespace "gc-9587"
Jan 12 01:45:58.633: INFO: Deleting pod "simpletest.rc-5cqbb" in namespace "gc-9587"
Jan 12 01:45:58.660: INFO: Deleting pod "simpletest.rc-68mzd" in namespace "gc-9587"
Jan 12 01:45:58.669: INFO: Deleting pod "simpletest.rc-6flkx" in namespace "gc-9587"
Jan 12 01:45:58.680: INFO: Deleting pod "simpletest.rc-6jh78" in namespace "gc-9587"
Jan 12 01:45:58.713: INFO: Deleting pod "simpletest.rc-7mhk9" in namespace "gc-9587"
Jan 12 01:45:58.738: INFO: Deleting pod "simpletest.rc-872vk" in namespace "gc-9587"
Jan 12 01:45:58.749: INFO: Deleting pod "simpletest.rc-8924f" in namespace "gc-9587"
Jan 12 01:45:58.768: INFO: Deleting pod "simpletest.rc-8cj6n" in namespace "gc-9587"
Jan 12 01:45:58.792: INFO: Deleting pod "simpletest.rc-9ppk4" in namespace "gc-9587"
Jan 12 01:45:58.807: INFO: Deleting pod "simpletest.rc-bfc6b" in namespace "gc-9587"
Jan 12 01:45:58.823: INFO: Deleting pod "simpletest.rc-bhgtp" in namespace "gc-9587"
Jan 12 01:45:58.836: INFO: Deleting pod "simpletest.rc-bjbqd" in namespace "gc-9587"
Jan 12 01:45:58.883: INFO: Deleting pod "simpletest.rc-blvgx" in namespace "gc-9587"
Jan 12 01:45:58.899: INFO: Deleting pod "simpletest.rc-bncwv" in namespace "gc-9587"
Jan 12 01:45:58.916: INFO: Deleting pod "simpletest.rc-bqm4w" in namespace "gc-9587"
Jan 12 01:45:58.930: INFO: Deleting pod "simpletest.rc-cdb9b" in namespace "gc-9587"
Jan 12 01:45:58.939: INFO: Deleting pod "simpletest.rc-ch7vx" in namespace "gc-9587"
Jan 12 01:45:58.965: INFO: Deleting pod "simpletest.rc-cpwmw" in namespace "gc-9587"
Jan 12 01:45:58.983: INFO: Deleting pod "simpletest.rc-crt82" in namespace "gc-9587"
Jan 12 01:45:58.995: INFO: Deleting pod "simpletest.rc-d5n2f" in namespace "gc-9587"
Jan 12 01:45:59.006: INFO: Deleting pod "simpletest.rc-d9r7p" in namespace "gc-9587"
Jan 12 01:45:59.014: INFO: Deleting pod "simpletest.rc-dwjg9" in namespace "gc-9587"
Jan 12 01:45:59.025: INFO: Deleting pod "simpletest.rc-f2pqh" in namespace "gc-9587"
Jan 12 01:45:59.035: INFO: Deleting pod "simpletest.rc-f52tt" in namespace "gc-9587"
Jan 12 01:45:59.043: INFO: Deleting pod "simpletest.rc-fbmj5" in namespace "gc-9587"
Jan 12 01:45:59.059: INFO: Deleting pod "simpletest.rc-fjd6t" in namespace "gc-9587"
Jan 12 01:45:59.081: INFO: Deleting pod "simpletest.rc-fjrxs" in namespace "gc-9587"
Jan 12 01:45:59.104: INFO: Deleting pod "simpletest.rc-fkxcn" in namespace "gc-9587"
Jan 12 01:45:59.121: INFO: Deleting pod "simpletest.rc-fqpng" in namespace "gc-9587"
Jan 12 01:45:59.142: INFO: Deleting pod "simpletest.rc-fxrd7" in namespace "gc-9587"
Jan 12 01:45:59.162: INFO: Deleting pod "simpletest.rc-g4gff" in namespace "gc-9587"
Jan 12 01:45:59.175: INFO: Deleting pod "simpletest.rc-gckbz" in namespace "gc-9587"
Jan 12 01:45:59.186: INFO: Deleting pod "simpletest.rc-gdm9l" in namespace "gc-9587"
Jan 12 01:45:59.201: INFO: Deleting pod "simpletest.rc-gdnz9" in namespace "gc-9587"
Jan 12 01:45:59.212: INFO: Deleting pod "simpletest.rc-glxlf" in namespace "gc-9587"
Jan 12 01:45:59.227: INFO: Deleting pod "simpletest.rc-gm7lc" in namespace "gc-9587"
Jan 12 01:45:59.250: INFO: Deleting pod "simpletest.rc-gwldb" in namespace "gc-9587"
Jan 12 01:45:59.270: INFO: Deleting pod "simpletest.rc-h5zkt" in namespace "gc-9587"
Jan 12 01:45:59.282: INFO: Deleting pod "simpletest.rc-hnlz6" in namespace "gc-9587"
Jan 12 01:45:59.297: INFO: Deleting pod "simpletest.rc-hvlf5" in namespace "gc-9587"
Jan 12 01:45:59.312: INFO: Deleting pod "simpletest.rc-j479h" in namespace "gc-9587"
Jan 12 01:45:59.325: INFO: Deleting pod "simpletest.rc-jbxtg" in namespace "gc-9587"
Jan 12 01:45:59.352: INFO: Deleting pod "simpletest.rc-jg7jf" in namespace "gc-9587"
Jan 12 01:45:59.363: INFO: Deleting pod "simpletest.rc-kkv5w" in namespace "gc-9587"
Jan 12 01:45:59.372: INFO: Deleting pod "simpletest.rc-l2n9l" in namespace "gc-9587"
Jan 12 01:45:59.385: INFO: Deleting pod "simpletest.rc-lbsgk" in namespace "gc-9587"
Jan 12 01:45:59.396: INFO: Deleting pod "simpletest.rc-lc97h" in namespace "gc-9587"
Jan 12 01:45:59.415: INFO: Deleting pod "simpletest.rc-lw26m" in namespace "gc-9587"
Jan 12 01:45:59.426: INFO: Deleting pod "simpletest.rc-m8cbh" in namespace "gc-9587"
Jan 12 01:45:59.441: INFO: Deleting pod "simpletest.rc-mf5fm" in namespace "gc-9587"
Jan 12 01:45:59.473: INFO: Deleting pod "simpletest.rc-mztmt" in namespace "gc-9587"
Jan 12 01:45:59.552: INFO: Deleting pod "simpletest.rc-n9kkn" in namespace "gc-9587"
Jan 12 01:45:59.574: INFO: Deleting pod "simpletest.rc-ndp5n" in namespace "gc-9587"
Jan 12 01:45:59.586: INFO: Deleting pod "simpletest.rc-nsf95" in namespace "gc-9587"
Jan 12 01:45:59.602: INFO: Deleting pod "simpletest.rc-nt7gr" in namespace "gc-9587"
Jan 12 01:45:59.612: INFO: Deleting pod "simpletest.rc-pg8b6" in namespace "gc-9587"
Jan 12 01:45:59.626: INFO: Deleting pod "simpletest.rc-pmmvh" in namespace "gc-9587"
Jan 12 01:45:59.634: INFO: Deleting pod "simpletest.rc-q6tjs" in namespace "gc-9587"
Jan 12 01:45:59.676: INFO: Deleting pod "simpletest.rc-qdh8t" in namespace "gc-9587"
Jan 12 01:45:59.687: INFO: Deleting pod "simpletest.rc-qfmms" in namespace "gc-9587"
Jan 12 01:45:59.708: INFO: Deleting pod "simpletest.rc-qxjrb" in namespace "gc-9587"
Jan 12 01:45:59.721: INFO: Deleting pod "simpletest.rc-rc2zv" in namespace "gc-9587"
Jan 12 01:45:59.734: INFO: Deleting pod "simpletest.rc-rp857" in namespace "gc-9587"
Jan 12 01:45:59.746: INFO: Deleting pod "simpletest.rc-s82vg" in namespace "gc-9587"
Jan 12 01:45:59.788: INFO: Deleting pod "simpletest.rc-sbcvl" in namespace "gc-9587"
Jan 12 01:45:59.835: INFO: Deleting pod "simpletest.rc-sbx8r" in namespace "gc-9587"
Jan 12 01:45:59.888: INFO: Deleting pod "simpletest.rc-sc7wc" in namespace "gc-9587"
Jan 12 01:45:59.941: INFO: Deleting pod "simpletest.rc-slp87" in namespace "gc-9587"
Jan 12 01:45:59.990: INFO: Deleting pod "simpletest.rc-strp8" in namespace "gc-9587"
Jan 12 01:46:00.037: INFO: Deleting pod "simpletest.rc-t5g27" in namespace "gc-9587"
Jan 12 01:46:00.089: INFO: Deleting pod "simpletest.rc-t5l2f" in namespace "gc-9587"
Jan 12 01:46:00.140: INFO: Deleting pod "simpletest.rc-t7mhl" in namespace "gc-9587"
Jan 12 01:46:00.190: INFO: Deleting pod "simpletest.rc-tkx6h" in namespace "gc-9587"
Jan 12 01:46:00.242: INFO: Deleting pod "simpletest.rc-vczjf" in namespace "gc-9587"
Jan 12 01:46:00.288: INFO: Deleting pod "simpletest.rc-vdmkj" in namespace "gc-9587"
Jan 12 01:46:00.336: INFO: Deleting pod "simpletest.rc-vhf25" in namespace "gc-9587"
Jan 12 01:46:00.385: INFO: Deleting pod "simpletest.rc-vmkp8" in namespace "gc-9587"
Jan 12 01:46:00.433: INFO: Deleting pod "simpletest.rc-vrhrn" in namespace "gc-9587"
Jan 12 01:46:00.487: INFO: Deleting pod "simpletest.rc-vxvrw" in namespace "gc-9587"
Jan 12 01:46:00.534: INFO: Deleting pod "simpletest.rc-vzqmm" in namespace "gc-9587"
Jan 12 01:46:00.590: INFO: Deleting pod "simpletest.rc-w2hq9" in namespace "gc-9587"
Jan 12 01:46:00.634: INFO: Deleting pod "simpletest.rc-w6cpb" in namespace "gc-9587"
Jan 12 01:46:00.690: INFO: Deleting pod "simpletest.rc-wbzms" in namespace "gc-9587"
Jan 12 01:46:00.745: INFO: Deleting pod "simpletest.rc-whxhm" in namespace "gc-9587"
Jan 12 01:46:00.788: INFO: Deleting pod "simpletest.rc-wj9tr" in namespace "gc-9587"
Jan 12 01:46:00.838: INFO: Deleting pod "simpletest.rc-wng9t" in namespace "gc-9587"
Jan 12 01:46:00.886: INFO: Deleting pod "simpletest.rc-x9tjw" in namespace "gc-9587"
Jan 12 01:46:00.938: INFO: Deleting pod "simpletest.rc-xsq55" in namespace "gc-9587"
Jan 12 01:46:00.992: INFO: Deleting pod "simpletest.rc-z97mq" in namespace "gc-9587"
Jan 12 01:46:01.039: INFO: Deleting pod "simpletest.rc-zpbkt" in namespace "gc-9587"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 12 01:46:01.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9587" for this suite. 01/12/23 01:46:01.153
------------------------------
• [SLOW TEST] [42.921 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:45:18.332
    Jan 12 01:45:18.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename gc 01/12/23 01:45:18.333
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:45:18.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:45:18.348
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/12/23 01:45:18.353
    STEP: delete the rc 01/12/23 01:45:23.363
    STEP: wait for the rc to be deleted 01/12/23 01:45:23.368
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/12/23 01:45:28.373
    STEP: Gathering metrics 01/12/23 01:45:58.392
    Jan 12 01:45:58.412: INFO: Waiting up to 5m0s for pod "kube-controller-manager-eqx04-flash04" in namespace "kube-system" to be "running and ready"
    Jan 12 01:45:58.414: INFO: Pod "kube-controller-manager-eqx04-flash04": Phase="Running", Reason="", readiness=true. Elapsed: 2.36877ms
    Jan 12 01:45:58.414: INFO: The phase of Pod kube-controller-manager-eqx04-flash04 is Running (Ready = true)
    Jan 12 01:45:58.414: INFO: Pod "kube-controller-manager-eqx04-flash04" satisfied condition "running and ready"
    Jan 12 01:45:58.468: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 12 01:45:58.468: INFO: Deleting pod "simpletest.rc-2sf6b" in namespace "gc-9587"
    Jan 12 01:45:58.477: INFO: Deleting pod "simpletest.rc-2ttmj" in namespace "gc-9587"
    Jan 12 01:45:58.489: INFO: Deleting pod "simpletest.rc-42bcx" in namespace "gc-9587"
    Jan 12 01:45:58.500: INFO: Deleting pod "simpletest.rc-458bc" in namespace "gc-9587"
    Jan 12 01:45:58.527: INFO: Deleting pod "simpletest.rc-4bgxk" in namespace "gc-9587"
    Jan 12 01:45:58.546: INFO: Deleting pod "simpletest.rc-4mprp" in namespace "gc-9587"
    Jan 12 01:45:58.572: INFO: Deleting pod "simpletest.rc-4n2k2" in namespace "gc-9587"
    Jan 12 01:45:58.602: INFO: Deleting pod "simpletest.rc-4v5p6" in namespace "gc-9587"
    Jan 12 01:45:58.614: INFO: Deleting pod "simpletest.rc-5585w" in namespace "gc-9587"
    Jan 12 01:45:58.622: INFO: Deleting pod "simpletest.rc-5c46m" in namespace "gc-9587"
    Jan 12 01:45:58.633: INFO: Deleting pod "simpletest.rc-5cqbb" in namespace "gc-9587"
    Jan 12 01:45:58.660: INFO: Deleting pod "simpletest.rc-68mzd" in namespace "gc-9587"
    Jan 12 01:45:58.669: INFO: Deleting pod "simpletest.rc-6flkx" in namespace "gc-9587"
    Jan 12 01:45:58.680: INFO: Deleting pod "simpletest.rc-6jh78" in namespace "gc-9587"
    Jan 12 01:45:58.713: INFO: Deleting pod "simpletest.rc-7mhk9" in namespace "gc-9587"
    Jan 12 01:45:58.738: INFO: Deleting pod "simpletest.rc-872vk" in namespace "gc-9587"
    Jan 12 01:45:58.749: INFO: Deleting pod "simpletest.rc-8924f" in namespace "gc-9587"
    Jan 12 01:45:58.768: INFO: Deleting pod "simpletest.rc-8cj6n" in namespace "gc-9587"
    Jan 12 01:45:58.792: INFO: Deleting pod "simpletest.rc-9ppk4" in namespace "gc-9587"
    Jan 12 01:45:58.807: INFO: Deleting pod "simpletest.rc-bfc6b" in namespace "gc-9587"
    Jan 12 01:45:58.823: INFO: Deleting pod "simpletest.rc-bhgtp" in namespace "gc-9587"
    Jan 12 01:45:58.836: INFO: Deleting pod "simpletest.rc-bjbqd" in namespace "gc-9587"
    Jan 12 01:45:58.883: INFO: Deleting pod "simpletest.rc-blvgx" in namespace "gc-9587"
    Jan 12 01:45:58.899: INFO: Deleting pod "simpletest.rc-bncwv" in namespace "gc-9587"
    Jan 12 01:45:58.916: INFO: Deleting pod "simpletest.rc-bqm4w" in namespace "gc-9587"
    Jan 12 01:45:58.930: INFO: Deleting pod "simpletest.rc-cdb9b" in namespace "gc-9587"
    Jan 12 01:45:58.939: INFO: Deleting pod "simpletest.rc-ch7vx" in namespace "gc-9587"
    Jan 12 01:45:58.965: INFO: Deleting pod "simpletest.rc-cpwmw" in namespace "gc-9587"
    Jan 12 01:45:58.983: INFO: Deleting pod "simpletest.rc-crt82" in namespace "gc-9587"
    Jan 12 01:45:58.995: INFO: Deleting pod "simpletest.rc-d5n2f" in namespace "gc-9587"
    Jan 12 01:45:59.006: INFO: Deleting pod "simpletest.rc-d9r7p" in namespace "gc-9587"
    Jan 12 01:45:59.014: INFO: Deleting pod "simpletest.rc-dwjg9" in namespace "gc-9587"
    Jan 12 01:45:59.025: INFO: Deleting pod "simpletest.rc-f2pqh" in namespace "gc-9587"
    Jan 12 01:45:59.035: INFO: Deleting pod "simpletest.rc-f52tt" in namespace "gc-9587"
    Jan 12 01:45:59.043: INFO: Deleting pod "simpletest.rc-fbmj5" in namespace "gc-9587"
    Jan 12 01:45:59.059: INFO: Deleting pod "simpletest.rc-fjd6t" in namespace "gc-9587"
    Jan 12 01:45:59.081: INFO: Deleting pod "simpletest.rc-fjrxs" in namespace "gc-9587"
    Jan 12 01:45:59.104: INFO: Deleting pod "simpletest.rc-fkxcn" in namespace "gc-9587"
    Jan 12 01:45:59.121: INFO: Deleting pod "simpletest.rc-fqpng" in namespace "gc-9587"
    Jan 12 01:45:59.142: INFO: Deleting pod "simpletest.rc-fxrd7" in namespace "gc-9587"
    Jan 12 01:45:59.162: INFO: Deleting pod "simpletest.rc-g4gff" in namespace "gc-9587"
    Jan 12 01:45:59.175: INFO: Deleting pod "simpletest.rc-gckbz" in namespace "gc-9587"
    Jan 12 01:45:59.186: INFO: Deleting pod "simpletest.rc-gdm9l" in namespace "gc-9587"
    Jan 12 01:45:59.201: INFO: Deleting pod "simpletest.rc-gdnz9" in namespace "gc-9587"
    Jan 12 01:45:59.212: INFO: Deleting pod "simpletest.rc-glxlf" in namespace "gc-9587"
    Jan 12 01:45:59.227: INFO: Deleting pod "simpletest.rc-gm7lc" in namespace "gc-9587"
    Jan 12 01:45:59.250: INFO: Deleting pod "simpletest.rc-gwldb" in namespace "gc-9587"
    Jan 12 01:45:59.270: INFO: Deleting pod "simpletest.rc-h5zkt" in namespace "gc-9587"
    Jan 12 01:45:59.282: INFO: Deleting pod "simpletest.rc-hnlz6" in namespace "gc-9587"
    Jan 12 01:45:59.297: INFO: Deleting pod "simpletest.rc-hvlf5" in namespace "gc-9587"
    Jan 12 01:45:59.312: INFO: Deleting pod "simpletest.rc-j479h" in namespace "gc-9587"
    Jan 12 01:45:59.325: INFO: Deleting pod "simpletest.rc-jbxtg" in namespace "gc-9587"
    Jan 12 01:45:59.352: INFO: Deleting pod "simpletest.rc-jg7jf" in namespace "gc-9587"
    Jan 12 01:45:59.363: INFO: Deleting pod "simpletest.rc-kkv5w" in namespace "gc-9587"
    Jan 12 01:45:59.372: INFO: Deleting pod "simpletest.rc-l2n9l" in namespace "gc-9587"
    Jan 12 01:45:59.385: INFO: Deleting pod "simpletest.rc-lbsgk" in namespace "gc-9587"
    Jan 12 01:45:59.396: INFO: Deleting pod "simpletest.rc-lc97h" in namespace "gc-9587"
    Jan 12 01:45:59.415: INFO: Deleting pod "simpletest.rc-lw26m" in namespace "gc-9587"
    Jan 12 01:45:59.426: INFO: Deleting pod "simpletest.rc-m8cbh" in namespace "gc-9587"
    Jan 12 01:45:59.441: INFO: Deleting pod "simpletest.rc-mf5fm" in namespace "gc-9587"
    Jan 12 01:45:59.473: INFO: Deleting pod "simpletest.rc-mztmt" in namespace "gc-9587"
    Jan 12 01:45:59.552: INFO: Deleting pod "simpletest.rc-n9kkn" in namespace "gc-9587"
    Jan 12 01:45:59.574: INFO: Deleting pod "simpletest.rc-ndp5n" in namespace "gc-9587"
    Jan 12 01:45:59.586: INFO: Deleting pod "simpletest.rc-nsf95" in namespace "gc-9587"
    Jan 12 01:45:59.602: INFO: Deleting pod "simpletest.rc-nt7gr" in namespace "gc-9587"
    Jan 12 01:45:59.612: INFO: Deleting pod "simpletest.rc-pg8b6" in namespace "gc-9587"
    Jan 12 01:45:59.626: INFO: Deleting pod "simpletest.rc-pmmvh" in namespace "gc-9587"
    Jan 12 01:45:59.634: INFO: Deleting pod "simpletest.rc-q6tjs" in namespace "gc-9587"
    Jan 12 01:45:59.676: INFO: Deleting pod "simpletest.rc-qdh8t" in namespace "gc-9587"
    Jan 12 01:45:59.687: INFO: Deleting pod "simpletest.rc-qfmms" in namespace "gc-9587"
    Jan 12 01:45:59.708: INFO: Deleting pod "simpletest.rc-qxjrb" in namespace "gc-9587"
    Jan 12 01:45:59.721: INFO: Deleting pod "simpletest.rc-rc2zv" in namespace "gc-9587"
    Jan 12 01:45:59.734: INFO: Deleting pod "simpletest.rc-rp857" in namespace "gc-9587"
    Jan 12 01:45:59.746: INFO: Deleting pod "simpletest.rc-s82vg" in namespace "gc-9587"
    Jan 12 01:45:59.788: INFO: Deleting pod "simpletest.rc-sbcvl" in namespace "gc-9587"
    Jan 12 01:45:59.835: INFO: Deleting pod "simpletest.rc-sbx8r" in namespace "gc-9587"
    Jan 12 01:45:59.888: INFO: Deleting pod "simpletest.rc-sc7wc" in namespace "gc-9587"
    Jan 12 01:45:59.941: INFO: Deleting pod "simpletest.rc-slp87" in namespace "gc-9587"
    Jan 12 01:45:59.990: INFO: Deleting pod "simpletest.rc-strp8" in namespace "gc-9587"
    Jan 12 01:46:00.037: INFO: Deleting pod "simpletest.rc-t5g27" in namespace "gc-9587"
    Jan 12 01:46:00.089: INFO: Deleting pod "simpletest.rc-t5l2f" in namespace "gc-9587"
    Jan 12 01:46:00.140: INFO: Deleting pod "simpletest.rc-t7mhl" in namespace "gc-9587"
    Jan 12 01:46:00.190: INFO: Deleting pod "simpletest.rc-tkx6h" in namespace "gc-9587"
    Jan 12 01:46:00.242: INFO: Deleting pod "simpletest.rc-vczjf" in namespace "gc-9587"
    Jan 12 01:46:00.288: INFO: Deleting pod "simpletest.rc-vdmkj" in namespace "gc-9587"
    Jan 12 01:46:00.336: INFO: Deleting pod "simpletest.rc-vhf25" in namespace "gc-9587"
    Jan 12 01:46:00.385: INFO: Deleting pod "simpletest.rc-vmkp8" in namespace "gc-9587"
    Jan 12 01:46:00.433: INFO: Deleting pod "simpletest.rc-vrhrn" in namespace "gc-9587"
    Jan 12 01:46:00.487: INFO: Deleting pod "simpletest.rc-vxvrw" in namespace "gc-9587"
    Jan 12 01:46:00.534: INFO: Deleting pod "simpletest.rc-vzqmm" in namespace "gc-9587"
    Jan 12 01:46:00.590: INFO: Deleting pod "simpletest.rc-w2hq9" in namespace "gc-9587"
    Jan 12 01:46:00.634: INFO: Deleting pod "simpletest.rc-w6cpb" in namespace "gc-9587"
    Jan 12 01:46:00.690: INFO: Deleting pod "simpletest.rc-wbzms" in namespace "gc-9587"
    Jan 12 01:46:00.745: INFO: Deleting pod "simpletest.rc-whxhm" in namespace "gc-9587"
    Jan 12 01:46:00.788: INFO: Deleting pod "simpletest.rc-wj9tr" in namespace "gc-9587"
    Jan 12 01:46:00.838: INFO: Deleting pod "simpletest.rc-wng9t" in namespace "gc-9587"
    Jan 12 01:46:00.886: INFO: Deleting pod "simpletest.rc-x9tjw" in namespace "gc-9587"
    Jan 12 01:46:00.938: INFO: Deleting pod "simpletest.rc-xsq55" in namespace "gc-9587"
    Jan 12 01:46:00.992: INFO: Deleting pod "simpletest.rc-z97mq" in namespace "gc-9587"
    Jan 12 01:46:01.039: INFO: Deleting pod "simpletest.rc-zpbkt" in namespace "gc-9587"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:46:01.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9587" for this suite. 01/12/23 01:46:01.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:46:01.262
Jan 12 01:46:01.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 01:46:01.263
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:01.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:01.283
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jan 12 01:46:01.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/12/23 01:46:03.715
Jan 12 01:46:03.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-6604 --namespace=crd-publish-openapi-6604 create -f -'
Jan 12 01:46:05.241: INFO: stderr: ""
Jan 12 01:46:05.241: INFO: stdout: "e2e-test-crd-publish-openapi-3097-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 12 01:46:05.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-6604 --namespace=crd-publish-openapi-6604 delete e2e-test-crd-publish-openapi-3097-crds test-cr'
Jan 12 01:46:05.489: INFO: stderr: ""
Jan 12 01:46:05.489: INFO: stdout: "e2e-test-crd-publish-openapi-3097-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 12 01:46:05.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-6604 --namespace=crd-publish-openapi-6604 apply -f -'
Jan 12 01:46:07.515: INFO: stderr: ""
Jan 12 01:46:07.515: INFO: stdout: "e2e-test-crd-publish-openapi-3097-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 12 01:46:07.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-6604 --namespace=crd-publish-openapi-6604 delete e2e-test-crd-publish-openapi-3097-crds test-cr'
Jan 12 01:46:07.986: INFO: stderr: ""
Jan 12 01:46:07.986: INFO: stdout: "e2e-test-crd-publish-openapi-3097-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/12/23 01:46:07.986
Jan 12 01:46:07.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-6604 explain e2e-test-crd-publish-openapi-3097-crds'
Jan 12 01:46:09.370: INFO: stderr: ""
Jan 12 01:46:09.370: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3097-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:46:13.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6604" for this suite. 01/12/23 01:46:13.077
------------------------------
• [SLOW TEST] [11.830 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:46:01.262
    Jan 12 01:46:01.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 01:46:01.263
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:01.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:01.283
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jan 12 01:46:01.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/12/23 01:46:03.715
    Jan 12 01:46:03.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-6604 --namespace=crd-publish-openapi-6604 create -f -'
    Jan 12 01:46:05.241: INFO: stderr: ""
    Jan 12 01:46:05.241: INFO: stdout: "e2e-test-crd-publish-openapi-3097-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 12 01:46:05.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-6604 --namespace=crd-publish-openapi-6604 delete e2e-test-crd-publish-openapi-3097-crds test-cr'
    Jan 12 01:46:05.489: INFO: stderr: ""
    Jan 12 01:46:05.489: INFO: stdout: "e2e-test-crd-publish-openapi-3097-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 12 01:46:05.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-6604 --namespace=crd-publish-openapi-6604 apply -f -'
    Jan 12 01:46:07.515: INFO: stderr: ""
    Jan 12 01:46:07.515: INFO: stdout: "e2e-test-crd-publish-openapi-3097-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 12 01:46:07.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-6604 --namespace=crd-publish-openapi-6604 delete e2e-test-crd-publish-openapi-3097-crds test-cr'
    Jan 12 01:46:07.986: INFO: stderr: ""
    Jan 12 01:46:07.986: INFO: stdout: "e2e-test-crd-publish-openapi-3097-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/12/23 01:46:07.986
    Jan 12 01:46:07.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=crd-publish-openapi-6604 explain e2e-test-crd-publish-openapi-3097-crds'
    Jan 12 01:46:09.370: INFO: stderr: ""
    Jan 12 01:46:09.370: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3097-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:46:13.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6604" for this suite. 01/12/23 01:46:13.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:46:13.092
Jan 12 01:46:13.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename events 01/12/23 01:46:13.093
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:13.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:13.106
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/12/23 01:46:13.111
STEP: listing all events in all namespaces 01/12/23 01:46:13.117
STEP: patching the test event 01/12/23 01:46:13.131
STEP: fetching the test event 01/12/23 01:46:13.139
STEP: updating the test event 01/12/23 01:46:13.143
STEP: getting the test event 01/12/23 01:46:13.183
STEP: deleting the test event 01/12/23 01:46:13.19
STEP: listing all events in all namespaces 01/12/23 01:46:13.202
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 12 01:46:13.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9614" for this suite. 01/12/23 01:46:13.235
------------------------------
• [0.162 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:46:13.092
    Jan 12 01:46:13.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename events 01/12/23 01:46:13.093
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:13.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:13.106
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/12/23 01:46:13.111
    STEP: listing all events in all namespaces 01/12/23 01:46:13.117
    STEP: patching the test event 01/12/23 01:46:13.131
    STEP: fetching the test event 01/12/23 01:46:13.139
    STEP: updating the test event 01/12/23 01:46:13.143
    STEP: getting the test event 01/12/23 01:46:13.183
    STEP: deleting the test event 01/12/23 01:46:13.19
    STEP: listing all events in all namespaces 01/12/23 01:46:13.202
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:46:13.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9614" for this suite. 01/12/23 01:46:13.235
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:46:13.264
Jan 12 01:46:13.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename containers 01/12/23 01:46:13.265
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:13.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:13.31
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 01/12/23 01:46:13.316
Jan 12 01:46:13.355: INFO: Waiting up to 5m0s for pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9" in namespace "containers-4315" to be "Succeeded or Failed"
Jan 12 01:46:13.363: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.585284ms
Jan 12 01:46:15.368: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012179015s
Jan 12 01:46:17.368: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011893978s
Jan 12 01:46:19.367: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011050807s
Jan 12 01:46:21.367: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011617887s
Jan 12 01:46:23.367: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011449306s
Jan 12 01:46:25.367: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011480142s
Jan 12 01:46:27.367: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011259522s
Jan 12 01:46:29.366: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010693362s
Jan 12 01:46:31.367: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.01102654s
STEP: Saw pod success 01/12/23 01:46:31.367
Jan 12 01:46:31.367: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9" satisfied condition "Succeeded or Failed"
Jan 12 01:46:31.369: INFO: Trying to get logs from node eqx04-flash06 pod client-containers-96f57767-77e5-4d58-9604-57a21f851de9 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 01:46:31.377
Jan 12 01:46:31.386: INFO: Waiting for pod client-containers-96f57767-77e5-4d58-9604-57a21f851de9 to disappear
Jan 12 01:46:31.388: INFO: Pod client-containers-96f57767-77e5-4d58-9604-57a21f851de9 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 12 01:46:31.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4315" for this suite. 01/12/23 01:46:31.392
------------------------------
• [SLOW TEST] [18.212 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:46:13.264
    Jan 12 01:46:13.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename containers 01/12/23 01:46:13.265
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:13.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:13.31
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 01/12/23 01:46:13.316
    Jan 12 01:46:13.355: INFO: Waiting up to 5m0s for pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9" in namespace "containers-4315" to be "Succeeded or Failed"
    Jan 12 01:46:13.363: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.585284ms
    Jan 12 01:46:15.368: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012179015s
    Jan 12 01:46:17.368: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011893978s
    Jan 12 01:46:19.367: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011050807s
    Jan 12 01:46:21.367: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011617887s
    Jan 12 01:46:23.367: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011449306s
    Jan 12 01:46:25.367: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011480142s
    Jan 12 01:46:27.367: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011259522s
    Jan 12 01:46:29.366: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010693362s
    Jan 12 01:46:31.367: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.01102654s
    STEP: Saw pod success 01/12/23 01:46:31.367
    Jan 12 01:46:31.367: INFO: Pod "client-containers-96f57767-77e5-4d58-9604-57a21f851de9" satisfied condition "Succeeded or Failed"
    Jan 12 01:46:31.369: INFO: Trying to get logs from node eqx04-flash06 pod client-containers-96f57767-77e5-4d58-9604-57a21f851de9 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 01:46:31.377
    Jan 12 01:46:31.386: INFO: Waiting for pod client-containers-96f57767-77e5-4d58-9604-57a21f851de9 to disappear
    Jan 12 01:46:31.388: INFO: Pod client-containers-96f57767-77e5-4d58-9604-57a21f851de9 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:46:31.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4315" for this suite. 01/12/23 01:46:31.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:46:31.478
Jan 12 01:46:31.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename limitrange 01/12/23 01:46:31.479
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:31.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:31.492
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-qwndb" in namespace "limitrange-3593" 01/12/23 01:46:31.494
STEP: Creating another limitRange in another namespace 01/12/23 01:46:31.498
Jan 12 01:46:31.510: INFO: Namespace "e2e-limitrange-qwndb-206" created
Jan 12 01:46:31.510: INFO: Creating LimitRange "e2e-limitrange-qwndb" in namespace "e2e-limitrange-qwndb-206"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-qwndb" 01/12/23 01:46:31.515
Jan 12 01:46:31.518: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-qwndb" in "limitrange-3593" namespace 01/12/23 01:46:31.518
Jan 12 01:46:31.523: INFO: LimitRange "e2e-limitrange-qwndb" has been patched
STEP: Delete LimitRange "e2e-limitrange-qwndb" by Collection with labelSelector: "e2e-limitrange-qwndb=patched" 01/12/23 01:46:31.523
STEP: Confirm that the limitRange "e2e-limitrange-qwndb" has been deleted 01/12/23 01:46:31.529
Jan 12 01:46:31.529: INFO: Requesting list of LimitRange to confirm quantity
Jan 12 01:46:31.531: INFO: Found 0 LimitRange with label "e2e-limitrange-qwndb=patched"
Jan 12 01:46:31.531: INFO: LimitRange "e2e-limitrange-qwndb" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-qwndb" 01/12/23 01:46:31.531
Jan 12 01:46:31.533: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 12 01:46:31.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-3593" for this suite. 01/12/23 01:46:31.537
STEP: Destroying namespace "e2e-limitrange-qwndb-206" for this suite. 01/12/23 01:46:31.601
------------------------------
• [0.213 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:46:31.478
    Jan 12 01:46:31.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename limitrange 01/12/23 01:46:31.479
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:31.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:31.492
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-qwndb" in namespace "limitrange-3593" 01/12/23 01:46:31.494
    STEP: Creating another limitRange in another namespace 01/12/23 01:46:31.498
    Jan 12 01:46:31.510: INFO: Namespace "e2e-limitrange-qwndb-206" created
    Jan 12 01:46:31.510: INFO: Creating LimitRange "e2e-limitrange-qwndb" in namespace "e2e-limitrange-qwndb-206"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-qwndb" 01/12/23 01:46:31.515
    Jan 12 01:46:31.518: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-qwndb" in "limitrange-3593" namespace 01/12/23 01:46:31.518
    Jan 12 01:46:31.523: INFO: LimitRange "e2e-limitrange-qwndb" has been patched
    STEP: Delete LimitRange "e2e-limitrange-qwndb" by Collection with labelSelector: "e2e-limitrange-qwndb=patched" 01/12/23 01:46:31.523
    STEP: Confirm that the limitRange "e2e-limitrange-qwndb" has been deleted 01/12/23 01:46:31.529
    Jan 12 01:46:31.529: INFO: Requesting list of LimitRange to confirm quantity
    Jan 12 01:46:31.531: INFO: Found 0 LimitRange with label "e2e-limitrange-qwndb=patched"
    Jan 12 01:46:31.531: INFO: LimitRange "e2e-limitrange-qwndb" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-qwndb" 01/12/23 01:46:31.531
    Jan 12 01:46:31.533: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:46:31.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-3593" for this suite. 01/12/23 01:46:31.537
    STEP: Destroying namespace "e2e-limitrange-qwndb-206" for this suite. 01/12/23 01:46:31.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:46:31.693
Jan 12 01:46:31.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename ingressclass 01/12/23 01:46:31.693
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:31.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:31.706
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/12/23 01:46:31.708
STEP: getting /apis/networking.k8s.io 01/12/23 01:46:31.71
STEP: getting /apis/networking.k8s.iov1 01/12/23 01:46:31.711
STEP: creating 01/12/23 01:46:31.712
STEP: getting 01/12/23 01:46:31.724
STEP: listing 01/12/23 01:46:31.727
STEP: watching 01/12/23 01:46:31.729
Jan 12 01:46:31.729: INFO: starting watch
STEP: patching 01/12/23 01:46:31.73
STEP: updating 01/12/23 01:46:31.734
Jan 12 01:46:31.738: INFO: waiting for watch events with expected annotations
Jan 12 01:46:31.738: INFO: saw patched and updated annotations
STEP: deleting 01/12/23 01:46:31.738
STEP: deleting a collection 01/12/23 01:46:31.747
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jan 12 01:46:31.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-9510" for this suite. 01/12/23 01:46:31.763
------------------------------
• [0.141 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:46:31.693
    Jan 12 01:46:31.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename ingressclass 01/12/23 01:46:31.693
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:31.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:31.706
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/12/23 01:46:31.708
    STEP: getting /apis/networking.k8s.io 01/12/23 01:46:31.71
    STEP: getting /apis/networking.k8s.iov1 01/12/23 01:46:31.711
    STEP: creating 01/12/23 01:46:31.712
    STEP: getting 01/12/23 01:46:31.724
    STEP: listing 01/12/23 01:46:31.727
    STEP: watching 01/12/23 01:46:31.729
    Jan 12 01:46:31.729: INFO: starting watch
    STEP: patching 01/12/23 01:46:31.73
    STEP: updating 01/12/23 01:46:31.734
    Jan 12 01:46:31.738: INFO: waiting for watch events with expected annotations
    Jan 12 01:46:31.738: INFO: saw patched and updated annotations
    STEP: deleting 01/12/23 01:46:31.738
    STEP: deleting a collection 01/12/23 01:46:31.747
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:46:31.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-9510" for this suite. 01/12/23 01:46:31.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:46:31.838
Jan 12 01:46:31.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename secrets 01/12/23 01:46:31.839
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:31.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:31.85
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-0756f6e2-9b58-4e90-923e-440e0df42747 01/12/23 01:46:31.852
STEP: Creating a pod to test consume secrets 01/12/23 01:46:31.855
Jan 12 01:46:32.031: INFO: Waiting up to 5m0s for pod "pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d" in namespace "secrets-6608" to be "Succeeded or Failed"
Jan 12 01:46:32.034: INFO: Pod "pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.384772ms
Jan 12 01:46:34.038: INFO: Pod "pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00672223s
Jan 12 01:46:36.039: INFO: Pod "pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007411561s
Jan 12 01:46:38.038: INFO: Pod "pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006725924s
STEP: Saw pod success 01/12/23 01:46:38.038
Jan 12 01:46:38.038: INFO: Pod "pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d" satisfied condition "Succeeded or Failed"
Jan 12 01:46:38.040: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 01:46:38.048
Jan 12 01:46:38.061: INFO: Waiting for pod pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d to disappear
Jan 12 01:46:38.063: INFO: Pod pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 01:46:38.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6608" for this suite. 01/12/23 01:46:38.067
------------------------------
• [SLOW TEST] [6.251 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:46:31.838
    Jan 12 01:46:31.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename secrets 01/12/23 01:46:31.839
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:31.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:31.85
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-0756f6e2-9b58-4e90-923e-440e0df42747 01/12/23 01:46:31.852
    STEP: Creating a pod to test consume secrets 01/12/23 01:46:31.855
    Jan 12 01:46:32.031: INFO: Waiting up to 5m0s for pod "pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d" in namespace "secrets-6608" to be "Succeeded or Failed"
    Jan 12 01:46:32.034: INFO: Pod "pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.384772ms
    Jan 12 01:46:34.038: INFO: Pod "pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00672223s
    Jan 12 01:46:36.039: INFO: Pod "pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007411561s
    Jan 12 01:46:38.038: INFO: Pod "pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006725924s
    STEP: Saw pod success 01/12/23 01:46:38.038
    Jan 12 01:46:38.038: INFO: Pod "pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d" satisfied condition "Succeeded or Failed"
    Jan 12 01:46:38.040: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 01:46:38.048
    Jan 12 01:46:38.061: INFO: Waiting for pod pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d to disappear
    Jan 12 01:46:38.063: INFO: Pod pod-secrets-8d69130f-f91b-428d-bd12-245b704c429d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:46:38.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6608" for this suite. 01/12/23 01:46:38.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:46:38.092
Jan 12 01:46:38.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 01:46:38.093
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:38.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:38.107
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-622f56fa-8393-40f9-aad9-a067a896b0a7 01/12/23 01:46:38.109
STEP: Creating a pod to test consume configMaps 01/12/23 01:46:38.113
Jan 12 01:46:38.187: INFO: Waiting up to 5m0s for pod "pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8" in namespace "configmap-3133" to be "Succeeded or Failed"
Jan 12 01:46:38.189: INFO: Pod "pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.5271ms
Jan 12 01:46:40.193: INFO: Pod "pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0061432s
Jan 12 01:46:42.194: INFO: Pod "pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006818384s
Jan 12 01:46:44.193: INFO: Pod "pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006247739s
STEP: Saw pod success 01/12/23 01:46:44.193
Jan 12 01:46:44.193: INFO: Pod "pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8" satisfied condition "Succeeded or Failed"
Jan 12 01:46:44.195: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 01:46:44.203
Jan 12 01:46:44.215: INFO: Waiting for pod pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8 to disappear
Jan 12 01:46:44.217: INFO: Pod pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:46:44.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3133" for this suite. 01/12/23 01:46:44.221
------------------------------
• [SLOW TEST] [6.150 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:46:38.092
    Jan 12 01:46:38.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 01:46:38.093
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:38.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:38.107
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-622f56fa-8393-40f9-aad9-a067a896b0a7 01/12/23 01:46:38.109
    STEP: Creating a pod to test consume configMaps 01/12/23 01:46:38.113
    Jan 12 01:46:38.187: INFO: Waiting up to 5m0s for pod "pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8" in namespace "configmap-3133" to be "Succeeded or Failed"
    Jan 12 01:46:38.189: INFO: Pod "pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.5271ms
    Jan 12 01:46:40.193: INFO: Pod "pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0061432s
    Jan 12 01:46:42.194: INFO: Pod "pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006818384s
    Jan 12 01:46:44.193: INFO: Pod "pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006247739s
    STEP: Saw pod success 01/12/23 01:46:44.193
    Jan 12 01:46:44.193: INFO: Pod "pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8" satisfied condition "Succeeded or Failed"
    Jan 12 01:46:44.195: INFO: Trying to get logs from node eqx04-flash06 pod pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 01:46:44.203
    Jan 12 01:46:44.215: INFO: Waiting for pod pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8 to disappear
    Jan 12 01:46:44.217: INFO: Pod pod-configmaps-934a9b00-eeb3-4536-9cf7-ba9cb7a363c8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:46:44.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3133" for this suite. 01/12/23 01:46:44.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:46:44.244
Jan 12 01:46:44.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 01:46:44.244
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:44.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:44.259
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 01:46:44.271
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:46:44.808
STEP: Deploying the webhook pod 01/12/23 01:46:44.816
STEP: Wait for the deployment to be ready 01/12/23 01:46:44.851
Jan 12 01:46:44.858: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 12 01:46:46.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 46, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 46, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 46, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 46, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 01:46:48.87
STEP: Verifying the service has paired with the endpoint 01/12/23 01:46:48.881
Jan 12 01:46:49.882: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jan 12 01:46:49.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/12/23 01:46:50.393
STEP: Creating a custom resource that should be denied by the webhook 01/12/23 01:46:50.414
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/12/23 01:46:52.458
STEP: Updating the custom resource with disallowed data should be denied 01/12/23 01:46:52.464
STEP: Deleting the custom resource should be denied 01/12/23 01:46:52.471
STEP: Remove the offending key and value from the custom resource data 01/12/23 01:46:52.476
STEP: Deleting the updated custom resource should be successful 01/12/23 01:46:52.484
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:46:53.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8776" for this suite. 01/12/23 01:46:53.042
STEP: Destroying namespace "webhook-8776-markers" for this suite. 01/12/23 01:46:53.083
------------------------------
• [SLOW TEST] [8.874 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:46:44.244
    Jan 12 01:46:44.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 01:46:44.244
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:44.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:44.259
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 01:46:44.271
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:46:44.808
    STEP: Deploying the webhook pod 01/12/23 01:46:44.816
    STEP: Wait for the deployment to be ready 01/12/23 01:46:44.851
    Jan 12 01:46:44.858: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 12 01:46:46.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 46, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 46, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 46, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 46, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 01:46:48.87
    STEP: Verifying the service has paired with the endpoint 01/12/23 01:46:48.881
    Jan 12 01:46:49.882: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jan 12 01:46:49.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/12/23 01:46:50.393
    STEP: Creating a custom resource that should be denied by the webhook 01/12/23 01:46:50.414
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/12/23 01:46:52.458
    STEP: Updating the custom resource with disallowed data should be denied 01/12/23 01:46:52.464
    STEP: Deleting the custom resource should be denied 01/12/23 01:46:52.471
    STEP: Remove the offending key and value from the custom resource data 01/12/23 01:46:52.476
    STEP: Deleting the updated custom resource should be successful 01/12/23 01:46:52.484
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:46:53.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8776" for this suite. 01/12/23 01:46:53.042
    STEP: Destroying namespace "webhook-8776-markers" for this suite. 01/12/23 01:46:53.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:46:53.119
Jan 12 01:46:53.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename dns 01/12/23 01:46:53.12
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:53.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:53.134
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/12/23 01:46:53.139
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/12/23 01:46:53.139
STEP: creating a pod to probe DNS 01/12/23 01:46:53.139
STEP: submitting the pod to kubernetes 01/12/23 01:46:53.139
Jan 12 01:46:53.217: INFO: Waiting up to 15m0s for pod "dns-test-e7f17c42-9681-43f9-86ee-00ff95bdca09" in namespace "dns-7989" to be "running"
Jan 12 01:46:53.220: INFO: Pod "dns-test-e7f17c42-9681-43f9-86ee-00ff95bdca09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.570624ms
Jan 12 01:46:55.224: INFO: Pod "dns-test-e7f17c42-9681-43f9-86ee-00ff95bdca09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006943438s
Jan 12 01:46:57.224: INFO: Pod "dns-test-e7f17c42-9681-43f9-86ee-00ff95bdca09": Phase="Running", Reason="", readiness=true. Elapsed: 4.006832033s
Jan 12 01:46:57.224: INFO: Pod "dns-test-e7f17c42-9681-43f9-86ee-00ff95bdca09" satisfied condition "running"
STEP: retrieving the pod 01/12/23 01:46:57.224
STEP: looking for the results for each expected name from probers 01/12/23 01:46:57.227
Jan 12 01:46:57.237: INFO: DNS probes using dns-7989/dns-test-e7f17c42-9681-43f9-86ee-00ff95bdca09 succeeded

STEP: deleting the pod 01/12/23 01:46:57.237
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 01:46:57.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7989" for this suite. 01/12/23 01:46:57.256
------------------------------
• [4.168 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:46:53.119
    Jan 12 01:46:53.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename dns 01/12/23 01:46:53.12
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:53.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:53.134
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/12/23 01:46:53.139
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/12/23 01:46:53.139
    STEP: creating a pod to probe DNS 01/12/23 01:46:53.139
    STEP: submitting the pod to kubernetes 01/12/23 01:46:53.139
    Jan 12 01:46:53.217: INFO: Waiting up to 15m0s for pod "dns-test-e7f17c42-9681-43f9-86ee-00ff95bdca09" in namespace "dns-7989" to be "running"
    Jan 12 01:46:53.220: INFO: Pod "dns-test-e7f17c42-9681-43f9-86ee-00ff95bdca09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.570624ms
    Jan 12 01:46:55.224: INFO: Pod "dns-test-e7f17c42-9681-43f9-86ee-00ff95bdca09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006943438s
    Jan 12 01:46:57.224: INFO: Pod "dns-test-e7f17c42-9681-43f9-86ee-00ff95bdca09": Phase="Running", Reason="", readiness=true. Elapsed: 4.006832033s
    Jan 12 01:46:57.224: INFO: Pod "dns-test-e7f17c42-9681-43f9-86ee-00ff95bdca09" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 01:46:57.224
    STEP: looking for the results for each expected name from probers 01/12/23 01:46:57.227
    Jan 12 01:46:57.237: INFO: DNS probes using dns-7989/dns-test-e7f17c42-9681-43f9-86ee-00ff95bdca09 succeeded

    STEP: deleting the pod 01/12/23 01:46:57.237
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:46:57.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7989" for this suite. 01/12/23 01:46:57.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:46:57.288
Jan 12 01:46:57.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename replicaset 01/12/23 01:46:57.289
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:57.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:57.302
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/12/23 01:46:57.311
STEP: Verify that the required pods have come up. 01/12/23 01:46:57.317
Jan 12 01:46:57.319: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 12 01:47:02.326: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/12/23 01:47:02.326
STEP: Getting /status 01/12/23 01:47:02.326
Jan 12 01:47:02.329: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/12/23 01:47:02.329
Jan 12 01:47:02.336: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/12/23 01:47:02.336
Jan 12 01:47:02.337: INFO: Observed &ReplicaSet event: ADDED
Jan 12 01:47:02.337: INFO: Observed &ReplicaSet event: MODIFIED
Jan 12 01:47:02.338: INFO: Observed &ReplicaSet event: MODIFIED
Jan 12 01:47:02.338: INFO: Observed &ReplicaSet event: MODIFIED
Jan 12 01:47:02.338: INFO: Found replicaset test-rs in namespace replicaset-5431 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 12 01:47:02.338: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/12/23 01:47:02.338
Jan 12 01:47:02.338: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 12 01:47:02.344: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/12/23 01:47:02.344
Jan 12 01:47:02.346: INFO: Observed &ReplicaSet event: ADDED
Jan 12 01:47:02.346: INFO: Observed &ReplicaSet event: MODIFIED
Jan 12 01:47:02.346: INFO: Observed &ReplicaSet event: MODIFIED
Jan 12 01:47:02.346: INFO: Observed &ReplicaSet event: MODIFIED
Jan 12 01:47:02.346: INFO: Observed replicaset test-rs in namespace replicaset-5431 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 12 01:47:02.347: INFO: Observed &ReplicaSet event: MODIFIED
Jan 12 01:47:02.347: INFO: Found replicaset test-rs in namespace replicaset-5431 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 12 01:47:02.347: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 12 01:47:02.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5431" for this suite. 01/12/23 01:47:02.35
------------------------------
• [SLOW TEST] [5.082 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:46:57.288
    Jan 12 01:46:57.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename replicaset 01/12/23 01:46:57.289
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:46:57.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:46:57.302
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/12/23 01:46:57.311
    STEP: Verify that the required pods have come up. 01/12/23 01:46:57.317
    Jan 12 01:46:57.319: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 12 01:47:02.326: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/12/23 01:47:02.326
    STEP: Getting /status 01/12/23 01:47:02.326
    Jan 12 01:47:02.329: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/12/23 01:47:02.329
    Jan 12 01:47:02.336: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/12/23 01:47:02.336
    Jan 12 01:47:02.337: INFO: Observed &ReplicaSet event: ADDED
    Jan 12 01:47:02.337: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 12 01:47:02.338: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 12 01:47:02.338: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 12 01:47:02.338: INFO: Found replicaset test-rs in namespace replicaset-5431 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 12 01:47:02.338: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/12/23 01:47:02.338
    Jan 12 01:47:02.338: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 12 01:47:02.344: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/12/23 01:47:02.344
    Jan 12 01:47:02.346: INFO: Observed &ReplicaSet event: ADDED
    Jan 12 01:47:02.346: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 12 01:47:02.346: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 12 01:47:02.346: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 12 01:47:02.346: INFO: Observed replicaset test-rs in namespace replicaset-5431 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 12 01:47:02.347: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 12 01:47:02.347: INFO: Found replicaset test-rs in namespace replicaset-5431 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 12 01:47:02.347: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:47:02.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5431" for this suite. 01/12/23 01:47:02.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:47:02.372
Jan 12 01:47:02.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename job 01/12/23 01:47:02.373
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:47:02.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:47:02.387
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 01/12/23 01:47:02.389
STEP: Ensuring job reaches completions 01/12/23 01:47:02.393
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 12 01:47:16.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-609" for this suite. 01/12/23 01:47:16.403
------------------------------
• [SLOW TEST] [14.043 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:47:02.372
    Jan 12 01:47:02.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename job 01/12/23 01:47:02.373
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:47:02.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:47:02.387
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 01/12/23 01:47:02.389
    STEP: Ensuring job reaches completions 01/12/23 01:47:02.393
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:47:16.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-609" for this suite. 01/12/23 01:47:16.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:47:16.417
Jan 12 01:47:16.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 01:47:16.418
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:47:16.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:47:16.435
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 01:47:16.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7228" for this suite. 01/12/23 01:47:16.444
------------------------------
• [0.060 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:47:16.417
    Jan 12 01:47:16.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 01:47:16.418
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:47:16.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:47:16.435
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:47:16.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7228" for this suite. 01/12/23 01:47:16.444
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:47:16.478
Jan 12 01:47:16.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir-wrapper 01/12/23 01:47:16.478
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:47:16.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:47:16.491
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/12/23 01:47:16.494
STEP: Creating RC which spawns configmap-volume pods 01/12/23 01:47:16.733
Jan 12 01:47:16.833: INFO: Pod name wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c: Found 1 pods out of 5
Jan 12 01:47:21.846: INFO: Pod name wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/12/23 01:47:21.846
Jan 12 01:47:21.846: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:47:21.856: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk": Phase="Pending", Reason="", readiness=false. Elapsed: 10.519194ms
Jan 12 01:47:23.860: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014069265s
Jan 12 01:47:25.862: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016043065s
Jan 12 01:47:27.860: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013946139s
Jan 12 01:47:29.861: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015286746s
Jan 12 01:47:31.860: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk": Phase="Running", Reason="", readiness=true. Elapsed: 10.014514232s
Jan 12 01:47:31.860: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk" satisfied condition "running"
Jan 12 01:47:31.860: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-gl7nz" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:47:31.863: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-gl7nz": Phase="Running", Reason="", readiness=true. Elapsed: 2.622348ms
Jan 12 01:47:31.863: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-gl7nz" satisfied condition "running"
Jan 12 01:47:31.863: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-k46nk" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:47:31.867: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-k46nk": Phase="Running", Reason="", readiness=true. Elapsed: 3.517626ms
Jan 12 01:47:31.867: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-k46nk" satisfied condition "running"
Jan 12 01:47:31.867: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-mf7jt" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:47:31.869: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-mf7jt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.423743ms
Jan 12 01:47:33.873: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-mf7jt": Phase="Running", Reason="", readiness=true. Elapsed: 2.006425728s
Jan 12 01:47:33.873: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-mf7jt" satisfied condition "running"
Jan 12 01:47:33.873: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-nvh4s" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:47:33.876: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-nvh4s": Phase="Running", Reason="", readiness=true. Elapsed: 2.625373ms
Jan 12 01:47:33.876: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-nvh4s" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c in namespace emptydir-wrapper-4515, will wait for the garbage collector to delete the pods 01/12/23 01:47:33.876
Jan 12 01:47:33.936: INFO: Deleting ReplicationController wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c took: 6.503621ms
Jan 12 01:47:34.037: INFO: Terminating ReplicationController wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c pods took: 101.07507ms
STEP: Creating RC which spawns configmap-volume pods 01/12/23 01:47:38.544
Jan 12 01:47:38.562: INFO: Pod name wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905: Found 0 pods out of 5
Jan 12 01:47:43.569: INFO: Pod name wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/12/23 01:47:43.569
Jan 12 01:47:43.569: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:47:43.571: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.56188ms
Jan 12 01:47:45.576: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007007089s
Jan 12 01:47:47.575: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006244202s
Jan 12 01:47:49.580: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01099247s
Jan 12 01:47:51.577: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008595626s
Jan 12 01:47:53.575: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb": Phase="Running", Reason="", readiness=true. Elapsed: 10.006627924s
Jan 12 01:47:53.575: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb" satisfied condition "running"
Jan 12 01:47:53.575: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-bkt9f" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:47:53.578: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-bkt9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.552693ms
Jan 12 01:47:53.578: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-bkt9f" satisfied condition "running"
Jan 12 01:47:53.578: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-p7g2r" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:47:53.581: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-p7g2r": Phase="Running", Reason="", readiness=true. Elapsed: 2.600073ms
Jan 12 01:47:53.581: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-p7g2r" satisfied condition "running"
Jan 12 01:47:53.581: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-v22c2" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:47:53.583: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-v22c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.52419ms
Jan 12 01:47:53.583: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-v22c2" satisfied condition "running"
Jan 12 01:47:53.583: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-zbhm8" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:47:53.586: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-zbhm8": Phase="Running", Reason="", readiness=true. Elapsed: 2.467142ms
Jan 12 01:47:53.586: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-zbhm8" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905 in namespace emptydir-wrapper-4515, will wait for the garbage collector to delete the pods 01/12/23 01:47:53.586
Jan 12 01:47:53.648: INFO: Deleting ReplicationController wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905 took: 8.235168ms
Jan 12 01:47:53.849: INFO: Terminating ReplicationController wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905 pods took: 200.871251ms
STEP: Creating RC which spawns configmap-volume pods 01/12/23 01:47:59.353
Jan 12 01:47:59.368: INFO: Pod name wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf: Found 0 pods out of 5
Jan 12 01:48:04.375: INFO: Pod name wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/12/23 01:48:04.375
Jan 12 01:48:04.375: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:48:04.377: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.617177ms
Jan 12 01:48:06.381: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006625178s
Jan 12 01:48:08.381: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006248959s
Jan 12 01:48:10.381: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006487923s
Jan 12 01:48:12.382: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007554899s
Jan 12 01:48:14.382: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006932498s
Jan 12 01:48:16.383: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n": Phase="Running", Reason="", readiness=true. Elapsed: 12.007813355s
Jan 12 01:48:16.383: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n" satisfied condition "running"
Jan 12 01:48:16.383: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-fqdr4" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:48:16.386: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-fqdr4": Phase="Running", Reason="", readiness=true. Elapsed: 2.869225ms
Jan 12 01:48:16.386: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-fqdr4" satisfied condition "running"
Jan 12 01:48:16.386: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-rm884" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:48:16.388: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-rm884": Phase="Running", Reason="", readiness=true. Elapsed: 2.704483ms
Jan 12 01:48:16.388: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-rm884" satisfied condition "running"
Jan 12 01:48:16.388: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-s6j4v" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:48:16.391: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-s6j4v": Phase="Running", Reason="", readiness=true. Elapsed: 2.883363ms
Jan 12 01:48:16.391: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-s6j4v" satisfied condition "running"
Jan 12 01:48:16.391: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-zpqhx" in namespace "emptydir-wrapper-4515" to be "running"
Jan 12 01:48:16.394: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-zpqhx": Phase="Running", Reason="", readiness=true. Elapsed: 2.786218ms
Jan 12 01:48:16.394: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-zpqhx" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf in namespace emptydir-wrapper-4515, will wait for the garbage collector to delete the pods 01/12/23 01:48:16.394
Jan 12 01:48:16.455: INFO: Deleting ReplicationController wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf took: 7.033889ms
Jan 12 01:48:16.555: INFO: Terminating ReplicationController wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf pods took: 100.401337ms
STEP: Cleaning up the configMaps 01/12/23 01:48:20.657
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:48:20.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-4515" for this suite. 01/12/23 01:48:20.958
------------------------------
• [SLOW TEST] [64.498 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:47:16.478
    Jan 12 01:47:16.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir-wrapper 01/12/23 01:47:16.478
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:47:16.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:47:16.491
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/12/23 01:47:16.494
    STEP: Creating RC which spawns configmap-volume pods 01/12/23 01:47:16.733
    Jan 12 01:47:16.833: INFO: Pod name wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c: Found 1 pods out of 5
    Jan 12 01:47:21.846: INFO: Pod name wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/12/23 01:47:21.846
    Jan 12 01:47:21.846: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:47:21.856: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk": Phase="Pending", Reason="", readiness=false. Elapsed: 10.519194ms
    Jan 12 01:47:23.860: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014069265s
    Jan 12 01:47:25.862: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016043065s
    Jan 12 01:47:27.860: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013946139s
    Jan 12 01:47:29.861: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015286746s
    Jan 12 01:47:31.860: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk": Phase="Running", Reason="", readiness=true. Elapsed: 10.014514232s
    Jan 12 01:47:31.860: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-7n9kk" satisfied condition "running"
    Jan 12 01:47:31.860: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-gl7nz" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:47:31.863: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-gl7nz": Phase="Running", Reason="", readiness=true. Elapsed: 2.622348ms
    Jan 12 01:47:31.863: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-gl7nz" satisfied condition "running"
    Jan 12 01:47:31.863: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-k46nk" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:47:31.867: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-k46nk": Phase="Running", Reason="", readiness=true. Elapsed: 3.517626ms
    Jan 12 01:47:31.867: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-k46nk" satisfied condition "running"
    Jan 12 01:47:31.867: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-mf7jt" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:47:31.869: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-mf7jt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.423743ms
    Jan 12 01:47:33.873: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-mf7jt": Phase="Running", Reason="", readiness=true. Elapsed: 2.006425728s
    Jan 12 01:47:33.873: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-mf7jt" satisfied condition "running"
    Jan 12 01:47:33.873: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-nvh4s" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:47:33.876: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-nvh4s": Phase="Running", Reason="", readiness=true. Elapsed: 2.625373ms
    Jan 12 01:47:33.876: INFO: Pod "wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c-nvh4s" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c in namespace emptydir-wrapper-4515, will wait for the garbage collector to delete the pods 01/12/23 01:47:33.876
    Jan 12 01:47:33.936: INFO: Deleting ReplicationController wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c took: 6.503621ms
    Jan 12 01:47:34.037: INFO: Terminating ReplicationController wrapped-volume-race-5188be42-e463-4173-b641-9d2230b28b2c pods took: 101.07507ms
    STEP: Creating RC which spawns configmap-volume pods 01/12/23 01:47:38.544
    Jan 12 01:47:38.562: INFO: Pod name wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905: Found 0 pods out of 5
    Jan 12 01:47:43.569: INFO: Pod name wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/12/23 01:47:43.569
    Jan 12 01:47:43.569: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:47:43.571: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.56188ms
    Jan 12 01:47:45.576: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007007089s
    Jan 12 01:47:47.575: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006244202s
    Jan 12 01:47:49.580: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01099247s
    Jan 12 01:47:51.577: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008595626s
    Jan 12 01:47:53.575: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb": Phase="Running", Reason="", readiness=true. Elapsed: 10.006627924s
    Jan 12 01:47:53.575: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-8w7bb" satisfied condition "running"
    Jan 12 01:47:53.575: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-bkt9f" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:47:53.578: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-bkt9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.552693ms
    Jan 12 01:47:53.578: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-bkt9f" satisfied condition "running"
    Jan 12 01:47:53.578: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-p7g2r" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:47:53.581: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-p7g2r": Phase="Running", Reason="", readiness=true. Elapsed: 2.600073ms
    Jan 12 01:47:53.581: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-p7g2r" satisfied condition "running"
    Jan 12 01:47:53.581: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-v22c2" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:47:53.583: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-v22c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.52419ms
    Jan 12 01:47:53.583: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-v22c2" satisfied condition "running"
    Jan 12 01:47:53.583: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-zbhm8" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:47:53.586: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-zbhm8": Phase="Running", Reason="", readiness=true. Elapsed: 2.467142ms
    Jan 12 01:47:53.586: INFO: Pod "wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905-zbhm8" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905 in namespace emptydir-wrapper-4515, will wait for the garbage collector to delete the pods 01/12/23 01:47:53.586
    Jan 12 01:47:53.648: INFO: Deleting ReplicationController wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905 took: 8.235168ms
    Jan 12 01:47:53.849: INFO: Terminating ReplicationController wrapped-volume-race-808f853a-27b2-4f72-a54f-17d5a96ac905 pods took: 200.871251ms
    STEP: Creating RC which spawns configmap-volume pods 01/12/23 01:47:59.353
    Jan 12 01:47:59.368: INFO: Pod name wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf: Found 0 pods out of 5
    Jan 12 01:48:04.375: INFO: Pod name wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/12/23 01:48:04.375
    Jan 12 01:48:04.375: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:48:04.377: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.617177ms
    Jan 12 01:48:06.381: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006625178s
    Jan 12 01:48:08.381: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006248959s
    Jan 12 01:48:10.381: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006487923s
    Jan 12 01:48:12.382: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007554899s
    Jan 12 01:48:14.382: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006932498s
    Jan 12 01:48:16.383: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n": Phase="Running", Reason="", readiness=true. Elapsed: 12.007813355s
    Jan 12 01:48:16.383: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-9q64n" satisfied condition "running"
    Jan 12 01:48:16.383: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-fqdr4" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:48:16.386: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-fqdr4": Phase="Running", Reason="", readiness=true. Elapsed: 2.869225ms
    Jan 12 01:48:16.386: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-fqdr4" satisfied condition "running"
    Jan 12 01:48:16.386: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-rm884" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:48:16.388: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-rm884": Phase="Running", Reason="", readiness=true. Elapsed: 2.704483ms
    Jan 12 01:48:16.388: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-rm884" satisfied condition "running"
    Jan 12 01:48:16.388: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-s6j4v" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:48:16.391: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-s6j4v": Phase="Running", Reason="", readiness=true. Elapsed: 2.883363ms
    Jan 12 01:48:16.391: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-s6j4v" satisfied condition "running"
    Jan 12 01:48:16.391: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-zpqhx" in namespace "emptydir-wrapper-4515" to be "running"
    Jan 12 01:48:16.394: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-zpqhx": Phase="Running", Reason="", readiness=true. Elapsed: 2.786218ms
    Jan 12 01:48:16.394: INFO: Pod "wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf-zpqhx" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf in namespace emptydir-wrapper-4515, will wait for the garbage collector to delete the pods 01/12/23 01:48:16.394
    Jan 12 01:48:16.455: INFO: Deleting ReplicationController wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf took: 7.033889ms
    Jan 12 01:48:16.555: INFO: Terminating ReplicationController wrapped-volume-race-f23165ac-9202-476f-a577-f7cdb7a7c7bf pods took: 100.401337ms
    STEP: Cleaning up the configMaps 01/12/23 01:48:20.657
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:48:20.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-4515" for this suite. 01/12/23 01:48:20.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:48:20.978
Jan 12 01:48:20.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename disruption 01/12/23 01:48:20.979
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:20.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:20.994
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:48:20.997
Jan 12 01:48:20.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename disruption-2 01/12/23 01:48:20.998
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:21.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:21.012
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 01/12/23 01:48:21.023
STEP: Waiting for the pdb to be processed 01/12/23 01:48:23.037
STEP: Waiting for the pdb to be processed 01/12/23 01:48:25.055
STEP: listing a collection of PDBs across all namespaces 01/12/23 01:48:27.06
STEP: listing a collection of PDBs in namespace disruption-2286 01/12/23 01:48:27.062
STEP: deleting a collection of PDBs 01/12/23 01:48:27.064
STEP: Waiting for the PDB collection to be deleted 01/12/23 01:48:27.074
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jan 12 01:48:27.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 12 01:48:27.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-8333" for this suite. 01/12/23 01:48:27.083
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2286" for this suite. 01/12/23 01:48:27.097
------------------------------
• [SLOW TEST] [6.135 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:48:20.978
    Jan 12 01:48:20.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename disruption 01/12/23 01:48:20.979
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:20.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:20.994
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:48:20.997
    Jan 12 01:48:20.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename disruption-2 01/12/23 01:48:20.998
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:21.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:21.012
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 01/12/23 01:48:21.023
    STEP: Waiting for the pdb to be processed 01/12/23 01:48:23.037
    STEP: Waiting for the pdb to be processed 01/12/23 01:48:25.055
    STEP: listing a collection of PDBs across all namespaces 01/12/23 01:48:27.06
    STEP: listing a collection of PDBs in namespace disruption-2286 01/12/23 01:48:27.062
    STEP: deleting a collection of PDBs 01/12/23 01:48:27.064
    STEP: Waiting for the PDB collection to be deleted 01/12/23 01:48:27.074
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:48:27.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:48:27.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-8333" for this suite. 01/12/23 01:48:27.083
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2286" for this suite. 01/12/23 01:48:27.097
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:48:27.114
Jan 12 01:48:27.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 01:48:27.115
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:27.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:27.133
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 01/12/23 01:48:27.135
Jan 12 01:48:27.169: INFO: Waiting up to 5m0s for pod "pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45" in namespace "emptydir-2222" to be "Succeeded or Failed"
Jan 12 01:48:27.171: INFO: Pod "pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45": Phase="Pending", Reason="", readiness=false. Elapsed: 1.879986ms
Jan 12 01:48:29.174: INFO: Pod "pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005153761s
Jan 12 01:48:31.175: INFO: Pod "pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006016062s
Jan 12 01:48:33.175: INFO: Pod "pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005802907s
STEP: Saw pod success 01/12/23 01:48:33.175
Jan 12 01:48:33.175: INFO: Pod "pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45" satisfied condition "Succeeded or Failed"
Jan 12 01:48:33.177: INFO: Trying to get logs from node eqx04-flash06 pod pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45 container test-container: <nil>
STEP: delete the pod 01/12/23 01:48:33.191
Jan 12 01:48:33.211: INFO: Waiting for pod pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45 to disappear
Jan 12 01:48:33.212: INFO: Pod pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:48:33.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2222" for this suite. 01/12/23 01:48:33.216
------------------------------
• [SLOW TEST] [6.119 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:48:27.114
    Jan 12 01:48:27.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 01:48:27.115
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:27.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:27.133
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/12/23 01:48:27.135
    Jan 12 01:48:27.169: INFO: Waiting up to 5m0s for pod "pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45" in namespace "emptydir-2222" to be "Succeeded or Failed"
    Jan 12 01:48:27.171: INFO: Pod "pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45": Phase="Pending", Reason="", readiness=false. Elapsed: 1.879986ms
    Jan 12 01:48:29.174: INFO: Pod "pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005153761s
    Jan 12 01:48:31.175: INFO: Pod "pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006016062s
    Jan 12 01:48:33.175: INFO: Pod "pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005802907s
    STEP: Saw pod success 01/12/23 01:48:33.175
    Jan 12 01:48:33.175: INFO: Pod "pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45" satisfied condition "Succeeded or Failed"
    Jan 12 01:48:33.177: INFO: Trying to get logs from node eqx04-flash06 pod pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45 container test-container: <nil>
    STEP: delete the pod 01/12/23 01:48:33.191
    Jan 12 01:48:33.211: INFO: Waiting for pod pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45 to disappear
    Jan 12 01:48:33.212: INFO: Pod pod-1721dc0d-bd53-4257-bca2-9f4c09fabe45 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:48:33.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2222" for this suite. 01/12/23 01:48:33.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:48:33.237
Jan 12 01:48:33.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 01:48:33.237
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:33.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:33.256
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 01/12/23 01:48:33.261
STEP: watching for the Service to be added 01/12/23 01:48:33.274
Jan 12 01:48:33.275: INFO: Found Service test-service-f42tl in namespace services-1025 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 12 01:48:33.275: INFO: Service test-service-f42tl created
STEP: Getting /status 01/12/23 01:48:33.275
Jan 12 01:48:33.277: INFO: Service test-service-f42tl has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/12/23 01:48:33.278
STEP: watching for the Service to be patched 01/12/23 01:48:33.285
Jan 12 01:48:33.287: INFO: observed Service test-service-f42tl in namespace services-1025 with annotations: map[] & LoadBalancer: {[]}
Jan 12 01:48:33.287: INFO: Found Service test-service-f42tl in namespace services-1025 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 12 01:48:33.287: INFO: Service test-service-f42tl has service status patched
STEP: updating the ServiceStatus 01/12/23 01:48:33.287
Jan 12 01:48:33.295: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/12/23 01:48:33.295
Jan 12 01:48:33.299: INFO: Observed Service test-service-f42tl in namespace services-1025 with annotations: map[] & Conditions: {[]}
Jan 12 01:48:33.299: INFO: Observed event: &Service{ObjectMeta:{test-service-f42tl  services-1025  372a1c1e-b147-4d13-a6c4-ba77eb926a39 20175405 0 2023-01-12 01:48:33 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-12 01:48:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-12 01:48:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.19.144.73,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.19.144.73],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 12 01:48:33.299: INFO: Found Service test-service-f42tl in namespace services-1025 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 12 01:48:33.299: INFO: Service test-service-f42tl has service status updated
STEP: patching the service 01/12/23 01:48:33.299
STEP: watching for the Service to be patched 01/12/23 01:48:33.313
Jan 12 01:48:33.315: INFO: observed Service test-service-f42tl in namespace services-1025 with labels: map[test-service-static:true]
Jan 12 01:48:33.315: INFO: observed Service test-service-f42tl in namespace services-1025 with labels: map[test-service-static:true]
Jan 12 01:48:33.315: INFO: observed Service test-service-f42tl in namespace services-1025 with labels: map[test-service-static:true]
Jan 12 01:48:33.315: INFO: Found Service test-service-f42tl in namespace services-1025 with labels: map[test-service:patched test-service-static:true]
Jan 12 01:48:33.315: INFO: Service test-service-f42tl patched
STEP: deleting the service 01/12/23 01:48:33.315
STEP: watching for the Service to be deleted 01/12/23 01:48:33.327
Jan 12 01:48:33.328: INFO: Observed event: ADDED
Jan 12 01:48:33.328: INFO: Observed event: MODIFIED
Jan 12 01:48:33.328: INFO: Observed event: MODIFIED
Jan 12 01:48:33.328: INFO: Observed event: MODIFIED
Jan 12 01:48:33.328: INFO: Found Service test-service-f42tl in namespace services-1025 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 12 01:48:33.328: INFO: Service test-service-f42tl deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 01:48:33.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1025" for this suite. 01/12/23 01:48:33.332
------------------------------
• [0.118 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:48:33.237
    Jan 12 01:48:33.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 01:48:33.237
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:33.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:33.256
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 01/12/23 01:48:33.261
    STEP: watching for the Service to be added 01/12/23 01:48:33.274
    Jan 12 01:48:33.275: INFO: Found Service test-service-f42tl in namespace services-1025 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 12 01:48:33.275: INFO: Service test-service-f42tl created
    STEP: Getting /status 01/12/23 01:48:33.275
    Jan 12 01:48:33.277: INFO: Service test-service-f42tl has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/12/23 01:48:33.278
    STEP: watching for the Service to be patched 01/12/23 01:48:33.285
    Jan 12 01:48:33.287: INFO: observed Service test-service-f42tl in namespace services-1025 with annotations: map[] & LoadBalancer: {[]}
    Jan 12 01:48:33.287: INFO: Found Service test-service-f42tl in namespace services-1025 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 12 01:48:33.287: INFO: Service test-service-f42tl has service status patched
    STEP: updating the ServiceStatus 01/12/23 01:48:33.287
    Jan 12 01:48:33.295: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/12/23 01:48:33.295
    Jan 12 01:48:33.299: INFO: Observed Service test-service-f42tl in namespace services-1025 with annotations: map[] & Conditions: {[]}
    Jan 12 01:48:33.299: INFO: Observed event: &Service{ObjectMeta:{test-service-f42tl  services-1025  372a1c1e-b147-4d13-a6c4-ba77eb926a39 20175405 0 2023-01-12 01:48:33 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-12 01:48:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-12 01:48:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.19.144.73,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.19.144.73],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 12 01:48:33.299: INFO: Found Service test-service-f42tl in namespace services-1025 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 12 01:48:33.299: INFO: Service test-service-f42tl has service status updated
    STEP: patching the service 01/12/23 01:48:33.299
    STEP: watching for the Service to be patched 01/12/23 01:48:33.313
    Jan 12 01:48:33.315: INFO: observed Service test-service-f42tl in namespace services-1025 with labels: map[test-service-static:true]
    Jan 12 01:48:33.315: INFO: observed Service test-service-f42tl in namespace services-1025 with labels: map[test-service-static:true]
    Jan 12 01:48:33.315: INFO: observed Service test-service-f42tl in namespace services-1025 with labels: map[test-service-static:true]
    Jan 12 01:48:33.315: INFO: Found Service test-service-f42tl in namespace services-1025 with labels: map[test-service:patched test-service-static:true]
    Jan 12 01:48:33.315: INFO: Service test-service-f42tl patched
    STEP: deleting the service 01/12/23 01:48:33.315
    STEP: watching for the Service to be deleted 01/12/23 01:48:33.327
    Jan 12 01:48:33.328: INFO: Observed event: ADDED
    Jan 12 01:48:33.328: INFO: Observed event: MODIFIED
    Jan 12 01:48:33.328: INFO: Observed event: MODIFIED
    Jan 12 01:48:33.328: INFO: Observed event: MODIFIED
    Jan 12 01:48:33.328: INFO: Found Service test-service-f42tl in namespace services-1025 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 12 01:48:33.328: INFO: Service test-service-f42tl deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:48:33.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1025" for this suite. 01/12/23 01:48:33.332
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:48:33.357
Jan 12 01:48:33.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename daemonsets 01/12/23 01:48:33.357
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:33.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:33.374
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 01/12/23 01:48:33.392
STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 01:48:33.446
Jan 12 01:48:33.450: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:48:33.450: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:48:33.450: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:48:33.452: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:48:33.452: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:48:34.456: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:48:34.456: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:48:34.456: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:48:34.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:48:34.459: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:48:35.457: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:48:35.457: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:48:35.457: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:48:35.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:48:35.460: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:48:36.456: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:48:36.456: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:48:36.456: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:48:36.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 01:48:36.459: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 01/12/23 01:48:36.461
Jan 12 01:48:36.463: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/12/23 01:48:36.463
Jan 12 01:48:36.472: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/12/23 01:48:36.472
Jan 12 01:48:36.474: INFO: Observed &DaemonSet event: ADDED
Jan 12 01:48:36.474: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 01:48:36.474: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 01:48:36.474: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 01:48:36.474: INFO: Found daemon set daemon-set in namespace daemonsets-4075 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 12 01:48:36.475: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/12/23 01:48:36.475
STEP: watching for the daemon set status to be patched 01/12/23 01:48:36.482
Jan 12 01:48:36.484: INFO: Observed &DaemonSet event: ADDED
Jan 12 01:48:36.484: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 01:48:36.484: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 01:48:36.485: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 01:48:36.485: INFO: Observed daemon set daemon-set in namespace daemonsets-4075 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 12 01:48:36.485: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 01:48:36.485: INFO: Found daemon set daemon-set in namespace daemonsets-4075 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 12 01:48:36.485: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/12/23 01:48:36.488
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4075, will wait for the garbage collector to delete the pods 01/12/23 01:48:36.488
Jan 12 01:48:36.548: INFO: Deleting DaemonSet.extensions daemon-set took: 7.488925ms
Jan 12 01:48:36.649: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.256372ms
Jan 12 01:48:39.452: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:48:39.452: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 12 01:48:39.454: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20175500"},"items":null}

Jan 12 01:48:39.456: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20175500"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:48:39.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4075" for this suite. 01/12/23 01:48:39.467
------------------------------
• [SLOW TEST] [6.136 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:48:33.357
    Jan 12 01:48:33.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename daemonsets 01/12/23 01:48:33.357
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:33.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:33.374
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 01/12/23 01:48:33.392
    STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 01:48:33.446
    Jan 12 01:48:33.450: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:48:33.450: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:48:33.450: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:48:33.452: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:48:33.452: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:48:34.456: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:48:34.456: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:48:34.456: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:48:34.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:48:34.459: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:48:35.457: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:48:35.457: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:48:35.457: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:48:35.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:48:35.460: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:48:36.456: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:48:36.456: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:48:36.456: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:48:36.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 01:48:36.459: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 01/12/23 01:48:36.461
    Jan 12 01:48:36.463: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/12/23 01:48:36.463
    Jan 12 01:48:36.472: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/12/23 01:48:36.472
    Jan 12 01:48:36.474: INFO: Observed &DaemonSet event: ADDED
    Jan 12 01:48:36.474: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 01:48:36.474: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 01:48:36.474: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 01:48:36.474: INFO: Found daemon set daemon-set in namespace daemonsets-4075 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 12 01:48:36.475: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/12/23 01:48:36.475
    STEP: watching for the daemon set status to be patched 01/12/23 01:48:36.482
    Jan 12 01:48:36.484: INFO: Observed &DaemonSet event: ADDED
    Jan 12 01:48:36.484: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 01:48:36.484: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 01:48:36.485: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 01:48:36.485: INFO: Observed daemon set daemon-set in namespace daemonsets-4075 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 12 01:48:36.485: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 01:48:36.485: INFO: Found daemon set daemon-set in namespace daemonsets-4075 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 12 01:48:36.485: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/12/23 01:48:36.488
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4075, will wait for the garbage collector to delete the pods 01/12/23 01:48:36.488
    Jan 12 01:48:36.548: INFO: Deleting DaemonSet.extensions daemon-set took: 7.488925ms
    Jan 12 01:48:36.649: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.256372ms
    Jan 12 01:48:39.452: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:48:39.452: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 12 01:48:39.454: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20175500"},"items":null}

    Jan 12 01:48:39.456: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20175500"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:48:39.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4075" for this suite. 01/12/23 01:48:39.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:48:39.508
Jan 12 01:48:39.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 01:48:39.51
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:39.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:39.529
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 01/12/23 01:48:39.531
Jan 12 01:48:39.531: INFO: Creating e2e-svc-a-btvzs
Jan 12 01:48:39.541: INFO: Creating e2e-svc-b-kd8wb
Jan 12 01:48:39.555: INFO: Creating e2e-svc-c-v8whm
STEP: deleting service collection 01/12/23 01:48:39.567
Jan 12 01:48:39.591: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 01:48:39.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-421" for this suite. 01/12/23 01:48:39.596
------------------------------
• [0.104 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:48:39.508
    Jan 12 01:48:39.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 01:48:39.51
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:39.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:39.529
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 01/12/23 01:48:39.531
    Jan 12 01:48:39.531: INFO: Creating e2e-svc-a-btvzs
    Jan 12 01:48:39.541: INFO: Creating e2e-svc-b-kd8wb
    Jan 12 01:48:39.555: INFO: Creating e2e-svc-c-v8whm
    STEP: deleting service collection 01/12/23 01:48:39.567
    Jan 12 01:48:39.591: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:48:39.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-421" for this suite. 01/12/23 01:48:39.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:48:39.613
Jan 12 01:48:39.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename job 01/12/23 01:48:39.614
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:39.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:39.628
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 01/12/23 01:48:39.63
STEP: Ensuring job reaches completions 01/12/23 01:48:39.635
STEP: Ensuring pods with index for job exist 01/12/23 01:48:49.638
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 12 01:48:49.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9968" for this suite. 01/12/23 01:48:49.645
------------------------------
• [SLOW TEST] [10.046 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:48:39.613
    Jan 12 01:48:39.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename job 01/12/23 01:48:39.614
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:39.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:39.628
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 01/12/23 01:48:39.63
    STEP: Ensuring job reaches completions 01/12/23 01:48:39.635
    STEP: Ensuring pods with index for job exist 01/12/23 01:48:49.638
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:48:49.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9968" for this suite. 01/12/23 01:48:49.645
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:48:49.66
Jan 12 01:48:49.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename gc 01/12/23 01:48:49.661
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:49.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:49.673
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/12/23 01:48:49.675
STEP: Wait for the Deployment to create new ReplicaSet 01/12/23 01:48:49.702
STEP: delete the deployment 01/12/23 01:48:50.21
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/12/23 01:48:50.215
STEP: Gathering metrics 01/12/23 01:48:50.727
Jan 12 01:48:50.750: INFO: Waiting up to 5m0s for pod "kube-controller-manager-eqx04-flash04" in namespace "kube-system" to be "running and ready"
Jan 12 01:48:50.753: INFO: Pod "kube-controller-manager-eqx04-flash04": Phase="Running", Reason="", readiness=true. Elapsed: 2.130183ms
Jan 12 01:48:50.753: INFO: The phase of Pod kube-controller-manager-eqx04-flash04 is Running (Ready = true)
Jan 12 01:48:50.753: INFO: Pod "kube-controller-manager-eqx04-flash04" satisfied condition "running and ready"
Jan 12 01:48:50.805: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 12 01:48:50.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-10" for this suite. 01/12/23 01:48:50.809
------------------------------
• [1.162 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:48:49.66
    Jan 12 01:48:49.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename gc 01/12/23 01:48:49.661
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:49.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:49.673
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/12/23 01:48:49.675
    STEP: Wait for the Deployment to create new ReplicaSet 01/12/23 01:48:49.702
    STEP: delete the deployment 01/12/23 01:48:50.21
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/12/23 01:48:50.215
    STEP: Gathering metrics 01/12/23 01:48:50.727
    Jan 12 01:48:50.750: INFO: Waiting up to 5m0s for pod "kube-controller-manager-eqx04-flash04" in namespace "kube-system" to be "running and ready"
    Jan 12 01:48:50.753: INFO: Pod "kube-controller-manager-eqx04-flash04": Phase="Running", Reason="", readiness=true. Elapsed: 2.130183ms
    Jan 12 01:48:50.753: INFO: The phase of Pod kube-controller-manager-eqx04-flash04 is Running (Ready = true)
    Jan 12 01:48:50.753: INFO: Pod "kube-controller-manager-eqx04-flash04" satisfied condition "running and ready"
    Jan 12 01:48:50.805: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:48:50.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-10" for this suite. 01/12/23 01:48:50.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:48:50.827
Jan 12 01:48:50.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 01:48:50.827
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:50.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:50.841
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6597 01/12/23 01:48:50.843
STEP: changing the ExternalName service to type=ClusterIP 01/12/23 01:48:50.847
STEP: creating replication controller externalname-service in namespace services-6597 01/12/23 01:48:50.859
I0112 01:48:50.865164      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6597, replica count: 2
I0112 01:48:53.917239      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 01:48:53.917: INFO: Creating new exec pod
Jan 12 01:48:53.944: INFO: Waiting up to 5m0s for pod "execpodzk8t7" in namespace "services-6597" to be "running"
Jan 12 01:48:53.947: INFO: Pod "execpodzk8t7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.488778ms
Jan 12 01:48:55.959: INFO: Pod "execpodzk8t7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014525758s
Jan 12 01:48:57.951: INFO: Pod "execpodzk8t7": Phase="Running", Reason="", readiness=true. Elapsed: 4.006338907s
Jan 12 01:48:57.951: INFO: Pod "execpodzk8t7" satisfied condition "running"
Jan 12 01:48:58.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-6597 exec execpodzk8t7 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 12 01:48:59.155: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 12 01:48:59.155: INFO: stdout: ""
Jan 12 01:48:59.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-6597 exec execpodzk8t7 -- /bin/sh -x -c nc -v -z -w 2 172.19.87.115 80'
Jan 12 01:48:59.354: INFO: stderr: "+ nc -v -z -w 2 172.19.87.115 80\nConnection to 172.19.87.115 80 port [tcp/http] succeeded!\n"
Jan 12 01:48:59.354: INFO: stdout: ""
Jan 12 01:48:59.355: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 01:48:59.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6597" for this suite. 01/12/23 01:48:59.374
------------------------------
• [SLOW TEST] [8.560 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:48:50.827
    Jan 12 01:48:50.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 01:48:50.827
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:50.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:50.841
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6597 01/12/23 01:48:50.843
    STEP: changing the ExternalName service to type=ClusterIP 01/12/23 01:48:50.847
    STEP: creating replication controller externalname-service in namespace services-6597 01/12/23 01:48:50.859
    I0112 01:48:50.865164      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6597, replica count: 2
    I0112 01:48:53.917239      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 01:48:53.917: INFO: Creating new exec pod
    Jan 12 01:48:53.944: INFO: Waiting up to 5m0s for pod "execpodzk8t7" in namespace "services-6597" to be "running"
    Jan 12 01:48:53.947: INFO: Pod "execpodzk8t7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.488778ms
    Jan 12 01:48:55.959: INFO: Pod "execpodzk8t7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014525758s
    Jan 12 01:48:57.951: INFO: Pod "execpodzk8t7": Phase="Running", Reason="", readiness=true. Elapsed: 4.006338907s
    Jan 12 01:48:57.951: INFO: Pod "execpodzk8t7" satisfied condition "running"
    Jan 12 01:48:58.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-6597 exec execpodzk8t7 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 12 01:48:59.155: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 12 01:48:59.155: INFO: stdout: ""
    Jan 12 01:48:59.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-6597 exec execpodzk8t7 -- /bin/sh -x -c nc -v -z -w 2 172.19.87.115 80'
    Jan 12 01:48:59.354: INFO: stderr: "+ nc -v -z -w 2 172.19.87.115 80\nConnection to 172.19.87.115 80 port [tcp/http] succeeded!\n"
    Jan 12 01:48:59.354: INFO: stdout: ""
    Jan 12 01:48:59.355: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:48:59.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6597" for this suite. 01/12/23 01:48:59.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:48:59.388
Jan 12 01:48:59.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename resourcequota 01/12/23 01:48:59.388
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:59.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:59.402
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 01/12/23 01:48:59.404
STEP: Creating a ResourceQuota 01/12/23 01:49:04.406
STEP: Ensuring resource quota status is calculated 01/12/23 01:49:04.412
STEP: Creating a ReplicaSet 01/12/23 01:49:06.415
STEP: Ensuring resource quota status captures replicaset creation 01/12/23 01:49:06.425
STEP: Deleting a ReplicaSet 01/12/23 01:49:08.429
STEP: Ensuring resource quota status released usage 01/12/23 01:49:08.434
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 01:49:10.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9011" for this suite. 01/12/23 01:49:10.456
------------------------------
• [SLOW TEST] [11.082 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:48:59.388
    Jan 12 01:48:59.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename resourcequota 01/12/23 01:48:59.388
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:48:59.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:48:59.402
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 01/12/23 01:48:59.404
    STEP: Creating a ResourceQuota 01/12/23 01:49:04.406
    STEP: Ensuring resource quota status is calculated 01/12/23 01:49:04.412
    STEP: Creating a ReplicaSet 01/12/23 01:49:06.415
    STEP: Ensuring resource quota status captures replicaset creation 01/12/23 01:49:06.425
    STEP: Deleting a ReplicaSet 01/12/23 01:49:08.429
    STEP: Ensuring resource quota status released usage 01/12/23 01:49:08.434
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:49:10.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9011" for this suite. 01/12/23 01:49:10.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:49:10.473
Jan 12 01:49:10.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename daemonsets 01/12/23 01:49:10.474
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:49:10.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:49:10.486
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 01/12/23 01:49:10.505
STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 01:49:10.534
Jan 12 01:49:10.539: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:49:10.539: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:49:10.540: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:49:10.542: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:49:10.542: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:49:11.548: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:49:11.548: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:49:11.548: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:49:11.550: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 01:49:11.550: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:49:12.549: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:49:12.549: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:49:12.549: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:49:12.552: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 01:49:12.552: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
Jan 12 01:49:13.546: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:49:13.546: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:49:13.546: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 12 01:49:13.549: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 01:49:13.549: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 01/12/23 01:49:13.551
STEP: DeleteCollection of the DaemonSets 01/12/23 01:49:13.554
STEP: Verify that ReplicaSets have been deleted 01/12/23 01:49:13.559
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Jan 12 01:49:13.567: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20176026"},"items":null}

Jan 12 01:49:13.569: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20176026"},"items":[{"metadata":{"name":"daemon-set-bqkf2","generateName":"daemon-set-","namespace":"daemonsets-6524","uid":"a4160d57-6f4c-42af-ac61-bf067afdafa7","resourceVersion":"20176021","creationTimestamp":"2023-01-12T01:49:10Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"4689547bd6534efa13be25cf9c988d12ef60858ec524220859f6892b07e8b4aa","cni.projectcalico.org/podIP":"172.21.117.138/32","cni.projectcalico.org/podIPs":"172.21.117.138/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"calico\",\n    \"ips\": [\n        \"172.21.117.138\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"calico\",\n    \"ips\": [\n        \"172.21.117.138\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c5149570-a4db-4759-8b0b-2e40932e58db","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5149570-a4db-4759-8b0b-2e40932e58db\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.117.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-clvl8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-clvl8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"eqx03-flash06","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["eqx03-flash06"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:10Z"}],"hostIP":"10.9.140.106","podIP":"172.21.117.138","podIPs":[{"ip":"172.21.117.138"}],"startTime":"2023-01-12T01:49:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-12T01:49:12Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"robin://249663d071dbc7385c9b099276478283faad301565d75d1f05d0a67be7c9290a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mtbts","generateName":"daemon-set-","namespace":"daemonsets-6524","uid":"3ef66726-e781-4ac4-84c6-a12065f8aac9","resourceVersion":"20176019","creationTimestamp":"2023-01-12T01:49:10Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"63ca12da30f9ed2679904dba95a1e65178da42b38b4db24d4e437bd4aa2eed8f","cni.projectcalico.org/podIP":"172.21.88.138/32","cni.projectcalico.org/podIPs":"172.21.88.138/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"calico\",\n    \"ips\": [\n        \"172.21.88.138\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"calico\",\n    \"ips\": [\n        \"172.21.88.138\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c5149570-a4db-4759-8b0b-2e40932e58db","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5149570-a4db-4759-8b0b-2e40932e58db\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l5vsc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l5vsc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"eqx04-flash06","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["eqx04-flash06"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:12Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:12Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:10Z"}],"hostIP":"10.9.40.106","podIP":"172.21.88.138","podIPs":[{"ip":"172.21.88.138"}],"startTime":"2023-01-12T01:49:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-12T01:49:12Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"robin://2df10570ffc578e29b17f598e81056218b349f4b1593c3c14f7cad5767c83c27","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:49:13.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6524" for this suite. 01/12/23 01:49:13.582
------------------------------
• [3.145 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:49:10.473
    Jan 12 01:49:10.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename daemonsets 01/12/23 01:49:10.474
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:49:10.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:49:10.486
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 01/12/23 01:49:10.505
    STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 01:49:10.534
    Jan 12 01:49:10.539: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:49:10.539: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:49:10.540: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:49:10.542: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:49:10.542: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:49:11.548: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:49:11.548: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:49:11.548: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:49:11.550: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 01:49:11.550: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:49:12.549: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:49:12.549: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:49:12.549: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:49:12.552: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 01:49:12.552: INFO: Node eqx03-flash06 is running 0 daemon pod, expected 1
    Jan 12 01:49:13.546: INFO: DaemonSet pods can't tolerate node eqx01-flash03 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:49:13.546: INFO: DaemonSet pods can't tolerate node eqx03-flash07 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:49:13.546: INFO: DaemonSet pods can't tolerate node eqx04-flash04 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 12 01:49:13.549: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 01:49:13.549: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 01/12/23 01:49:13.551
    STEP: DeleteCollection of the DaemonSets 01/12/23 01:49:13.554
    STEP: Verify that ReplicaSets have been deleted 01/12/23 01:49:13.559
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Jan 12 01:49:13.567: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20176026"},"items":null}

    Jan 12 01:49:13.569: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20176026"},"items":[{"metadata":{"name":"daemon-set-bqkf2","generateName":"daemon-set-","namespace":"daemonsets-6524","uid":"a4160d57-6f4c-42af-ac61-bf067afdafa7","resourceVersion":"20176021","creationTimestamp":"2023-01-12T01:49:10Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"4689547bd6534efa13be25cf9c988d12ef60858ec524220859f6892b07e8b4aa","cni.projectcalico.org/podIP":"172.21.117.138/32","cni.projectcalico.org/podIPs":"172.21.117.138/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"calico\",\n    \"ips\": [\n        \"172.21.117.138\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"calico\",\n    \"ips\": [\n        \"172.21.117.138\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c5149570-a4db-4759-8b0b-2e40932e58db","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5149570-a4db-4759-8b0b-2e40932e58db\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.117.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-clvl8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-clvl8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"eqx03-flash06","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["eqx03-flash06"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:10Z"}],"hostIP":"10.9.140.106","podIP":"172.21.117.138","podIPs":[{"ip":"172.21.117.138"}],"startTime":"2023-01-12T01:49:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-12T01:49:12Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"robin://249663d071dbc7385c9b099276478283faad301565d75d1f05d0a67be7c9290a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mtbts","generateName":"daemon-set-","namespace":"daemonsets-6524","uid":"3ef66726-e781-4ac4-84c6-a12065f8aac9","resourceVersion":"20176019","creationTimestamp":"2023-01-12T01:49:10Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"63ca12da30f9ed2679904dba95a1e65178da42b38b4db24d4e437bd4aa2eed8f","cni.projectcalico.org/podIP":"172.21.88.138/32","cni.projectcalico.org/podIPs":"172.21.88.138/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"calico\",\n    \"ips\": [\n        \"172.21.88.138\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"calico\",\n    \"ips\": [\n        \"172.21.88.138\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c5149570-a4db-4759-8b0b-2e40932e58db","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5149570-a4db-4759-8b0b-2e40932e58db\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-12T01:49:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l5vsc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l5vsc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"eqx04-flash06","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["eqx04-flash06"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:12Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:12Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T01:49:10Z"}],"hostIP":"10.9.40.106","podIP":"172.21.88.138","podIPs":[{"ip":"172.21.88.138"}],"startTime":"2023-01-12T01:49:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-12T01:49:12Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"robin://2df10570ffc578e29b17f598e81056218b349f4b1593c3c14f7cad5767c83c27","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:49:13.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6524" for this suite. 01/12/23 01:49:13.582
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:49:13.62
Jan 12 01:49:13.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 01:49:13.621
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:49:13.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:49:13.632
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/12/23 01:49:13.634
Jan 12 01:49:13.721: INFO: Waiting up to 5m0s for pod "pod-63712586-cde5-4998-b0ac-6cf3a5a77220" in namespace "emptydir-8547" to be "Succeeded or Failed"
Jan 12 01:49:13.723: INFO: Pod "pod-63712586-cde5-4998-b0ac-6cf3a5a77220": Phase="Pending", Reason="", readiness=false. Elapsed: 2.393605ms
Jan 12 01:49:15.727: INFO: Pod "pod-63712586-cde5-4998-b0ac-6cf3a5a77220": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006014584s
Jan 12 01:49:17.727: INFO: Pod "pod-63712586-cde5-4998-b0ac-6cf3a5a77220": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005694416s
Jan 12 01:49:19.727: INFO: Pod "pod-63712586-cde5-4998-b0ac-6cf3a5a77220": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006017924s
STEP: Saw pod success 01/12/23 01:49:19.727
Jan 12 01:49:19.727: INFO: Pod "pod-63712586-cde5-4998-b0ac-6cf3a5a77220" satisfied condition "Succeeded or Failed"
Jan 12 01:49:19.729: INFO: Trying to get logs from node eqx04-flash06 pod pod-63712586-cde5-4998-b0ac-6cf3a5a77220 container test-container: <nil>
STEP: delete the pod 01/12/23 01:49:19.737
Jan 12 01:49:19.747: INFO: Waiting for pod pod-63712586-cde5-4998-b0ac-6cf3a5a77220 to disappear
Jan 12 01:49:19.748: INFO: Pod pod-63712586-cde5-4998-b0ac-6cf3a5a77220 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 01:49:19.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8547" for this suite. 01/12/23 01:49:19.752
------------------------------
• [SLOW TEST] [6.157 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:49:13.62
    Jan 12 01:49:13.620: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 01:49:13.621
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:49:13.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:49:13.632
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/12/23 01:49:13.634
    Jan 12 01:49:13.721: INFO: Waiting up to 5m0s for pod "pod-63712586-cde5-4998-b0ac-6cf3a5a77220" in namespace "emptydir-8547" to be "Succeeded or Failed"
    Jan 12 01:49:13.723: INFO: Pod "pod-63712586-cde5-4998-b0ac-6cf3a5a77220": Phase="Pending", Reason="", readiness=false. Elapsed: 2.393605ms
    Jan 12 01:49:15.727: INFO: Pod "pod-63712586-cde5-4998-b0ac-6cf3a5a77220": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006014584s
    Jan 12 01:49:17.727: INFO: Pod "pod-63712586-cde5-4998-b0ac-6cf3a5a77220": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005694416s
    Jan 12 01:49:19.727: INFO: Pod "pod-63712586-cde5-4998-b0ac-6cf3a5a77220": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006017924s
    STEP: Saw pod success 01/12/23 01:49:19.727
    Jan 12 01:49:19.727: INFO: Pod "pod-63712586-cde5-4998-b0ac-6cf3a5a77220" satisfied condition "Succeeded or Failed"
    Jan 12 01:49:19.729: INFO: Trying to get logs from node eqx04-flash06 pod pod-63712586-cde5-4998-b0ac-6cf3a5a77220 container test-container: <nil>
    STEP: delete the pod 01/12/23 01:49:19.737
    Jan 12 01:49:19.747: INFO: Waiting for pod pod-63712586-cde5-4998-b0ac-6cf3a5a77220 to disappear
    Jan 12 01:49:19.748: INFO: Pod pod-63712586-cde5-4998-b0ac-6cf3a5a77220 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:49:19.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8547" for this suite. 01/12/23 01:49:19.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:49:19.779
Jan 12 01:49:19.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:49:19.78
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:49:19.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:49:19.793
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 01/12/23 01:49:19.796
Jan 12 01:49:19.856: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1" in namespace "projected-7682" to be "Succeeded or Failed"
Jan 12 01:49:19.858: INFO: Pod "downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25124ms
Jan 12 01:49:21.862: INFO: Pod "downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005439121s
Jan 12 01:49:23.862: INFO: Pod "downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005722518s
Jan 12 01:49:25.861: INFO: Pod "downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005140686s
STEP: Saw pod success 01/12/23 01:49:25.861
Jan 12 01:49:25.861: INFO: Pod "downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1" satisfied condition "Succeeded or Failed"
Jan 12 01:49:25.863: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1 container client-container: <nil>
STEP: delete the pod 01/12/23 01:49:25.873
Jan 12 01:49:25.884: INFO: Waiting for pod downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1 to disappear
Jan 12 01:49:25.886: INFO: Pod downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 01:49:25.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7682" for this suite. 01/12/23 01:49:25.89
------------------------------
• [SLOW TEST] [6.135 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:49:19.779
    Jan 12 01:49:19.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:49:19.78
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:49:19.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:49:19.793
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 01/12/23 01:49:19.796
    Jan 12 01:49:19.856: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1" in namespace "projected-7682" to be "Succeeded or Failed"
    Jan 12 01:49:19.858: INFO: Pod "downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25124ms
    Jan 12 01:49:21.862: INFO: Pod "downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005439121s
    Jan 12 01:49:23.862: INFO: Pod "downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005722518s
    Jan 12 01:49:25.861: INFO: Pod "downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005140686s
    STEP: Saw pod success 01/12/23 01:49:25.861
    Jan 12 01:49:25.861: INFO: Pod "downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1" satisfied condition "Succeeded or Failed"
    Jan 12 01:49:25.863: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1 container client-container: <nil>
    STEP: delete the pod 01/12/23 01:49:25.873
    Jan 12 01:49:25.884: INFO: Waiting for pod downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1 to disappear
    Jan 12 01:49:25.886: INFO: Pod downwardapi-volume-c4a608e4-cb00-4595-984d-3780e06df9c1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:49:25.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7682" for this suite. 01/12/23 01:49:25.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:49:25.917
Jan 12 01:49:25.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:49:25.918
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:49:25.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:49:25.932
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-30741c96-56ff-4490-aed1-49cc552b07bd 01/12/23 01:49:25.938
STEP: Creating the pod 01/12/23 01:49:25.941
Jan 12 01:49:26.005: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992" in namespace "projected-3777" to be "running and ready"
Jan 12 01:49:26.007: INFO: Pod "pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992": Phase="Pending", Reason="", readiness=false. Elapsed: 2.280992ms
Jan 12 01:49:26.007: INFO: The phase of Pod pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:49:28.011: INFO: Pod "pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006096904s
Jan 12 01:49:28.011: INFO: The phase of Pod pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:49:30.012: INFO: Pod "pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992": Phase="Running", Reason="", readiness=true. Elapsed: 4.007542376s
Jan 12 01:49:30.012: INFO: The phase of Pod pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992 is Running (Ready = true)
Jan 12 01:49:30.012: INFO: Pod "pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-30741c96-56ff-4490-aed1-49cc552b07bd 01/12/23 01:49:30.026
STEP: waiting to observe update in volume 01/12/23 01:49:30.031
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 01:50:58.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3777" for this suite. 01/12/23 01:50:58.536
------------------------------
• [SLOW TEST] [92.668 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:49:25.917
    Jan 12 01:49:25.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:49:25.918
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:49:25.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:49:25.932
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-30741c96-56ff-4490-aed1-49cc552b07bd 01/12/23 01:49:25.938
    STEP: Creating the pod 01/12/23 01:49:25.941
    Jan 12 01:49:26.005: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992" in namespace "projected-3777" to be "running and ready"
    Jan 12 01:49:26.007: INFO: Pod "pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992": Phase="Pending", Reason="", readiness=false. Elapsed: 2.280992ms
    Jan 12 01:49:26.007: INFO: The phase of Pod pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:49:28.011: INFO: Pod "pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006096904s
    Jan 12 01:49:28.011: INFO: The phase of Pod pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:49:30.012: INFO: Pod "pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992": Phase="Running", Reason="", readiness=true. Elapsed: 4.007542376s
    Jan 12 01:49:30.012: INFO: The phase of Pod pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992 is Running (Ready = true)
    Jan 12 01:49:30.012: INFO: Pod "pod-projected-configmaps-e7e49002-17e3-4111-a91b-f9e81123b992" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-30741c96-56ff-4490-aed1-49cc552b07bd 01/12/23 01:49:30.026
    STEP: waiting to observe update in volume 01/12/23 01:49:30.031
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:50:58.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3777" for this suite. 01/12/23 01:50:58.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:50:58.589
Jan 12 01:50:58.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename cronjob 01/12/23 01:50:58.589
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:50:58.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:50:58.614
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/12/23 01:50:58.616
STEP: Ensuring more than one job is running at a time 01/12/23 01:50:58.622
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/12/23 01:52:00.626
STEP: Removing cronjob 01/12/23 01:52:00.628
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 12 01:52:00.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3087" for this suite. 01/12/23 01:52:00.638
------------------------------
• [SLOW TEST] [62.066 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:50:58.589
    Jan 12 01:50:58.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename cronjob 01/12/23 01:50:58.589
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:50:58.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:50:58.614
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/12/23 01:50:58.616
    STEP: Ensuring more than one job is running at a time 01/12/23 01:50:58.622
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/12/23 01:52:00.626
    STEP: Removing cronjob 01/12/23 01:52:00.628
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:52:00.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3087" for this suite. 01/12/23 01:52:00.638
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:52:00.655
Jan 12 01:52:00.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 01:52:00.656
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:52:00.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:52:00.675
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 01/12/23 01:52:00.678
Jan 12 01:52:00.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 create -f -'
Jan 12 01:52:02.099: INFO: stderr: ""
Jan 12 01:52:02.099: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 01:52:02.099
Jan 12 01:52:02.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 12 01:52:02.164: INFO: stderr: ""
Jan 12 01:52:02.164: INFO: stdout: "update-demo-nautilus-q5dkm "
STEP: Replicas for name=update-demo: expected=2 actual=1 01/12/23 01:52:02.164
Jan 12 01:52:07.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 12 01:52:07.237: INFO: stderr: ""
Jan 12 01:52:07.237: INFO: stdout: "update-demo-nautilus-psx5m update-demo-nautilus-q5dkm "
Jan 12 01:52:07.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-psx5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 01:52:07.301: INFO: stderr: ""
Jan 12 01:52:07.301: INFO: stdout: "true"
Jan 12 01:52:07.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-psx5m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 12 01:52:07.362: INFO: stderr: ""
Jan 12 01:52:07.362: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 12 01:52:07.362: INFO: validating pod update-demo-nautilus-psx5m
Jan 12 01:52:07.367: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 12 01:52:07.367: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 12 01:52:07.367: INFO: update-demo-nautilus-psx5m is verified up and running
Jan 12 01:52:07.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-q5dkm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 01:52:07.430: INFO: stderr: ""
Jan 12 01:52:07.430: INFO: stdout: "true"
Jan 12 01:52:07.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-q5dkm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 12 01:52:07.493: INFO: stderr: ""
Jan 12 01:52:07.493: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 12 01:52:07.493: INFO: validating pod update-demo-nautilus-q5dkm
Jan 12 01:52:07.496: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 12 01:52:07.497: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 12 01:52:07.497: INFO: update-demo-nautilus-q5dkm is verified up and running
STEP: scaling down the replication controller 01/12/23 01:52:07.497
Jan 12 01:52:07.498: INFO: scanned /root for discovery docs: <nil>
Jan 12 01:52:07.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 12 01:52:08.589: INFO: stderr: ""
Jan 12 01:52:08.589: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 01:52:08.589
Jan 12 01:52:08.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 12 01:52:08.707: INFO: stderr: ""
Jan 12 01:52:08.707: INFO: stdout: "update-demo-nautilus-psx5m "
Jan 12 01:52:08.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-psx5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 01:52:08.771: INFO: stderr: ""
Jan 12 01:52:08.771: INFO: stdout: "true"
Jan 12 01:52:08.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-psx5m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 12 01:52:08.834: INFO: stderr: ""
Jan 12 01:52:08.834: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 12 01:52:08.834: INFO: validating pod update-demo-nautilus-psx5m
Jan 12 01:52:08.837: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 12 01:52:08.837: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 12 01:52:08.837: INFO: update-demo-nautilus-psx5m is verified up and running
STEP: scaling up the replication controller 01/12/23 01:52:08.837
Jan 12 01:52:08.838: INFO: scanned /root for discovery docs: <nil>
Jan 12 01:52:08.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 12 01:52:09.917: INFO: stderr: ""
Jan 12 01:52:09.917: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 01:52:09.917
Jan 12 01:52:09.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 12 01:52:09.991: INFO: stderr: ""
Jan 12 01:52:09.991: INFO: stdout: "update-demo-nautilus-g28xw update-demo-nautilus-psx5m "
Jan 12 01:52:09.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-g28xw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 01:52:10.083: INFO: stderr: ""
Jan 12 01:52:10.083: INFO: stdout: ""
Jan 12 01:52:10.083: INFO: update-demo-nautilus-g28xw is created but not running
Jan 12 01:52:15.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 12 01:52:15.168: INFO: stderr: ""
Jan 12 01:52:15.168: INFO: stdout: "update-demo-nautilus-g28xw update-demo-nautilus-psx5m "
Jan 12 01:52:15.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-g28xw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 01:52:15.233: INFO: stderr: ""
Jan 12 01:52:15.233: INFO: stdout: "true"
Jan 12 01:52:15.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-g28xw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 12 01:52:15.299: INFO: stderr: ""
Jan 12 01:52:15.299: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 12 01:52:15.299: INFO: validating pod update-demo-nautilus-g28xw
Jan 12 01:52:15.303: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 12 01:52:15.303: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 12 01:52:15.303: INFO: update-demo-nautilus-g28xw is verified up and running
Jan 12 01:52:15.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-psx5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 01:52:15.366: INFO: stderr: ""
Jan 12 01:52:15.366: INFO: stdout: "true"
Jan 12 01:52:15.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-psx5m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 12 01:52:15.430: INFO: stderr: ""
Jan 12 01:52:15.430: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 12 01:52:15.430: INFO: validating pod update-demo-nautilus-psx5m
Jan 12 01:52:15.434: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 12 01:52:15.434: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 12 01:52:15.434: INFO: update-demo-nautilus-psx5m is verified up and running
STEP: using delete to clean up resources 01/12/23 01:52:15.434
Jan 12 01:52:15.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 delete --grace-period=0 --force -f -'
Jan 12 01:52:15.509: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 01:52:15.509: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 12 01:52:15.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get rc,svc -l name=update-demo --no-headers'
Jan 12 01:52:15.594: INFO: stderr: "No resources found in kubectl-7367 namespace.\n"
Jan 12 01:52:15.594: INFO: stdout: ""
Jan 12 01:52:15.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 12 01:52:15.689: INFO: stderr: ""
Jan 12 01:52:15.689: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 01:52:15.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7367" for this suite. 01/12/23 01:52:15.693
------------------------------
• [SLOW TEST] [15.056 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:52:00.655
    Jan 12 01:52:00.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 01:52:00.656
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:52:00.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:52:00.675
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 01/12/23 01:52:00.678
    Jan 12 01:52:00.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 create -f -'
    Jan 12 01:52:02.099: INFO: stderr: ""
    Jan 12 01:52:02.099: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 01:52:02.099
    Jan 12 01:52:02.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 12 01:52:02.164: INFO: stderr: ""
    Jan 12 01:52:02.164: INFO: stdout: "update-demo-nautilus-q5dkm "
    STEP: Replicas for name=update-demo: expected=2 actual=1 01/12/23 01:52:02.164
    Jan 12 01:52:07.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 12 01:52:07.237: INFO: stderr: ""
    Jan 12 01:52:07.237: INFO: stdout: "update-demo-nautilus-psx5m update-demo-nautilus-q5dkm "
    Jan 12 01:52:07.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-psx5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 01:52:07.301: INFO: stderr: ""
    Jan 12 01:52:07.301: INFO: stdout: "true"
    Jan 12 01:52:07.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-psx5m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 12 01:52:07.362: INFO: stderr: ""
    Jan 12 01:52:07.362: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 12 01:52:07.362: INFO: validating pod update-demo-nautilus-psx5m
    Jan 12 01:52:07.367: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 12 01:52:07.367: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 12 01:52:07.367: INFO: update-demo-nautilus-psx5m is verified up and running
    Jan 12 01:52:07.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-q5dkm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 01:52:07.430: INFO: stderr: ""
    Jan 12 01:52:07.430: INFO: stdout: "true"
    Jan 12 01:52:07.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-q5dkm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 12 01:52:07.493: INFO: stderr: ""
    Jan 12 01:52:07.493: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 12 01:52:07.493: INFO: validating pod update-demo-nautilus-q5dkm
    Jan 12 01:52:07.496: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 12 01:52:07.497: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 12 01:52:07.497: INFO: update-demo-nautilus-q5dkm is verified up and running
    STEP: scaling down the replication controller 01/12/23 01:52:07.497
    Jan 12 01:52:07.498: INFO: scanned /root for discovery docs: <nil>
    Jan 12 01:52:07.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 12 01:52:08.589: INFO: stderr: ""
    Jan 12 01:52:08.589: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 01:52:08.589
    Jan 12 01:52:08.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 12 01:52:08.707: INFO: stderr: ""
    Jan 12 01:52:08.707: INFO: stdout: "update-demo-nautilus-psx5m "
    Jan 12 01:52:08.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-psx5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 01:52:08.771: INFO: stderr: ""
    Jan 12 01:52:08.771: INFO: stdout: "true"
    Jan 12 01:52:08.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-psx5m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 12 01:52:08.834: INFO: stderr: ""
    Jan 12 01:52:08.834: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 12 01:52:08.834: INFO: validating pod update-demo-nautilus-psx5m
    Jan 12 01:52:08.837: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 12 01:52:08.837: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 12 01:52:08.837: INFO: update-demo-nautilus-psx5m is verified up and running
    STEP: scaling up the replication controller 01/12/23 01:52:08.837
    Jan 12 01:52:08.838: INFO: scanned /root for discovery docs: <nil>
    Jan 12 01:52:08.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 12 01:52:09.917: INFO: stderr: ""
    Jan 12 01:52:09.917: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 01:52:09.917
    Jan 12 01:52:09.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 12 01:52:09.991: INFO: stderr: ""
    Jan 12 01:52:09.991: INFO: stdout: "update-demo-nautilus-g28xw update-demo-nautilus-psx5m "
    Jan 12 01:52:09.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-g28xw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 01:52:10.083: INFO: stderr: ""
    Jan 12 01:52:10.083: INFO: stdout: ""
    Jan 12 01:52:10.083: INFO: update-demo-nautilus-g28xw is created but not running
    Jan 12 01:52:15.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 12 01:52:15.168: INFO: stderr: ""
    Jan 12 01:52:15.168: INFO: stdout: "update-demo-nautilus-g28xw update-demo-nautilus-psx5m "
    Jan 12 01:52:15.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-g28xw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 01:52:15.233: INFO: stderr: ""
    Jan 12 01:52:15.233: INFO: stdout: "true"
    Jan 12 01:52:15.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-g28xw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 12 01:52:15.299: INFO: stderr: ""
    Jan 12 01:52:15.299: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 12 01:52:15.299: INFO: validating pod update-demo-nautilus-g28xw
    Jan 12 01:52:15.303: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 12 01:52:15.303: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 12 01:52:15.303: INFO: update-demo-nautilus-g28xw is verified up and running
    Jan 12 01:52:15.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-psx5m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 01:52:15.366: INFO: stderr: ""
    Jan 12 01:52:15.366: INFO: stdout: "true"
    Jan 12 01:52:15.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods update-demo-nautilus-psx5m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 12 01:52:15.430: INFO: stderr: ""
    Jan 12 01:52:15.430: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 12 01:52:15.430: INFO: validating pod update-demo-nautilus-psx5m
    Jan 12 01:52:15.434: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 12 01:52:15.434: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 12 01:52:15.434: INFO: update-demo-nautilus-psx5m is verified up and running
    STEP: using delete to clean up resources 01/12/23 01:52:15.434
    Jan 12 01:52:15.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 delete --grace-period=0 --force -f -'
    Jan 12 01:52:15.509: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 01:52:15.509: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 12 01:52:15.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get rc,svc -l name=update-demo --no-headers'
    Jan 12 01:52:15.594: INFO: stderr: "No resources found in kubectl-7367 namespace.\n"
    Jan 12 01:52:15.594: INFO: stdout: ""
    Jan 12 01:52:15.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-7367 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 12 01:52:15.689: INFO: stderr: ""
    Jan 12 01:52:15.689: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:52:15.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7367" for this suite. 01/12/23 01:52:15.693
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:52:15.712
Jan 12 01:52:15.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename events 01/12/23 01:52:15.713
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:52:15.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:52:15.727
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/12/23 01:52:15.729
STEP: get a list of Events with a label in the current namespace 01/12/23 01:52:15.741
STEP: delete a list of events 01/12/23 01:52:15.744
Jan 12 01:52:15.744: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/12/23 01:52:15.759
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 12 01:52:15.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4024" for this suite. 01/12/23 01:52:15.765
------------------------------
• [0.071 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:52:15.712
    Jan 12 01:52:15.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename events 01/12/23 01:52:15.713
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:52:15.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:52:15.727
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/12/23 01:52:15.729
    STEP: get a list of Events with a label in the current namespace 01/12/23 01:52:15.741
    STEP: delete a list of events 01/12/23 01:52:15.744
    Jan 12 01:52:15.744: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/12/23 01:52:15.759
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:52:15.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4024" for this suite. 01/12/23 01:52:15.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:52:15.785
Jan 12 01:52:15.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename dns 01/12/23 01:52:15.785
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:52:15.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:52:15.799
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/12/23 01:52:15.801
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4291 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4291;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4291 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4291;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4291.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4291.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4291.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4291.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4291.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4291.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4291.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4291.svc;check="$$(dig +notcp +noall +answer +search 171.144.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.144.171_udp@PTR;check="$$(dig +tcp +noall +answer +search 171.144.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.144.171_tcp@PTR;sleep 1; done
 01/12/23 01:52:15.815
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4291 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4291;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4291 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4291;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4291.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4291.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4291.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4291.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4291.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4291.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4291.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4291.svc;check="$$(dig +notcp +noall +answer +search 171.144.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.144.171_udp@PTR;check="$$(dig +tcp +noall +answer +search 171.144.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.144.171_tcp@PTR;sleep 1; done
 01/12/23 01:52:15.816
STEP: creating a pod to probe DNS 01/12/23 01:52:15.816
STEP: submitting the pod to kubernetes 01/12/23 01:52:15.816
Jan 12 01:52:15.851: INFO: Waiting up to 15m0s for pod "dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739" in namespace "dns-4291" to be "running"
Jan 12 01:52:15.854: INFO: Pod "dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739": Phase="Pending", Reason="", readiness=false. Elapsed: 2.499256ms
Jan 12 01:52:17.858: INFO: Pod "dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006989296s
Jan 12 01:52:19.859: INFO: Pod "dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739": Phase="Running", Reason="", readiness=true. Elapsed: 4.007250613s
Jan 12 01:52:19.859: INFO: Pod "dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739" satisfied condition "running"
STEP: retrieving the pod 01/12/23 01:52:19.859
STEP: looking for the results for each expected name from probers 01/12/23 01:52:19.862
Jan 12 01:52:19.866: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.869: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.872: INFO: Unable to read wheezy_udp@dns-test-service.dns-4291 from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.875: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4291 from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.878: INFO: Unable to read wheezy_udp@dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.880: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.883: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.886: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.900: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.903: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.906: INFO: Unable to read jessie_udp@dns-test-service.dns-4291 from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.909: INFO: Unable to read jessie_tcp@dns-test-service.dns-4291 from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.912: INFO: Unable to read jessie_udp@dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.914: INFO: Unable to read jessie_tcp@dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.917: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.919: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
Jan 12 01:52:19.930: INFO: Lookups using dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4291 wheezy_tcp@dns-test-service.dns-4291 wheezy_udp@dns-test-service.dns-4291.svc wheezy_tcp@dns-test-service.dns-4291.svc wheezy_udp@_http._tcp.dns-test-service.dns-4291.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4291.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4291 jessie_tcp@dns-test-service.dns-4291 jessie_udp@dns-test-service.dns-4291.svc jessie_tcp@dns-test-service.dns-4291.svc jessie_udp@_http._tcp.dns-test-service.dns-4291.svc jessie_tcp@_http._tcp.dns-test-service.dns-4291.svc]

Jan 12 01:52:25.000: INFO: DNS probes using dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739 succeeded

STEP: deleting the pod 01/12/23 01:52:25
STEP: deleting the test service 01/12/23 01:52:25.012
STEP: deleting the test headless service 01/12/23 01:52:25.041
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 01:52:25.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4291" for this suite. 01/12/23 01:52:25.056
------------------------------
• [SLOW TEST] [9.313 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:52:15.785
    Jan 12 01:52:15.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename dns 01/12/23 01:52:15.785
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:52:15.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:52:15.799
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/12/23 01:52:15.801
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4291 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4291;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4291 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4291;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4291.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4291.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4291.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4291.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4291.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4291.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4291.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4291.svc;check="$$(dig +notcp +noall +answer +search 171.144.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.144.171_udp@PTR;check="$$(dig +tcp +noall +answer +search 171.144.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.144.171_tcp@PTR;sleep 1; done
     01/12/23 01:52:15.815
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4291 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4291;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4291 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4291;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4291.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4291.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4291.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4291.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4291.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4291.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4291.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4291.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4291.svc;check="$$(dig +notcp +noall +answer +search 171.144.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.144.171_udp@PTR;check="$$(dig +tcp +noall +answer +search 171.144.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.144.171_tcp@PTR;sleep 1; done
     01/12/23 01:52:15.816
    STEP: creating a pod to probe DNS 01/12/23 01:52:15.816
    STEP: submitting the pod to kubernetes 01/12/23 01:52:15.816
    Jan 12 01:52:15.851: INFO: Waiting up to 15m0s for pod "dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739" in namespace "dns-4291" to be "running"
    Jan 12 01:52:15.854: INFO: Pod "dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739": Phase="Pending", Reason="", readiness=false. Elapsed: 2.499256ms
    Jan 12 01:52:17.858: INFO: Pod "dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006989296s
    Jan 12 01:52:19.859: INFO: Pod "dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739": Phase="Running", Reason="", readiness=true. Elapsed: 4.007250613s
    Jan 12 01:52:19.859: INFO: Pod "dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 01:52:19.859
    STEP: looking for the results for each expected name from probers 01/12/23 01:52:19.862
    Jan 12 01:52:19.866: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.869: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.872: INFO: Unable to read wheezy_udp@dns-test-service.dns-4291 from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.875: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4291 from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.878: INFO: Unable to read wheezy_udp@dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.880: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.883: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.886: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.900: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.903: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.906: INFO: Unable to read jessie_udp@dns-test-service.dns-4291 from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.909: INFO: Unable to read jessie_tcp@dns-test-service.dns-4291 from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.912: INFO: Unable to read jessie_udp@dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.914: INFO: Unable to read jessie_tcp@dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.917: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.919: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4291.svc from pod dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739: the server could not find the requested resource (get pods dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739)
    Jan 12 01:52:19.930: INFO: Lookups using dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4291 wheezy_tcp@dns-test-service.dns-4291 wheezy_udp@dns-test-service.dns-4291.svc wheezy_tcp@dns-test-service.dns-4291.svc wheezy_udp@_http._tcp.dns-test-service.dns-4291.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4291.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4291 jessie_tcp@dns-test-service.dns-4291 jessie_udp@dns-test-service.dns-4291.svc jessie_tcp@dns-test-service.dns-4291.svc jessie_udp@_http._tcp.dns-test-service.dns-4291.svc jessie_tcp@_http._tcp.dns-test-service.dns-4291.svc]

    Jan 12 01:52:25.000: INFO: DNS probes using dns-4291/dns-test-fc7786d3-7e5b-4ed8-88cf-8a2f81c1a739 succeeded

    STEP: deleting the pod 01/12/23 01:52:25
    STEP: deleting the test service 01/12/23 01:52:25.012
    STEP: deleting the test headless service 01/12/23 01:52:25.041
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:52:25.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4291" for this suite. 01/12/23 01:52:25.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:52:25.115
Jan 12 01:52:25.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename disruption 01/12/23 01:52:25.125
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:52:25.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:52:25.139
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 01/12/23 01:52:25.146
STEP: Waiting for all pods to be running 01/12/23 01:52:27.289
Jan 12 01:52:27.292: INFO: running pods: 0 < 3
Jan 12 01:52:29.296: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 12 01:52:31.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8651" for this suite. 01/12/23 01:52:31.306
------------------------------
• [SLOW TEST] [6.207 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:52:25.115
    Jan 12 01:52:25.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename disruption 01/12/23 01:52:25.125
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:52:25.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:52:25.139
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 01/12/23 01:52:25.146
    STEP: Waiting for all pods to be running 01/12/23 01:52:27.289
    Jan 12 01:52:27.292: INFO: running pods: 0 < 3
    Jan 12 01:52:29.296: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:52:31.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8651" for this suite. 01/12/23 01:52:31.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:52:31.33
Jan 12 01:52:31.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename cronjob 01/12/23 01:52:31.33
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:52:31.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:52:31.344
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/12/23 01:52:31.348
STEP: Ensuring a job is scheduled 01/12/23 01:52:31.352
STEP: Ensuring exactly one is scheduled 01/12/23 01:53:01.356
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/12/23 01:53:01.358
STEP: Ensuring the job is replaced with a new one 01/12/23 01:53:01.36
STEP: Removing cronjob 01/12/23 01:54:01.364
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 12 01:54:01.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7402" for this suite. 01/12/23 01:54:01.375
------------------------------
• [SLOW TEST] [90.092 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:52:31.33
    Jan 12 01:52:31.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename cronjob 01/12/23 01:52:31.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:52:31.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:52:31.344
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/12/23 01:52:31.348
    STEP: Ensuring a job is scheduled 01/12/23 01:52:31.352
    STEP: Ensuring exactly one is scheduled 01/12/23 01:53:01.356
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/12/23 01:53:01.358
    STEP: Ensuring the job is replaced with a new one 01/12/23 01:53:01.36
    STEP: Removing cronjob 01/12/23 01:54:01.364
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:54:01.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7402" for this suite. 01/12/23 01:54:01.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:54:01.422
Jan 12 01:54:01.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 01:54:01.423
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:01.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:01.437
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-5733 01/12/23 01:54:01.439
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5733 to expose endpoints map[] 01/12/23 01:54:01.453
Jan 12 01:54:01.456: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 12 01:54:02.462: INFO: successfully validated that service multi-endpoint-test in namespace services-5733 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5733 01/12/23 01:54:02.462
Jan 12 01:54:02.494: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5733" to be "running and ready"
Jan 12 01:54:02.497: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.785011ms
Jan 12 01:54:02.497: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:54:04.501: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006254871s
Jan 12 01:54:04.501: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 12 01:54:04.501: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5733 to expose endpoints map[pod1:[100]] 01/12/23 01:54:04.504
Jan 12 01:54:04.520: INFO: successfully validated that service multi-endpoint-test in namespace services-5733 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5733 01/12/23 01:54:04.52
Jan 12 01:54:04.550: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5733" to be "running and ready"
Jan 12 01:54:04.553: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.578946ms
Jan 12 01:54:04.553: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:54:06.559: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008799234s
Jan 12 01:54:06.559: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:54:08.557: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.006969159s
Jan 12 01:54:08.557: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 12 01:54:08.557: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5733 to expose endpoints map[pod1:[100] pod2:[101]] 01/12/23 01:54:08.559
Jan 12 01:54:08.569: INFO: successfully validated that service multi-endpoint-test in namespace services-5733 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/12/23 01:54:08.569
Jan 12 01:54:08.569: INFO: Creating new exec pod
Jan 12 01:54:08.599: INFO: Waiting up to 5m0s for pod "execpod758kf" in namespace "services-5733" to be "running"
Jan 12 01:54:08.602: INFO: Pod "execpod758kf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.767231ms
Jan 12 01:54:10.606: INFO: Pod "execpod758kf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006668951s
Jan 12 01:54:12.607: INFO: Pod "execpod758kf": Phase="Running", Reason="", readiness=true. Elapsed: 4.007255046s
Jan 12 01:54:12.607: INFO: Pod "execpod758kf" satisfied condition "running"
Jan 12 01:54:13.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5733 exec execpod758kf -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jan 12 01:54:13.815: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 12 01:54:13.815: INFO: stdout: ""
Jan 12 01:54:13.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5733 exec execpod758kf -- /bin/sh -x -c nc -v -z -w 2 172.19.140.59 80'
Jan 12 01:54:14.043: INFO: stderr: "+ nc -v -z -w 2 172.19.140.59 80\nConnection to 172.19.140.59 80 port [tcp/http] succeeded!\n"
Jan 12 01:54:14.043: INFO: stdout: ""
Jan 12 01:54:14.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5733 exec execpod758kf -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jan 12 01:54:14.245: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 12 01:54:14.245: INFO: stdout: ""
Jan 12 01:54:14.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5733 exec execpod758kf -- /bin/sh -x -c nc -v -z -w 2 172.19.140.59 81'
Jan 12 01:54:14.441: INFO: stderr: "+ nc -v -z -w 2 172.19.140.59 81\nConnection to 172.19.140.59 81 port [tcp/*] succeeded!\n"
Jan 12 01:54:14.441: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-5733 01/12/23 01:54:14.441
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5733 to expose endpoints map[pod2:[101]] 01/12/23 01:54:14.459
Jan 12 01:54:14.472: INFO: successfully validated that service multi-endpoint-test in namespace services-5733 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5733 01/12/23 01:54:14.472
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5733 to expose endpoints map[] 01/12/23 01:54:14.488
Jan 12 01:54:15.509: INFO: successfully validated that service multi-endpoint-test in namespace services-5733 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 01:54:15.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5733" for this suite. 01/12/23 01:54:15.537
------------------------------
• [SLOW TEST] [14.131 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:54:01.422
    Jan 12 01:54:01.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 01:54:01.423
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:01.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:01.437
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-5733 01/12/23 01:54:01.439
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5733 to expose endpoints map[] 01/12/23 01:54:01.453
    Jan 12 01:54:01.456: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 12 01:54:02.462: INFO: successfully validated that service multi-endpoint-test in namespace services-5733 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5733 01/12/23 01:54:02.462
    Jan 12 01:54:02.494: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5733" to be "running and ready"
    Jan 12 01:54:02.497: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.785011ms
    Jan 12 01:54:02.497: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:54:04.501: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006254871s
    Jan 12 01:54:04.501: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 12 01:54:04.501: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5733 to expose endpoints map[pod1:[100]] 01/12/23 01:54:04.504
    Jan 12 01:54:04.520: INFO: successfully validated that service multi-endpoint-test in namespace services-5733 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-5733 01/12/23 01:54:04.52
    Jan 12 01:54:04.550: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5733" to be "running and ready"
    Jan 12 01:54:04.553: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.578946ms
    Jan 12 01:54:04.553: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:54:06.559: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008799234s
    Jan 12 01:54:06.559: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:54:08.557: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.006969159s
    Jan 12 01:54:08.557: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 12 01:54:08.557: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5733 to expose endpoints map[pod1:[100] pod2:[101]] 01/12/23 01:54:08.559
    Jan 12 01:54:08.569: INFO: successfully validated that service multi-endpoint-test in namespace services-5733 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/12/23 01:54:08.569
    Jan 12 01:54:08.569: INFO: Creating new exec pod
    Jan 12 01:54:08.599: INFO: Waiting up to 5m0s for pod "execpod758kf" in namespace "services-5733" to be "running"
    Jan 12 01:54:08.602: INFO: Pod "execpod758kf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.767231ms
    Jan 12 01:54:10.606: INFO: Pod "execpod758kf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006668951s
    Jan 12 01:54:12.607: INFO: Pod "execpod758kf": Phase="Running", Reason="", readiness=true. Elapsed: 4.007255046s
    Jan 12 01:54:12.607: INFO: Pod "execpod758kf" satisfied condition "running"
    Jan 12 01:54:13.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5733 exec execpod758kf -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jan 12 01:54:13.815: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 12 01:54:13.815: INFO: stdout: ""
    Jan 12 01:54:13.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5733 exec execpod758kf -- /bin/sh -x -c nc -v -z -w 2 172.19.140.59 80'
    Jan 12 01:54:14.043: INFO: stderr: "+ nc -v -z -w 2 172.19.140.59 80\nConnection to 172.19.140.59 80 port [tcp/http] succeeded!\n"
    Jan 12 01:54:14.043: INFO: stdout: ""
    Jan 12 01:54:14.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5733 exec execpod758kf -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jan 12 01:54:14.245: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 12 01:54:14.245: INFO: stdout: ""
    Jan 12 01:54:14.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-5733 exec execpod758kf -- /bin/sh -x -c nc -v -z -w 2 172.19.140.59 81'
    Jan 12 01:54:14.441: INFO: stderr: "+ nc -v -z -w 2 172.19.140.59 81\nConnection to 172.19.140.59 81 port [tcp/*] succeeded!\n"
    Jan 12 01:54:14.441: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-5733 01/12/23 01:54:14.441
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5733 to expose endpoints map[pod2:[101]] 01/12/23 01:54:14.459
    Jan 12 01:54:14.472: INFO: successfully validated that service multi-endpoint-test in namespace services-5733 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-5733 01/12/23 01:54:14.472
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5733 to expose endpoints map[] 01/12/23 01:54:14.488
    Jan 12 01:54:15.509: INFO: successfully validated that service multi-endpoint-test in namespace services-5733 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:54:15.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5733" for this suite. 01/12/23 01:54:15.537
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:54:15.557
Jan 12 01:54:15.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 01:54:15.558
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:15.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:15.578
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 01:54:15.593
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:54:16.14
STEP: Deploying the webhook pod 01/12/23 01:54:16.148
STEP: Wait for the deployment to be ready 01/12/23 01:54:16.194
Jan 12 01:54:16.199: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 12 01:54:18.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 54, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 54, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 54, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 54, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 01:54:20.212
STEP: Verifying the service has paired with the endpoint 01/12/23 01:54:20.225
Jan 12 01:54:21.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 01/12/23 01:54:21.229
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/12/23 01:54:21.246
STEP: Creating a configMap that should not be mutated 01/12/23 01:54:21.254
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/12/23 01:54:21.266
STEP: Creating a configMap that should be mutated 01/12/23 01:54:21.275
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:54:21.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3650" for this suite. 01/12/23 01:54:21.347
STEP: Destroying namespace "webhook-3650-markers" for this suite. 01/12/23 01:54:21.409
------------------------------
• [SLOW TEST] [5.915 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:54:15.557
    Jan 12 01:54:15.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 01:54:15.558
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:15.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:15.578
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 01:54:15.593
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:54:16.14
    STEP: Deploying the webhook pod 01/12/23 01:54:16.148
    STEP: Wait for the deployment to be ready 01/12/23 01:54:16.194
    Jan 12 01:54:16.199: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 12 01:54:18.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 54, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 54, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 54, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 54, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 01:54:20.212
    STEP: Verifying the service has paired with the endpoint 01/12/23 01:54:20.225
    Jan 12 01:54:21.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 01/12/23 01:54:21.229
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/12/23 01:54:21.246
    STEP: Creating a configMap that should not be mutated 01/12/23 01:54:21.254
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/12/23 01:54:21.266
    STEP: Creating a configMap that should be mutated 01/12/23 01:54:21.275
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:54:21.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3650" for this suite. 01/12/23 01:54:21.347
    STEP: Destroying namespace "webhook-3650-markers" for this suite. 01/12/23 01:54:21.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:54:21.472
Jan 12 01:54:21.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 01:54:21.473
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:21.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:21.487
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 01:54:21.506
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:54:21.824
STEP: Deploying the webhook pod 01/12/23 01:54:21.831
STEP: Wait for the deployment to be ready 01/12/23 01:54:21.87
Jan 12 01:54:21.878: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 12 01:54:23.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 54, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 54, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 54, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 54, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 01:54:25.89
STEP: Verifying the service has paired with the endpoint 01/12/23 01:54:25.906
Jan 12 01:54:26.907: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jan 12 01:54:26.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6904-crds.webhook.example.com via the AdmissionRegistration API 01/12/23 01:54:27.423
STEP: Creating a custom resource while v1 is storage version 01/12/23 01:54:27.439
STEP: Patching Custom Resource Definition to set v2 as storage 01/12/23 01:54:29.51
STEP: Patching the custom resource while v2 is storage version 01/12/23 01:54:29.53
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:54:30.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8992" for this suite. 01/12/23 01:54:30.151
STEP: Destroying namespace "webhook-8992-markers" for this suite. 01/12/23 01:54:30.186
------------------------------
• [SLOW TEST] [8.803 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:54:21.472
    Jan 12 01:54:21.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 01:54:21.473
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:21.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:21.487
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 01:54:21.506
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:54:21.824
    STEP: Deploying the webhook pod 01/12/23 01:54:21.831
    STEP: Wait for the deployment to be ready 01/12/23 01:54:21.87
    Jan 12 01:54:21.878: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 12 01:54:23.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 54, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 54, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 54, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 54, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 01:54:25.89
    STEP: Verifying the service has paired with the endpoint 01/12/23 01:54:25.906
    Jan 12 01:54:26.907: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jan 12 01:54:26.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6904-crds.webhook.example.com via the AdmissionRegistration API 01/12/23 01:54:27.423
    STEP: Creating a custom resource while v1 is storage version 01/12/23 01:54:27.439
    STEP: Patching Custom Resource Definition to set v2 as storage 01/12/23 01:54:29.51
    STEP: Patching the custom resource while v2 is storage version 01/12/23 01:54:29.53
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:54:30.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8992" for this suite. 01/12/23 01:54:30.151
    STEP: Destroying namespace "webhook-8992-markers" for this suite. 01/12/23 01:54:30.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:54:30.28
Jan 12 01:54:30.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename svcaccounts 01/12/23 01:54:30.282
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:30.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:30.3
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Jan 12 01:54:30.387: INFO: created pod pod-service-account-defaultsa
Jan 12 01:54:30.387: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 12 01:54:30.555: INFO: created pod pod-service-account-mountsa
Jan 12 01:54:30.555: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 12 01:54:30.680: INFO: created pod pod-service-account-nomountsa
Jan 12 01:54:30.680: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 12 01:54:30.728: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 12 01:54:30.728: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 12 01:54:30.854: INFO: created pod pod-service-account-mountsa-mountspec
Jan 12 01:54:30.854: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 12 01:54:30.905: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 12 01:54:30.905: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 12 01:54:30.996: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 12 01:54:30.996: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 12 01:54:31.048: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 12 01:54:31.048: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 12 01:54:31.101: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 12 01:54:31.101: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 12 01:54:31.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6740" for this suite. 01/12/23 01:54:31.105
------------------------------
• [0.842 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:54:30.28
    Jan 12 01:54:30.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename svcaccounts 01/12/23 01:54:30.282
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:30.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:30.3
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Jan 12 01:54:30.387: INFO: created pod pod-service-account-defaultsa
    Jan 12 01:54:30.387: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 12 01:54:30.555: INFO: created pod pod-service-account-mountsa
    Jan 12 01:54:30.555: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 12 01:54:30.680: INFO: created pod pod-service-account-nomountsa
    Jan 12 01:54:30.680: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 12 01:54:30.728: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 12 01:54:30.728: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 12 01:54:30.854: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 12 01:54:30.854: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 12 01:54:30.905: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 12 01:54:30.905: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 12 01:54:30.996: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 12 01:54:30.996: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 12 01:54:31.048: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 12 01:54:31.048: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 12 01:54:31.101: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 12 01:54:31.101: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:54:31.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6740" for this suite. 01/12/23 01:54:31.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:54:31.123
Jan 12 01:54:31.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename replication-controller 01/12/23 01:54:31.124
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:31.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:31.138
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 01/12/23 01:54:31.142
STEP: waiting for RC to be added 01/12/23 01:54:31.147
STEP: waiting for available Replicas 01/12/23 01:54:31.147
STEP: patching ReplicationController 01/12/23 01:54:36.966
STEP: waiting for RC to be modified 01/12/23 01:54:36.979
STEP: patching ReplicationController status 01/12/23 01:54:36.979
STEP: waiting for RC to be modified 01/12/23 01:54:36.995
STEP: waiting for available Replicas 01/12/23 01:54:36.995
STEP: fetching ReplicationController status 01/12/23 01:54:36.996
STEP: patching ReplicationController scale 01/12/23 01:54:36.998
STEP: waiting for RC to be modified 01/12/23 01:54:37.005
STEP: waiting for ReplicationController's scale to be the max amount 01/12/23 01:54:37.005
STEP: fetching ReplicationController; ensuring that it's patched 01/12/23 01:54:39.178
STEP: updating ReplicationController status 01/12/23 01:54:39.18
STEP: waiting for RC to be modified 01/12/23 01:54:39.186
STEP: listing all ReplicationControllers 01/12/23 01:54:39.186
STEP: checking that ReplicationController has expected values 01/12/23 01:54:39.188
STEP: deleting ReplicationControllers by collection 01/12/23 01:54:39.188
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/12/23 01:54:39.194
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 12 01:54:39.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7849" for this suite. 01/12/23 01:54:39.219
------------------------------
• [SLOW TEST] [8.109 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:54:31.123
    Jan 12 01:54:31.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename replication-controller 01/12/23 01:54:31.124
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:31.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:31.138
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 01/12/23 01:54:31.142
    STEP: waiting for RC to be added 01/12/23 01:54:31.147
    STEP: waiting for available Replicas 01/12/23 01:54:31.147
    STEP: patching ReplicationController 01/12/23 01:54:36.966
    STEP: waiting for RC to be modified 01/12/23 01:54:36.979
    STEP: patching ReplicationController status 01/12/23 01:54:36.979
    STEP: waiting for RC to be modified 01/12/23 01:54:36.995
    STEP: waiting for available Replicas 01/12/23 01:54:36.995
    STEP: fetching ReplicationController status 01/12/23 01:54:36.996
    STEP: patching ReplicationController scale 01/12/23 01:54:36.998
    STEP: waiting for RC to be modified 01/12/23 01:54:37.005
    STEP: waiting for ReplicationController's scale to be the max amount 01/12/23 01:54:37.005
    STEP: fetching ReplicationController; ensuring that it's patched 01/12/23 01:54:39.178
    STEP: updating ReplicationController status 01/12/23 01:54:39.18
    STEP: waiting for RC to be modified 01/12/23 01:54:39.186
    STEP: listing all ReplicationControllers 01/12/23 01:54:39.186
    STEP: checking that ReplicationController has expected values 01/12/23 01:54:39.188
    STEP: deleting ReplicationControllers by collection 01/12/23 01:54:39.188
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/12/23 01:54:39.194
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:54:39.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7849" for this suite. 01/12/23 01:54:39.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:54:39.236
Jan 12 01:54:39.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename endpointslicemirroring 01/12/23 01:54:39.236
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:39.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:39.248
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/12/23 01:54:39.26
Jan 12 01:54:39.265: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/12/23 01:54:41.268
Jan 12 01:54:41.275: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/12/23 01:54:43.279
Jan 12 01:54:43.287: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jan 12 01:54:45.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-3521" for this suite. 01/12/23 01:54:45.294
------------------------------
• [SLOW TEST] [6.072 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:54:39.236
    Jan 12 01:54:39.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename endpointslicemirroring 01/12/23 01:54:39.236
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:39.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:39.248
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/12/23 01:54:39.26
    Jan 12 01:54:39.265: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/12/23 01:54:41.268
    Jan 12 01:54:41.275: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/12/23 01:54:43.279
    Jan 12 01:54:43.287: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:54:45.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-3521" for this suite. 01/12/23 01:54:45.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:54:45.31
Jan 12 01:54:45.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename deployment 01/12/23 01:54:45.311
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:45.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:45.325
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 12 01:54:45.328: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 12 01:54:45.333: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 12 01:54:50.338: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/12/23 01:54:50.338
Jan 12 01:54:50.338: INFO: Creating deployment "test-rolling-update-deployment"
Jan 12 01:54:50.447: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 12 01:54:50.451: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 12 01:54:52.457: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 12 01:54:52.459: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 54, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 54, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 54, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 54, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 01:54:54.463: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 01:54:54.469: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9966  9eb579d2-e5c4-49f3-a191-5a3872edc9fd 20178206 1 2023-01-12 01:54:50 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-12 01:54:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:54:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00232a728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-12 01:54:50 +0000 UTC,LastTransitionTime:2023-01-12 01:54:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-12 01:54:52 +0000 UTC,LastTransitionTime:2023-01-12 01:54:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 12 01:54:54.471: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-9966  90b3e6f6-8812-453a-a349-2a524ee9b2e3 20178197 1 2023-01-12 01:54:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9eb579d2-e5c4-49f3-a191-5a3872edc9fd 0xc00232ac27 0xc00232ac28}] [] [{kube-controller-manager Update apps/v1 2023-01-12 01:54:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9eb579d2-e5c4-49f3-a191-5a3872edc9fd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:54:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00232acd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 12 01:54:54.471: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 12 01:54:54.471: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9966  88ac5769-0c47-42d2-abe3-af3818c99495 20178205 2 2023-01-12 01:54:45 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9eb579d2-e5c4-49f3-a191-5a3872edc9fd 0xc00232aaf7 0xc00232aaf8}] [] [{e2e.test Update apps/v1 2023-01-12 01:54:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:54:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9eb579d2-e5c4-49f3-a191-5a3872edc9fd\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:54:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00232abb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 12 01:54:54.474: INFO: Pod "test-rolling-update-deployment-7549d9f46d-4t75x" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-4t75x test-rolling-update-deployment-7549d9f46d- deployment-9966  7b999c76-6fa3-42b8-8058-a41454ad4867 20178196 0 2023-01-12 01:54:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:1b567a32485a2bd0746cad4728858e2fe04295d76b41f9d3c19c350b8ffb3ee5 cni.projectcalico.org/podIP:172.21.88.186/32 cni.projectcalico.org/podIPs:172.21.88.186/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.186"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "calico",
    "ips": [
        "172.21.88.186"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 90b3e6f6-8812-453a-a349-2a524ee9b2e3 0xc00543f757 0xc00543f758}] [] [{kube-controller-manager Update v1 2023-01-12 01:54:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90b3e6f6-8812-453a-a349-2a524ee9b2e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:54:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:54:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:54:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fm6tf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fm6tf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:54:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:54:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:54:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:54:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.186,StartTime:2023-01-12 01:54:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:54:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:robin://c5df9047e4ea959b9ce3433817e2eb1619a7369debe677efc8c7288171089cae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 01:54:54.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9966" for this suite. 01/12/23 01:54:54.477
------------------------------
• [SLOW TEST] [9.181 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:54:45.31
    Jan 12 01:54:45.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename deployment 01/12/23 01:54:45.311
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:45.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:45.325
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 12 01:54:45.328: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 12 01:54:45.333: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 12 01:54:50.338: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/12/23 01:54:50.338
    Jan 12 01:54:50.338: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 12 01:54:50.447: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 12 01:54:50.451: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 12 01:54:52.457: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 12 01:54:52.459: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 54, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 54, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 54, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 54, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 01:54:54.463: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 01:54:54.469: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9966  9eb579d2-e5c4-49f3-a191-5a3872edc9fd 20178206 1 2023-01-12 01:54:50 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-12 01:54:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:54:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00232a728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-12 01:54:50 +0000 UTC,LastTransitionTime:2023-01-12 01:54:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-12 01:54:52 +0000 UTC,LastTransitionTime:2023-01-12 01:54:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 12 01:54:54.471: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-9966  90b3e6f6-8812-453a-a349-2a524ee9b2e3 20178197 1 2023-01-12 01:54:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9eb579d2-e5c4-49f3-a191-5a3872edc9fd 0xc00232ac27 0xc00232ac28}] [] [{kube-controller-manager Update apps/v1 2023-01-12 01:54:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9eb579d2-e5c4-49f3-a191-5a3872edc9fd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:54:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00232acd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 01:54:54.471: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 12 01:54:54.471: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9966  88ac5769-0c47-42d2-abe3-af3818c99495 20178205 2 2023-01-12 01:54:45 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9eb579d2-e5c4-49f3-a191-5a3872edc9fd 0xc00232aaf7 0xc00232aaf8}] [] [{e2e.test Update apps/v1 2023-01-12 01:54:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:54:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9eb579d2-e5c4-49f3-a191-5a3872edc9fd\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-12 01:54:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00232abb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 01:54:54.474: INFO: Pod "test-rolling-update-deployment-7549d9f46d-4t75x" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-4t75x test-rolling-update-deployment-7549d9f46d- deployment-9966  7b999c76-6fa3-42b8-8058-a41454ad4867 20178196 0 2023-01-12 01:54:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:1b567a32485a2bd0746cad4728858e2fe04295d76b41f9d3c19c350b8ffb3ee5 cni.projectcalico.org/podIP:172.21.88.186/32 cni.projectcalico.org/podIPs:172.21.88.186/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.186"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "calico",
        "ips": [
            "172.21.88.186"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 90b3e6f6-8812-453a-a349-2a524ee9b2e3 0xc00543f757 0xc00543f758}] [] [{kube-controller-manager Update v1 2023-01-12 01:54:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90b3e6f6-8812-453a-a349-2a524ee9b2e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-12 01:54:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-01-12 01:54:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-12 01:54:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.88.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fm6tf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fm6tf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:eqx04-flash06,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:54:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:54:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:54:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 01:54:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.9.40.106,PodIP:172.21.88.186,StartTime:2023-01-12 01:54:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 01:54:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:robin://c5df9047e4ea959b9ce3433817e2eb1619a7369debe677efc8c7288171089cae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.88.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:54:54.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9966" for this suite. 01/12/23 01:54:54.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:54:54.492
Jan 12 01:54:54.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 01:54:54.493
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:54.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:54.505
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 01/12/23 01:54:54.507
Jan 12 01:54:54.538: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8" in namespace "downward-api-9726" to be "Succeeded or Failed"
Jan 12 01:54:54.541: INFO: Pod "downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.350847ms
Jan 12 01:54:56.545: INFO: Pod "downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007124362s
Jan 12 01:54:58.550: INFO: Pod "downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012469806s
Jan 12 01:55:00.545: INFO: Pod "downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007469915s
STEP: Saw pod success 01/12/23 01:55:00.546
Jan 12 01:55:00.546: INFO: Pod "downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8" satisfied condition "Succeeded or Failed"
Jan 12 01:55:00.548: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8 container client-container: <nil>
STEP: delete the pod 01/12/23 01:55:00.564
Jan 12 01:55:00.575: INFO: Waiting for pod downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8 to disappear
Jan 12 01:55:00.577: INFO: Pod downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 01:55:00.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9726" for this suite. 01/12/23 01:55:00.581
------------------------------
• [SLOW TEST] [6.498 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:54:54.492
    Jan 12 01:54:54.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 01:54:54.493
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:54:54.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:54:54.505
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 01/12/23 01:54:54.507
    Jan 12 01:54:54.538: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8" in namespace "downward-api-9726" to be "Succeeded or Failed"
    Jan 12 01:54:54.541: INFO: Pod "downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.350847ms
    Jan 12 01:54:56.545: INFO: Pod "downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007124362s
    Jan 12 01:54:58.550: INFO: Pod "downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012469806s
    Jan 12 01:55:00.545: INFO: Pod "downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007469915s
    STEP: Saw pod success 01/12/23 01:55:00.546
    Jan 12 01:55:00.546: INFO: Pod "downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8" satisfied condition "Succeeded or Failed"
    Jan 12 01:55:00.548: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8 container client-container: <nil>
    STEP: delete the pod 01/12/23 01:55:00.564
    Jan 12 01:55:00.575: INFO: Waiting for pod downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8 to disappear
    Jan 12 01:55:00.577: INFO: Pod downwardapi-volume-b18ebae6-740b-41c8-a2a2-0159b80cd8f8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:55:00.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9726" for this suite. 01/12/23 01:55:00.581
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:55:00.992
Jan 12 01:55:00.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename subpath 01/12/23 01:55:00.993
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:55:01.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:55:01.005
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/12/23 01:55:01.007
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-6dfl 01/12/23 01:55:01.015
STEP: Creating a pod to test atomic-volume-subpath 01/12/23 01:55:01.015
Jan 12 01:55:01.159: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6dfl" in namespace "subpath-4841" to be "Succeeded or Failed"
Jan 12 01:55:01.161: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075676ms
Jan 12 01:55:03.164: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005323958s
Jan 12 01:55:05.164: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 4.005122736s
Jan 12 01:55:07.164: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 6.005110096s
Jan 12 01:55:09.164: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 8.005227711s
Jan 12 01:55:11.163: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 10.004645927s
Jan 12 01:55:13.164: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 12.004955015s
Jan 12 01:55:15.165: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 14.006581947s
Jan 12 01:55:17.164: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 16.005310684s
Jan 12 01:55:19.165: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 18.005916463s
Jan 12 01:55:21.165: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 20.005777691s
Jan 12 01:55:23.165: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 22.006453789s
Jan 12 01:55:25.164: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=false. Elapsed: 24.005226467s
Jan 12 01:55:27.165: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.006075546s
STEP: Saw pod success 01/12/23 01:55:27.165
Jan 12 01:55:27.165: INFO: Pod "pod-subpath-test-configmap-6dfl" satisfied condition "Succeeded or Failed"
Jan 12 01:55:27.168: INFO: Trying to get logs from node eqx04-flash06 pod pod-subpath-test-configmap-6dfl container test-container-subpath-configmap-6dfl: <nil>
STEP: delete the pod 01/12/23 01:55:27.177
Jan 12 01:55:27.191: INFO: Waiting for pod pod-subpath-test-configmap-6dfl to disappear
Jan 12 01:55:27.193: INFO: Pod pod-subpath-test-configmap-6dfl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6dfl 01/12/23 01:55:27.193
Jan 12 01:55:27.193: INFO: Deleting pod "pod-subpath-test-configmap-6dfl" in namespace "subpath-4841"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 12 01:55:27.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4841" for this suite. 01/12/23 01:55:27.199
------------------------------
• [SLOW TEST] [26.231 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:55:00.992
    Jan 12 01:55:00.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename subpath 01/12/23 01:55:00.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:55:01.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:55:01.005
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/12/23 01:55:01.007
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-6dfl 01/12/23 01:55:01.015
    STEP: Creating a pod to test atomic-volume-subpath 01/12/23 01:55:01.015
    Jan 12 01:55:01.159: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6dfl" in namespace "subpath-4841" to be "Succeeded or Failed"
    Jan 12 01:55:01.161: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075676ms
    Jan 12 01:55:03.164: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005323958s
    Jan 12 01:55:05.164: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 4.005122736s
    Jan 12 01:55:07.164: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 6.005110096s
    Jan 12 01:55:09.164: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 8.005227711s
    Jan 12 01:55:11.163: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 10.004645927s
    Jan 12 01:55:13.164: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 12.004955015s
    Jan 12 01:55:15.165: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 14.006581947s
    Jan 12 01:55:17.164: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 16.005310684s
    Jan 12 01:55:19.165: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 18.005916463s
    Jan 12 01:55:21.165: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 20.005777691s
    Jan 12 01:55:23.165: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=true. Elapsed: 22.006453789s
    Jan 12 01:55:25.164: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Running", Reason="", readiness=false. Elapsed: 24.005226467s
    Jan 12 01:55:27.165: INFO: Pod "pod-subpath-test-configmap-6dfl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.006075546s
    STEP: Saw pod success 01/12/23 01:55:27.165
    Jan 12 01:55:27.165: INFO: Pod "pod-subpath-test-configmap-6dfl" satisfied condition "Succeeded or Failed"
    Jan 12 01:55:27.168: INFO: Trying to get logs from node eqx04-flash06 pod pod-subpath-test-configmap-6dfl container test-container-subpath-configmap-6dfl: <nil>
    STEP: delete the pod 01/12/23 01:55:27.177
    Jan 12 01:55:27.191: INFO: Waiting for pod pod-subpath-test-configmap-6dfl to disappear
    Jan 12 01:55:27.193: INFO: Pod pod-subpath-test-configmap-6dfl no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-6dfl 01/12/23 01:55:27.193
    Jan 12 01:55:27.193: INFO: Deleting pod "pod-subpath-test-configmap-6dfl" in namespace "subpath-4841"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:55:27.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4841" for this suite. 01/12/23 01:55:27.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:55:27.226
Jan 12 01:55:27.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename namespaces 01/12/23 01:55:27.227
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:55:27.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:55:27.242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 01/12/23 01:55:27.245
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:55:27.256
STEP: Creating a service in the namespace 01/12/23 01:55:27.258
STEP: Deleting the namespace 01/12/23 01:55:27.268
STEP: Waiting for the namespace to be removed. 01/12/23 01:55:27.338
STEP: Recreating the namespace 01/12/23 01:55:33.342
STEP: Verifying there is no service in the namespace 01/12/23 01:55:33.354
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:55:33.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3642" for this suite. 01/12/23 01:55:33.36
STEP: Destroying namespace "nsdeletetest-2115" for this suite. 01/12/23 01:55:33.372
Jan 12 01:55:33.375: INFO: Namespace nsdeletetest-2115 was already deleted
STEP: Destroying namespace "nsdeletetest-1034" for this suite. 01/12/23 01:55:33.375
------------------------------
• [SLOW TEST] [6.161 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:55:27.226
    Jan 12 01:55:27.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename namespaces 01/12/23 01:55:27.227
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:55:27.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:55:27.242
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 01/12/23 01:55:27.245
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:55:27.256
    STEP: Creating a service in the namespace 01/12/23 01:55:27.258
    STEP: Deleting the namespace 01/12/23 01:55:27.268
    STEP: Waiting for the namespace to be removed. 01/12/23 01:55:27.338
    STEP: Recreating the namespace 01/12/23 01:55:33.342
    STEP: Verifying there is no service in the namespace 01/12/23 01:55:33.354
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:55:33.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3642" for this suite. 01/12/23 01:55:33.36
    STEP: Destroying namespace "nsdeletetest-2115" for this suite. 01/12/23 01:55:33.372
    Jan 12 01:55:33.375: INFO: Namespace nsdeletetest-2115 was already deleted
    STEP: Destroying namespace "nsdeletetest-1034" for this suite. 01/12/23 01:55:33.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:55:33.389
Jan 12 01:55:33.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename secrets 01/12/23 01:55:33.389
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:55:33.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:55:33.403
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 01:55:33.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7361" for this suite. 01/12/23 01:55:33.438
------------------------------
• [0.064 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:55:33.389
    Jan 12 01:55:33.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename secrets 01/12/23 01:55:33.389
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:55:33.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:55:33.403
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:55:33.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7361" for this suite. 01/12/23 01:55:33.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:55:33.453
Jan 12 01:55:33.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename init-container 01/12/23 01:55:33.454
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:55:33.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:55:33.467
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 01/12/23 01:55:33.47
Jan 12 01:55:33.470: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:55:39.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2030" for this suite. 01/12/23 01:55:39.782
------------------------------
• [SLOW TEST] [6.468 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:55:33.453
    Jan 12 01:55:33.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename init-container 01/12/23 01:55:33.454
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:55:33.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:55:33.467
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 01/12/23 01:55:33.47
    Jan 12 01:55:33.470: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:55:39.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2030" for this suite. 01/12/23 01:55:39.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:55:39.922
Jan 12 01:55:39.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename watch 01/12/23 01:55:39.923
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:55:39.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:55:39.937
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/12/23 01:55:39.939
STEP: creating a watch on configmaps with label B 01/12/23 01:55:39.94
STEP: creating a watch on configmaps with label A or B 01/12/23 01:55:39.941
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/12/23 01:55:39.942
Jan 12 01:55:39.946: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178583 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 01:55:39.946: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178583 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/12/23 01:55:39.946
Jan 12 01:55:39.953: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178584 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 01:55:39.953: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178584 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/12/23 01:55:39.953
Jan 12 01:55:39.959: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178585 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 01:55:39.959: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178585 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/12/23 01:55:39.959
Jan 12 01:55:39.963: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178586 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 01:55:39.963: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178586 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/12/23 01:55:39.963
Jan 12 01:55:39.967: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9313  2c091c16-ce8a-4194-9230-871a0fcb4d7a 20178587 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 01:55:39.967: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9313  2c091c16-ce8a-4194-9230-871a0fcb4d7a 20178587 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/12/23 01:55:49.968
Jan 12 01:55:49.975: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9313  2c091c16-ce8a-4194-9230-871a0fcb4d7a 20178633 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 01:55:49.975: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9313  2c091c16-ce8a-4194-9230-871a0fcb4d7a 20178633 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 12 01:55:59.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9313" for this suite. 01/12/23 01:55:59.983
------------------------------
• [SLOW TEST] [20.134 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:55:39.922
    Jan 12 01:55:39.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename watch 01/12/23 01:55:39.923
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:55:39.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:55:39.937
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/12/23 01:55:39.939
    STEP: creating a watch on configmaps with label B 01/12/23 01:55:39.94
    STEP: creating a watch on configmaps with label A or B 01/12/23 01:55:39.941
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/12/23 01:55:39.942
    Jan 12 01:55:39.946: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178583 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 01:55:39.946: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178583 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/12/23 01:55:39.946
    Jan 12 01:55:39.953: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178584 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 01:55:39.953: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178584 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/12/23 01:55:39.953
    Jan 12 01:55:39.959: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178585 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 01:55:39.959: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178585 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/12/23 01:55:39.959
    Jan 12 01:55:39.963: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178586 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 01:55:39.963: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9313  3b1d68f4-2ab6-4960-9c0e-ebef2dec10e7 20178586 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/12/23 01:55:39.963
    Jan 12 01:55:39.967: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9313  2c091c16-ce8a-4194-9230-871a0fcb4d7a 20178587 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 01:55:39.967: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9313  2c091c16-ce8a-4194-9230-871a0fcb4d7a 20178587 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/12/23 01:55:49.968
    Jan 12 01:55:49.975: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9313  2c091c16-ce8a-4194-9230-871a0fcb4d7a 20178633 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 01:55:49.975: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9313  2c091c16-ce8a-4194-9230-871a0fcb4d7a 20178633 0 2023-01-12 01:55:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 01:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:55:59.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9313" for this suite. 01/12/23 01:55:59.983
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:56:00.057
Jan 12 01:56:00.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename resourcequota 01/12/23 01:56:00.058
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:00.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:00.083
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 01/12/23 01:56:00.086
STEP: Creating a ResourceQuota 01/12/23 01:56:05.088
STEP: Ensuring resource quota status is calculated 01/12/23 01:56:05.092
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 01:56:07.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3091" for this suite. 01/12/23 01:56:07.099
------------------------------
• [SLOW TEST] [7.055 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:56:00.057
    Jan 12 01:56:00.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename resourcequota 01/12/23 01:56:00.058
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:00.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:00.083
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 01/12/23 01:56:00.086
    STEP: Creating a ResourceQuota 01/12/23 01:56:05.088
    STEP: Ensuring resource quota status is calculated 01/12/23 01:56:05.092
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:56:07.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3091" for this suite. 01/12/23 01:56:07.099
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:56:07.113
Jan 12 01:56:07.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 01:56:07.114
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:07.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:07.125
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/12/23 01:56:07.127
Jan 12 01:56:07.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:56:09.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:56:18.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-341" for this suite. 01/12/23 01:56:18.187
------------------------------
• [SLOW TEST] [11.098 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:56:07.113
    Jan 12 01:56:07.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 01:56:07.114
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:07.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:07.125
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/12/23 01:56:07.127
    Jan 12 01:56:07.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:56:09.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:56:18.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-341" for this suite. 01/12/23 01:56:18.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:56:18.213
Jan 12 01:56:18.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 01:56:18.213
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:18.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:18.237
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 01:56:18.254
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:56:18.868
STEP: Deploying the webhook pod 01/12/23 01:56:18.878
STEP: Wait for the deployment to be ready 01/12/23 01:56:18.927
Jan 12 01:56:18.935: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 12 01:56:20.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 56, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 56, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 56, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 56, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 01:56:22.949
STEP: Verifying the service has paired with the endpoint 01/12/23 01:56:22.963
Jan 12 01:56:23.963: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 01/12/23 01:56:23.967
STEP: Creating a custom resource definition that should be denied by the webhook 01/12/23 01:56:23.984
Jan 12 01:56:23.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:56:23.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4980" for this suite. 01/12/23 01:56:24.039
STEP: Destroying namespace "webhook-4980-markers" for this suite. 01/12/23 01:56:24.104
------------------------------
• [SLOW TEST] [5.968 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:56:18.213
    Jan 12 01:56:18.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 01:56:18.213
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:18.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:18.237
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 01:56:18.254
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 01:56:18.868
    STEP: Deploying the webhook pod 01/12/23 01:56:18.878
    STEP: Wait for the deployment to be ready 01/12/23 01:56:18.927
    Jan 12 01:56:18.935: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 12 01:56:20.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 1, 56, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 56, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 1, 56, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 1, 56, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 01:56:22.949
    STEP: Verifying the service has paired with the endpoint 01/12/23 01:56:22.963
    Jan 12 01:56:23.963: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/12/23 01:56:23.967
    STEP: Creating a custom resource definition that should be denied by the webhook 01/12/23 01:56:23.984
    Jan 12 01:56:23.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:56:23.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4980" for this suite. 01/12/23 01:56:24.039
    STEP: Destroying namespace "webhook-4980-markers" for this suite. 01/12/23 01:56:24.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:56:24.181
Jan 12 01:56:24.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 01:56:24.182
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:24.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:24.202
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 12 01:56:24.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:56:30.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8615" for this suite. 01/12/23 01:56:30.497
------------------------------
• [SLOW TEST] [6.338 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:56:24.181
    Jan 12 01:56:24.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 01:56:24.182
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:24.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:24.202
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 12 01:56:24.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:56:30.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8615" for this suite. 01/12/23 01:56:30.497
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:56:30.523
Jan 12 01:56:30.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename var-expansion 01/12/23 01:56:30.526
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:30.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:30.547
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 01/12/23 01:56:30.55
Jan 12 01:56:30.590: INFO: Waiting up to 5m0s for pod "var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb" in namespace "var-expansion-1815" to be "Succeeded or Failed"
Jan 12 01:56:30.593: INFO: Pod "var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.272995ms
Jan 12 01:56:32.598: INFO: Pod "var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008033517s
Jan 12 01:56:34.600: INFO: Pod "var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01006312s
Jan 12 01:56:36.599: INFO: Pod "var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008508055s
STEP: Saw pod success 01/12/23 01:56:36.599
Jan 12 01:56:36.599: INFO: Pod "var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb" satisfied condition "Succeeded or Failed"
Jan 12 01:56:36.603: INFO: Trying to get logs from node eqx04-flash06 pod var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb container dapi-container: <nil>
STEP: delete the pod 01/12/23 01:56:36.62
Jan 12 01:56:36.634: INFO: Waiting for pod var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb to disappear
Jan 12 01:56:36.637: INFO: Pod var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 01:56:36.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1815" for this suite. 01/12/23 01:56:36.642
------------------------------
• [SLOW TEST] [6.136 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:56:30.523
    Jan 12 01:56:30.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename var-expansion 01/12/23 01:56:30.526
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:30.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:30.547
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 01/12/23 01:56:30.55
    Jan 12 01:56:30.590: INFO: Waiting up to 5m0s for pod "var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb" in namespace "var-expansion-1815" to be "Succeeded or Failed"
    Jan 12 01:56:30.593: INFO: Pod "var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.272995ms
    Jan 12 01:56:32.598: INFO: Pod "var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008033517s
    Jan 12 01:56:34.600: INFO: Pod "var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01006312s
    Jan 12 01:56:36.599: INFO: Pod "var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008508055s
    STEP: Saw pod success 01/12/23 01:56:36.599
    Jan 12 01:56:36.599: INFO: Pod "var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb" satisfied condition "Succeeded or Failed"
    Jan 12 01:56:36.603: INFO: Trying to get logs from node eqx04-flash06 pod var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb container dapi-container: <nil>
    STEP: delete the pod 01/12/23 01:56:36.62
    Jan 12 01:56:36.634: INFO: Waiting for pod var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb to disappear
    Jan 12 01:56:36.637: INFO: Pod var-expansion-696f8c25-17a3-4d82-b891-ebc09b2fe6cb no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:56:36.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1815" for this suite. 01/12/23 01:56:36.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:56:36.659
Jan 12 01:56:36.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 01:56:36.66
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:36.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:36.681
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9171 01/12/23 01:56:36.684
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/12/23 01:56:36.7
STEP: creating service externalsvc in namespace services-9171 01/12/23 01:56:36.701
STEP: creating replication controller externalsvc in namespace services-9171 01/12/23 01:56:36.716
I0112 01:56:36.721848      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9171, replica count: 2
I0112 01:56:39.773003      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/12/23 01:56:39.776
Jan 12 01:56:39.796: INFO: Creating new exec pod
Jan 12 01:56:39.830: INFO: Waiting up to 5m0s for pod "execpodghx5w" in namespace "services-9171" to be "running"
Jan 12 01:56:39.833: INFO: Pod "execpodghx5w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.999089ms
Jan 12 01:56:41.837: INFO: Pod "execpodghx5w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007565537s
Jan 12 01:56:43.839: INFO: Pod "execpodghx5w": Phase="Running", Reason="", readiness=true. Elapsed: 4.009003793s
Jan 12 01:56:43.839: INFO: Pod "execpodghx5w" satisfied condition "running"
Jan 12 01:56:43.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9171 exec execpodghx5w -- /bin/sh -x -c nslookup nodeport-service.services-9171.svc.cluster.local'
Jan 12 01:56:44.066: INFO: stderr: "+ nslookup nodeport-service.services-9171.svc.cluster.local\n"
Jan 12 01:56:44.066: INFO: stdout: "Server:\t\t172.19.0.10\nAddress:\t172.19.0.10#53\n\nnodeport-service.services-9171.svc.cluster.local\tcanonical name = externalsvc.services-9171.svc.cluster.local.\nName:\texternalsvc.services-9171.svc.cluster.local\nAddress: 172.19.249.99\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9171, will wait for the garbage collector to delete the pods 01/12/23 01:56:44.066
Jan 12 01:56:44.127: INFO: Deleting ReplicationController externalsvc took: 6.220651ms
Jan 12 01:56:44.227: INFO: Terminating ReplicationController externalsvc pods took: 100.41999ms
Jan 12 01:56:46.551: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 01:56:46.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9171" for this suite. 01/12/23 01:56:46.569
------------------------------
• [SLOW TEST] [9.927 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:56:36.659
    Jan 12 01:56:36.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 01:56:36.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:36.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:36.681
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-9171 01/12/23 01:56:36.684
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/12/23 01:56:36.7
    STEP: creating service externalsvc in namespace services-9171 01/12/23 01:56:36.701
    STEP: creating replication controller externalsvc in namespace services-9171 01/12/23 01:56:36.716
    I0112 01:56:36.721848      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9171, replica count: 2
    I0112 01:56:39.773003      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/12/23 01:56:39.776
    Jan 12 01:56:39.796: INFO: Creating new exec pod
    Jan 12 01:56:39.830: INFO: Waiting up to 5m0s for pod "execpodghx5w" in namespace "services-9171" to be "running"
    Jan 12 01:56:39.833: INFO: Pod "execpodghx5w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.999089ms
    Jan 12 01:56:41.837: INFO: Pod "execpodghx5w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007565537s
    Jan 12 01:56:43.839: INFO: Pod "execpodghx5w": Phase="Running", Reason="", readiness=true. Elapsed: 4.009003793s
    Jan 12 01:56:43.839: INFO: Pod "execpodghx5w" satisfied condition "running"
    Jan 12 01:56:43.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-9171 exec execpodghx5w -- /bin/sh -x -c nslookup nodeport-service.services-9171.svc.cluster.local'
    Jan 12 01:56:44.066: INFO: stderr: "+ nslookup nodeport-service.services-9171.svc.cluster.local\n"
    Jan 12 01:56:44.066: INFO: stdout: "Server:\t\t172.19.0.10\nAddress:\t172.19.0.10#53\n\nnodeport-service.services-9171.svc.cluster.local\tcanonical name = externalsvc.services-9171.svc.cluster.local.\nName:\texternalsvc.services-9171.svc.cluster.local\nAddress: 172.19.249.99\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9171, will wait for the garbage collector to delete the pods 01/12/23 01:56:44.066
    Jan 12 01:56:44.127: INFO: Deleting ReplicationController externalsvc took: 6.220651ms
    Jan 12 01:56:44.227: INFO: Terminating ReplicationController externalsvc pods took: 100.41999ms
    Jan 12 01:56:46.551: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:56:46.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9171" for this suite. 01/12/23 01:56:46.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:56:46.587
Jan 12 01:56:46.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename init-container 01/12/23 01:56:46.588
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:46.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:46.61
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 01/12/23 01:56:46.612
Jan 12 01:56:46.612: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 12 01:56:51.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8706" for this suite. 01/12/23 01:56:51.778
------------------------------
• [SLOW TEST] [5.263 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:56:46.587
    Jan 12 01:56:46.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename init-container 01/12/23 01:56:46.588
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:46.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:46.61
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 01/12/23 01:56:46.612
    Jan 12 01:56:46.612: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:56:51.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8706" for this suite. 01/12/23 01:56:51.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:56:51.852
Jan 12 01:56:51.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 01:56:51.853
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:51.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:51.872
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 01:56:51.875
Jan 12 01:56:51.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1114 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 12 01:56:52.066: INFO: stderr: ""
Jan 12 01:56:52.066: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/12/23 01:56:52.066
STEP: verifying the pod e2e-test-httpd-pod was created 01/12/23 01:56:57.117
Jan 12 01:56:57.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1114 get pod e2e-test-httpd-pod -o json'
Jan 12 01:56:57.187: INFO: stderr: ""
Jan 12 01:56:57.187: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"eb182472e9a99da896c8791286d2a75ed1ffa9e11a3edf544152a82014a00e42\",\n            \"cni.projectcalico.org/podIP\": \"172.21.88.168/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.21.88.168/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"calico\\\",\\n    \\\"ips\\\": [\\n        \\\"172.21.88.168\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"calico\\\",\\n    \\\"ips\\\": [\\n        \\\"172.21.88.168\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2023-01-12T01:56:51Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1114\",\n        \"resourceVersion\": \"20179208\",\n        \"uid\": \"ed256b43-1be7-4919-9561-be6cb9d5dc72\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-z7wcf\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"eqx04-flash06\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-z7wcf\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T01:56:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T01:56:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T01:56:54Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T01:56:52Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"robin://47d5bd2188c52a1fe1c9b5bc9e448363a3f7911c9e4f470e66117ba7a34de401\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-12T01:56:53Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.9.40.106\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.21.88.168\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.21.88.168\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-12T01:56:52Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/12/23 01:56:57.188
Jan 12 01:56:57.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1114 replace -f -'
Jan 12 01:56:57.958: INFO: stderr: ""
Jan 12 01:56:57.958: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/12/23 01:56:57.958
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jan 12 01:56:57.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1114 delete pods e2e-test-httpd-pod'
Jan 12 01:57:00.084: INFO: stderr: ""
Jan 12 01:57:00.084: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 01:57:00.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1114" for this suite. 01/12/23 01:57:00.09
------------------------------
• [SLOW TEST] [8.253 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:56:51.852
    Jan 12 01:56:51.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 01:56:51.853
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:56:51.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:56:51.872
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 01:56:51.875
    Jan 12 01:56:51.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1114 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 12 01:56:52.066: INFO: stderr: ""
    Jan 12 01:56:52.066: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/12/23 01:56:52.066
    STEP: verifying the pod e2e-test-httpd-pod was created 01/12/23 01:56:57.117
    Jan 12 01:56:57.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1114 get pod e2e-test-httpd-pod -o json'
    Jan 12 01:56:57.187: INFO: stderr: ""
    Jan 12 01:56:57.187: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"eb182472e9a99da896c8791286d2a75ed1ffa9e11a3edf544152a82014a00e42\",\n            \"cni.projectcalico.org/podIP\": \"172.21.88.168/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.21.88.168/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"calico\\\",\\n    \\\"ips\\\": [\\n        \\\"172.21.88.168\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"calico\\\",\\n    \\\"ips\\\": [\\n        \\\"172.21.88.168\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2023-01-12T01:56:51Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1114\",\n        \"resourceVersion\": \"20179208\",\n        \"uid\": \"ed256b43-1be7-4919-9561-be6cb9d5dc72\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-z7wcf\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"eqx04-flash06\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-z7wcf\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T01:56:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T01:56:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T01:56:54Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T01:56:52Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"robin://47d5bd2188c52a1fe1c9b5bc9e448363a3f7911c9e4f470e66117ba7a34de401\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-12T01:56:53Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.9.40.106\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.21.88.168\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.21.88.168\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-12T01:56:52Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/12/23 01:56:57.188
    Jan 12 01:56:57.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1114 replace -f -'
    Jan 12 01:56:57.958: INFO: stderr: ""
    Jan 12 01:56:57.958: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/12/23 01:56:57.958
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jan 12 01:56:57.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1114 delete pods e2e-test-httpd-pod'
    Jan 12 01:57:00.084: INFO: stderr: ""
    Jan 12 01:57:00.084: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:57:00.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1114" for this suite. 01/12/23 01:57:00.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:57:00.108
Jan 12 01:57:00.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename prestop 01/12/23 01:57:00.108
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:57:00.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:57:00.126
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-1749 01/12/23 01:57:00.129
STEP: Waiting for pods to come up. 01/12/23 01:57:00.162
Jan 12 01:57:00.162: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1749" to be "running"
Jan 12 01:57:00.165: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.151529ms
Jan 12 01:57:02.171: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008555979s
Jan 12 01:57:02.171: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-1749 01/12/23 01:57:02.174
Jan 12 01:57:02.207: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1749" to be "running"
Jan 12 01:57:02.210: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.853478ms
Jan 12 01:57:04.215: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.00789262s
Jan 12 01:57:04.215: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/12/23 01:57:04.215
Jan 12 01:57:09.228: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/12/23 01:57:09.229
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jan 12 01:57:09.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-1749" for this suite. 01/12/23 01:57:09.251
------------------------------
• [SLOW TEST] [9.183 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:57:00.108
    Jan 12 01:57:00.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename prestop 01/12/23 01:57:00.108
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:57:00.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:57:00.126
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-1749 01/12/23 01:57:00.129
    STEP: Waiting for pods to come up. 01/12/23 01:57:00.162
    Jan 12 01:57:00.162: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1749" to be "running"
    Jan 12 01:57:00.165: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.151529ms
    Jan 12 01:57:02.171: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008555979s
    Jan 12 01:57:02.171: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-1749 01/12/23 01:57:02.174
    Jan 12 01:57:02.207: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1749" to be "running"
    Jan 12 01:57:02.210: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.853478ms
    Jan 12 01:57:04.215: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.00789262s
    Jan 12 01:57:04.215: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/12/23 01:57:04.215
    Jan 12 01:57:09.228: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/12/23 01:57:09.229
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:57:09.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-1749" for this suite. 01/12/23 01:57:09.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:57:09.292
Jan 12 01:57:09.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 01:57:09.293
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:57:09.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:57:09.312
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-7f923cde-1128-4600-96ff-60f831079887 01/12/23 01:57:09.315
STEP: Creating a pod to test consume secrets 01/12/23 01:57:09.32
Jan 12 01:57:09.351: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade" in namespace "projected-916" to be "Succeeded or Failed"
Jan 12 01:57:09.354: INFO: Pod "pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade": Phase="Pending", Reason="", readiness=false. Elapsed: 2.931834ms
Jan 12 01:57:11.358: INFO: Pod "pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007385799s
Jan 12 01:57:13.359: INFO: Pod "pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008058s
Jan 12 01:57:15.360: INFO: Pod "pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009332708s
STEP: Saw pod success 01/12/23 01:57:15.36
Jan 12 01:57:15.361: INFO: Pod "pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade" satisfied condition "Succeeded or Failed"
Jan 12 01:57:15.363: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade container projected-secret-volume-test: <nil>
STEP: delete the pod 01/12/23 01:57:15.373
Jan 12 01:57:15.388: INFO: Waiting for pod pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade to disappear
Jan 12 01:57:15.391: INFO: Pod pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 12 01:57:15.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-916" for this suite. 01/12/23 01:57:15.402
------------------------------
• [SLOW TEST] [6.591 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:57:09.292
    Jan 12 01:57:09.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 01:57:09.293
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:57:09.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:57:09.312
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-7f923cde-1128-4600-96ff-60f831079887 01/12/23 01:57:09.315
    STEP: Creating a pod to test consume secrets 01/12/23 01:57:09.32
    Jan 12 01:57:09.351: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade" in namespace "projected-916" to be "Succeeded or Failed"
    Jan 12 01:57:09.354: INFO: Pod "pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade": Phase="Pending", Reason="", readiness=false. Elapsed: 2.931834ms
    Jan 12 01:57:11.358: INFO: Pod "pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007385799s
    Jan 12 01:57:13.359: INFO: Pod "pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008058s
    Jan 12 01:57:15.360: INFO: Pod "pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009332708s
    STEP: Saw pod success 01/12/23 01:57:15.36
    Jan 12 01:57:15.361: INFO: Pod "pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade" satisfied condition "Succeeded or Failed"
    Jan 12 01:57:15.363: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 01:57:15.373
    Jan 12 01:57:15.388: INFO: Waiting for pod pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade to disappear
    Jan 12 01:57:15.391: INFO: Pod pod-projected-secrets-35962b0d-5bad-4761-95f8-94d790e52ade no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:57:15.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-916" for this suite. 01/12/23 01:57:15.402
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:57:15.884
Jan 12 01:57:15.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 01:57:15.885
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:57:15.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:57:15.904
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 01/12/23 01:57:15.907
Jan 12 01:57:16.108: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8" in namespace "downward-api-6104" to be "Succeeded or Failed"
Jan 12 01:57:16.111: INFO: Pod "downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.122463ms
Jan 12 01:57:18.115: INFO: Pod "downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007435874s
Jan 12 01:57:20.116: INFO: Pod "downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007839751s
Jan 12 01:57:22.115: INFO: Pod "downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007064421s
STEP: Saw pod success 01/12/23 01:57:22.115
Jan 12 01:57:22.115: INFO: Pod "downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8" satisfied condition "Succeeded or Failed"
Jan 12 01:57:22.118: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8 container client-container: <nil>
STEP: delete the pod 01/12/23 01:57:22.127
Jan 12 01:57:22.143: INFO: Waiting for pod downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8 to disappear
Jan 12 01:57:22.145: INFO: Pod downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 01:57:22.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6104" for this suite. 01/12/23 01:57:22.15
------------------------------
• [SLOW TEST] [6.284 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:57:15.884
    Jan 12 01:57:15.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 01:57:15.885
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:57:15.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:57:15.904
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 01/12/23 01:57:15.907
    Jan 12 01:57:16.108: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8" in namespace "downward-api-6104" to be "Succeeded or Failed"
    Jan 12 01:57:16.111: INFO: Pod "downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.122463ms
    Jan 12 01:57:18.115: INFO: Pod "downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007435874s
    Jan 12 01:57:20.116: INFO: Pod "downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007839751s
    Jan 12 01:57:22.115: INFO: Pod "downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007064421s
    STEP: Saw pod success 01/12/23 01:57:22.115
    Jan 12 01:57:22.115: INFO: Pod "downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8" satisfied condition "Succeeded or Failed"
    Jan 12 01:57:22.118: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8 container client-container: <nil>
    STEP: delete the pod 01/12/23 01:57:22.127
    Jan 12 01:57:22.143: INFO: Waiting for pod downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8 to disappear
    Jan 12 01:57:22.145: INFO: Pod downwardapi-volume-2ca1954a-9f8f-4095-a416-ec3c3c0df3d8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:57:22.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6104" for this suite. 01/12/23 01:57:22.15
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:57:22.168
Jan 12 01:57:22.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename statefulset 01/12/23 01:57:22.169
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:57:22.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:57:22.186
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-935 01/12/23 01:57:22.189
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 01/12/23 01:57:22.194
Jan 12 01:57:22.228: INFO: Found 0 stateful pods, waiting for 3
Jan 12 01:57:32.233: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 01:57:32.233: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 01:57:32.233: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 01:57:32.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-935 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 01:57:32.538: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 01:57:32.538: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 01:57:32.538: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/12/23 01:57:42.555
Jan 12 01:57:42.575: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/12/23 01:57:42.575
STEP: Updating Pods in reverse ordinal order 01/12/23 01:57:52.594
Jan 12 01:57:52.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-935 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 01:57:52.889: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 12 01:57:52.889: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 01:57:52.889: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 01:58:02.911: INFO: Waiting for StatefulSet statefulset-935/ss2 to complete update
STEP: Rolling back to a previous revision 01/12/23 01:58:12.92
Jan 12 01:58:12.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-935 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 01:58:13.207: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 01:58:13.207: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 01:58:13.207: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 01:58:23.245: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/12/23 01:58:33.261
Jan 12 01:58:33.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-935 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 01:58:33.563: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 12 01:58:33.563: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 01:58:33.563: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 01:58:43.585: INFO: Waiting for StatefulSet statefulset-935/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 01:58:53.594: INFO: Deleting all statefulset in ns statefulset-935
Jan 12 01:58:53.597: INFO: Scaling statefulset ss2 to 0
Jan 12 01:59:03.617: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 01:59:03.619: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 01:59:03.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-935" for this suite. 01/12/23 01:59:03.64
------------------------------
• [SLOW TEST] [101.531 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:57:22.168
    Jan 12 01:57:22.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename statefulset 01/12/23 01:57:22.169
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:57:22.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:57:22.186
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-935 01/12/23 01:57:22.189
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 01/12/23 01:57:22.194
    Jan 12 01:57:22.228: INFO: Found 0 stateful pods, waiting for 3
    Jan 12 01:57:32.233: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 01:57:32.233: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 01:57:32.233: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 01:57:32.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-935 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 01:57:32.538: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 01:57:32.538: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 01:57:32.538: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/12/23 01:57:42.555
    Jan 12 01:57:42.575: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/12/23 01:57:42.575
    STEP: Updating Pods in reverse ordinal order 01/12/23 01:57:52.594
    Jan 12 01:57:52.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-935 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 01:57:52.889: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 12 01:57:52.889: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 01:57:52.889: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 01:58:02.911: INFO: Waiting for StatefulSet statefulset-935/ss2 to complete update
    STEP: Rolling back to a previous revision 01/12/23 01:58:12.92
    Jan 12 01:58:12.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-935 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 01:58:13.207: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 01:58:13.207: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 01:58:13.207: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 01:58:23.245: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/12/23 01:58:33.261
    Jan 12 01:58:33.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-935 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 01:58:33.563: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 12 01:58:33.563: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 01:58:33.563: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 01:58:43.585: INFO: Waiting for StatefulSet statefulset-935/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 01:58:53.594: INFO: Deleting all statefulset in ns statefulset-935
    Jan 12 01:58:53.597: INFO: Scaling statefulset ss2 to 0
    Jan 12 01:59:03.617: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 01:59:03.619: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:59:03.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-935" for this suite. 01/12/23 01:59:03.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:59:03.699
Jan 12 01:59:03.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename secrets 01/12/23 01:59:03.7
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:59:03.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:59:03.727
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-b96c2eb5-b3dc-46a0-ba20-d737ec455830 01/12/23 01:59:03.729
STEP: Creating a pod to test consume secrets 01/12/23 01:59:03.735
Jan 12 01:59:04.089: INFO: Waiting up to 5m0s for pod "pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192" in namespace "secrets-3663" to be "Succeeded or Failed"
Jan 12 01:59:04.092: INFO: Pod "pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192": Phase="Pending", Reason="", readiness=false. Elapsed: 3.128324ms
Jan 12 01:59:06.096: INFO: Pod "pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007129855s
Jan 12 01:59:08.097: INFO: Pod "pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007455386s
Jan 12 01:59:10.098: INFO: Pod "pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008829911s
STEP: Saw pod success 01/12/23 01:59:10.098
Jan 12 01:59:10.098: INFO: Pod "pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192" satisfied condition "Succeeded or Failed"
Jan 12 01:59:10.102: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192 container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 01:59:10.12
Jan 12 01:59:10.141: INFO: Waiting for pod pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192 to disappear
Jan 12 01:59:10.144: INFO: Pod pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 01:59:10.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3663" for this suite. 01/12/23 01:59:10.149
------------------------------
• [SLOW TEST] [6.467 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:59:03.699
    Jan 12 01:59:03.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename secrets 01/12/23 01:59:03.7
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:59:03.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:59:03.727
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-b96c2eb5-b3dc-46a0-ba20-d737ec455830 01/12/23 01:59:03.729
    STEP: Creating a pod to test consume secrets 01/12/23 01:59:03.735
    Jan 12 01:59:04.089: INFO: Waiting up to 5m0s for pod "pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192" in namespace "secrets-3663" to be "Succeeded or Failed"
    Jan 12 01:59:04.092: INFO: Pod "pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192": Phase="Pending", Reason="", readiness=false. Elapsed: 3.128324ms
    Jan 12 01:59:06.096: INFO: Pod "pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007129855s
    Jan 12 01:59:08.097: INFO: Pod "pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007455386s
    Jan 12 01:59:10.098: INFO: Pod "pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008829911s
    STEP: Saw pod success 01/12/23 01:59:10.098
    Jan 12 01:59:10.098: INFO: Pod "pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192" satisfied condition "Succeeded or Failed"
    Jan 12 01:59:10.102: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192 container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 01:59:10.12
    Jan 12 01:59:10.141: INFO: Waiting for pod pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192 to disappear
    Jan 12 01:59:10.144: INFO: Pod pod-secrets-f29c7d5e-0a3e-415a-b881-c8ab991c2192 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:59:10.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3663" for this suite. 01/12/23 01:59:10.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:59:10.167
Jan 12 01:59:10.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/12/23 01:59:10.168
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:59:10.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:59:10.194
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/12/23 01:59:10.196
STEP: Creating hostNetwork=false pod 01/12/23 01:59:10.196
Jan 12 01:59:10.227: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1137" to be "running and ready"
Jan 12 01:59:10.230: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.794741ms
Jan 12 01:59:10.230: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:59:12.235: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007657856s
Jan 12 01:59:12.235: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:59:14.235: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007932587s
Jan 12 01:59:14.235: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 12 01:59:14.235: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/12/23 01:59:14.239
Jan 12 01:59:14.286: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1137" to be "running and ready"
Jan 12 01:59:14.289: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.793351ms
Jan 12 01:59:14.289: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:59:16.293: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006736279s
Jan 12 01:59:16.293: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 12 01:59:18.297: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010897305s
Jan 12 01:59:18.297: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 12 01:59:18.297: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/12/23 01:59:18.301
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/12/23 01:59:18.301
Jan 12 01:59:18.301: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:59:18.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:59:18.301: INFO: ExecWithOptions: Clientset creation
Jan 12 01:59:18.301: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 12 01:59:18.443: INFO: Exec stderr: ""
Jan 12 01:59:18.443: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:59:18.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:59:18.444: INFO: ExecWithOptions: Clientset creation
Jan 12 01:59:18.444: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 12 01:59:18.643: INFO: Exec stderr: ""
Jan 12 01:59:18.644: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:59:18.644: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:59:18.644: INFO: ExecWithOptions: Clientset creation
Jan 12 01:59:18.644: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 12 01:59:18.825: INFO: Exec stderr: ""
Jan 12 01:59:18.825: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:59:18.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:59:18.825: INFO: ExecWithOptions: Clientset creation
Jan 12 01:59:18.825: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 12 01:59:18.974: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/12/23 01:59:18.974
Jan 12 01:59:18.974: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:59:18.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:59:18.975: INFO: ExecWithOptions: Clientset creation
Jan 12 01:59:18.975: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 12 01:59:19.115: INFO: Exec stderr: ""
Jan 12 01:59:19.115: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:59:19.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:59:19.115: INFO: ExecWithOptions: Clientset creation
Jan 12 01:59:19.115: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 12 01:59:19.274: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/12/23 01:59:19.274
Jan 12 01:59:19.274: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:59:19.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:59:19.275: INFO: ExecWithOptions: Clientset creation
Jan 12 01:59:19.275: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 12 01:59:19.419: INFO: Exec stderr: ""
Jan 12 01:59:19.419: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:59:19.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:59:19.419: INFO: ExecWithOptions: Clientset creation
Jan 12 01:59:19.419: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 12 01:59:19.588: INFO: Exec stderr: ""
Jan 12 01:59:19.588: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:59:19.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:59:19.589: INFO: ExecWithOptions: Clientset creation
Jan 12 01:59:19.589: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 12 01:59:19.720: INFO: Exec stderr: ""
Jan 12 01:59:19.720: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 01:59:19.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 01:59:19.721: INFO: ExecWithOptions: Clientset creation
Jan 12 01:59:19.721: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 12 01:59:19.851: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jan 12 01:59:19.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1137" for this suite. 01/12/23 01:59:19.856
------------------------------
• [SLOW TEST] [9.769 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:59:10.167
    Jan 12 01:59:10.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/12/23 01:59:10.168
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:59:10.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:59:10.194
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/12/23 01:59:10.196
    STEP: Creating hostNetwork=false pod 01/12/23 01:59:10.196
    Jan 12 01:59:10.227: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1137" to be "running and ready"
    Jan 12 01:59:10.230: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.794741ms
    Jan 12 01:59:10.230: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:59:12.235: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007657856s
    Jan 12 01:59:12.235: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:59:14.235: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007932587s
    Jan 12 01:59:14.235: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 12 01:59:14.235: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/12/23 01:59:14.239
    Jan 12 01:59:14.286: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1137" to be "running and ready"
    Jan 12 01:59:14.289: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.793351ms
    Jan 12 01:59:14.289: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:59:16.293: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006736279s
    Jan 12 01:59:16.293: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 01:59:18.297: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010897305s
    Jan 12 01:59:18.297: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 12 01:59:18.297: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/12/23 01:59:18.301
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/12/23 01:59:18.301
    Jan 12 01:59:18.301: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:59:18.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:59:18.301: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:59:18.301: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 12 01:59:18.443: INFO: Exec stderr: ""
    Jan 12 01:59:18.443: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:59:18.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:59:18.444: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:59:18.444: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 12 01:59:18.643: INFO: Exec stderr: ""
    Jan 12 01:59:18.644: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:59:18.644: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:59:18.644: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:59:18.644: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 12 01:59:18.825: INFO: Exec stderr: ""
    Jan 12 01:59:18.825: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:59:18.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:59:18.825: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:59:18.825: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 12 01:59:18.974: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/12/23 01:59:18.974
    Jan 12 01:59:18.974: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:59:18.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:59:18.975: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:59:18.975: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 12 01:59:19.115: INFO: Exec stderr: ""
    Jan 12 01:59:19.115: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:59:19.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:59:19.115: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:59:19.115: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 12 01:59:19.274: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/12/23 01:59:19.274
    Jan 12 01:59:19.274: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:59:19.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:59:19.275: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:59:19.275: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 12 01:59:19.419: INFO: Exec stderr: ""
    Jan 12 01:59:19.419: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:59:19.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:59:19.419: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:59:19.419: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 12 01:59:19.588: INFO: Exec stderr: ""
    Jan 12 01:59:19.588: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:59:19.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:59:19.589: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:59:19.589: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 12 01:59:19.720: INFO: Exec stderr: ""
    Jan 12 01:59:19.720: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1137 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 01:59:19.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 01:59:19.721: INFO: ExecWithOptions: Clientset creation
    Jan 12 01:59:19.721: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1137/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 12 01:59:19.851: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jan 12 01:59:19.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-1137" for this suite. 01/12/23 01:59:19.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 01:59:19.937
Jan 12 01:59:19.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename cronjob 01/12/23 01:59:19.938
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:59:19.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:59:19.957
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/12/23 01:59:19.959
STEP: Ensuring a job is scheduled 01/12/23 01:59:19.967
STEP: Ensuring exactly one is scheduled 01/12/23 02:00:01.972
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/12/23 02:00:01.975
STEP: Ensuring no more jobs are scheduled 01/12/23 02:00:01.978
STEP: Removing cronjob 01/12/23 02:05:01.985
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 12 02:05:01.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4211" for this suite. 01/12/23 02:05:01.996
------------------------------
• [SLOW TEST] [342.189 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 01:59:19.937
    Jan 12 01:59:19.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename cronjob 01/12/23 01:59:19.938
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 01:59:19.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 01:59:19.957
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/12/23 01:59:19.959
    STEP: Ensuring a job is scheduled 01/12/23 01:59:19.967
    STEP: Ensuring exactly one is scheduled 01/12/23 02:00:01.972
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/12/23 02:00:01.975
    STEP: Ensuring no more jobs are scheduled 01/12/23 02:00:01.978
    STEP: Removing cronjob 01/12/23 02:05:01.985
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:05:01.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4211" for this suite. 01/12/23 02:05:01.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:05:02.128
Jan 12 02:05:02.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-runtime 01/12/23 02:05:02.13
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:05:02.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:05:02.153
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/12/23 02:05:02.335
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/12/23 02:05:23.436
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/12/23 02:05:23.439
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/12/23 02:05:23.445
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/12/23 02:05:23.445
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/12/23 02:05:23.517
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/12/23 02:05:26.532
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/12/23 02:05:28.546
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/12/23 02:05:28.553
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/12/23 02:05:28.553
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/12/23 02:05:28.621
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/12/23 02:05:29.629
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/12/23 02:05:33.652
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/12/23 02:05:33.658
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/12/23 02:05:33.658
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 12 02:05:33.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5442" for this suite. 01/12/23 02:05:33.69
------------------------------
• [SLOW TEST] [31.578 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:05:02.128
    Jan 12 02:05:02.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-runtime 01/12/23 02:05:02.13
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:05:02.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:05:02.153
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/12/23 02:05:02.335
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/12/23 02:05:23.436
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/12/23 02:05:23.439
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/12/23 02:05:23.445
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/12/23 02:05:23.445
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/12/23 02:05:23.517
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/12/23 02:05:26.532
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/12/23 02:05:28.546
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/12/23 02:05:28.553
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/12/23 02:05:28.553
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/12/23 02:05:28.621
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/12/23 02:05:29.629
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/12/23 02:05:33.652
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/12/23 02:05:33.658
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/12/23 02:05:33.658
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:05:33.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5442" for this suite. 01/12/23 02:05:33.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:05:33.708
Jan 12 02:05:33.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename services 01/12/23 02:05:33.709
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:05:33.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:05:33.728
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-3954 01/12/23 02:05:33.731
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3954 to expose endpoints map[] 01/12/23 02:05:33.743
Jan 12 02:05:33.753: INFO: successfully validated that service endpoint-test2 in namespace services-3954 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3954 01/12/23 02:05:33.753
Jan 12 02:05:33.807: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3954" to be "running and ready"
Jan 12 02:05:33.810: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.985373ms
Jan 12 02:05:33.810: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:05:35.814: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007686298s
Jan 12 02:05:35.814: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 12 02:05:35.814: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3954 to expose endpoints map[pod1:[80]] 01/12/23 02:05:35.817
Jan 12 02:05:35.827: INFO: successfully validated that service endpoint-test2 in namespace services-3954 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/12/23 02:05:35.827
Jan 12 02:05:35.828: INFO: Creating new exec pod
Jan 12 02:05:35.859: INFO: Waiting up to 5m0s for pod "execpodx627w" in namespace "services-3954" to be "running"
Jan 12 02:05:35.862: INFO: Pod "execpodx627w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.862422ms
Jan 12 02:05:37.868: INFO: Pod "execpodx627w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009580724s
Jan 12 02:05:39.867: INFO: Pod "execpodx627w": Phase="Running", Reason="", readiness=true. Elapsed: 4.008706462s
Jan 12 02:05:39.868: INFO: Pod "execpodx627w" satisfied condition "running"
Jan 12 02:05:40.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-3954 exec execpodx627w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 12 02:05:41.097: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 12 02:05:41.097: INFO: stdout: ""
Jan 12 02:05:41.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-3954 exec execpodx627w -- /bin/sh -x -c nc -v -z -w 2 172.19.15.52 80'
Jan 12 02:05:41.300: INFO: stderr: "+ nc -v -z -w 2 172.19.15.52 80\nConnection to 172.19.15.52 80 port [tcp/http] succeeded!\n"
Jan 12 02:05:41.300: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-3954 01/12/23 02:05:41.3
Jan 12 02:05:41.412: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3954" to be "running and ready"
Jan 12 02:05:41.415: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.250793ms
Jan 12 02:05:41.415: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:05:43.420: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008302915s
Jan 12 02:05:43.420: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:05:45.420: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008209916s
Jan 12 02:05:45.420: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 12 02:05:45.420: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3954 to expose endpoints map[pod1:[80] pod2:[80]] 01/12/23 02:05:45.423
Jan 12 02:05:45.435: INFO: successfully validated that service endpoint-test2 in namespace services-3954 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/12/23 02:05:45.435
Jan 12 02:05:46.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-3954 exec execpodx627w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 12 02:05:46.669: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 12 02:05:46.669: INFO: stdout: ""
Jan 12 02:05:46.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-3954 exec execpodx627w -- /bin/sh -x -c nc -v -z -w 2 172.19.15.52 80'
Jan 12 02:05:46.866: INFO: stderr: "+ nc -v -z -w 2 172.19.15.52 80\nConnection to 172.19.15.52 80 port [tcp/http] succeeded!\n"
Jan 12 02:05:46.866: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-3954 01/12/23 02:05:46.866
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3954 to expose endpoints map[pod2:[80]] 01/12/23 02:05:46.878
Jan 12 02:05:46.895: INFO: successfully validated that service endpoint-test2 in namespace services-3954 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/12/23 02:05:46.895
Jan 12 02:05:47.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-3954 exec execpodx627w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 12 02:05:48.113: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 12 02:05:48.113: INFO: stdout: ""
Jan 12 02:05:48.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-3954 exec execpodx627w -- /bin/sh -x -c nc -v -z -w 2 172.19.15.52 80'
Jan 12 02:05:48.317: INFO: stderr: "+ nc -v -z -w 2 172.19.15.52 80\nConnection to 172.19.15.52 80 port [tcp/http] succeeded!\n"
Jan 12 02:05:48.317: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-3954 01/12/23 02:05:48.317
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3954 to expose endpoints map[] 01/12/23 02:05:48.329
Jan 12 02:05:49.346: INFO: successfully validated that service endpoint-test2 in namespace services-3954 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 02:05:49.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3954" for this suite. 01/12/23 02:05:49.371
------------------------------
• [SLOW TEST] [15.682 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:05:33.708
    Jan 12 02:05:33.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename services 01/12/23 02:05:33.709
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:05:33.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:05:33.728
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-3954 01/12/23 02:05:33.731
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3954 to expose endpoints map[] 01/12/23 02:05:33.743
    Jan 12 02:05:33.753: INFO: successfully validated that service endpoint-test2 in namespace services-3954 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3954 01/12/23 02:05:33.753
    Jan 12 02:05:33.807: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3954" to be "running and ready"
    Jan 12 02:05:33.810: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.985373ms
    Jan 12 02:05:33.810: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:05:35.814: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007686298s
    Jan 12 02:05:35.814: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 12 02:05:35.814: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3954 to expose endpoints map[pod1:[80]] 01/12/23 02:05:35.817
    Jan 12 02:05:35.827: INFO: successfully validated that service endpoint-test2 in namespace services-3954 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/12/23 02:05:35.827
    Jan 12 02:05:35.828: INFO: Creating new exec pod
    Jan 12 02:05:35.859: INFO: Waiting up to 5m0s for pod "execpodx627w" in namespace "services-3954" to be "running"
    Jan 12 02:05:35.862: INFO: Pod "execpodx627w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.862422ms
    Jan 12 02:05:37.868: INFO: Pod "execpodx627w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009580724s
    Jan 12 02:05:39.867: INFO: Pod "execpodx627w": Phase="Running", Reason="", readiness=true. Elapsed: 4.008706462s
    Jan 12 02:05:39.868: INFO: Pod "execpodx627w" satisfied condition "running"
    Jan 12 02:05:40.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-3954 exec execpodx627w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 12 02:05:41.097: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 12 02:05:41.097: INFO: stdout: ""
    Jan 12 02:05:41.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-3954 exec execpodx627w -- /bin/sh -x -c nc -v -z -w 2 172.19.15.52 80'
    Jan 12 02:05:41.300: INFO: stderr: "+ nc -v -z -w 2 172.19.15.52 80\nConnection to 172.19.15.52 80 port [tcp/http] succeeded!\n"
    Jan 12 02:05:41.300: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-3954 01/12/23 02:05:41.3
    Jan 12 02:05:41.412: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3954" to be "running and ready"
    Jan 12 02:05:41.415: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.250793ms
    Jan 12 02:05:41.415: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:05:43.420: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008302915s
    Jan 12 02:05:43.420: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:05:45.420: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008209916s
    Jan 12 02:05:45.420: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 12 02:05:45.420: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3954 to expose endpoints map[pod1:[80] pod2:[80]] 01/12/23 02:05:45.423
    Jan 12 02:05:45.435: INFO: successfully validated that service endpoint-test2 in namespace services-3954 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/12/23 02:05:45.435
    Jan 12 02:05:46.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-3954 exec execpodx627w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 12 02:05:46.669: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 12 02:05:46.669: INFO: stdout: ""
    Jan 12 02:05:46.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-3954 exec execpodx627w -- /bin/sh -x -c nc -v -z -w 2 172.19.15.52 80'
    Jan 12 02:05:46.866: INFO: stderr: "+ nc -v -z -w 2 172.19.15.52 80\nConnection to 172.19.15.52 80 port [tcp/http] succeeded!\n"
    Jan 12 02:05:46.866: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-3954 01/12/23 02:05:46.866
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3954 to expose endpoints map[pod2:[80]] 01/12/23 02:05:46.878
    Jan 12 02:05:46.895: INFO: successfully validated that service endpoint-test2 in namespace services-3954 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/12/23 02:05:46.895
    Jan 12 02:05:47.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-3954 exec execpodx627w -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 12 02:05:48.113: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 12 02:05:48.113: INFO: stdout: ""
    Jan 12 02:05:48.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=services-3954 exec execpodx627w -- /bin/sh -x -c nc -v -z -w 2 172.19.15.52 80'
    Jan 12 02:05:48.317: INFO: stderr: "+ nc -v -z -w 2 172.19.15.52 80\nConnection to 172.19.15.52 80 port [tcp/http] succeeded!\n"
    Jan 12 02:05:48.317: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-3954 01/12/23 02:05:48.317
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3954 to expose endpoints map[] 01/12/23 02:05:48.329
    Jan 12 02:05:49.346: INFO: successfully validated that service endpoint-test2 in namespace services-3954 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:05:49.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3954" for this suite. 01/12/23 02:05:49.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:05:49.391
Jan 12 02:05:49.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename init-container 01/12/23 02:05:49.391
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:05:49.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:05:49.413
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 01/12/23 02:05:49.415
Jan 12 02:05:49.415: INFO: PodSpec: initContainers in spec.initContainers
Jan 12 02:06:31.518: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b44fff3e-1bb0-4859-8e27-38704afa5734", GenerateName:"", Namespace:"init-container-5167", SelfLink:"", UID:"50f31706-c580-4fea-bdd9-72176b6d8103", ResourceVersion:"20182082", Generation:0, CreationTimestamp:time.Date(2023, time.January, 12, 2, 5, 49, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"415939436"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"c439917676a8ff6908923c35cfc40902f19423a826757d157c83cf47e584f2b0", "cni.projectcalico.org/podIP":"172.21.88.184/32", "cni.projectcalico.org/podIPs":"172.21.88.184/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"calico\",\n    \"ips\": [\n        \"172.21.88.184\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"calico\",\n    \"ips\": [\n        \"172.21.88.184\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 12, 2, 5, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006a2c078), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 12, 2, 5, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006a2c0a8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 12, 2, 5, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006a2c0d8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 12, 2, 6, 31, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006a2c108), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-tr5d8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004ba4040), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tr5d8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tr5d8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tr5d8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000944708), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"eqx04-flash06", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0054a4000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0009447a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0009447c0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0009447c8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0009447cc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0061d6030), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 2, 5, 49, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 2, 5, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 2, 5, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 2, 5, 49, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.9.40.106", PodIP:"172.21.88.184", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.21.88.184"}}, StartTime:time.Date(2023, time.January, 12, 2, 5, 49, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0054a40e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0054a4150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"docker-pullable://registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"robin://d63733621df1054a59c5bbc2da01dcbd305521888e7240e54bb3434dd9c551b7", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004ba40c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004ba40a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00094484f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 12 02:06:31.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5167" for this suite. 01/12/23 02:06:31.534
------------------------------
• [SLOW TEST] [42.168 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:05:49.391
    Jan 12 02:05:49.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename init-container 01/12/23 02:05:49.391
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:05:49.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:05:49.413
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 01/12/23 02:05:49.415
    Jan 12 02:05:49.415: INFO: PodSpec: initContainers in spec.initContainers
    Jan 12 02:06:31.518: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b44fff3e-1bb0-4859-8e27-38704afa5734", GenerateName:"", Namespace:"init-container-5167", SelfLink:"", UID:"50f31706-c580-4fea-bdd9-72176b6d8103", ResourceVersion:"20182082", Generation:0, CreationTimestamp:time.Date(2023, time.January, 12, 2, 5, 49, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"415939436"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"c439917676a8ff6908923c35cfc40902f19423a826757d157c83cf47e584f2b0", "cni.projectcalico.org/podIP":"172.21.88.184/32", "cni.projectcalico.org/podIPs":"172.21.88.184/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"calico\",\n    \"ips\": [\n        \"172.21.88.184\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"calico\",\n    \"ips\": [\n        \"172.21.88.184\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 12, 2, 5, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006a2c078), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 12, 2, 5, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006a2c0a8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 12, 2, 5, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006a2c0d8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 12, 2, 6, 31, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006a2c108), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-tr5d8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004ba4040), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tr5d8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tr5d8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tr5d8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000944708), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"eqx04-flash06", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0054a4000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0009447a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0009447c0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0009447c8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0009447cc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0061d6030), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 2, 5, 49, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 2, 5, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 2, 5, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 2, 5, 49, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.9.40.106", PodIP:"172.21.88.184", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.21.88.184"}}, StartTime:time.Date(2023, time.January, 12, 2, 5, 49, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0054a40e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0054a4150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"docker-pullable://registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"robin://d63733621df1054a59c5bbc2da01dcbd305521888e7240e54bb3434dd9c551b7", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004ba40c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004ba40a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00094484f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:06:31.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5167" for this suite. 01/12/23 02:06:31.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:06:31.56
Jan 12 02:06:31.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 02:06:31.561
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:06:31.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:06:31.583
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-a8620d5b-1767-4580-aed8-4a36e5cc8cdf 01/12/23 02:06:31.585
STEP: Creating a pod to test consume secrets 01/12/23 02:06:31.59
Jan 12 02:06:31.646: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2" in namespace "projected-4303" to be "Succeeded or Failed"
Jan 12 02:06:31.650: INFO: Pod "pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.02538ms
Jan 12 02:06:33.654: INFO: Pod "pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007914809s
Jan 12 02:06:35.654: INFO: Pod "pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007332518s
Jan 12 02:06:37.653: INFO: Pod "pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00697625s
STEP: Saw pod success 01/12/23 02:06:37.653
Jan 12 02:06:37.654: INFO: Pod "pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2" satisfied condition "Succeeded or Failed"
Jan 12 02:06:37.656: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/12/23 02:06:37.679
Jan 12 02:06:37.694: INFO: Waiting for pod pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2 to disappear
Jan 12 02:06:37.697: INFO: Pod pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 12 02:06:37.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4303" for this suite. 01/12/23 02:06:37.702
------------------------------
• [SLOW TEST] [6.167 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:06:31.56
    Jan 12 02:06:31.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 02:06:31.561
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:06:31.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:06:31.583
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-a8620d5b-1767-4580-aed8-4a36e5cc8cdf 01/12/23 02:06:31.585
    STEP: Creating a pod to test consume secrets 01/12/23 02:06:31.59
    Jan 12 02:06:31.646: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2" in namespace "projected-4303" to be "Succeeded or Failed"
    Jan 12 02:06:31.650: INFO: Pod "pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.02538ms
    Jan 12 02:06:33.654: INFO: Pod "pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007914809s
    Jan 12 02:06:35.654: INFO: Pod "pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007332518s
    Jan 12 02:06:37.653: INFO: Pod "pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00697625s
    STEP: Saw pod success 01/12/23 02:06:37.653
    Jan 12 02:06:37.654: INFO: Pod "pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2" satisfied condition "Succeeded or Failed"
    Jan 12 02:06:37.656: INFO: Trying to get logs from node eqx04-flash06 pod pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 02:06:37.679
    Jan 12 02:06:37.694: INFO: Waiting for pod pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2 to disappear
    Jan 12 02:06:37.697: INFO: Pod pod-projected-secrets-6ce80b27-d842-4b38-9136-956d2427fae2 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:06:37.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4303" for this suite. 01/12/23 02:06:37.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:06:37.73
Jan 12 02:06:37.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename replicaset 01/12/23 02:06:37.731
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:06:37.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:06:37.749
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 12 02:06:37.769: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 12 02:06:42.774: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/12/23 02:06:42.774
STEP: Scaling up "test-rs" replicaset  01/12/23 02:06:42.774
Jan 12 02:06:42.785: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/12/23 02:06:42.785
W0112 02:06:42.795168      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 12 02:06:42.796: INFO: observed ReplicaSet test-rs in namespace replicaset-798 with ReadyReplicas 1, AvailableReplicas 1
Jan 12 02:06:42.843: INFO: observed ReplicaSet test-rs in namespace replicaset-798 with ReadyReplicas 1, AvailableReplicas 1
Jan 12 02:06:42.907: INFO: observed ReplicaSet test-rs in namespace replicaset-798 with ReadyReplicas 1, AvailableReplicas 1
Jan 12 02:06:42.912: INFO: observed ReplicaSet test-rs in namespace replicaset-798 with ReadyReplicas 1, AvailableReplicas 1
Jan 12 02:06:45.086: INFO: observed ReplicaSet test-rs in namespace replicaset-798 with ReadyReplicas 2, AvailableReplicas 2
Jan 12 02:06:45.648: INFO: observed Replicaset test-rs in namespace replicaset-798 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 12 02:06:45.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-798" for this suite. 01/12/23 02:06:45.653
------------------------------
• [SLOW TEST] [7.956 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:06:37.73
    Jan 12 02:06:37.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename replicaset 01/12/23 02:06:37.731
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:06:37.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:06:37.749
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 12 02:06:37.769: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 12 02:06:42.774: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/12/23 02:06:42.774
    STEP: Scaling up "test-rs" replicaset  01/12/23 02:06:42.774
    Jan 12 02:06:42.785: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/12/23 02:06:42.785
    W0112 02:06:42.795168      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 12 02:06:42.796: INFO: observed ReplicaSet test-rs in namespace replicaset-798 with ReadyReplicas 1, AvailableReplicas 1
    Jan 12 02:06:42.843: INFO: observed ReplicaSet test-rs in namespace replicaset-798 with ReadyReplicas 1, AvailableReplicas 1
    Jan 12 02:06:42.907: INFO: observed ReplicaSet test-rs in namespace replicaset-798 with ReadyReplicas 1, AvailableReplicas 1
    Jan 12 02:06:42.912: INFO: observed ReplicaSet test-rs in namespace replicaset-798 with ReadyReplicas 1, AvailableReplicas 1
    Jan 12 02:06:45.086: INFO: observed ReplicaSet test-rs in namespace replicaset-798 with ReadyReplicas 2, AvailableReplicas 2
    Jan 12 02:06:45.648: INFO: observed Replicaset test-rs in namespace replicaset-798 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:06:45.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-798" for this suite. 01/12/23 02:06:45.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:06:45.688
Jan 12 02:06:45.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pods 01/12/23 02:06:45.688
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:06:45.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:06:45.706
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 01/12/23 02:06:45.709
STEP: submitting the pod to kubernetes 01/12/23 02:06:45.709
Jan 12 02:06:45.741: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3" in namespace "pods-3013" to be "running and ready"
Jan 12 02:06:45.744: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.791484ms
Jan 12 02:06:45.744: INFO: The phase of Pod pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:06:47.749: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007571802s
Jan 12 02:06:47.749: INFO: The phase of Pod pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:06:49.750: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3": Phase="Running", Reason="", readiness=true. Elapsed: 4.008565529s
Jan 12 02:06:49.750: INFO: The phase of Pod pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3 is Running (Ready = true)
Jan 12 02:06:49.750: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/12/23 02:06:49.753
STEP: updating the pod 01/12/23 02:06:49.756
Jan 12 02:06:50.271: INFO: Successfully updated pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3"
Jan 12 02:06:50.271: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3" in namespace "pods-3013" to be "terminated with reason DeadlineExceeded"
Jan 12 02:06:50.274: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.932876ms
Jan 12 02:06:52.277: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3": Phase="Running", Reason="", readiness=false. Elapsed: 2.006198462s
Jan 12 02:06:54.278: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007794014s
Jan 12 02:06:54.278: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 02:06:54.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3013" for this suite. 01/12/23 02:06:54.284
------------------------------
• [SLOW TEST] [8.614 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:06:45.688
    Jan 12 02:06:45.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pods 01/12/23 02:06:45.688
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:06:45.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:06:45.706
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 01/12/23 02:06:45.709
    STEP: submitting the pod to kubernetes 01/12/23 02:06:45.709
    Jan 12 02:06:45.741: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3" in namespace "pods-3013" to be "running and ready"
    Jan 12 02:06:45.744: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.791484ms
    Jan 12 02:06:45.744: INFO: The phase of Pod pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:06:47.749: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007571802s
    Jan 12 02:06:47.749: INFO: The phase of Pod pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:06:49.750: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3": Phase="Running", Reason="", readiness=true. Elapsed: 4.008565529s
    Jan 12 02:06:49.750: INFO: The phase of Pod pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3 is Running (Ready = true)
    Jan 12 02:06:49.750: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/12/23 02:06:49.753
    STEP: updating the pod 01/12/23 02:06:49.756
    Jan 12 02:06:50.271: INFO: Successfully updated pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3"
    Jan 12 02:06:50.271: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3" in namespace "pods-3013" to be "terminated with reason DeadlineExceeded"
    Jan 12 02:06:50.274: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.932876ms
    Jan 12 02:06:52.277: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3": Phase="Running", Reason="", readiness=false. Elapsed: 2.006198462s
    Jan 12 02:06:54.278: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007794014s
    Jan 12 02:06:54.278: INFO: Pod "pod-update-activedeadlineseconds-62990668-c15a-49d3-827c-4868297b85b3" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:06:54.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3013" for this suite. 01/12/23 02:06:54.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:06:54.302
Jan 12 02:06:54.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename replication-controller 01/12/23 02:06:54.303
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:06:54.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:06:54.323
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 01/12/23 02:06:54.325
STEP: When the matched label of one of its pods change 01/12/23 02:06:54.331
Jan 12 02:06:54.334: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 12 02:06:59.341: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/12/23 02:06:59.353
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 12 02:07:00.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1443" for this suite. 01/12/23 02:07:00.366
------------------------------
• [SLOW TEST] [6.080 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:06:54.302
    Jan 12 02:06:54.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename replication-controller 01/12/23 02:06:54.303
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:06:54.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:06:54.323
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 01/12/23 02:06:54.325
    STEP: When the matched label of one of its pods change 01/12/23 02:06:54.331
    Jan 12 02:06:54.334: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 12 02:06:59.341: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/12/23 02:06:59.353
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:07:00.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1443" for this suite. 01/12/23 02:07:00.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:07:00.383
Jan 12 02:07:00.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename statefulset 01/12/23 02:07:00.384
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:07:00.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:07:00.402
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5420 01/12/23 02:07:00.405
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 01/12/23 02:07:00.41
STEP: Creating stateful set ss in namespace statefulset-5420 01/12/23 02:07:00.414
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5420 01/12/23 02:07:00.446
Jan 12 02:07:00.449: INFO: Found 0 stateful pods, waiting for 1
Jan 12 02:07:10.454: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/12/23 02:07:10.454
Jan 12 02:07:10.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 02:07:10.706: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 02:07:10.706: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 02:07:10.706: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 02:07:10.709: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 12 02:07:20.716: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 02:07:20.716: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 02:07:20.732: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999797s
Jan 12 02:07:21.736: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996783953s
Jan 12 02:07:22.740: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992560352s
Jan 12 02:07:23.744: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988365613s
Jan 12 02:07:24.748: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984257628s
Jan 12 02:07:25.752: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979923629s
Jan 12 02:07:26.757: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.975507384s
Jan 12 02:07:27.761: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.97175086s
Jan 12 02:07:28.765: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.967069366s
Jan 12 02:07:29.770: INFO: Verifying statefulset ss doesn't scale past 1 for another 963.195495ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5420 01/12/23 02:07:30.77
Jan 12 02:07:30.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 02:07:30.998: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 12 02:07:30.998: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 02:07:30.998: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 02:07:31.003: INFO: Found 1 stateful pods, waiting for 3
Jan 12 02:07:41.009: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 02:07:41.009: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 02:07:41.009: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/12/23 02:07:41.009
STEP: Scale down will halt with unhealthy stateful pod 01/12/23 02:07:41.009
Jan 12 02:07:41.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 02:07:41.230: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 02:07:41.230: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 02:07:41.230: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 02:07:41.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 02:07:41.528: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 02:07:41.528: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 02:07:41.528: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 02:07:41.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 02:07:41.755: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 02:07:41.755: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 02:07:41.755: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 02:07:41.755: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 02:07:41.758: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 12 02:07:51.770: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 02:07:51.770: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 02:07:51.770: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 02:07:51.787: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999769s
Jan 12 02:07:52.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992570433s
Jan 12 02:07:53.797: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987859808s
Jan 12 02:07:54.801: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983333673s
Jan 12 02:07:55.806: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978736123s
Jan 12 02:07:56.811: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973353623s
Jan 12 02:07:57.816: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969185674s
Jan 12 02:07:58.820: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964807503s
Jan 12 02:07:59.826: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959834034s
Jan 12 02:08:00.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.787587ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5420 01/12/23 02:08:01.831
Jan 12 02:08:01.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 02:08:02.038: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 12 02:08:02.038: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 02:08:02.038: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 02:08:02.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 02:08:02.332: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 12 02:08:02.332: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 02:08:02.332: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 02:08:02.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 02:08:02.577: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 12 02:08:02.577: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 02:08:02.577: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 02:08:02.577: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/12/23 02:08:12.595
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 02:08:12.595: INFO: Deleting all statefulset in ns statefulset-5420
Jan 12 02:08:12.598: INFO: Scaling statefulset ss to 0
Jan 12 02:08:12.612: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 02:08:12.615: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 02:08:12.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5420" for this suite. 01/12/23 02:08:12.634
------------------------------
• [SLOW TEST] [72.282 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:07:00.383
    Jan 12 02:07:00.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename statefulset 01/12/23 02:07:00.384
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:07:00.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:07:00.402
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5420 01/12/23 02:07:00.405
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/12/23 02:07:00.41
    STEP: Creating stateful set ss in namespace statefulset-5420 01/12/23 02:07:00.414
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5420 01/12/23 02:07:00.446
    Jan 12 02:07:00.449: INFO: Found 0 stateful pods, waiting for 1
    Jan 12 02:07:10.454: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/12/23 02:07:10.454
    Jan 12 02:07:10.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 02:07:10.706: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 02:07:10.706: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 02:07:10.706: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 02:07:10.709: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 12 02:07:20.716: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 02:07:20.716: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 02:07:20.732: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999797s
    Jan 12 02:07:21.736: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996783953s
    Jan 12 02:07:22.740: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992560352s
    Jan 12 02:07:23.744: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988365613s
    Jan 12 02:07:24.748: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984257628s
    Jan 12 02:07:25.752: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979923629s
    Jan 12 02:07:26.757: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.975507384s
    Jan 12 02:07:27.761: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.97175086s
    Jan 12 02:07:28.765: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.967069366s
    Jan 12 02:07:29.770: INFO: Verifying statefulset ss doesn't scale past 1 for another 963.195495ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5420 01/12/23 02:07:30.77
    Jan 12 02:07:30.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 02:07:30.998: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 12 02:07:30.998: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 02:07:30.998: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 02:07:31.003: INFO: Found 1 stateful pods, waiting for 3
    Jan 12 02:07:41.009: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 02:07:41.009: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 02:07:41.009: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/12/23 02:07:41.009
    STEP: Scale down will halt with unhealthy stateful pod 01/12/23 02:07:41.009
    Jan 12 02:07:41.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 02:07:41.230: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 02:07:41.230: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 02:07:41.230: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 02:07:41.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 02:07:41.528: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 02:07:41.528: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 02:07:41.528: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 02:07:41.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 02:07:41.755: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 02:07:41.755: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 02:07:41.755: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 02:07:41.755: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 02:07:41.758: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 12 02:07:51.770: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 02:07:51.770: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 02:07:51.770: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 02:07:51.787: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999769s
    Jan 12 02:07:52.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992570433s
    Jan 12 02:07:53.797: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987859808s
    Jan 12 02:07:54.801: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983333673s
    Jan 12 02:07:55.806: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978736123s
    Jan 12 02:07:56.811: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973353623s
    Jan 12 02:07:57.816: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969185674s
    Jan 12 02:07:58.820: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964807503s
    Jan 12 02:07:59.826: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959834034s
    Jan 12 02:08:00.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.787587ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5420 01/12/23 02:08:01.831
    Jan 12 02:08:01.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 02:08:02.038: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 12 02:08:02.038: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 02:08:02.038: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 02:08:02.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 02:08:02.332: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 12 02:08:02.332: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 02:08:02.332: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 02:08:02.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=statefulset-5420 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 02:08:02.577: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 12 02:08:02.577: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 02:08:02.577: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 02:08:02.577: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/12/23 02:08:12.595
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 02:08:12.595: INFO: Deleting all statefulset in ns statefulset-5420
    Jan 12 02:08:12.598: INFO: Scaling statefulset ss to 0
    Jan 12 02:08:12.612: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 02:08:12.615: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:08:12.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5420" for this suite. 01/12/23 02:08:12.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:08:12.668
Jan 12 02:08:12.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename namespaces 01/12/23 02:08:12.669
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:08:12.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:08:12.688
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 01/12/23 02:08:12.691
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:08:12.706
STEP: Creating a pod in the namespace 01/12/23 02:08:12.709
STEP: Waiting for the pod to have running status 01/12/23 02:08:12.817
Jan 12 02:08:12.817: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2129" to be "running"
Jan 12 02:08:12.820: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.038662ms
Jan 12 02:08:14.824: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007223082s
Jan 12 02:08:16.824: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007125831s
Jan 12 02:08:16.824: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/12/23 02:08:16.824
STEP: Waiting for the namespace to be removed. 01/12/23 02:08:16.841
STEP: Recreating the namespace 01/12/23 02:08:27.845
STEP: Verifying there are no pods in the namespace 01/12/23 02:08:27.863
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 02:08:27.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6004" for this suite. 01/12/23 02:08:27.871
STEP: Destroying namespace "nsdeletetest-2129" for this suite. 01/12/23 02:08:27.964
Jan 12 02:08:27.967: INFO: Namespace nsdeletetest-2129 was already deleted
STEP: Destroying namespace "nsdeletetest-9238" for this suite. 01/12/23 02:08:27.967
------------------------------
• [SLOW TEST] [15.391 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:08:12.668
    Jan 12 02:08:12.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename namespaces 01/12/23 02:08:12.669
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:08:12.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:08:12.688
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 01/12/23 02:08:12.691
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:08:12.706
    STEP: Creating a pod in the namespace 01/12/23 02:08:12.709
    STEP: Waiting for the pod to have running status 01/12/23 02:08:12.817
    Jan 12 02:08:12.817: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2129" to be "running"
    Jan 12 02:08:12.820: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.038662ms
    Jan 12 02:08:14.824: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007223082s
    Jan 12 02:08:16.824: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007125831s
    Jan 12 02:08:16.824: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/12/23 02:08:16.824
    STEP: Waiting for the namespace to be removed. 01/12/23 02:08:16.841
    STEP: Recreating the namespace 01/12/23 02:08:27.845
    STEP: Verifying there are no pods in the namespace 01/12/23 02:08:27.863
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:08:27.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6004" for this suite. 01/12/23 02:08:27.871
    STEP: Destroying namespace "nsdeletetest-2129" for this suite. 01/12/23 02:08:27.964
    Jan 12 02:08:27.967: INFO: Namespace nsdeletetest-2129 was already deleted
    STEP: Destroying namespace "nsdeletetest-9238" for this suite. 01/12/23 02:08:27.967
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:08:28.059
Jan 12 02:08:28.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename cronjob 01/12/23 02:08:28.06
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:08:28.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:08:28.079
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/12/23 02:08:28.082
STEP: Ensuring no jobs are scheduled 01/12/23 02:08:28.089
STEP: Ensuring no job exists by listing jobs explicitly 01/12/23 02:13:28.096
STEP: Removing cronjob 01/12/23 02:13:28.098
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 12 02:13:28.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2100" for this suite. 01/12/23 02:13:28.109
------------------------------
• [SLOW TEST] [300.072 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:08:28.059
    Jan 12 02:08:28.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename cronjob 01/12/23 02:08:28.06
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:08:28.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:08:28.079
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/12/23 02:08:28.082
    STEP: Ensuring no jobs are scheduled 01/12/23 02:08:28.089
    STEP: Ensuring no job exists by listing jobs explicitly 01/12/23 02:13:28.096
    STEP: Removing cronjob 01/12/23 02:13:28.098
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:13:28.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2100" for this suite. 01/12/23 02:13:28.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:13:28.132
Jan 12 02:13:28.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 02:13:28.133
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:13:28.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:13:28.157
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/12/23 02:13:28.164
Jan 12 02:13:28.214: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2060" to be "running and ready"
Jan 12 02:13:28.217: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.825865ms
Jan 12 02:13:28.217: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:13:30.222: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008451619s
Jan 12 02:13:30.222: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:13:32.222: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.007720126s
Jan 12 02:13:32.222: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 12 02:13:32.222: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 01/12/23 02:13:32.225
Jan 12 02:13:32.262: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2060" to be "running and ready"
Jan 12 02:13:32.265: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.962398ms
Jan 12 02:13:32.265: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:13:34.270: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0082355s
Jan 12 02:13:34.270: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:13:36.270: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.008020074s
Jan 12 02:13:36.270: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 12 02:13:36.270: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/12/23 02:13:36.273
STEP: delete the pod with lifecycle hook 01/12/23 02:13:36.292
Jan 12 02:13:36.302: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 12 02:13:36.306: INFO: Pod pod-with-poststart-http-hook still exists
Jan 12 02:13:38.306: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 12 02:13:38.310: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 12 02:13:38.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2060" for this suite. 01/12/23 02:13:38.315
------------------------------
• [SLOW TEST] [10.204 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:13:28.132
    Jan 12 02:13:28.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 02:13:28.133
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:13:28.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:13:28.157
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/12/23 02:13:28.164
    Jan 12 02:13:28.214: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2060" to be "running and ready"
    Jan 12 02:13:28.217: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.825865ms
    Jan 12 02:13:28.217: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:13:30.222: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008451619s
    Jan 12 02:13:30.222: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:13:32.222: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.007720126s
    Jan 12 02:13:32.222: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 12 02:13:32.222: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 01/12/23 02:13:32.225
    Jan 12 02:13:32.262: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2060" to be "running and ready"
    Jan 12 02:13:32.265: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.962398ms
    Jan 12 02:13:32.265: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:13:34.270: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0082355s
    Jan 12 02:13:34.270: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:13:36.270: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.008020074s
    Jan 12 02:13:36.270: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 12 02:13:36.270: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/12/23 02:13:36.273
    STEP: delete the pod with lifecycle hook 01/12/23 02:13:36.292
    Jan 12 02:13:36.302: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 12 02:13:36.306: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 12 02:13:38.306: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 12 02:13:38.310: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:13:38.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2060" for this suite. 01/12/23 02:13:38.315
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:13:38.336
Jan 12 02:13:38.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 02:13:38.337
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:13:38.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:13:38.36
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 01/12/23 02:13:38.363
Jan 12 02:13:38.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: mark a version not serverd 01/12/23 02:13:43.089
STEP: check the unserved version gets removed 01/12/23 02:13:43.108
STEP: check the other version is not changed 01/12/23 02:13:44.964
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 02:13:49.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3404" for this suite. 01/12/23 02:13:49.187
------------------------------
• [SLOW TEST] [10.867 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:13:38.336
    Jan 12 02:13:38.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 02:13:38.337
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:13:38.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:13:38.36
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 01/12/23 02:13:38.363
    Jan 12 02:13:38.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: mark a version not serverd 01/12/23 02:13:43.089
    STEP: check the unserved version gets removed 01/12/23 02:13:43.108
    STEP: check the other version is not changed 01/12/23 02:13:44.964
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:13:49.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3404" for this suite. 01/12/23 02:13:49.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:13:49.205
Jan 12 02:13:49.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename csiinlinevolumes 01/12/23 02:13:49.206
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:13:49.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:13:49.231
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 01/12/23 02:13:49.233
STEP: getting 01/12/23 02:13:49.312
STEP: listing in namespace 01/12/23 02:13:49.315
STEP: patching 01/12/23 02:13:49.318
STEP: deleting 01/12/23 02:13:49.327
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 12 02:13:49.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2163" for this suite. 01/12/23 02:13:49.35
------------------------------
• [0.159 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:13:49.205
    Jan 12 02:13:49.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename csiinlinevolumes 01/12/23 02:13:49.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:13:49.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:13:49.231
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 01/12/23 02:13:49.233
    STEP: getting 01/12/23 02:13:49.312
    STEP: listing in namespace 01/12/23 02:13:49.315
    STEP: patching 01/12/23 02:13:49.318
    STEP: deleting 01/12/23 02:13:49.327
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:13:49.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2163" for this suite. 01/12/23 02:13:49.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:13:49.366
Jan 12 02:13:49.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 02:13:49.366
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:13:49.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:13:49.389
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 01/12/23 02:13:49.391
Jan 12 02:13:49.424: INFO: Waiting up to 5m0s for pod "labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e" in namespace "projected-5042" to be "running and ready"
Jan 12 02:13:49.427: INFO: Pod "labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.835271ms
Jan 12 02:13:49.427: INFO: The phase of Pod labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:13:51.431: INFO: Pod "labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006784766s
Jan 12 02:13:51.431: INFO: The phase of Pod labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:13:53.431: INFO: Pod "labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e": Phase="Running", Reason="", readiness=true. Elapsed: 4.00679452s
Jan 12 02:13:53.431: INFO: The phase of Pod labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e is Running (Ready = true)
Jan 12 02:13:53.431: INFO: Pod "labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e" satisfied condition "running and ready"
Jan 12 02:13:53.962: INFO: Successfully updated pod "labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 02:13:55.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5042" for this suite. 01/12/23 02:13:55.996
------------------------------
• [SLOW TEST] [6.652 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:13:49.366
    Jan 12 02:13:49.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 02:13:49.366
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:13:49.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:13:49.389
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 01/12/23 02:13:49.391
    Jan 12 02:13:49.424: INFO: Waiting up to 5m0s for pod "labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e" in namespace "projected-5042" to be "running and ready"
    Jan 12 02:13:49.427: INFO: Pod "labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.835271ms
    Jan 12 02:13:49.427: INFO: The phase of Pod labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:13:51.431: INFO: Pod "labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006784766s
    Jan 12 02:13:51.431: INFO: The phase of Pod labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:13:53.431: INFO: Pod "labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e": Phase="Running", Reason="", readiness=true. Elapsed: 4.00679452s
    Jan 12 02:13:53.431: INFO: The phase of Pod labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e is Running (Ready = true)
    Jan 12 02:13:53.431: INFO: Pod "labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e" satisfied condition "running and ready"
    Jan 12 02:13:53.962: INFO: Successfully updated pod "labelsupdate0aaf4b7c-7c22-48fc-a9b6-de3287f7650e"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:13:55.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5042" for this suite. 01/12/23 02:13:55.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:13:56.022
Jan 12 02:13:56.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 02:13:56.023
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:13:56.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:13:56.047
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 02:13:56.049
Jan 12 02:13:56.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-5646 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 12 02:13:56.149: INFO: stderr: ""
Jan 12 02:13:56.149: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/12/23 02:13:56.149
Jan 12 02:13:56.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-5646 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Jan 12 02:13:56.667: INFO: stderr: ""
Jan 12 02:13:56.667: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 02:13:56.667
Jan 12 02:13:56.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-5646 delete pods e2e-test-httpd-pod'
Jan 12 02:13:59.190: INFO: stderr: ""
Jan 12 02:13:59.190: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 02:13:59.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5646" for this suite. 01/12/23 02:13:59.197
------------------------------
• [3.255 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:13:56.022
    Jan 12 02:13:56.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 02:13:56.023
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:13:56.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:13:56.047
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 02:13:56.049
    Jan 12 02:13:56.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-5646 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 12 02:13:56.149: INFO: stderr: ""
    Jan 12 02:13:56.149: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/12/23 02:13:56.149
    Jan 12 02:13:56.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-5646 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Jan 12 02:13:56.667: INFO: stderr: ""
    Jan 12 02:13:56.667: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 02:13:56.667
    Jan 12 02:13:56.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-5646 delete pods e2e-test-httpd-pod'
    Jan 12 02:13:59.190: INFO: stderr: ""
    Jan 12 02:13:59.190: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:13:59.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5646" for this suite. 01/12/23 02:13:59.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:13:59.279
Jan 12 02:13:59.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename init-container 01/12/23 02:13:59.28
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:13:59.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:13:59.301
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 01/12/23 02:13:59.304
Jan 12 02:13:59.304: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 12 02:14:04.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4105" for this suite. 01/12/23 02:14:04.535
------------------------------
• [SLOW TEST] [5.273 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:13:59.279
    Jan 12 02:13:59.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename init-container 01/12/23 02:13:59.28
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:13:59.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:13:59.301
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 01/12/23 02:13:59.304
    Jan 12 02:13:59.304: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:14:04.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4105" for this suite. 01/12/23 02:14:04.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:14:04.556
Jan 12 02:14:04.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename resourcequota 01/12/23 02:14:04.558
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:04.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:04.582
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-2p2hj" 01/12/23 02:14:04.588
Jan 12 02:14:04.599: INFO: Resource quota "e2e-rq-status-2p2hj" reports spec: hard cpu limit of 500m
Jan 12 02:14:04.599: INFO: Resource quota "e2e-rq-status-2p2hj" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-2p2hj" /status 01/12/23 02:14:04.599
STEP: Confirm /status for "e2e-rq-status-2p2hj" resourceQuota via watch 01/12/23 02:14:04.611
Jan 12 02:14:04.612: INFO: observed resourceQuota "e2e-rq-status-2p2hj" in namespace "resourcequota-8138" with hard status: v1.ResourceList(nil)
Jan 12 02:14:04.612: INFO: Found resourceQuota "e2e-rq-status-2p2hj" in namespace "resourcequota-8138" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 12 02:14:04.612: INFO: ResourceQuota "e2e-rq-status-2p2hj" /status was updated
STEP: Patching hard spec values for cpu & memory 01/12/23 02:14:04.615
Jan 12 02:14:04.623: INFO: Resource quota "e2e-rq-status-2p2hj" reports spec: hard cpu limit of 1
Jan 12 02:14:04.623: INFO: Resource quota "e2e-rq-status-2p2hj" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-2p2hj" /status 01/12/23 02:14:04.623
STEP: Confirm /status for "e2e-rq-status-2p2hj" resourceQuota via watch 01/12/23 02:14:04.634
Jan 12 02:14:04.636: INFO: observed resourceQuota "e2e-rq-status-2p2hj" in namespace "resourcequota-8138" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 12 02:14:04.636: INFO: Found resourceQuota "e2e-rq-status-2p2hj" in namespace "resourcequota-8138" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jan 12 02:14:04.636: INFO: ResourceQuota "e2e-rq-status-2p2hj" /status was patched
STEP: Get "e2e-rq-status-2p2hj" /status 01/12/23 02:14:04.636
Jan 12 02:14:04.643: INFO: Resourcequota "e2e-rq-status-2p2hj" reports status: hard cpu of 1
Jan 12 02:14:04.643: INFO: Resourcequota "e2e-rq-status-2p2hj" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-2p2hj" /status before checking Spec is unchanged 01/12/23 02:14:04.646
Jan 12 02:14:04.652: INFO: Resourcequota "e2e-rq-status-2p2hj" reports status: hard cpu of 2
Jan 12 02:14:04.652: INFO: Resourcequota "e2e-rq-status-2p2hj" reports status: hard memory of 2Gi
Jan 12 02:14:04.653: INFO: Found resourceQuota "e2e-rq-status-2p2hj" in namespace "resourcequota-8138" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jan 12 02:14:14.660: INFO: ResourceQuota "e2e-rq-status-2p2hj" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 02:14:14.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8138" for this suite. 01/12/23 02:14:14.665
------------------------------
• [SLOW TEST] [10.189 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:14:04.556
    Jan 12 02:14:04.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename resourcequota 01/12/23 02:14:04.558
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:04.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:04.582
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-2p2hj" 01/12/23 02:14:04.588
    Jan 12 02:14:04.599: INFO: Resource quota "e2e-rq-status-2p2hj" reports spec: hard cpu limit of 500m
    Jan 12 02:14:04.599: INFO: Resource quota "e2e-rq-status-2p2hj" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-2p2hj" /status 01/12/23 02:14:04.599
    STEP: Confirm /status for "e2e-rq-status-2p2hj" resourceQuota via watch 01/12/23 02:14:04.611
    Jan 12 02:14:04.612: INFO: observed resourceQuota "e2e-rq-status-2p2hj" in namespace "resourcequota-8138" with hard status: v1.ResourceList(nil)
    Jan 12 02:14:04.612: INFO: Found resourceQuota "e2e-rq-status-2p2hj" in namespace "resourcequota-8138" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 12 02:14:04.612: INFO: ResourceQuota "e2e-rq-status-2p2hj" /status was updated
    STEP: Patching hard spec values for cpu & memory 01/12/23 02:14:04.615
    Jan 12 02:14:04.623: INFO: Resource quota "e2e-rq-status-2p2hj" reports spec: hard cpu limit of 1
    Jan 12 02:14:04.623: INFO: Resource quota "e2e-rq-status-2p2hj" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-2p2hj" /status 01/12/23 02:14:04.623
    STEP: Confirm /status for "e2e-rq-status-2p2hj" resourceQuota via watch 01/12/23 02:14:04.634
    Jan 12 02:14:04.636: INFO: observed resourceQuota "e2e-rq-status-2p2hj" in namespace "resourcequota-8138" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 12 02:14:04.636: INFO: Found resourceQuota "e2e-rq-status-2p2hj" in namespace "resourcequota-8138" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jan 12 02:14:04.636: INFO: ResourceQuota "e2e-rq-status-2p2hj" /status was patched
    STEP: Get "e2e-rq-status-2p2hj" /status 01/12/23 02:14:04.636
    Jan 12 02:14:04.643: INFO: Resourcequota "e2e-rq-status-2p2hj" reports status: hard cpu of 1
    Jan 12 02:14:04.643: INFO: Resourcequota "e2e-rq-status-2p2hj" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-2p2hj" /status before checking Spec is unchanged 01/12/23 02:14:04.646
    Jan 12 02:14:04.652: INFO: Resourcequota "e2e-rq-status-2p2hj" reports status: hard cpu of 2
    Jan 12 02:14:04.652: INFO: Resourcequota "e2e-rq-status-2p2hj" reports status: hard memory of 2Gi
    Jan 12 02:14:04.653: INFO: Found resourceQuota "e2e-rq-status-2p2hj" in namespace "resourcequota-8138" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jan 12 02:14:14.660: INFO: ResourceQuota "e2e-rq-status-2p2hj" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:14:14.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8138" for this suite. 01/12/23 02:14:14.665
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:14:14.746
Jan 12 02:14:14.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename secrets 01/12/23 02:14:14.746
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:14.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:14.772
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-99d4da75-48f0-40f5-ae7b-94bb38e52333 01/12/23 02:14:14.775
STEP: Creating a pod to test consume secrets 01/12/23 02:14:14.78
Jan 12 02:14:14.912: INFO: Waiting up to 5m0s for pod "pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba" in namespace "secrets-593" to be "Succeeded or Failed"
Jan 12 02:14:14.915: INFO: Pod "pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.636587ms
Jan 12 02:14:16.922: INFO: Pod "pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009513005s
Jan 12 02:14:18.920: INFO: Pod "pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007524241s
Jan 12 02:14:20.922: INFO: Pod "pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00921478s
STEP: Saw pod success 01/12/23 02:14:20.922
Jan 12 02:14:20.922: INFO: Pod "pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba" satisfied condition "Succeeded or Failed"
Jan 12 02:14:20.925: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 02:14:20.935
Jan 12 02:14:20.951: INFO: Waiting for pod pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba to disappear
Jan 12 02:14:20.954: INFO: Pod pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 02:14:20.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-593" for this suite. 01/12/23 02:14:20.959
------------------------------
• [SLOW TEST] [6.229 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:14:14.746
    Jan 12 02:14:14.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename secrets 01/12/23 02:14:14.746
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:14.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:14.772
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-99d4da75-48f0-40f5-ae7b-94bb38e52333 01/12/23 02:14:14.775
    STEP: Creating a pod to test consume secrets 01/12/23 02:14:14.78
    Jan 12 02:14:14.912: INFO: Waiting up to 5m0s for pod "pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba" in namespace "secrets-593" to be "Succeeded or Failed"
    Jan 12 02:14:14.915: INFO: Pod "pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.636587ms
    Jan 12 02:14:16.922: INFO: Pod "pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009513005s
    Jan 12 02:14:18.920: INFO: Pod "pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007524241s
    Jan 12 02:14:20.922: INFO: Pod "pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00921478s
    STEP: Saw pod success 01/12/23 02:14:20.922
    Jan 12 02:14:20.922: INFO: Pod "pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba" satisfied condition "Succeeded or Failed"
    Jan 12 02:14:20.925: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 02:14:20.935
    Jan 12 02:14:20.951: INFO: Waiting for pod pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba to disappear
    Jan 12 02:14:20.954: INFO: Pod pod-secrets-6454a745-a62f-4e50-9825-9baa75eeb5ba no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:14:20.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-593" for this suite. 01/12/23 02:14:20.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:14:20.977
Jan 12 02:14:20.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 02:14:20.978
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:20.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:20.999
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-660a8eb9-2dd6-43ff-a2be-5e2ef5e201d0 01/12/23 02:14:21.006
STEP: Creating secret with name s-test-opt-upd-4a6579e1-338d-4f72-ae6c-0a31eb7fe157 01/12/23 02:14:21.013
STEP: Creating the pod 01/12/23 02:14:21.019
Jan 12 02:14:21.053: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42" in namespace "projected-4439" to be "running and ready"
Jan 12 02:14:21.056: INFO: Pod "pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.871386ms
Jan 12 02:14:21.056: INFO: The phase of Pod pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:14:23.060: INFO: Pod "pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007089608s
Jan 12 02:14:23.060: INFO: The phase of Pod pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:14:25.062: INFO: Pod "pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42": Phase="Running", Reason="", readiness=true. Elapsed: 4.008845406s
Jan 12 02:14:25.062: INFO: The phase of Pod pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42 is Running (Ready = true)
Jan 12 02:14:25.062: INFO: Pod "pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-660a8eb9-2dd6-43ff-a2be-5e2ef5e201d0 01/12/23 02:14:25.093
STEP: Updating secret s-test-opt-upd-4a6579e1-338d-4f72-ae6c-0a31eb7fe157 01/12/23 02:14:25.099
STEP: Creating secret with name s-test-opt-create-7d468cc6-6f24-48fa-8dae-e2829e5d2089 01/12/23 02:14:25.104
STEP: waiting to observe update in volume 01/12/23 02:14:25.109
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 12 02:14:27.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4439" for this suite. 01/12/23 02:14:27.152
------------------------------
• [SLOW TEST] [6.192 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:14:20.977
    Jan 12 02:14:20.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 02:14:20.978
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:20.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:20.999
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-660a8eb9-2dd6-43ff-a2be-5e2ef5e201d0 01/12/23 02:14:21.006
    STEP: Creating secret with name s-test-opt-upd-4a6579e1-338d-4f72-ae6c-0a31eb7fe157 01/12/23 02:14:21.013
    STEP: Creating the pod 01/12/23 02:14:21.019
    Jan 12 02:14:21.053: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42" in namespace "projected-4439" to be "running and ready"
    Jan 12 02:14:21.056: INFO: Pod "pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.871386ms
    Jan 12 02:14:21.056: INFO: The phase of Pod pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:14:23.060: INFO: Pod "pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007089608s
    Jan 12 02:14:23.060: INFO: The phase of Pod pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:14:25.062: INFO: Pod "pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42": Phase="Running", Reason="", readiness=true. Elapsed: 4.008845406s
    Jan 12 02:14:25.062: INFO: The phase of Pod pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42 is Running (Ready = true)
    Jan 12 02:14:25.062: INFO: Pod "pod-projected-secrets-cecb6f74-2b9f-46bc-8ab2-c34ce2187b42" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-660a8eb9-2dd6-43ff-a2be-5e2ef5e201d0 01/12/23 02:14:25.093
    STEP: Updating secret s-test-opt-upd-4a6579e1-338d-4f72-ae6c-0a31eb7fe157 01/12/23 02:14:25.099
    STEP: Creating secret with name s-test-opt-create-7d468cc6-6f24-48fa-8dae-e2829e5d2089 01/12/23 02:14:25.104
    STEP: waiting to observe update in volume 01/12/23 02:14:25.109
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:14:27.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4439" for this suite. 01/12/23 02:14:27.152
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:14:27.173
Jan 12 02:14:27.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename secrets 01/12/23 02:14:27.173
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:27.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:27.192
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-32c81db5-9cea-471b-8a91-72d2aed33afc 01/12/23 02:14:27.194
STEP: Creating a pod to test consume secrets 01/12/23 02:14:27.2
Jan 12 02:14:27.231: INFO: Waiting up to 5m0s for pod "pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7" in namespace "secrets-8582" to be "Succeeded or Failed"
Jan 12 02:14:27.234: INFO: Pod "pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.780262ms
Jan 12 02:14:29.238: INFO: Pod "pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006276758s
Jan 12 02:14:31.240: INFO: Pod "pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008550421s
Jan 12 02:14:33.238: INFO: Pod "pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006880558s
STEP: Saw pod success 01/12/23 02:14:33.238
Jan 12 02:14:33.239: INFO: Pod "pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7" satisfied condition "Succeeded or Failed"
Jan 12 02:14:33.241: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7 container secret-env-test: <nil>
STEP: delete the pod 01/12/23 02:14:33.252
Jan 12 02:14:33.265: INFO: Waiting for pod pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7 to disappear
Jan 12 02:14:33.268: INFO: Pod pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 02:14:33.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8582" for this suite. 01/12/23 02:14:33.273
------------------------------
• [SLOW TEST] [6.179 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:14:27.173
    Jan 12 02:14:27.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename secrets 01/12/23 02:14:27.173
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:27.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:27.192
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-32c81db5-9cea-471b-8a91-72d2aed33afc 01/12/23 02:14:27.194
    STEP: Creating a pod to test consume secrets 01/12/23 02:14:27.2
    Jan 12 02:14:27.231: INFO: Waiting up to 5m0s for pod "pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7" in namespace "secrets-8582" to be "Succeeded or Failed"
    Jan 12 02:14:27.234: INFO: Pod "pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.780262ms
    Jan 12 02:14:29.238: INFO: Pod "pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006276758s
    Jan 12 02:14:31.240: INFO: Pod "pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008550421s
    Jan 12 02:14:33.238: INFO: Pod "pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006880558s
    STEP: Saw pod success 01/12/23 02:14:33.238
    Jan 12 02:14:33.239: INFO: Pod "pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7" satisfied condition "Succeeded or Failed"
    Jan 12 02:14:33.241: INFO: Trying to get logs from node eqx04-flash06 pod pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7 container secret-env-test: <nil>
    STEP: delete the pod 01/12/23 02:14:33.252
    Jan 12 02:14:33.265: INFO: Waiting for pod pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7 to disappear
    Jan 12 02:14:33.268: INFO: Pod pod-secrets-a3e6eb6c-1cfe-4996-94da-cc57ddcbe0e7 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:14:33.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8582" for this suite. 01/12/23 02:14:33.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:14:33.352
Jan 12 02:14:33.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 02:14:33.354
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:33.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:33.371
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 02:14:33.388
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 02:14:33.833
STEP: Deploying the webhook pod 01/12/23 02:14:33.842
STEP: Wait for the deployment to be ready 01/12/23 02:14:33.916
Jan 12 02:14:33.924: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 12 02:14:35.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 2, 14, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 14, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 2, 14, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 14, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 02:14:37.941
STEP: Verifying the service has paired with the endpoint 01/12/23 02:14:37.956
Jan 12 02:14:38.956: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 01/12/23 02:14:38.96
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/12/23 02:14:38.961
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/12/23 02:14:38.961
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/12/23 02:14:38.961
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/12/23 02:14:38.962
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/12/23 02:14:38.962
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/12/23 02:14:38.963
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 02:14:38.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3698" for this suite. 01/12/23 02:14:39.01
STEP: Destroying namespace "webhook-3698-markers" for this suite. 01/12/23 02:14:39.065
------------------------------
• [SLOW TEST] [5.793 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:14:33.352
    Jan 12 02:14:33.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 02:14:33.354
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:33.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:33.371
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 02:14:33.388
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 02:14:33.833
    STEP: Deploying the webhook pod 01/12/23 02:14:33.842
    STEP: Wait for the deployment to be ready 01/12/23 02:14:33.916
    Jan 12 02:14:33.924: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 12 02:14:35.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 2, 14, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 14, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 2, 14, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 14, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 02:14:37.941
    STEP: Verifying the service has paired with the endpoint 01/12/23 02:14:37.956
    Jan 12 02:14:38.956: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 01/12/23 02:14:38.96
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/12/23 02:14:38.961
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/12/23 02:14:38.961
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/12/23 02:14:38.961
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/12/23 02:14:38.962
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/12/23 02:14:38.962
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/12/23 02:14:38.963
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:14:38.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3698" for this suite. 01/12/23 02:14:39.01
    STEP: Destroying namespace "webhook-3698-markers" for this suite. 01/12/23 02:14:39.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:14:39.146
Jan 12 02:14:39.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename configmap 01/12/23 02:14:39.147
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:39.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:39.165
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 02:14:39.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6135" for this suite. 01/12/23 02:14:39.211
------------------------------
• [0.079 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:14:39.146
    Jan 12 02:14:39.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename configmap 01/12/23 02:14:39.147
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:39.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:39.165
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:14:39.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6135" for this suite. 01/12/23 02:14:39.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:14:39.227
Jan 12 02:14:39.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 02:14:39.228
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:39.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:39.245
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 01/12/23 02:14:39.248
Jan 12 02:14:39.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 create -f -'
Jan 12 02:14:40.034: INFO: stderr: ""
Jan 12 02:14:40.034: INFO: stdout: "pod/pause created\n"
Jan 12 02:14:40.034: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 12 02:14:40.034: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4720" to be "running and ready"
Jan 12 02:14:40.038: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.60778ms
Jan 12 02:14:40.038: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
Jan 12 02:14:42.042: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008186053s
Jan 12 02:14:42.042: INFO: Pod "pause" satisfied condition "running and ready"
Jan 12 02:14:42.043: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 01/12/23 02:14:42.043
Jan 12 02:14:42.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 label pods pause testing-label=testing-label-value'
Jan 12 02:14:42.123: INFO: stderr: ""
Jan 12 02:14:42.123: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/12/23 02:14:42.123
Jan 12 02:14:42.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 get pod pause -L testing-label'
Jan 12 02:14:42.191: INFO: stderr: ""
Jan 12 02:14:42.191: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/12/23 02:14:42.191
Jan 12 02:14:42.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 label pods pause testing-label-'
Jan 12 02:14:42.264: INFO: stderr: ""
Jan 12 02:14:42.264: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/12/23 02:14:42.264
Jan 12 02:14:42.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 get pod pause -L testing-label'
Jan 12 02:14:42.332: INFO: stderr: ""
Jan 12 02:14:42.332: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 01/12/23 02:14:42.332
Jan 12 02:14:42.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 delete --grace-period=0 --force -f -'
Jan 12 02:14:42.408: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 02:14:42.408: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 12 02:14:42.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 get rc,svc -l name=pause --no-headers'
Jan 12 02:14:42.505: INFO: stderr: "No resources found in kubectl-4720 namespace.\n"
Jan 12 02:14:42.505: INFO: stdout: ""
Jan 12 02:14:42.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 12 02:14:42.596: INFO: stderr: ""
Jan 12 02:14:42.596: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 02:14:42.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4720" for this suite. 01/12/23 02:14:42.601
------------------------------
• [3.391 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:14:39.227
    Jan 12 02:14:39.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 02:14:39.228
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:39.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:39.245
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 01/12/23 02:14:39.248
    Jan 12 02:14:39.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 create -f -'
    Jan 12 02:14:40.034: INFO: stderr: ""
    Jan 12 02:14:40.034: INFO: stdout: "pod/pause created\n"
    Jan 12 02:14:40.034: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 12 02:14:40.034: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4720" to be "running and ready"
    Jan 12 02:14:40.038: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.60778ms
    Jan 12 02:14:40.038: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
    Jan 12 02:14:42.042: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008186053s
    Jan 12 02:14:42.042: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 12 02:14:42.043: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 01/12/23 02:14:42.043
    Jan 12 02:14:42.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 label pods pause testing-label=testing-label-value'
    Jan 12 02:14:42.123: INFO: stderr: ""
    Jan 12 02:14:42.123: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/12/23 02:14:42.123
    Jan 12 02:14:42.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 get pod pause -L testing-label'
    Jan 12 02:14:42.191: INFO: stderr: ""
    Jan 12 02:14:42.191: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/12/23 02:14:42.191
    Jan 12 02:14:42.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 label pods pause testing-label-'
    Jan 12 02:14:42.264: INFO: stderr: ""
    Jan 12 02:14:42.264: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/12/23 02:14:42.264
    Jan 12 02:14:42.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 get pod pause -L testing-label'
    Jan 12 02:14:42.332: INFO: stderr: ""
    Jan 12 02:14:42.332: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 01/12/23 02:14:42.332
    Jan 12 02:14:42.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 delete --grace-period=0 --force -f -'
    Jan 12 02:14:42.408: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 02:14:42.408: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 12 02:14:42.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 get rc,svc -l name=pause --no-headers'
    Jan 12 02:14:42.505: INFO: stderr: "No resources found in kubectl-4720 namespace.\n"
    Jan 12 02:14:42.505: INFO: stdout: ""
    Jan 12 02:14:42.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-4720 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 12 02:14:42.596: INFO: stderr: ""
    Jan 12 02:14:42.596: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:14:42.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4720" for this suite. 01/12/23 02:14:42.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:14:42.618
Jan 12 02:14:42.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 02:14:42.619
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:42.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:42.639
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 01/12/23 02:14:42.642
Jan 12 02:14:42.675: INFO: Waiting up to 5m0s for pod "downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731" in namespace "downward-api-4593" to be "Succeeded or Failed"
Jan 12 02:14:42.677: INFO: Pod "downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731": Phase="Pending", Reason="", readiness=false. Elapsed: 2.832164ms
Jan 12 02:14:44.682: INFO: Pod "downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006920818s
Jan 12 02:14:46.682: INFO: Pod "downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006923521s
Jan 12 02:14:48.682: INFO: Pod "downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007714931s
STEP: Saw pod success 01/12/23 02:14:48.682
Jan 12 02:14:48.682: INFO: Pod "downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731" satisfied condition "Succeeded or Failed"
Jan 12 02:14:48.685: INFO: Trying to get logs from node eqx04-flash06 pod downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731 container dapi-container: <nil>
STEP: delete the pod 01/12/23 02:14:48.694
Jan 12 02:14:48.708: INFO: Waiting for pod downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731 to disappear
Jan 12 02:14:48.711: INFO: Pod downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 12 02:14:48.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4593" for this suite. 01/12/23 02:14:48.716
------------------------------
• [SLOW TEST] [6.152 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:14:42.618
    Jan 12 02:14:42.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 02:14:42.619
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:42.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:42.639
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 01/12/23 02:14:42.642
    Jan 12 02:14:42.675: INFO: Waiting up to 5m0s for pod "downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731" in namespace "downward-api-4593" to be "Succeeded or Failed"
    Jan 12 02:14:42.677: INFO: Pod "downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731": Phase="Pending", Reason="", readiness=false. Elapsed: 2.832164ms
    Jan 12 02:14:44.682: INFO: Pod "downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006920818s
    Jan 12 02:14:46.682: INFO: Pod "downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006923521s
    Jan 12 02:14:48.682: INFO: Pod "downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007714931s
    STEP: Saw pod success 01/12/23 02:14:48.682
    Jan 12 02:14:48.682: INFO: Pod "downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731" satisfied condition "Succeeded or Failed"
    Jan 12 02:14:48.685: INFO: Trying to get logs from node eqx04-flash06 pod downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731 container dapi-container: <nil>
    STEP: delete the pod 01/12/23 02:14:48.694
    Jan 12 02:14:48.708: INFO: Waiting for pod downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731 to disappear
    Jan 12 02:14:48.711: INFO: Pod downward-api-22aecfdb-60fb-44e7-90da-b1b380a94731 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:14:48.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4593" for this suite. 01/12/23 02:14:48.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:14:48.772
Jan 12 02:14:48.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename emptydir 01/12/23 02:14:48.773
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:48.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:48.791
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 01/12/23 02:14:48.794
Jan 12 02:14:48.826: INFO: Waiting up to 5m0s for pod "pod-0bbbea97-8336-487a-bdd9-449b2237805f" in namespace "emptydir-2109" to be "Succeeded or Failed"
Jan 12 02:14:48.829: INFO: Pod "pod-0bbbea97-8336-487a-bdd9-449b2237805f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.784274ms
Jan 12 02:14:50.834: INFO: Pod "pod-0bbbea97-8336-487a-bdd9-449b2237805f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007692039s
Jan 12 02:14:52.835: INFO: Pod "pod-0bbbea97-8336-487a-bdd9-449b2237805f": Phase="Running", Reason="", readiness=false. Elapsed: 4.008722284s
Jan 12 02:14:54.835: INFO: Pod "pod-0bbbea97-8336-487a-bdd9-449b2237805f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009043303s
STEP: Saw pod success 01/12/23 02:14:54.835
Jan 12 02:14:54.835: INFO: Pod "pod-0bbbea97-8336-487a-bdd9-449b2237805f" satisfied condition "Succeeded or Failed"
Jan 12 02:14:54.839: INFO: Trying to get logs from node eqx04-flash06 pod pod-0bbbea97-8336-487a-bdd9-449b2237805f container test-container: <nil>
STEP: delete the pod 01/12/23 02:14:54.848
Jan 12 02:14:54.863: INFO: Waiting for pod pod-0bbbea97-8336-487a-bdd9-449b2237805f to disappear
Jan 12 02:14:54.865: INFO: Pod pod-0bbbea97-8336-487a-bdd9-449b2237805f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 02:14:54.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2109" for this suite. 01/12/23 02:14:54.87
------------------------------
• [SLOW TEST] [6.114 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:14:48.772
    Jan 12 02:14:48.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename emptydir 01/12/23 02:14:48.773
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:48.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:48.791
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/12/23 02:14:48.794
    Jan 12 02:14:48.826: INFO: Waiting up to 5m0s for pod "pod-0bbbea97-8336-487a-bdd9-449b2237805f" in namespace "emptydir-2109" to be "Succeeded or Failed"
    Jan 12 02:14:48.829: INFO: Pod "pod-0bbbea97-8336-487a-bdd9-449b2237805f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.784274ms
    Jan 12 02:14:50.834: INFO: Pod "pod-0bbbea97-8336-487a-bdd9-449b2237805f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007692039s
    Jan 12 02:14:52.835: INFO: Pod "pod-0bbbea97-8336-487a-bdd9-449b2237805f": Phase="Running", Reason="", readiness=false. Elapsed: 4.008722284s
    Jan 12 02:14:54.835: INFO: Pod "pod-0bbbea97-8336-487a-bdd9-449b2237805f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009043303s
    STEP: Saw pod success 01/12/23 02:14:54.835
    Jan 12 02:14:54.835: INFO: Pod "pod-0bbbea97-8336-487a-bdd9-449b2237805f" satisfied condition "Succeeded or Failed"
    Jan 12 02:14:54.839: INFO: Trying to get logs from node eqx04-flash06 pod pod-0bbbea97-8336-487a-bdd9-449b2237805f container test-container: <nil>
    STEP: delete the pod 01/12/23 02:14:54.848
    Jan 12 02:14:54.863: INFO: Waiting for pod pod-0bbbea97-8336-487a-bdd9-449b2237805f to disappear
    Jan 12 02:14:54.865: INFO: Pod pod-0bbbea97-8336-487a-bdd9-449b2237805f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:14:54.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2109" for this suite. 01/12/23 02:14:54.87
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:14:54.886
Jan 12 02:14:54.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 02:14:54.888
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:54.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:54.907
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 01/12/23 02:14:54.91
Jan 12 02:14:54.944: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c" in namespace "downward-api-9143" to be "Succeeded or Failed"
Jan 12 02:14:54.947: INFO: Pod "downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.004969ms
Jan 12 02:14:56.951: INFO: Pod "downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007283753s
Jan 12 02:14:58.951: INFO: Pod "downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007074995s
STEP: Saw pod success 01/12/23 02:14:58.951
Jan 12 02:14:58.951: INFO: Pod "downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c" satisfied condition "Succeeded or Failed"
Jan 12 02:14:58.954: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c container client-container: <nil>
STEP: delete the pod 01/12/23 02:14:58.964
Jan 12 02:14:58.979: INFO: Waiting for pod downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c to disappear
Jan 12 02:14:58.982: INFO: Pod downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 02:14:58.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9143" for this suite. 01/12/23 02:14:58.987
------------------------------
• [4.411 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:14:54.886
    Jan 12 02:14:54.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 02:14:54.888
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:54.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:54.907
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 01/12/23 02:14:54.91
    Jan 12 02:14:54.944: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c" in namespace "downward-api-9143" to be "Succeeded or Failed"
    Jan 12 02:14:54.947: INFO: Pod "downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.004969ms
    Jan 12 02:14:56.951: INFO: Pod "downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007283753s
    Jan 12 02:14:58.951: INFO: Pod "downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007074995s
    STEP: Saw pod success 01/12/23 02:14:58.951
    Jan 12 02:14:58.951: INFO: Pod "downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c" satisfied condition "Succeeded or Failed"
    Jan 12 02:14:58.954: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c container client-container: <nil>
    STEP: delete the pod 01/12/23 02:14:58.964
    Jan 12 02:14:58.979: INFO: Waiting for pod downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c to disappear
    Jan 12 02:14:58.982: INFO: Pod downwardapi-volume-20138970-76b9-4ccb-b4c6-3a888332676c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:14:58.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9143" for this suite. 01/12/23 02:14:58.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:14:59.299
Jan 12 02:14:59.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename kubectl 01/12/23 02:14:59.3
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:59.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:59.32
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 01/12/23 02:14:59.323
Jan 12 02:14:59.323: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1442 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/12/23 02:14:59.374
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 02:14:59.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1442" for this suite. 01/12/23 02:14:59.387
------------------------------
• [0.129 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:14:59.299
    Jan 12 02:14:59.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename kubectl 01/12/23 02:14:59.3
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:59.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:59.32
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 01/12/23 02:14:59.323
    Jan 12 02:14:59.323: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1006763425 --namespace=kubectl-1442 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/12/23 02:14:59.374
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:14:59.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1442" for this suite. 01/12/23 02:14:59.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:14:59.43
Jan 12 02:14:59.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename namespaces 01/12/23 02:14:59.43
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:59.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:59.45
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-9645" 01/12/23 02:14:59.452
Jan 12 02:14:59.461: INFO: Namespace "namespaces-9645" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"57cc750e-de8d-467f-a423-c8fd052edd1e", "kubernetes.io/metadata.name":"namespaces-9645", "namespaces-9645":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 02:14:59.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9645" for this suite. 01/12/23 02:14:59.466
------------------------------
• [0.057 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:14:59.43
    Jan 12 02:14:59.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename namespaces 01/12/23 02:14:59.43
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:59.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:59.45
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-9645" 01/12/23 02:14:59.452
    Jan 12 02:14:59.461: INFO: Namespace "namespaces-9645" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"57cc750e-de8d-467f-a423-c8fd052edd1e", "kubernetes.io/metadata.name":"namespaces-9645", "namespaces-9645":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:14:59.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9645" for this suite. 01/12/23 02:14:59.466
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:14:59.489
Jan 12 02:14:59.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pod-network-test 01/12/23 02:14:59.49
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:59.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:59.507
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-7086 01/12/23 02:14:59.509
STEP: creating a selector 01/12/23 02:14:59.51
STEP: Creating the service pods in kubernetes 01/12/23 02:14:59.51
Jan 12 02:14:59.510: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 12 02:14:59.618: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7086" to be "running and ready"
Jan 12 02:14:59.621: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.999628ms
Jan 12 02:14:59.621: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:15:01.625: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006994681s
Jan 12 02:15:01.625: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 02:15:03.626: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007765071s
Jan 12 02:15:03.626: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 02:15:05.625: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007103204s
Jan 12 02:15:05.625: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 02:15:07.625: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.007553995s
Jan 12 02:15:07.625: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 02:15:09.625: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007238034s
Jan 12 02:15:09.625: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 02:15:11.625: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.007631095s
Jan 12 02:15:11.625: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 02:15:13.625: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.007564827s
Jan 12 02:15:13.625: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 02:15:15.625: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.006826039s
Jan 12 02:15:15.625: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 02:15:17.625: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.006753229s
Jan 12 02:15:17.625: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 02:15:19.626: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.008155802s
Jan 12 02:15:19.626: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 02:15:21.626: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008068209s
Jan 12 02:15:21.626: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 12 02:15:21.626: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 12 02:15:21.629: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7086" to be "running and ready"
Jan 12 02:15:21.632: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.869044ms
Jan 12 02:15:21.632: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 12 02:15:21.632: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/12/23 02:15:21.635
Jan 12 02:15:21.718: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7086" to be "running"
Jan 12 02:15:21.722: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369583ms
Jan 12 02:15:23.726: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007960921s
Jan 12 02:15:25.726: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007725401s
Jan 12 02:15:25.726: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 12 02:15:25.729: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7086" to be "running"
Jan 12 02:15:25.732: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.829013ms
Jan 12 02:15:25.732: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 12 02:15:25.735: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 12 02:15:25.735: INFO: Going to poll 172.21.117.132 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 12 02:15:25.737: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.117.132:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7086 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 02:15:25.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 02:15:25.738: INFO: ExecWithOptions: Clientset creation
Jan 12 02:15:25.738: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7086/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.21.117.132%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 12 02:15:25.882: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 12 02:15:25.882: INFO: Going to poll 172.21.88.153 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 12 02:15:25.885: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.88.153:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7086 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 02:15:25.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
Jan 12 02:15:25.886: INFO: ExecWithOptions: Clientset creation
Jan 12 02:15:25.886: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7086/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.21.88.153%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 12 02:15:26.030: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 12 02:15:26.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7086" for this suite. 01/12/23 02:15:26.034
------------------------------
• [SLOW TEST] [26.565 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:14:59.489
    Jan 12 02:14:59.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pod-network-test 01/12/23 02:14:59.49
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:14:59.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:14:59.507
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-7086 01/12/23 02:14:59.509
    STEP: creating a selector 01/12/23 02:14:59.51
    STEP: Creating the service pods in kubernetes 01/12/23 02:14:59.51
    Jan 12 02:14:59.510: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 12 02:14:59.618: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7086" to be "running and ready"
    Jan 12 02:14:59.621: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.999628ms
    Jan 12 02:14:59.621: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:15:01.625: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006994681s
    Jan 12 02:15:01.625: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 02:15:03.626: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007765071s
    Jan 12 02:15:03.626: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 02:15:05.625: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007103204s
    Jan 12 02:15:05.625: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 02:15:07.625: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.007553995s
    Jan 12 02:15:07.625: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 02:15:09.625: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007238034s
    Jan 12 02:15:09.625: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 02:15:11.625: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.007631095s
    Jan 12 02:15:11.625: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 02:15:13.625: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.007564827s
    Jan 12 02:15:13.625: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 02:15:15.625: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.006826039s
    Jan 12 02:15:15.625: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 02:15:17.625: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.006753229s
    Jan 12 02:15:17.625: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 02:15:19.626: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.008155802s
    Jan 12 02:15:19.626: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 02:15:21.626: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008068209s
    Jan 12 02:15:21.626: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 12 02:15:21.626: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 12 02:15:21.629: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7086" to be "running and ready"
    Jan 12 02:15:21.632: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.869044ms
    Jan 12 02:15:21.632: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 12 02:15:21.632: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/12/23 02:15:21.635
    Jan 12 02:15:21.718: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7086" to be "running"
    Jan 12 02:15:21.722: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369583ms
    Jan 12 02:15:23.726: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007960921s
    Jan 12 02:15:25.726: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007725401s
    Jan 12 02:15:25.726: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 12 02:15:25.729: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7086" to be "running"
    Jan 12 02:15:25.732: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.829013ms
    Jan 12 02:15:25.732: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 12 02:15:25.735: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 12 02:15:25.735: INFO: Going to poll 172.21.117.132 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 12 02:15:25.737: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.117.132:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7086 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 02:15:25.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 02:15:25.738: INFO: ExecWithOptions: Clientset creation
    Jan 12 02:15:25.738: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7086/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.21.117.132%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 12 02:15:25.882: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 12 02:15:25.882: INFO: Going to poll 172.21.88.153 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 12 02:15:25.885: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.88.153:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7086 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 02:15:25.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    Jan 12 02:15:25.886: INFO: ExecWithOptions: Clientset creation
    Jan 12 02:15:25.886: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7086/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.21.88.153%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 12 02:15:26.030: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:15:26.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7086" for this suite. 01/12/23 02:15:26.034
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:15:26.055
Jan 12 02:15:26.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename resourcequota 01/12/23 02:15:26.055
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:15:26.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:15:26.076
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 01/12/23 02:15:26.079
STEP: Ensuring ResourceQuota status is calculated 01/12/23 02:15:26.088
STEP: Creating a ResourceQuota with not best effort scope 01/12/23 02:15:28.092
STEP: Ensuring ResourceQuota status is calculated 01/12/23 02:15:28.098
STEP: Creating a best-effort pod 01/12/23 02:15:30.102
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/12/23 02:15:30.143
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/12/23 02:15:32.146
STEP: Deleting the pod 01/12/23 02:15:34.152
STEP: Ensuring resource quota status released the pod usage 01/12/23 02:15:34.167
STEP: Creating a not best-effort pod 01/12/23 02:15:36.171
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/12/23 02:15:36.276
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/12/23 02:15:38.281
STEP: Deleting the pod 01/12/23 02:15:40.285
STEP: Ensuring resource quota status released the pod usage 01/12/23 02:15:40.3
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 02:15:42.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9421" for this suite. 01/12/23 02:15:42.313
------------------------------
• [SLOW TEST] [16.280 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:15:26.055
    Jan 12 02:15:26.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename resourcequota 01/12/23 02:15:26.055
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:15:26.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:15:26.076
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 01/12/23 02:15:26.079
    STEP: Ensuring ResourceQuota status is calculated 01/12/23 02:15:26.088
    STEP: Creating a ResourceQuota with not best effort scope 01/12/23 02:15:28.092
    STEP: Ensuring ResourceQuota status is calculated 01/12/23 02:15:28.098
    STEP: Creating a best-effort pod 01/12/23 02:15:30.102
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/12/23 02:15:30.143
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/12/23 02:15:32.146
    STEP: Deleting the pod 01/12/23 02:15:34.152
    STEP: Ensuring resource quota status released the pod usage 01/12/23 02:15:34.167
    STEP: Creating a not best-effort pod 01/12/23 02:15:36.171
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/12/23 02:15:36.276
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/12/23 02:15:38.281
    STEP: Deleting the pod 01/12/23 02:15:40.285
    STEP: Ensuring resource quota status released the pod usage 01/12/23 02:15:40.3
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:15:42.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9421" for this suite. 01/12/23 02:15:42.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:15:42.337
Jan 12 02:15:42.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename webhook 01/12/23 02:15:42.337
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:15:42.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:15:42.356
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 02:15:42.372
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 02:15:42.922
STEP: Deploying the webhook pod 01/12/23 02:15:42.93
STEP: Wait for the deployment to be ready 01/12/23 02:15:43.066
Jan 12 02:15:43.072: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 12 02:15:45.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 2, 15, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 15, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 2, 15, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 15, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 02:15:47.089
STEP: Verifying the service has paired with the endpoint 01/12/23 02:15:47.1
Jan 12 02:15:48.101: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/12/23 02:15:48.104
STEP: create a namespace for the webhook 01/12/23 02:15:48.12
STEP: create a configmap should be unconditionally rejected by the webhook 01/12/23 02:15:48.13
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 02:15:48.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7014" for this suite. 01/12/23 02:15:48.197
STEP: Destroying namespace "webhook-7014-markers" for this suite. 01/12/23 02:15:48.258
------------------------------
• [SLOW TEST] [5.958 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:15:42.337
    Jan 12 02:15:42.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename webhook 01/12/23 02:15:42.337
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:15:42.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:15:42.356
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 02:15:42.372
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 02:15:42.922
    STEP: Deploying the webhook pod 01/12/23 02:15:42.93
    STEP: Wait for the deployment to be ready 01/12/23 02:15:43.066
    Jan 12 02:15:43.072: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 12 02:15:45.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 2, 15, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 15, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 2, 15, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 15, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 02:15:47.089
    STEP: Verifying the service has paired with the endpoint 01/12/23 02:15:47.1
    Jan 12 02:15:48.101: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/12/23 02:15:48.104
    STEP: create a namespace for the webhook 01/12/23 02:15:48.12
    STEP: create a configmap should be unconditionally rejected by the webhook 01/12/23 02:15:48.13
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:15:48.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7014" for this suite. 01/12/23 02:15:48.197
    STEP: Destroying namespace "webhook-7014-markers" for this suite. 01/12/23 02:15:48.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:15:48.297
Jan 12 02:15:48.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename sched-pred 01/12/23 02:15:48.298
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:15:48.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:15:48.318
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 12 02:15:48.320: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 12 02:15:48.329: INFO: Waiting for terminating namespaces to be deleted...
Jan 12 02:15:48.334: INFO: 
Logging pods the apiserver thinks is on node eqx03-flash06 before test
Jan 12 02:15:48.381: INFO: calico-kube-controllers-5c9fd9b888-vc4sl from kube-system started at 2023-01-11 19:29:05 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 12 02:15:48.381: INFO: calico-node-lwrhk from kube-system started at 2023-01-11 08:01:00 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container calico-node ready: true, restart count 0
Jan 12 02:15:48.381: INFO: kube-multus-ds-amd64-gsh8m from kube-system started at 2022-10-10 22:16:50 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container kube-multus ready: true, restart count 2
Jan 12 02:15:48.381: INFO: kube-proxy-h8fqt from kube-system started at 2023-01-11 07:45:11 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 02:15:48.381: INFO: kube-sriov-device-plugin-amd64-4wg6g from kube-system started at 2022-10-10 22:14:57 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container kube-sriovdp ready: true, restart count 2
Jan 12 02:15:48.381: INFO: mariadb-1665683911-master-0 from maria started at 2023-01-11 07:55:52 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mariadb ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mariadb-1665683911-slave-0 from maria started at 2023-01-11 07:56:03 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mariadb ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1664779250-7dc5656c45-vcb4f from mg started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1664779250 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1664779251-858bf7fdc-6scml from mg started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1664779251 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1664779253-866d78dd7-n8j9t from mg started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1664779253 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1664779256-5c6fc69c89-9kvgw from mg started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1664779256 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779263-controller-bf7587b7f-8f6bc from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 22
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779263-default-backend-cdc6d499b-ng78h from ng started at 2023-01-11 07:55:44 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779266-controller-677944959d-dnpbb from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 22
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779266-default-backend-bbdfc88b7-7ln9d from ng started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779278-controller-5fb4f984c-cshwh from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 23
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779278-default-backend-5dfb4f9db4-bdjmg from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779281-controller-75b6585875-6fwmp from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 22
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779281-default-backend-d5fb87996-kl2gz from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779288-controller-9744ff446-rhjs8 from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 22
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779288-default-backend-594965dbd7-kb8tt from ng started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779291-controller-ff67b7844-jqvh6 from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 22
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779291-default-backend-66f95c66fc-2vfvf from ng started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779294-controller-84785d75f7-x4w9c from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 22
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779294-default-backend-6765dd7578-v796m from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779297-controller-57f654c69d-vzn6l from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 22
Jan 12 02:15:48.381: INFO: nginx-ingress-1664779297-default-backend-74df75bf95-n6xwg from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Jan 12 02:15:48.381: INFO: csi-nodeplugin-robin-hnwgz from robinio started at 2023-01-11 08:44:24 +0000 UTC (3 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container driver-registrar ready: true, restart count 0
Jan 12 02:15:48.381: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 12 02:15:48.381: INFO: 	Container robin ready: true, restart count 0
Jan 12 02:15:48.381: INFO: robin-nfs-watchdog-qrzrq from robinio started at 2023-01-11 08:28:03 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
Jan 12 02:15:48.381: INFO: robin-worker-jlr7d from robinio started at 2023-01-11 08:27:55 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container robinrcm ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089095-66b956f5d-vxj2b from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089095 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089145-598df7974-kn9cv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089145 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089149-5574fb7774-t7pqg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089149 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089152-84f657bf94-dvsbc from sa-ns-user started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089152 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089179-5bf57c5944-tp4xz from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089179 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089182-6c58488f6d-5jn9v from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089182 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089185-677dc7ffb6-b9pl6 from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089185 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089188-58475957bd-5nfbd from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089188 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089191-66859c96dd-lkchj from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089191 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089205-5ff87d5b8d-wch4v from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089205 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089217-76648fcb6f-9hkfm from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089217 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089220-595bdf59bf-wzg6h from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089220 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089231-fd7d54889-stkqh from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089231 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089236-66c6896dcf-jjztv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089236 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089247-845bdb94dc-z5mxg from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089247 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089249-68588bc459-psrj7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089249 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089253-7d549cf945-mhld7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089253 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089257-6c4b6dd79c-tz5d5 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089257 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089259-58c44567c7-jmzbp from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089259 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089262-744cbfcf5c-lzzbl from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089262 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089268-5867478f97-c44xg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089268 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089337-9dbc9475f-zzk9n from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089337 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665089342-69f7c77fd7-9l5qq from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665089342 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665088984-56cbf7747c-8zfh5 from sa-ns started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665088984 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: mysql-1665088986-7785c569-hfqvj from sa-ns started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql-1665088986 ready: true, restart count 0
Jan 12 02:15:48.381: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-ppd9w from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 02:15:48.381: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 12 02:15:48.381: INFO: ravi-ravi-mysql-0 from t001-u000004 started at 2023-01-11 07:55:59 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.381: INFO: 	Container mysql ready: true, restart count 0
Jan 12 02:15:48.381: INFO: 
Logging pods the apiserver thinks is on node eqx04-flash06 before test
Jan 12 02:15:48.405: INFO: calico-node-wh5zd from kube-system started at 2023-01-11 08:02:20 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.405: INFO: 	Container calico-node ready: true, restart count 0
Jan 12 02:15:48.405: INFO: kube-multus-ds-amd64-qllb4 from kube-system started at 2023-01-12 01:12:37 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.405: INFO: 	Container kube-multus ready: true, restart count 0
Jan 12 02:15:48.405: INFO: kube-proxy-kvvhz from kube-system started at 2023-01-11 07:45:13 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.405: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 02:15:48.405: INFO: kube-sriov-device-plugin-amd64-b2hg7 from kube-system started at 2023-01-12 01:12:30 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.405: INFO: 	Container kube-sriovdp ready: true, restart count 0
Jan 12 02:15:48.405: INFO: csi-nodeplugin-robin-z9mk4 from robinio started at 2023-01-12 01:12:37 +0000 UTC (3 container statuses recorded)
Jan 12 02:15:48.405: INFO: 	Container driver-registrar ready: true, restart count 0
Jan 12 02:15:48.405: INFO: 	Container liveness-probe ready: true, restart count 0
Jan 12 02:15:48.405: INFO: 	Container robin ready: true, restart count 0
Jan 12 02:15:48.405: INFO: robin-nfs-watchdog-cpvh5 from robinio started at 2023-01-12 01:12:30 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.406: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
Jan 12 02:15:48.406: INFO: robin-worker-pczc5 from robinio started at 2023-01-12 01:13:06 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.406: INFO: 	Container robinrcm ready: true, restart count 0
Jan 12 02:15:48.406: INFO: sonobuoy from sonobuoy started at 2023-01-12 00:40:33 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.406: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 12 02:15:48.406: INFO: sonobuoy-e2e-job-90575ca5f8b04bb8 from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
Jan 12 02:15:48.406: INFO: 	Container e2e ready: true, restart count 0
Jan 12 02:15:48.406: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 02:15:48.406: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-mkhnx from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
Jan 12 02:15:48.406: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 02:15:48.406: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 12 02:15:48.406: INFO: cent-1-server-01 from t001-u000004 started at 2023-01-12 01:14:56 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.406: INFO: 	Container cent-1-server-01 ready: true, restart count 0
Jan 12 02:15:48.406: INFO: cent-2-server-01 from t001-u000004 started at 2023-01-12 01:14:56 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.406: INFO: 	Container cent-2-server-01 ready: true, restart count 0
Jan 12 02:15:48.406: INFO: sq-mysql-01 from t001-u000004 started at 2023-01-12 01:14:45 +0000 UTC (1 container statuses recorded)
Jan 12 02:15:48.406: INFO: 	Container sq-mysql-01 ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node eqx03-flash06 01/12/23 02:15:48.461
STEP: verifying the node has the label node eqx04-flash06 01/12/23 02:15:48.479
Jan 12 02:15:48.553: INFO: Pod calico-kube-controllers-5c9fd9b888-vc4sl requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.553: INFO: Pod calico-node-lwrhk requesting resource cpu=250m on Node eqx03-flash06
Jan 12 02:15:48.553: INFO: Pod calico-node-wh5zd requesting resource cpu=250m on Node eqx04-flash06
Jan 12 02:15:48.553: INFO: Pod kube-multus-ds-amd64-gsh8m requesting resource cpu=100m on Node eqx03-flash06
Jan 12 02:15:48.553: INFO: Pod kube-multus-ds-amd64-qllb4 requesting resource cpu=100m on Node eqx04-flash06
Jan 12 02:15:48.553: INFO: Pod kube-proxy-h8fqt requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.553: INFO: Pod kube-proxy-kvvhz requesting resource cpu=0m on Node eqx04-flash06
Jan 12 02:15:48.553: INFO: Pod kube-sriov-device-plugin-amd64-4wg6g requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.553: INFO: Pod kube-sriov-device-plugin-amd64-b2hg7 requesting resource cpu=0m on Node eqx04-flash06
Jan 12 02:15:48.553: INFO: Pod mariadb-1665683911-master-0 requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.553: INFO: Pod mariadb-1665683911-slave-0 requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.553: INFO: Pod mysql-1664779250-7dc5656c45-vcb4f requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.553: INFO: Pod mysql-1664779251-858bf7fdc-6scml requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.553: INFO: Pod mysql-1664779253-866d78dd7-n8j9t requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.553: INFO: Pod mysql-1664779256-5c6fc69c89-9kvgw requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.553: INFO: Pod nginx-ingress-1664779263-controller-bf7587b7f-8f6bc requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.553: INFO: Pod nginx-ingress-1664779263-default-backend-cdc6d499b-ng78h requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.553: INFO: Pod nginx-ingress-1664779266-controller-677944959d-dnpbb requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779266-default-backend-bbdfc88b7-7ln9d requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779278-controller-5fb4f984c-cshwh requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779278-default-backend-5dfb4f9db4-bdjmg requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779281-controller-75b6585875-6fwmp requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779281-default-backend-d5fb87996-kl2gz requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779288-controller-9744ff446-rhjs8 requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779288-default-backend-594965dbd7-kb8tt requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779291-controller-ff67b7844-jqvh6 requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779291-default-backend-66f95c66fc-2vfvf requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779294-controller-84785d75f7-x4w9c requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779294-default-backend-6765dd7578-v796m requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779297-controller-57f654c69d-vzn6l requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779297-default-backend-74df75bf95-n6xwg requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod csi-nodeplugin-robin-hnwgz requesting resource cpu=250m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod csi-nodeplugin-robin-z9mk4 requesting resource cpu=250m on Node eqx04-flash06
Jan 12 02:15:48.554: INFO: Pod robin-nfs-watchdog-cpvh5 requesting resource cpu=0m on Node eqx04-flash06
Jan 12 02:15:48.554: INFO: Pod robin-nfs-watchdog-qrzrq requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod robin-worker-jlr7d requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod robin-worker-pczc5 requesting resource cpu=1000m on Node eqx04-flash06
Jan 12 02:15:48.554: INFO: Pod mysql-1665089095-66b956f5d-vxj2b requesting resource cpu=100m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod mysql-1665089145-598df7974-kn9cv requesting resource cpu=100m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod mysql-1665089149-5574fb7774-t7pqg requesting resource cpu=100m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod mysql-1665089152-84f657bf94-dvsbc requesting resource cpu=100m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod mysql-1665089179-5bf57c5944-tp4xz requesting resource cpu=100m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod mysql-1665089182-6c58488f6d-5jn9v requesting resource cpu=100m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod mysql-1665089185-677dc7ffb6-b9pl6 requesting resource cpu=100m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod mysql-1665089188-58475957bd-5nfbd requesting resource cpu=100m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod mysql-1665089191-66859c96dd-lkchj requesting resource cpu=100m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod mysql-1665089205-5ff87d5b8d-wch4v requesting resource cpu=100m on Node eqx03-flash06
Jan 12 02:15:48.554: INFO: Pod mysql-1665089217-76648fcb6f-9hkfm requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod mysql-1665089220-595bdf59bf-wzg6h requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod mysql-1665089231-fd7d54889-stkqh requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod mysql-1665089236-66c6896dcf-jjztv requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod mysql-1665089247-845bdb94dc-z5mxg requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod mysql-1665089249-68588bc459-psrj7 requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod mysql-1665089253-7d549cf945-mhld7 requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod mysql-1665089257-6c4b6dd79c-tz5d5 requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod mysql-1665089259-58c44567c7-jmzbp requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod mysql-1665089262-744cbfcf5c-lzzbl requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod mysql-1665089268-5867478f97-c44xg requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod mysql-1665089337-9dbc9475f-zzk9n requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod mysql-1665089342-69f7c77fd7-9l5qq requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod mysql-1665088984-56cbf7747c-8zfh5 requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod mysql-1665088986-7785c569-hfqvj requesting resource cpu=1000m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod sonobuoy requesting resource cpu=0m on Node eqx04-flash06
Jan 12 02:15:48.555: INFO: Pod sonobuoy-e2e-job-90575ca5f8b04bb8 requesting resource cpu=0m on Node eqx04-flash06
Jan 12 02:15:48.555: INFO: Pod sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-mkhnx requesting resource cpu=0m on Node eqx04-flash06
Jan 12 02:15:48.555: INFO: Pod sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-ppd9w requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod cent-1-server-01 requesting resource cpu=0m on Node eqx04-flash06
Jan 12 02:15:48.555: INFO: Pod cent-2-server-01 requesting resource cpu=0m on Node eqx04-flash06
Jan 12 02:15:48.555: INFO: Pod ravi-ravi-mysql-0 requesting resource cpu=0m on Node eqx03-flash06
Jan 12 02:15:48.555: INFO: Pod sq-mysql-01 requesting resource cpu=0m on Node eqx04-flash06
STEP: Starting Pods to consume most of the cluster CPU. 01/12/23 02:15:48.555
Jan 12 02:15:48.555: INFO: Creating a pod which consumes cpu=12180m on Node eqx03-flash06
Jan 12 02:15:48.626: INFO: Creating a pod which consumes cpu=7280m on Node eqx04-flash06
Jan 12 02:15:48.724: INFO: Waiting up to 5m0s for pod "filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e" in namespace "sched-pred-7457" to be "running"
Jan 12 02:15:48.729: INFO: Pod "filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.712123ms
Jan 12 02:15:50.732: INFO: Pod "filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008491527s
Jan 12 02:15:50.732: INFO: Pod "filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e" satisfied condition "running"
Jan 12 02:15:50.732: INFO: Waiting up to 5m0s for pod "filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0" in namespace "sched-pred-7457" to be "running"
Jan 12 02:15:50.735: INFO: Pod "filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.004072ms
Jan 12 02:15:52.740: INFO: Pod "filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.00733852s
Jan 12 02:15:52.740: INFO: Pod "filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/12/23 02:15:52.74
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e.17396e0f5a8e492a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7457/filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e to eqx03-flash06] 01/12/23 02:15:52.744
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e.17396e0f9d374f5c], Reason = [AddedInterface], Message = [Add eth0 [172.21.117.134/32] from calico] 01/12/23 02:15:52.744
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e.17396e0f9f0c9194], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/12/23 02:15:52.744
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e.17396e0fa3292a9d], Reason = [Created], Message = [Created container filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e] 01/12/23 02:15:52.744
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e.17396e0fb25bfc0a], Reason = [Started], Message = [Started container filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e] 01/12/23 02:15:52.744
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0.17396e0f5eee5503], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7457/filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0 to eqx04-flash06] 01/12/23 02:15:52.744
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0.17396e0fa367bd97], Reason = [AddedInterface], Message = [Add eth0 [172.21.88.181/32] from calico] 01/12/23 02:15:52.744
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0.17396e0fa6397d3d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/12/23 02:15:52.744
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0.17396e0fad07576c], Reason = [Created], Message = [Created container filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0] 01/12/23 02:15:52.744
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0.17396e0fba0e3ba1], Reason = [Started], Message = [Started container filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0] 01/12/23 02:15:52.744
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17396e10497aea5c], Reason = [FailedScheduling], Message = [0/5 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/5 nodes are available: 2 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 01/12/23 02:15:52.783
STEP: removing the label node off the node eqx03-flash06 01/12/23 02:15:53.781
STEP: verifying the node doesn't have the label node 01/12/23 02:15:53.8
STEP: removing the label node off the node eqx04-flash06 01/12/23 02:15:53.803
STEP: verifying the node doesn't have the label node 01/12/23 02:15:53.819
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 02:15:53.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7457" for this suite. 01/12/23 02:15:53.83
------------------------------
• [SLOW TEST] [5.588 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:15:48.297
    Jan 12 02:15:48.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename sched-pred 01/12/23 02:15:48.298
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:15:48.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:15:48.318
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 12 02:15:48.320: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 12 02:15:48.329: INFO: Waiting for terminating namespaces to be deleted...
    Jan 12 02:15:48.334: INFO: 
    Logging pods the apiserver thinks is on node eqx03-flash06 before test
    Jan 12 02:15:48.381: INFO: calico-kube-controllers-5c9fd9b888-vc4sl from kube-system started at 2023-01-11 19:29:05 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: calico-node-lwrhk from kube-system started at 2023-01-11 08:01:00 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container calico-node ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: kube-multus-ds-amd64-gsh8m from kube-system started at 2022-10-10 22:16:50 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container kube-multus ready: true, restart count 2
    Jan 12 02:15:48.381: INFO: kube-proxy-h8fqt from kube-system started at 2023-01-11 07:45:11 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: kube-sriov-device-plugin-amd64-4wg6g from kube-system started at 2022-10-10 22:14:57 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container kube-sriovdp ready: true, restart count 2
    Jan 12 02:15:48.381: INFO: mariadb-1665683911-master-0 from maria started at 2023-01-11 07:55:52 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mariadb ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mariadb-1665683911-slave-0 from maria started at 2023-01-11 07:56:03 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mariadb ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1664779250-7dc5656c45-vcb4f from mg started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1664779250 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1664779251-858bf7fdc-6scml from mg started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1664779251 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1664779253-866d78dd7-n8j9t from mg started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1664779253 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1664779256-5c6fc69c89-9kvgw from mg started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1664779256 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779263-controller-bf7587b7f-8f6bc from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 22
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779263-default-backend-cdc6d499b-ng78h from ng started at 2023-01-11 07:55:44 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779266-controller-677944959d-dnpbb from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 22
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779266-default-backend-bbdfc88b7-7ln9d from ng started at 2023-01-11 07:55:42 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779278-controller-5fb4f984c-cshwh from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 23
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779278-default-backend-5dfb4f9db4-bdjmg from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779281-controller-75b6585875-6fwmp from ng started at 2023-01-12 01:12:06 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 22
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779281-default-backend-d5fb87996-kl2gz from ng started at 2023-01-11 07:55:43 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779288-controller-9744ff446-rhjs8 from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 22
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779288-default-backend-594965dbd7-kb8tt from ng started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779291-controller-ff67b7844-jqvh6 from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 22
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779291-default-backend-66f95c66fc-2vfvf from ng started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779294-controller-84785d75f7-x4w9c from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 22
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779294-default-backend-6765dd7578-v796m from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779297-controller-57f654c69d-vzn6l from ng started at 2023-01-12 01:12:05 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-controller ready: false, restart count 22
    Jan 12 02:15:48.381: INFO: nginx-ingress-1664779297-default-backend-74df75bf95-n6xwg from ng started at 2023-01-11 07:55:38 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: csi-nodeplugin-robin-hnwgz from robinio started at 2023-01-11 08:44:24 +0000 UTC (3 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: 	Container robin ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: robin-nfs-watchdog-qrzrq from robinio started at 2023-01-11 08:28:03 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: robin-worker-jlr7d from robinio started at 2023-01-11 08:27:55 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container robinrcm ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089095-66b956f5d-vxj2b from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089095 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089145-598df7974-kn9cv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089145 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089149-5574fb7774-t7pqg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089149 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089152-84f657bf94-dvsbc from sa-ns-user started at 2023-01-11 07:55:45 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089152 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089179-5bf57c5944-tp4xz from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089179 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089182-6c58488f6d-5jn9v from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089182 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089185-677dc7ffb6-b9pl6 from sa-ns-user started at 2023-01-11 07:55:47 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089185 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089188-58475957bd-5nfbd from sa-ns-user started at 2023-01-11 07:55:49 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089188 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089191-66859c96dd-lkchj from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089191 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089205-5ff87d5b8d-wch4v from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089205 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089217-76648fcb6f-9hkfm from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089217 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089220-595bdf59bf-wzg6h from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089220 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089231-fd7d54889-stkqh from sa-ns-user started at 2023-01-11 07:55:39 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089231 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089236-66c6896dcf-jjztv from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089236 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089247-845bdb94dc-z5mxg from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089247 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089249-68588bc459-psrj7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089249 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089253-7d549cf945-mhld7 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089253 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089257-6c4b6dd79c-tz5d5 from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089257 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089259-58c44567c7-jmzbp from sa-ns-user started at 2023-01-11 07:55:40 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089259 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089262-744cbfcf5c-lzzbl from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089262 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089268-5867478f97-c44xg from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089268 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089337-9dbc9475f-zzk9n from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089337 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665089342-69f7c77fd7-9l5qq from sa-ns-user started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665089342 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665088984-56cbf7747c-8zfh5 from sa-ns started at 2023-01-11 07:55:41 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665088984 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: mysql-1665088986-7785c569-hfqvj from sa-ns started at 2023-01-11 07:55:17 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql-1665088986 ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-ppd9w from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: ravi-ravi-mysql-0 from t001-u000004 started at 2023-01-11 07:55:59 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.381: INFO: 	Container mysql ready: true, restart count 0
    Jan 12 02:15:48.381: INFO: 
    Logging pods the apiserver thinks is on node eqx04-flash06 before test
    Jan 12 02:15:48.405: INFO: calico-node-wh5zd from kube-system started at 2023-01-11 08:02:20 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.405: INFO: 	Container calico-node ready: true, restart count 0
    Jan 12 02:15:48.405: INFO: kube-multus-ds-amd64-qllb4 from kube-system started at 2023-01-12 01:12:37 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.405: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 12 02:15:48.405: INFO: kube-proxy-kvvhz from kube-system started at 2023-01-11 07:45:13 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.405: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 02:15:48.405: INFO: kube-sriov-device-plugin-amd64-b2hg7 from kube-system started at 2023-01-12 01:12:30 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.405: INFO: 	Container kube-sriovdp ready: true, restart count 0
    Jan 12 02:15:48.405: INFO: csi-nodeplugin-robin-z9mk4 from robinio started at 2023-01-12 01:12:37 +0000 UTC (3 container statuses recorded)
    Jan 12 02:15:48.405: INFO: 	Container driver-registrar ready: true, restart count 0
    Jan 12 02:15:48.405: INFO: 	Container liveness-probe ready: true, restart count 0
    Jan 12 02:15:48.405: INFO: 	Container robin ready: true, restart count 0
    Jan 12 02:15:48.405: INFO: robin-nfs-watchdog-cpvh5 from robinio started at 2023-01-12 01:12:30 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.406: INFO: 	Container robin-nfs-watchdog ready: true, restart count 0
    Jan 12 02:15:48.406: INFO: robin-worker-pczc5 from robinio started at 2023-01-12 01:13:06 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.406: INFO: 	Container robinrcm ready: true, restart count 0
    Jan 12 02:15:48.406: INFO: sonobuoy from sonobuoy started at 2023-01-12 00:40:33 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.406: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 12 02:15:48.406: INFO: sonobuoy-e2e-job-90575ca5f8b04bb8 from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
    Jan 12 02:15:48.406: INFO: 	Container e2e ready: true, restart count 0
    Jan 12 02:15:48.406: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 02:15:48.406: INFO: sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-mkhnx from sonobuoy started at 2023-01-12 00:40:35 +0000 UTC (2 container statuses recorded)
    Jan 12 02:15:48.406: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 02:15:48.406: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 12 02:15:48.406: INFO: cent-1-server-01 from t001-u000004 started at 2023-01-12 01:14:56 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.406: INFO: 	Container cent-1-server-01 ready: true, restart count 0
    Jan 12 02:15:48.406: INFO: cent-2-server-01 from t001-u000004 started at 2023-01-12 01:14:56 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.406: INFO: 	Container cent-2-server-01 ready: true, restart count 0
    Jan 12 02:15:48.406: INFO: sq-mysql-01 from t001-u000004 started at 2023-01-12 01:14:45 +0000 UTC (1 container statuses recorded)
    Jan 12 02:15:48.406: INFO: 	Container sq-mysql-01 ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node eqx03-flash06 01/12/23 02:15:48.461
    STEP: verifying the node has the label node eqx04-flash06 01/12/23 02:15:48.479
    Jan 12 02:15:48.553: INFO: Pod calico-kube-controllers-5c9fd9b888-vc4sl requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.553: INFO: Pod calico-node-lwrhk requesting resource cpu=250m on Node eqx03-flash06
    Jan 12 02:15:48.553: INFO: Pod calico-node-wh5zd requesting resource cpu=250m on Node eqx04-flash06
    Jan 12 02:15:48.553: INFO: Pod kube-multus-ds-amd64-gsh8m requesting resource cpu=100m on Node eqx03-flash06
    Jan 12 02:15:48.553: INFO: Pod kube-multus-ds-amd64-qllb4 requesting resource cpu=100m on Node eqx04-flash06
    Jan 12 02:15:48.553: INFO: Pod kube-proxy-h8fqt requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.553: INFO: Pod kube-proxy-kvvhz requesting resource cpu=0m on Node eqx04-flash06
    Jan 12 02:15:48.553: INFO: Pod kube-sriov-device-plugin-amd64-4wg6g requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.553: INFO: Pod kube-sriov-device-plugin-amd64-b2hg7 requesting resource cpu=0m on Node eqx04-flash06
    Jan 12 02:15:48.553: INFO: Pod mariadb-1665683911-master-0 requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.553: INFO: Pod mariadb-1665683911-slave-0 requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.553: INFO: Pod mysql-1664779250-7dc5656c45-vcb4f requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.553: INFO: Pod mysql-1664779251-858bf7fdc-6scml requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.553: INFO: Pod mysql-1664779253-866d78dd7-n8j9t requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.553: INFO: Pod mysql-1664779256-5c6fc69c89-9kvgw requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.553: INFO: Pod nginx-ingress-1664779263-controller-bf7587b7f-8f6bc requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.553: INFO: Pod nginx-ingress-1664779263-default-backend-cdc6d499b-ng78h requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.553: INFO: Pod nginx-ingress-1664779266-controller-677944959d-dnpbb requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779266-default-backend-bbdfc88b7-7ln9d requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779278-controller-5fb4f984c-cshwh requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779278-default-backend-5dfb4f9db4-bdjmg requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779281-controller-75b6585875-6fwmp requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779281-default-backend-d5fb87996-kl2gz requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779288-controller-9744ff446-rhjs8 requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779288-default-backend-594965dbd7-kb8tt requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779291-controller-ff67b7844-jqvh6 requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779291-default-backend-66f95c66fc-2vfvf requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779294-controller-84785d75f7-x4w9c requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779294-default-backend-6765dd7578-v796m requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779297-controller-57f654c69d-vzn6l requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod nginx-ingress-1664779297-default-backend-74df75bf95-n6xwg requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod csi-nodeplugin-robin-hnwgz requesting resource cpu=250m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod csi-nodeplugin-robin-z9mk4 requesting resource cpu=250m on Node eqx04-flash06
    Jan 12 02:15:48.554: INFO: Pod robin-nfs-watchdog-cpvh5 requesting resource cpu=0m on Node eqx04-flash06
    Jan 12 02:15:48.554: INFO: Pod robin-nfs-watchdog-qrzrq requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod robin-worker-jlr7d requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod robin-worker-pczc5 requesting resource cpu=1000m on Node eqx04-flash06
    Jan 12 02:15:48.554: INFO: Pod mysql-1665089095-66b956f5d-vxj2b requesting resource cpu=100m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod mysql-1665089145-598df7974-kn9cv requesting resource cpu=100m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod mysql-1665089149-5574fb7774-t7pqg requesting resource cpu=100m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod mysql-1665089152-84f657bf94-dvsbc requesting resource cpu=100m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod mysql-1665089179-5bf57c5944-tp4xz requesting resource cpu=100m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod mysql-1665089182-6c58488f6d-5jn9v requesting resource cpu=100m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod mysql-1665089185-677dc7ffb6-b9pl6 requesting resource cpu=100m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod mysql-1665089188-58475957bd-5nfbd requesting resource cpu=100m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod mysql-1665089191-66859c96dd-lkchj requesting resource cpu=100m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod mysql-1665089205-5ff87d5b8d-wch4v requesting resource cpu=100m on Node eqx03-flash06
    Jan 12 02:15:48.554: INFO: Pod mysql-1665089217-76648fcb6f-9hkfm requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod mysql-1665089220-595bdf59bf-wzg6h requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod mysql-1665089231-fd7d54889-stkqh requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod mysql-1665089236-66c6896dcf-jjztv requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod mysql-1665089247-845bdb94dc-z5mxg requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod mysql-1665089249-68588bc459-psrj7 requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod mysql-1665089253-7d549cf945-mhld7 requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod mysql-1665089257-6c4b6dd79c-tz5d5 requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod mysql-1665089259-58c44567c7-jmzbp requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod mysql-1665089262-744cbfcf5c-lzzbl requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod mysql-1665089268-5867478f97-c44xg requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod mysql-1665089337-9dbc9475f-zzk9n requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod mysql-1665089342-69f7c77fd7-9l5qq requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod mysql-1665088984-56cbf7747c-8zfh5 requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod mysql-1665088986-7785c569-hfqvj requesting resource cpu=1000m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod sonobuoy requesting resource cpu=0m on Node eqx04-flash06
    Jan 12 02:15:48.555: INFO: Pod sonobuoy-e2e-job-90575ca5f8b04bb8 requesting resource cpu=0m on Node eqx04-flash06
    Jan 12 02:15:48.555: INFO: Pod sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-mkhnx requesting resource cpu=0m on Node eqx04-flash06
    Jan 12 02:15:48.555: INFO: Pod sonobuoy-systemd-logs-daemon-set-a79a1bb5decd4e2d-ppd9w requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod cent-1-server-01 requesting resource cpu=0m on Node eqx04-flash06
    Jan 12 02:15:48.555: INFO: Pod cent-2-server-01 requesting resource cpu=0m on Node eqx04-flash06
    Jan 12 02:15:48.555: INFO: Pod ravi-ravi-mysql-0 requesting resource cpu=0m on Node eqx03-flash06
    Jan 12 02:15:48.555: INFO: Pod sq-mysql-01 requesting resource cpu=0m on Node eqx04-flash06
    STEP: Starting Pods to consume most of the cluster CPU. 01/12/23 02:15:48.555
    Jan 12 02:15:48.555: INFO: Creating a pod which consumes cpu=12180m on Node eqx03-flash06
    Jan 12 02:15:48.626: INFO: Creating a pod which consumes cpu=7280m on Node eqx04-flash06
    Jan 12 02:15:48.724: INFO: Waiting up to 5m0s for pod "filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e" in namespace "sched-pred-7457" to be "running"
    Jan 12 02:15:48.729: INFO: Pod "filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.712123ms
    Jan 12 02:15:50.732: INFO: Pod "filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008491527s
    Jan 12 02:15:50.732: INFO: Pod "filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e" satisfied condition "running"
    Jan 12 02:15:50.732: INFO: Waiting up to 5m0s for pod "filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0" in namespace "sched-pred-7457" to be "running"
    Jan 12 02:15:50.735: INFO: Pod "filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.004072ms
    Jan 12 02:15:52.740: INFO: Pod "filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.00733852s
    Jan 12 02:15:52.740: INFO: Pod "filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/12/23 02:15:52.74
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e.17396e0f5a8e492a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7457/filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e to eqx03-flash06] 01/12/23 02:15:52.744
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e.17396e0f9d374f5c], Reason = [AddedInterface], Message = [Add eth0 [172.21.117.134/32] from calico] 01/12/23 02:15:52.744
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e.17396e0f9f0c9194], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/12/23 02:15:52.744
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e.17396e0fa3292a9d], Reason = [Created], Message = [Created container filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e] 01/12/23 02:15:52.744
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e.17396e0fb25bfc0a], Reason = [Started], Message = [Started container filler-pod-c17d7f77-dd47-4b5d-888b-023edfb81d6e] 01/12/23 02:15:52.744
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0.17396e0f5eee5503], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7457/filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0 to eqx04-flash06] 01/12/23 02:15:52.744
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0.17396e0fa367bd97], Reason = [AddedInterface], Message = [Add eth0 [172.21.88.181/32] from calico] 01/12/23 02:15:52.744
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0.17396e0fa6397d3d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/12/23 02:15:52.744
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0.17396e0fad07576c], Reason = [Created], Message = [Created container filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0] 01/12/23 02:15:52.744
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0.17396e0fba0e3ba1], Reason = [Started], Message = [Started container filler-pod-f060bebd-ed36-4022-83bd-c4e8e07e17a0] 01/12/23 02:15:52.744
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17396e10497aea5c], Reason = [FailedScheduling], Message = [0/5 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/5 nodes are available: 2 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 01/12/23 02:15:52.783
    STEP: removing the label node off the node eqx03-flash06 01/12/23 02:15:53.781
    STEP: verifying the node doesn't have the label node 01/12/23 02:15:53.8
    STEP: removing the label node off the node eqx04-flash06 01/12/23 02:15:53.803
    STEP: verifying the node doesn't have the label node 01/12/23 02:15:53.819
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:15:53.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7457" for this suite. 01/12/23 02:15:53.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:15:53.888
Jan 12 02:15:53.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename projected 01/12/23 02:15:53.889
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:15:53.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:15:53.908
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 01/12/23 02:15:53.911
Jan 12 02:15:53.945: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89" in namespace "projected-8482" to be "Succeeded or Failed"
Jan 12 02:15:53.948: INFO: Pod "downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.907793ms
Jan 12 02:15:55.954: INFO: Pod "downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008409503s
Jan 12 02:15:57.952: INFO: Pod "downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006378111s
STEP: Saw pod success 01/12/23 02:15:57.952
Jan 12 02:15:57.952: INFO: Pod "downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89" satisfied condition "Succeeded or Failed"
Jan 12 02:15:57.955: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89 container client-container: <nil>
STEP: delete the pod 01/12/23 02:15:57.964
Jan 12 02:15:57.979: INFO: Waiting for pod downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89 to disappear
Jan 12 02:15:57.981: INFO: Pod downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 02:15:57.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8482" for this suite. 01/12/23 02:15:57.986
------------------------------
• [4.207 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:15:53.888
    Jan 12 02:15:53.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename projected 01/12/23 02:15:53.889
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:15:53.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:15:53.908
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 01/12/23 02:15:53.911
    Jan 12 02:15:53.945: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89" in namespace "projected-8482" to be "Succeeded or Failed"
    Jan 12 02:15:53.948: INFO: Pod "downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.907793ms
    Jan 12 02:15:55.954: INFO: Pod "downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008409503s
    Jan 12 02:15:57.952: INFO: Pod "downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006378111s
    STEP: Saw pod success 01/12/23 02:15:57.952
    Jan 12 02:15:57.952: INFO: Pod "downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89" satisfied condition "Succeeded or Failed"
    Jan 12 02:15:57.955: INFO: Trying to get logs from node eqx04-flash06 pod downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89 container client-container: <nil>
    STEP: delete the pod 01/12/23 02:15:57.964
    Jan 12 02:15:57.979: INFO: Waiting for pod downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89 to disappear
    Jan 12 02:15:57.981: INFO: Pod downwardapi-volume-6d1f4082-e899-4672-a97a-0e846c950a89 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:15:57.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8482" for this suite. 01/12/23 02:15:57.986
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:15:58.096
Jan 12 02:15:58.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename deployment 01/12/23 02:15:58.096
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:15:58.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:15:58.116
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 12 02:15:58.118: INFO: Creating deployment "test-recreate-deployment"
Jan 12 02:15:58.262: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 12 02:15:58.268: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 12 02:16:00.275: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 12 02:16:00.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 2, 15, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 15, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 2, 15, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 15, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 02:16:02.283: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 12 02:16:02.294: INFO: Updating deployment test-recreate-deployment
Jan 12 02:16:02.294: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 02:16:02.397: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-483  8585da1a-91ab-4377-8d3a-da22488e7929 20185438 2 2023-01-12 02:15:58 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-12 02:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 02:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00489cae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-12 02:16:02 +0000 UTC,LastTransitionTime:2023-01-12 02:16:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-12 02:16:02 +0000 UTC,LastTransitionTime:2023-01-12 02:15:58 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 12 02:16:02.401: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-483  6c847ba3-26c1-462f-ba7c-cbf0112bb6f4 20185437 1 2023-01-12 02:16:02 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8585da1a-91ab-4377-8d3a-da22488e7929 0xc00489cfe0 0xc00489cfe1}] [] [{kube-controller-manager Update apps/v1 2023-01-12 02:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8585da1a-91ab-4377-8d3a-da22488e7929\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 02:16:02 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00489d078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 12 02:16:02.401: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 12 02:16:02.401: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-483  a4de2bd2-ae9e-425e-a545-92bd8409d7cb 20185429 2 2023-01-12 02:15:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8585da1a-91ab-4377-8d3a-da22488e7929 0xc00489cec7 0xc00489cec8}] [] [{kube-controller-manager Update apps/v1 2023-01-12 02:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8585da1a-91ab-4377-8d3a-da22488e7929\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 02:16:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00489cf78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 12 02:16:02.404: INFO: Pod "test-recreate-deployment-cff6dc657-dn2cg" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-dn2cg test-recreate-deployment-cff6dc657- deployment-483  0f33a488-135a-42b4-b286-2330e8056f2c 20185434 0 2023-01-12 02:16:02 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 6c847ba3-26c1-462f-ba7c-cbf0112bb6f4 0xc00489d500 0xc00489d501}] [] [{kube-controller-manager Update v1 2023-01-12 02:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c847ba3-26c1-462f-ba7c-cbf0112bb6f4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6ktkk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6ktkk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 02:16:02.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-483" for this suite. 01/12/23 02:16:02.409
------------------------------
• [4.345 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:15:58.096
    Jan 12 02:15:58.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename deployment 01/12/23 02:15:58.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:15:58.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:15:58.116
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 12 02:15:58.118: INFO: Creating deployment "test-recreate-deployment"
    Jan 12 02:15:58.262: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 12 02:15:58.268: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan 12 02:16:00.275: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 12 02:16:00.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 2, 15, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 15, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 2, 15, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 15, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 02:16:02.283: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 12 02:16:02.294: INFO: Updating deployment test-recreate-deployment
    Jan 12 02:16:02.294: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 02:16:02.397: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-483  8585da1a-91ab-4377-8d3a-da22488e7929 20185438 2 2023-01-12 02:15:58 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-12 02:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 02:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00489cae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-12 02:16:02 +0000 UTC,LastTransitionTime:2023-01-12 02:16:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-12 02:16:02 +0000 UTC,LastTransitionTime:2023-01-12 02:15:58 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 12 02:16:02.401: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-483  6c847ba3-26c1-462f-ba7c-cbf0112bb6f4 20185437 1 2023-01-12 02:16:02 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8585da1a-91ab-4377-8d3a-da22488e7929 0xc00489cfe0 0xc00489cfe1}] [] [{kube-controller-manager Update apps/v1 2023-01-12 02:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8585da1a-91ab-4377-8d3a-da22488e7929\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 02:16:02 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00489d078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 02:16:02.401: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 12 02:16:02.401: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-483  a4de2bd2-ae9e-425e-a545-92bd8409d7cb 20185429 2 2023-01-12 02:15:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8585da1a-91ab-4377-8d3a-da22488e7929 0xc00489cec7 0xc00489cec8}] [] [{kube-controller-manager Update apps/v1 2023-01-12 02:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8585da1a-91ab-4377-8d3a-da22488e7929\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 02:16:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00489cf78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 02:16:02.404: INFO: Pod "test-recreate-deployment-cff6dc657-dn2cg" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-dn2cg test-recreate-deployment-cff6dc657- deployment-483  0f33a488-135a-42b4-b286-2330e8056f2c 20185434 0 2023-01-12 02:16:02 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 6c847ba3-26c1-462f-ba7c-cbf0112bb6f4 0xc00489d500 0xc00489d501}] [] [{kube-controller-manager Update v1 2023-01-12 02:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c847ba3-26c1-462f-ba7c-cbf0112bb6f4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6ktkk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6ktkk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:16:02.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-483" for this suite. 01/12/23 02:16:02.409
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:16:02.44
Jan 12 02:16:02.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename pods 01/12/23 02:16:02.441
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:16:02.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:16:02.462
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 01/12/23 02:16:02.465
Jan 12 02:16:02.537: INFO: Waiting up to 5m0s for pod "pod-h2zkt" in namespace "pods-3886" to be "running"
Jan 12 02:16:02.540: INFO: Pod "pod-h2zkt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.314812ms
Jan 12 02:16:04.545: INFO: Pod "pod-h2zkt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007781876s
Jan 12 02:16:06.545: INFO: Pod "pod-h2zkt": Phase="Running", Reason="", readiness=true. Elapsed: 4.008430238s
Jan 12 02:16:06.545: INFO: Pod "pod-h2zkt" satisfied condition "running"
STEP: patching /status 01/12/23 02:16:06.545
Jan 12 02:16:06.556: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 02:16:06.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3886" for this suite. 01/12/23 02:16:06.562
------------------------------
• [4.139 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:16:02.44
    Jan 12 02:16:02.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename pods 01/12/23 02:16:02.441
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:16:02.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:16:02.462
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 01/12/23 02:16:02.465
    Jan 12 02:16:02.537: INFO: Waiting up to 5m0s for pod "pod-h2zkt" in namespace "pods-3886" to be "running"
    Jan 12 02:16:02.540: INFO: Pod "pod-h2zkt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.314812ms
    Jan 12 02:16:04.545: INFO: Pod "pod-h2zkt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007781876s
    Jan 12 02:16:06.545: INFO: Pod "pod-h2zkt": Phase="Running", Reason="", readiness=true. Elapsed: 4.008430238s
    Jan 12 02:16:06.545: INFO: Pod "pod-h2zkt" satisfied condition "running"
    STEP: patching /status 01/12/23 02:16:06.545
    Jan 12 02:16:06.556: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:16:06.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3886" for this suite. 01/12/23 02:16:06.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:16:06.58
Jan 12 02:16:06.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename crd-webhook 01/12/23 02:16:06.581
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:16:06.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:16:06.599
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/12/23 02:16:06.601
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/12/23 02:16:07.272
STEP: Deploying the custom resource conversion webhook pod 01/12/23 02:16:07.279
STEP: Wait for the deployment to be ready 01/12/23 02:16:07.338
Jan 12 02:16:07.344: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 12 02:16:09.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 2, 16, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 16, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 2, 16, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 16, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 02:16:11.363
STEP: Verifying the service has paired with the endpoint 01/12/23 02:16:11.376
Jan 12 02:16:12.377: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 12 02:16:12.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Creating a v1 custom resource 01/12/23 02:16:14.98
STEP: Create a v2 custom resource 01/12/23 02:16:14.998
STEP: List CRs in v1 01/12/23 02:16:15.072
STEP: List CRs in v2 01/12/23 02:16:15.077
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 02:16:15.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-7404" for this suite. 01/12/23 02:16:15.639
------------------------------
• [SLOW TEST] [9.111 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:16:06.58
    Jan 12 02:16:06.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename crd-webhook 01/12/23 02:16:06.581
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:16:06.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:16:06.599
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/12/23 02:16:06.601
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/12/23 02:16:07.272
    STEP: Deploying the custom resource conversion webhook pod 01/12/23 02:16:07.279
    STEP: Wait for the deployment to be ready 01/12/23 02:16:07.338
    Jan 12 02:16:07.344: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Jan 12 02:16:09.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 2, 16, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 16, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 2, 16, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 2, 16, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 02:16:11.363
    STEP: Verifying the service has paired with the endpoint 01/12/23 02:16:11.376
    Jan 12 02:16:12.377: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 12 02:16:12.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Creating a v1 custom resource 01/12/23 02:16:14.98
    STEP: Create a v2 custom resource 01/12/23 02:16:14.998
    STEP: List CRs in v1 01/12/23 02:16:15.072
    STEP: List CRs in v2 01/12/23 02:16:15.077
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:16:15.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-7404" for this suite. 01/12/23 02:16:15.639
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:16:15.692
Jan 12 02:16:15.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename sysctl 01/12/23 02:16:15.693
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:16:15.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:16:15.713
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/12/23 02:16:15.716
STEP: Watching for error events or started pod 01/12/23 02:16:15.788
STEP: Waiting for pod completion 01/12/23 02:16:17.793
Jan 12 02:16:17.793: INFO: Waiting up to 3m0s for pod "sysctl-b92ff211-0976-412e-bc6f-cf0cd6a13994" in namespace "sysctl-7952" to be "completed"
Jan 12 02:16:17.796: INFO: Pod "sysctl-b92ff211-0976-412e-bc6f-cf0cd6a13994": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980444ms
Jan 12 02:16:19.801: INFO: Pod "sysctl-b92ff211-0976-412e-bc6f-cf0cd6a13994": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008081535s
Jan 12 02:16:21.801: INFO: Pod "sysctl-b92ff211-0976-412e-bc6f-cf0cd6a13994": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007657109s
Jan 12 02:16:21.801: INFO: Pod "sysctl-b92ff211-0976-412e-bc6f-cf0cd6a13994" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/12/23 02:16:21.804
STEP: Getting logs from the pod 01/12/23 02:16:21.804
STEP: Checking that the sysctl is actually updated 01/12/23 02:16:21.813
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 12 02:16:21.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-7952" for this suite. 01/12/23 02:16:21.818
------------------------------
• [SLOW TEST] [6.142 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:16:15.692
    Jan 12 02:16:15.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename sysctl 01/12/23 02:16:15.693
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:16:15.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:16:15.713
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/12/23 02:16:15.716
    STEP: Watching for error events or started pod 01/12/23 02:16:15.788
    STEP: Waiting for pod completion 01/12/23 02:16:17.793
    Jan 12 02:16:17.793: INFO: Waiting up to 3m0s for pod "sysctl-b92ff211-0976-412e-bc6f-cf0cd6a13994" in namespace "sysctl-7952" to be "completed"
    Jan 12 02:16:17.796: INFO: Pod "sysctl-b92ff211-0976-412e-bc6f-cf0cd6a13994": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980444ms
    Jan 12 02:16:19.801: INFO: Pod "sysctl-b92ff211-0976-412e-bc6f-cf0cd6a13994": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008081535s
    Jan 12 02:16:21.801: INFO: Pod "sysctl-b92ff211-0976-412e-bc6f-cf0cd6a13994": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007657109s
    Jan 12 02:16:21.801: INFO: Pod "sysctl-b92ff211-0976-412e-bc6f-cf0cd6a13994" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/12/23 02:16:21.804
    STEP: Getting logs from the pod 01/12/23 02:16:21.804
    STEP: Checking that the sysctl is actually updated 01/12/23 02:16:21.813
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:16:21.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-7952" for this suite. 01/12/23 02:16:21.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:16:21.843
Jan 12 02:16:21.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename downward-api 01/12/23 02:16:21.844
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:16:21.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:16:21.862
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 01/12/23 02:16:21.865
Jan 12 02:16:21.895: INFO: Waiting up to 5m0s for pod "downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b" in namespace "downward-api-3457" to be "Succeeded or Failed"
Jan 12 02:16:21.898: INFO: Pod "downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.707743ms
Jan 12 02:16:23.903: INFO: Pod "downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007426566s
Jan 12 02:16:25.902: INFO: Pod "downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006955217s
Jan 12 02:16:27.903: INFO: Pod "downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007651587s
STEP: Saw pod success 01/12/23 02:16:27.903
Jan 12 02:16:27.903: INFO: Pod "downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b" satisfied condition "Succeeded or Failed"
Jan 12 02:16:27.906: INFO: Trying to get logs from node eqx04-flash06 pod downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b container dapi-container: <nil>
STEP: delete the pod 01/12/23 02:16:27.916
Jan 12 02:16:27.932: INFO: Waiting for pod downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b to disappear
Jan 12 02:16:27.934: INFO: Pod downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 12 02:16:27.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3457" for this suite. 01/12/23 02:16:27.939
------------------------------
• [SLOW TEST] [6.467 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:16:21.843
    Jan 12 02:16:21.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename downward-api 01/12/23 02:16:21.844
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:16:21.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:16:21.862
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 01/12/23 02:16:21.865
    Jan 12 02:16:21.895: INFO: Waiting up to 5m0s for pod "downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b" in namespace "downward-api-3457" to be "Succeeded or Failed"
    Jan 12 02:16:21.898: INFO: Pod "downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.707743ms
    Jan 12 02:16:23.903: INFO: Pod "downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007426566s
    Jan 12 02:16:25.902: INFO: Pod "downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006955217s
    Jan 12 02:16:27.903: INFO: Pod "downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007651587s
    STEP: Saw pod success 01/12/23 02:16:27.903
    Jan 12 02:16:27.903: INFO: Pod "downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b" satisfied condition "Succeeded or Failed"
    Jan 12 02:16:27.906: INFO: Trying to get logs from node eqx04-flash06 pod downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b container dapi-container: <nil>
    STEP: delete the pod 01/12/23 02:16:27.916
    Jan 12 02:16:27.932: INFO: Waiting for pod downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b to disappear
    Jan 12 02:16:27.934: INFO: Pod downward-api-01cd005b-1cb1-4fa3-a3ff-23ef094be21b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:16:27.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3457" for this suite. 01/12/23 02:16:27.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:16:28.311
Jan 12 02:16:28.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename taint-single-pod 01/12/23 02:16:28.312
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:16:28.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:16:28.329
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jan 12 02:16:28.332: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 12 02:17:28.437: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jan 12 02:17:28.443: INFO: Starting informer...
STEP: Starting pod... 01/12/23 02:17:28.443
Jan 12 02:17:28.685: INFO: Pod is running on eqx04-flash06. Tainting Node
STEP: Trying to apply a taint on the Node 01/12/23 02:17:28.685
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 02:17:28.702
STEP: Waiting short time to make sure Pod is queued for deletion 01/12/23 02:17:28.706
Jan 12 02:17:28.706: INFO: Pod wasn't evicted. Proceeding
Jan 12 02:17:28.706: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 02:17:28.726
STEP: Waiting some time to make sure that toleration time passed. 01/12/23 02:17:28.746
Jan 12 02:18:43.748: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 02:18:43.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-4853" for this suite. 01/12/23 02:18:43.754
------------------------------
• [SLOW TEST] [135.470 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:16:28.311
    Jan 12 02:16:28.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename taint-single-pod 01/12/23 02:16:28.312
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:16:28.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:16:28.329
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jan 12 02:16:28.332: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 12 02:17:28.437: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jan 12 02:17:28.443: INFO: Starting informer...
    STEP: Starting pod... 01/12/23 02:17:28.443
    Jan 12 02:17:28.685: INFO: Pod is running on eqx04-flash06. Tainting Node
    STEP: Trying to apply a taint on the Node 01/12/23 02:17:28.685
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 02:17:28.702
    STEP: Waiting short time to make sure Pod is queued for deletion 01/12/23 02:17:28.706
    Jan 12 02:17:28.706: INFO: Pod wasn't evicted. Proceeding
    Jan 12 02:17:28.706: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 02:17:28.726
    STEP: Waiting some time to make sure that toleration time passed. 01/12/23 02:17:28.746
    Jan 12 02:18:43.748: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:18:43.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-4853" for this suite. 01/12/23 02:18:43.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 02:18:43.783
Jan 12 02:18:43.783: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
STEP: Building a namespace api object, basename runtimeclass 01/12/23 02:18:43.784
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:18:43.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:18:43.812
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/12/23 02:18:43.815
STEP: getting /apis/node.k8s.io 01/12/23 02:18:43.817
STEP: getting /apis/node.k8s.io/v1 01/12/23 02:18:43.818
STEP: creating 01/12/23 02:18:43.819
STEP: watching 01/12/23 02:18:43.836
Jan 12 02:18:43.836: INFO: starting watch
STEP: getting 01/12/23 02:18:43.842
STEP: listing 01/12/23 02:18:43.844
STEP: patching 01/12/23 02:18:43.847
STEP: updating 01/12/23 02:18:43.852
Jan 12 02:18:43.857: INFO: waiting for watch events with expected annotations
STEP: deleting 01/12/23 02:18:43.857
STEP: deleting a collection 01/12/23 02:18:43.868
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 12 02:18:43.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1947" for this suite. 01/12/23 02:18:43.889
------------------------------
• [0.123 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 02:18:43.783
    Jan 12 02:18:43.783: INFO: >>> kubeConfig: /tmp/kubeconfig-1006763425
    STEP: Building a namespace api object, basename runtimeclass 01/12/23 02:18:43.784
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 02:18:43.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 02:18:43.812
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/12/23 02:18:43.815
    STEP: getting /apis/node.k8s.io 01/12/23 02:18:43.817
    STEP: getting /apis/node.k8s.io/v1 01/12/23 02:18:43.818
    STEP: creating 01/12/23 02:18:43.819
    STEP: watching 01/12/23 02:18:43.836
    Jan 12 02:18:43.836: INFO: starting watch
    STEP: getting 01/12/23 02:18:43.842
    STEP: listing 01/12/23 02:18:43.844
    STEP: patching 01/12/23 02:18:43.847
    STEP: updating 01/12/23 02:18:43.852
    Jan 12 02:18:43.857: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/12/23 02:18:43.857
    STEP: deleting a collection 01/12/23 02:18:43.868
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 12 02:18:43.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1947" for this suite. 01/12/23 02:18:43.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jan 12 02:18:43.908: INFO: Running AfterSuite actions on node 1
Jan 12 02:18:43.908: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jan 12 02:18:43.908: INFO: Running AfterSuite actions on node 1
    Jan 12 02:18:43.908: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.081 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5887.002 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h38m7.384046168s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

