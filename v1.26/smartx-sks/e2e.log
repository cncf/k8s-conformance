I0609 10:18:26.015840      18 e2e.go:126] Starting e2e run "1e3b8b3b-6336-49ab-9439-1cfe1adcddf6" on Ginkgo node 1
Jun  9 10:18:26.036: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1686305905 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jun  9 10:18:26.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:18:26.226: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun  9 10:18:26.249: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun  9 10:18:26.284: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun  9 10:18:26.284: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jun  9 10:18:26.284: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun  9 10:18:26.291: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun  9 10:18:26.291: INFO: e2e test version: v1.26.4
Jun  9 10:18:26.292: INFO: kube-apiserver version: v1.26.4
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jun  9 10:18:26.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:18:26.299: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.075 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jun  9 10:18:26.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:18:26.226: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jun  9 10:18:26.249: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jun  9 10:18:26.284: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jun  9 10:18:26.284: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
    Jun  9 10:18:26.284: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jun  9 10:18:26.291: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jun  9 10:18:26.291: INFO: e2e test version: v1.26.4
    Jun  9 10:18:26.292: INFO: kube-apiserver version: v1.26.4
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jun  9 10:18:26.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:18:26.299: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:18:26.331
Jun  9 10:18:26.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename sched-preemption 06/09/23 10:18:26.331
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:18:26.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:18:26.577
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jun  9 10:18:26.610: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  9 10:19:26.716: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:19:26.721
Jun  9 10:19:26.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename sched-preemption-path 06/09/23 10:19:26.723
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:19:26.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:19:26.957
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Jun  9 10:19:27.178: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jun  9 10:19:27.188: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jun  9 10:19:27.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:19:27.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-6959" for this suite. 06/09/23 10:19:27.686
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9861" for this suite. 06/09/23 10:19:27.705
------------------------------
• [SLOW TEST] [61.392 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:18:26.331
    Jun  9 10:18:26.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename sched-preemption 06/09/23 10:18:26.331
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:18:26.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:18:26.577
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jun  9 10:18:26.610: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun  9 10:19:26.716: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:19:26.721
    Jun  9 10:19:26.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename sched-preemption-path 06/09/23 10:19:26.723
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:19:26.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:19:26.957
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Jun  9 10:19:27.178: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jun  9 10:19:27.188: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:19:27.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:19:27.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-6959" for this suite. 06/09/23 10:19:27.686
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9861" for this suite. 06/09/23 10:19:27.705
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:19:27.722
Jun  9 10:19:27.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 10:19:27.723
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:19:27.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:19:27.762
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 06/09/23 10:19:27.77
Jun  9 10:19:27.787: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269" in namespace "downward-api-1757" to be "Succeeded or Failed"
Jun  9 10:19:27.796: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Pending", Reason="", readiness=false. Elapsed: 9.893982ms
Jun  9 10:19:29.803: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016546619s
Jun  9 10:19:31.804: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017260736s
Jun  9 10:19:33.805: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018434298s
Jun  9 10:19:35.804: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017401444s
Jun  9 10:19:37.806: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01931033s
Jun  9 10:19:39.811: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Pending", Reason="", readiness=false. Elapsed: 12.024061516s
Jun  9 10:19:41.805: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.018285609s
STEP: Saw pod success 06/09/23 10:19:41.805
Jun  9 10:19:41.805: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269" satisfied condition "Succeeded or Failed"
Jun  9 10:19:41.812: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269 container client-container: <nil>
STEP: delete the pod 06/09/23 10:19:41.846
Jun  9 10:19:41.863: INFO: Waiting for pod downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269 to disappear
Jun  9 10:19:41.870: INFO: Pod downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  9 10:19:41.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1757" for this suite. 06/09/23 10:19:41.879
------------------------------
• [SLOW TEST] [14.168 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:19:27.722
    Jun  9 10:19:27.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 10:19:27.723
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:19:27.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:19:27.762
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 06/09/23 10:19:27.77
    Jun  9 10:19:27.787: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269" in namespace "downward-api-1757" to be "Succeeded or Failed"
    Jun  9 10:19:27.796: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Pending", Reason="", readiness=false. Elapsed: 9.893982ms
    Jun  9 10:19:29.803: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016546619s
    Jun  9 10:19:31.804: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017260736s
    Jun  9 10:19:33.805: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018434298s
    Jun  9 10:19:35.804: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017401444s
    Jun  9 10:19:37.806: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01931033s
    Jun  9 10:19:39.811: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Pending", Reason="", readiness=false. Elapsed: 12.024061516s
    Jun  9 10:19:41.805: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.018285609s
    STEP: Saw pod success 06/09/23 10:19:41.805
    Jun  9 10:19:41.805: INFO: Pod "downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269" satisfied condition "Succeeded or Failed"
    Jun  9 10:19:41.812: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269 container client-container: <nil>
    STEP: delete the pod 06/09/23 10:19:41.846
    Jun  9 10:19:41.863: INFO: Waiting for pod downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269 to disappear
    Jun  9 10:19:41.870: INFO: Pod downwardapi-volume-2288849b-b9f3-4f44-b69c-00c2acc4f269 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:19:41.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1757" for this suite. 06/09/23 10:19:41.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:19:41.891
Jun  9 10:19:41.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 10:19:41.892
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:19:41.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:19:41.932
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 06/09/23 10:19:41.936
Jun  9 10:19:41.949: INFO: Waiting up to 5m0s for pod "annotationupdate42ffc131-17ed-4edb-85d3-dcea5c0e536e" in namespace "downward-api-2028" to be "running and ready"
Jun  9 10:19:41.958: INFO: Pod "annotationupdate42ffc131-17ed-4edb-85d3-dcea5c0e536e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.362505ms
Jun  9 10:19:41.958: INFO: The phase of Pod annotationupdate42ffc131-17ed-4edb-85d3-dcea5c0e536e is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:19:43.964: INFO: Pod "annotationupdate42ffc131-17ed-4edb-85d3-dcea5c0e536e": Phase="Running", Reason="", readiness=true. Elapsed: 2.014621743s
Jun  9 10:19:43.964: INFO: The phase of Pod annotationupdate42ffc131-17ed-4edb-85d3-dcea5c0e536e is Running (Ready = true)
Jun  9 10:19:43.964: INFO: Pod "annotationupdate42ffc131-17ed-4edb-85d3-dcea5c0e536e" satisfied condition "running and ready"
Jun  9 10:19:44.500: INFO: Successfully updated pod "annotationupdate42ffc131-17ed-4edb-85d3-dcea5c0e536e"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  9 10:19:46.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2028" for this suite. 06/09/23 10:19:46.529
------------------------------
• [4.649 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:19:41.891
    Jun  9 10:19:41.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 10:19:41.892
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:19:41.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:19:41.932
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 06/09/23 10:19:41.936
    Jun  9 10:19:41.949: INFO: Waiting up to 5m0s for pod "annotationupdate42ffc131-17ed-4edb-85d3-dcea5c0e536e" in namespace "downward-api-2028" to be "running and ready"
    Jun  9 10:19:41.958: INFO: Pod "annotationupdate42ffc131-17ed-4edb-85d3-dcea5c0e536e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.362505ms
    Jun  9 10:19:41.958: INFO: The phase of Pod annotationupdate42ffc131-17ed-4edb-85d3-dcea5c0e536e is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:19:43.964: INFO: Pod "annotationupdate42ffc131-17ed-4edb-85d3-dcea5c0e536e": Phase="Running", Reason="", readiness=true. Elapsed: 2.014621743s
    Jun  9 10:19:43.964: INFO: The phase of Pod annotationupdate42ffc131-17ed-4edb-85d3-dcea5c0e536e is Running (Ready = true)
    Jun  9 10:19:43.964: INFO: Pod "annotationupdate42ffc131-17ed-4edb-85d3-dcea5c0e536e" satisfied condition "running and ready"
    Jun  9 10:19:44.500: INFO: Successfully updated pod "annotationupdate42ffc131-17ed-4edb-85d3-dcea5c0e536e"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:19:46.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2028" for this suite. 06/09/23 10:19:46.529
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:19:46.542
Jun  9 10:19:46.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 10:19:46.543
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:19:46.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:19:46.574
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/09/23 10:19:46.581
Jun  9 10:19:46.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1532 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun  9 10:19:46.711: INFO: stderr: ""
Jun  9 10:19:46.711: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 06/09/23 10:19:46.711
STEP: verifying the pod e2e-test-httpd-pod was created 06/09/23 10:20:16.765
Jun  9 10:20:16.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1532 get pod e2e-test-httpd-pod -o json'
Jun  9 10:20:16.854: INFO: stderr: ""
Jun  9 10:20:16.854: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"f3077e3008392fd980efc1721f60e1a54502dfcf3dc0fe3a5d4a3cc042af150e\",\n            \"cni.projectcalico.org/podIP\": \"172.27.53.72/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.27.53.72/32\"\n        },\n        \"creationTimestamp\": \"2023-06-09T10:19:46Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1532\",\n        \"resourceVersion\": \"67259\",\n        \"uid\": \"9feaadd8-8dec-4f10-96cb-c66827339145\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-67sjj\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"sks-test-v1-26.4-workergroup-qdprq\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-67sjj\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-09T10:19:46Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-09T10:20:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-09T10:20:13Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-09T10:19:46Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://49164290dd0846aaae1b4a644a045b9c4c8ddc55d6adab03a364f4a77132c313\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-09T10:20:12Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.255.64.103\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.27.53.72\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.27.53.72\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-09T10:19:46Z\"\n    }\n}\n"
STEP: replace the image in the pod 06/09/23 10:20:16.854
Jun  9 10:20:16.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1532 replace -f -'
Jun  9 10:20:18.127: INFO: stderr: ""
Jun  9 10:20:18.127: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 06/09/23 10:20:18.127
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jun  9 10:20:18.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1532 delete pods e2e-test-httpd-pod'
Jun  9 10:20:31.845: INFO: stderr: ""
Jun  9 10:20:31.845: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 10:20:31.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1532" for this suite. 06/09/23 10:20:31.856
------------------------------
• [SLOW TEST] [45.326 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:19:46.542
    Jun  9 10:19:46.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 10:19:46.543
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:19:46.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:19:46.574
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/09/23 10:19:46.581
    Jun  9 10:19:46.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1532 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jun  9 10:19:46.711: INFO: stderr: ""
    Jun  9 10:19:46.711: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 06/09/23 10:19:46.711
    STEP: verifying the pod e2e-test-httpd-pod was created 06/09/23 10:20:16.765
    Jun  9 10:20:16.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1532 get pod e2e-test-httpd-pod -o json'
    Jun  9 10:20:16.854: INFO: stderr: ""
    Jun  9 10:20:16.854: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"f3077e3008392fd980efc1721f60e1a54502dfcf3dc0fe3a5d4a3cc042af150e\",\n            \"cni.projectcalico.org/podIP\": \"172.27.53.72/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.27.53.72/32\"\n        },\n        \"creationTimestamp\": \"2023-06-09T10:19:46Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1532\",\n        \"resourceVersion\": \"67259\",\n        \"uid\": \"9feaadd8-8dec-4f10-96cb-c66827339145\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-67sjj\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"sks-test-v1-26.4-workergroup-qdprq\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-67sjj\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-09T10:19:46Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-09T10:20:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-09T10:20:13Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-09T10:19:46Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://49164290dd0846aaae1b4a644a045b9c4c8ddc55d6adab03a364f4a77132c313\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-09T10:20:12Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.255.64.103\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.27.53.72\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.27.53.72\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-09T10:19:46Z\"\n    }\n}\n"
    STEP: replace the image in the pod 06/09/23 10:20:16.854
    Jun  9 10:20:16.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1532 replace -f -'
    Jun  9 10:20:18.127: INFO: stderr: ""
    Jun  9 10:20:18.127: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 06/09/23 10:20:18.127
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jun  9 10:20:18.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1532 delete pods e2e-test-httpd-pod'
    Jun  9 10:20:31.845: INFO: stderr: ""
    Jun  9 10:20:31.845: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:20:31.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1532" for this suite. 06/09/23 10:20:31.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:20:31.869
Jun  9 10:20:31.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename events 06/09/23 10:20:31.871
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:20:31.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:20:31.901
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 06/09/23 10:20:31.906
Jun  9 10:20:31.919: INFO: created test-event-1
Jun  9 10:20:31.928: INFO: created test-event-2
Jun  9 10:20:31.937: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 06/09/23 10:20:31.937
STEP: delete collection of events 06/09/23 10:20:31.946
Jun  9 10:20:31.946: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 06/09/23 10:20:31.984
Jun  9 10:20:31.984: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jun  9 10:20:31.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3429" for this suite. 06/09/23 10:20:31.997
------------------------------
• [0.144 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:20:31.869
    Jun  9 10:20:31.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename events 06/09/23 10:20:31.871
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:20:31.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:20:31.901
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 06/09/23 10:20:31.906
    Jun  9 10:20:31.919: INFO: created test-event-1
    Jun  9 10:20:31.928: INFO: created test-event-2
    Jun  9 10:20:31.937: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 06/09/23 10:20:31.937
    STEP: delete collection of events 06/09/23 10:20:31.946
    Jun  9 10:20:31.946: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 06/09/23 10:20:31.984
    Jun  9 10:20:31.984: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:20:31.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3429" for this suite. 06/09/23 10:20:31.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:20:32.014
Jun  9 10:20:32.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pods 06/09/23 10:20:32.015
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:20:32.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:20:32.045
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 06/09/23 10:20:32.05
STEP: submitting the pod to kubernetes 06/09/23 10:20:32.051
Jun  9 10:20:32.067: INFO: Waiting up to 5m0s for pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885" in namespace "pods-7662" to be "running and ready"
Jun  9 10:20:32.074: INFO: Pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885": Phase="Pending", Reason="", readiness=false. Elapsed: 6.681485ms
Jun  9 10:20:32.074: INFO: The phase of Pod pod-update-f818ec09-5139-460c-85cd-32151bf2f885 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:20:34.082: INFO: Pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015350453s
Jun  9 10:20:34.082: INFO: The phase of Pod pod-update-f818ec09-5139-460c-85cd-32151bf2f885 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:20:36.079: INFO: Pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012184491s
Jun  9 10:20:36.079: INFO: The phase of Pod pod-update-f818ec09-5139-460c-85cd-32151bf2f885 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:20:38.083: INFO: Pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885": Phase="Running", Reason="", readiness=true. Elapsed: 6.015751514s
Jun  9 10:20:38.083: INFO: The phase of Pod pod-update-f818ec09-5139-460c-85cd-32151bf2f885 is Running (Ready = true)
Jun  9 10:20:38.083: INFO: Pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 06/09/23 10:20:38.088
STEP: updating the pod 06/09/23 10:20:38.095
Jun  9 10:20:38.615: INFO: Successfully updated pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885"
Jun  9 10:20:38.615: INFO: Waiting up to 5m0s for pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885" in namespace "pods-7662" to be "running"
Jun  9 10:20:38.620: INFO: Pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885": Phase="Running", Reason="", readiness=true. Elapsed: 5.041788ms
Jun  9 10:20:38.620: INFO: Pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 06/09/23 10:20:38.62
Jun  9 10:20:38.626: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  9 10:20:38.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7662" for this suite. 06/09/23 10:20:38.633
------------------------------
• [SLOW TEST] [6.628 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:20:32.014
    Jun  9 10:20:32.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pods 06/09/23 10:20:32.015
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:20:32.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:20:32.045
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 06/09/23 10:20:32.05
    STEP: submitting the pod to kubernetes 06/09/23 10:20:32.051
    Jun  9 10:20:32.067: INFO: Waiting up to 5m0s for pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885" in namespace "pods-7662" to be "running and ready"
    Jun  9 10:20:32.074: INFO: Pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885": Phase="Pending", Reason="", readiness=false. Elapsed: 6.681485ms
    Jun  9 10:20:32.074: INFO: The phase of Pod pod-update-f818ec09-5139-460c-85cd-32151bf2f885 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:20:34.082: INFO: Pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015350453s
    Jun  9 10:20:34.082: INFO: The phase of Pod pod-update-f818ec09-5139-460c-85cd-32151bf2f885 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:20:36.079: INFO: Pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012184491s
    Jun  9 10:20:36.079: INFO: The phase of Pod pod-update-f818ec09-5139-460c-85cd-32151bf2f885 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:20:38.083: INFO: Pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885": Phase="Running", Reason="", readiness=true. Elapsed: 6.015751514s
    Jun  9 10:20:38.083: INFO: The phase of Pod pod-update-f818ec09-5139-460c-85cd-32151bf2f885 is Running (Ready = true)
    Jun  9 10:20:38.083: INFO: Pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 06/09/23 10:20:38.088
    STEP: updating the pod 06/09/23 10:20:38.095
    Jun  9 10:20:38.615: INFO: Successfully updated pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885"
    Jun  9 10:20:38.615: INFO: Waiting up to 5m0s for pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885" in namespace "pods-7662" to be "running"
    Jun  9 10:20:38.620: INFO: Pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885": Phase="Running", Reason="", readiness=true. Elapsed: 5.041788ms
    Jun  9 10:20:38.620: INFO: Pod "pod-update-f818ec09-5139-460c-85cd-32151bf2f885" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 06/09/23 10:20:38.62
    Jun  9 10:20:38.626: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:20:38.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7662" for this suite. 06/09/23 10:20:38.633
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:20:38.642
Jun  9 10:20:38.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 10:20:38.644
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:20:38.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:20:38.667
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 06/09/23 10:20:38.676
Jun  9 10:20:38.690: INFO: Waiting up to 5m0s for pod "downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402" in namespace "downward-api-9652" to be "Succeeded or Failed"
Jun  9 10:20:38.695: INFO: Pod "downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402": Phase="Pending", Reason="", readiness=false. Elapsed: 5.586035ms
Jun  9 10:20:40.701: INFO: Pod "downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011584235s
Jun  9 10:20:42.703: INFO: Pod "downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012874367s
STEP: Saw pod success 06/09/23 10:20:42.703
Jun  9 10:20:42.703: INFO: Pod "downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402" satisfied condition "Succeeded or Failed"
Jun  9 10:20:42.708: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402 container client-container: <nil>
STEP: delete the pod 06/09/23 10:20:42.717
Jun  9 10:20:42.750: INFO: Waiting for pod downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402 to disappear
Jun  9 10:20:42.755: INFO: Pod downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  9 10:20:42.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9652" for this suite. 06/09/23 10:20:42.764
------------------------------
• [4.132 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:20:38.642
    Jun  9 10:20:38.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 10:20:38.644
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:20:38.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:20:38.667
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 06/09/23 10:20:38.676
    Jun  9 10:20:38.690: INFO: Waiting up to 5m0s for pod "downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402" in namespace "downward-api-9652" to be "Succeeded or Failed"
    Jun  9 10:20:38.695: INFO: Pod "downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402": Phase="Pending", Reason="", readiness=false. Elapsed: 5.586035ms
    Jun  9 10:20:40.701: INFO: Pod "downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011584235s
    Jun  9 10:20:42.703: INFO: Pod "downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012874367s
    STEP: Saw pod success 06/09/23 10:20:42.703
    Jun  9 10:20:42.703: INFO: Pod "downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402" satisfied condition "Succeeded or Failed"
    Jun  9 10:20:42.708: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402 container client-container: <nil>
    STEP: delete the pod 06/09/23 10:20:42.717
    Jun  9 10:20:42.750: INFO: Waiting for pod downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402 to disappear
    Jun  9 10:20:42.755: INFO: Pod downwardapi-volume-991a6921-c87e-447b-9b1e-cad936caa402 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:20:42.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9652" for this suite. 06/09/23 10:20:42.764
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:20:42.776
Jun  9 10:20:42.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename daemonsets 06/09/23 10:20:42.777
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:20:42.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:20:42.81
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 06/09/23 10:20:42.856
STEP: Check that daemon pods launch on every node of the cluster. 06/09/23 10:20:42.864
Jun  9 10:20:42.872: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:42.872: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:42.872: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:42.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:20:42.877: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:43.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:43.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:43.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:43.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:43.891: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:44.883: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:44.883: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:44.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:44.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:44.892: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:45.887: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:45.887: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:45.887: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:45.893: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:45.893: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:46.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:46.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:46.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:46.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:46.890: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:47.888: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:47.888: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:47.888: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:47.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:47.894: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:48.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:48.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:48.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:48.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:48.894: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:49.994: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:49.995: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:49.995: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:50.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:50.006: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:50.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:50.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:50.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:50.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:50.891: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:51.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:51.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:51.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:51.898: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:51.898: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:52.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:52.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:52.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:52.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:52.894: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:53.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:53.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:53.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:53.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:53.892: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:54.888: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:54.888: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:54.888: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:54.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:54.897: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:55.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:55.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:55.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:55.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:55.892: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:56.895: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:56.895: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:56.895: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:56.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:56.901: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:57.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:57.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:57.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:57.893: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:57.893: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:58.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:58.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:58.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:58.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:58.890: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:20:59.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:59.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:59.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:20:59.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:20:59.895: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:21:00.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:00.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:00.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:00.888: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:21:00.888: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:21:01.887: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:01.887: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:01.887: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:01.895: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:21:01.895: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:21:02.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:02.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:02.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:02.893: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:21:02.893: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:21:03.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:03.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:03.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:03.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:21:03.890: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:21:04.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:04.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:04.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:21:04.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  9 10:21:04.890: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 06/09/23 10:21:04.894
Jun  9 10:21:04.902: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 06/09/23 10:21:04.902
Jun  9 10:21:04.921: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 06/09/23 10:21:04.921
Jun  9 10:21:04.925: INFO: Observed &DaemonSet event: ADDED
Jun  9 10:21:04.925: INFO: Observed &DaemonSet event: MODIFIED
Jun  9 10:21:04.925: INFO: Observed &DaemonSet event: MODIFIED
Jun  9 10:21:04.925: INFO: Observed &DaemonSet event: MODIFIED
Jun  9 10:21:04.925: INFO: Observed &DaemonSet event: MODIFIED
Jun  9 10:21:04.925: INFO: Found daemon set daemon-set in namespace daemonsets-7475 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  9 10:21:04.925: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 06/09/23 10:21:04.925
STEP: watching for the daemon set status to be patched 06/09/23 10:21:04.938
Jun  9 10:21:04.940: INFO: Observed &DaemonSet event: ADDED
Jun  9 10:21:04.940: INFO: Observed &DaemonSet event: MODIFIED
Jun  9 10:21:04.941: INFO: Observed &DaemonSet event: MODIFIED
Jun  9 10:21:04.941: INFO: Observed &DaemonSet event: MODIFIED
Jun  9 10:21:04.941: INFO: Observed &DaemonSet event: MODIFIED
Jun  9 10:21:04.941: INFO: Observed daemon set daemon-set in namespace daemonsets-7475 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  9 10:21:04.941: INFO: Observed &DaemonSet event: MODIFIED
Jun  9 10:21:04.941: INFO: Found daemon set daemon-set in namespace daemonsets-7475 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jun  9 10:21:04.941: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/09/23 10:21:04.946
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7475, will wait for the garbage collector to delete the pods 06/09/23 10:21:04.946
Jun  9 10:21:05.015: INFO: Deleting DaemonSet.extensions daemon-set took: 10.801447ms
Jun  9 10:21:05.115: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.459243ms
Jun  9 10:21:07.122: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:21:07.122: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  9 10:21:07.128: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"67674"},"items":null}

Jun  9 10:21:07.134: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"67674"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:21:07.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7475" for this suite. 06/09/23 10:21:07.165
------------------------------
• [SLOW TEST] [24.401 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:20:42.776
    Jun  9 10:20:42.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename daemonsets 06/09/23 10:20:42.777
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:20:42.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:20:42.81
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 06/09/23 10:20:42.856
    STEP: Check that daemon pods launch on every node of the cluster. 06/09/23 10:20:42.864
    Jun  9 10:20:42.872: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:42.872: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:42.872: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:42.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:20:42.877: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:43.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:43.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:43.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:43.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:43.891: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:44.883: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:44.883: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:44.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:44.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:44.892: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:45.887: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:45.887: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:45.887: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:45.893: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:45.893: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:46.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:46.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:46.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:46.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:46.890: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:47.888: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:47.888: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:47.888: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:47.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:47.894: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:48.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:48.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:48.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:48.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:48.894: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:49.994: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:49.995: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:49.995: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:50.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:50.006: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:50.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:50.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:50.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:50.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:50.891: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:51.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:51.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:51.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:51.898: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:51.898: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:52.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:52.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:52.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:52.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:52.894: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:53.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:53.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:53.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:53.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:53.892: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:54.888: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:54.888: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:54.888: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:54.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:54.897: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:55.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:55.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:55.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:55.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:55.892: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:56.895: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:56.895: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:56.895: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:56.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:56.901: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:57.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:57.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:57.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:57.893: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:57.893: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:58.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:58.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:58.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:58.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:58.890: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:20:59.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:59.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:59.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:20:59.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:20:59.895: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:21:00.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:00.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:00.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:00.888: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:21:00.888: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:21:01.887: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:01.887: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:01.887: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:01.895: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:21:01.895: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:21:02.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:02.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:02.886: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:02.893: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:21:02.893: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:21:03.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:03.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:03.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:03.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:21:03.890: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:21:04.884: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:04.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:04.885: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:21:04.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun  9 10:21:04.890: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 06/09/23 10:21:04.894
    Jun  9 10:21:04.902: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 06/09/23 10:21:04.902
    Jun  9 10:21:04.921: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 06/09/23 10:21:04.921
    Jun  9 10:21:04.925: INFO: Observed &DaemonSet event: ADDED
    Jun  9 10:21:04.925: INFO: Observed &DaemonSet event: MODIFIED
    Jun  9 10:21:04.925: INFO: Observed &DaemonSet event: MODIFIED
    Jun  9 10:21:04.925: INFO: Observed &DaemonSet event: MODIFIED
    Jun  9 10:21:04.925: INFO: Observed &DaemonSet event: MODIFIED
    Jun  9 10:21:04.925: INFO: Found daemon set daemon-set in namespace daemonsets-7475 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun  9 10:21:04.925: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 06/09/23 10:21:04.925
    STEP: watching for the daemon set status to be patched 06/09/23 10:21:04.938
    Jun  9 10:21:04.940: INFO: Observed &DaemonSet event: ADDED
    Jun  9 10:21:04.940: INFO: Observed &DaemonSet event: MODIFIED
    Jun  9 10:21:04.941: INFO: Observed &DaemonSet event: MODIFIED
    Jun  9 10:21:04.941: INFO: Observed &DaemonSet event: MODIFIED
    Jun  9 10:21:04.941: INFO: Observed &DaemonSet event: MODIFIED
    Jun  9 10:21:04.941: INFO: Observed daemon set daemon-set in namespace daemonsets-7475 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun  9 10:21:04.941: INFO: Observed &DaemonSet event: MODIFIED
    Jun  9 10:21:04.941: INFO: Found daemon set daemon-set in namespace daemonsets-7475 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jun  9 10:21:04.941: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/09/23 10:21:04.946
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7475, will wait for the garbage collector to delete the pods 06/09/23 10:21:04.946
    Jun  9 10:21:05.015: INFO: Deleting DaemonSet.extensions daemon-set took: 10.801447ms
    Jun  9 10:21:05.115: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.459243ms
    Jun  9 10:21:07.122: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:21:07.122: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun  9 10:21:07.128: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"67674"},"items":null}

    Jun  9 10:21:07.134: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"67674"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:21:07.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7475" for this suite. 06/09/23 10:21:07.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:21:07.178
Jun  9 10:21:07.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename disruption 06/09/23 10:21:07.181
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:07.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:07.205
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 06/09/23 10:21:07.212
STEP: Waiting for the pdb to be processed 06/09/23 10:21:07.22
STEP: updating the pdb 06/09/23 10:21:09.235
STEP: Waiting for the pdb to be processed 06/09/23 10:21:09.252
STEP: patching the pdb 06/09/23 10:21:09.26
STEP: Waiting for the pdb to be processed 06/09/23 10:21:09.279
STEP: Waiting for the pdb to be deleted 06/09/23 10:21:09.295
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun  9 10:21:09.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9911" for this suite. 06/09/23 10:21:09.308
------------------------------
• [2.142 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:21:07.178
    Jun  9 10:21:07.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename disruption 06/09/23 10:21:07.181
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:07.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:07.205
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 06/09/23 10:21:07.212
    STEP: Waiting for the pdb to be processed 06/09/23 10:21:07.22
    STEP: updating the pdb 06/09/23 10:21:09.235
    STEP: Waiting for the pdb to be processed 06/09/23 10:21:09.252
    STEP: patching the pdb 06/09/23 10:21:09.26
    STEP: Waiting for the pdb to be processed 06/09/23 10:21:09.279
    STEP: Waiting for the pdb to be deleted 06/09/23 10:21:09.295
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:21:09.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9911" for this suite. 06/09/23 10:21:09.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:21:09.322
Jun  9 10:21:09.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 10:21:09.324
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:09.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:09.351
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-4bcdee33-d848-4ab9-8657-5534d2452b32 06/09/23 10:21:09.357
STEP: Creating a pod to test consume configMaps 06/09/23 10:21:09.366
Jun  9 10:21:09.379: INFO: Waiting up to 5m0s for pod "pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567" in namespace "configmap-780" to be "Succeeded or Failed"
Jun  9 10:21:09.389: INFO: Pod "pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567": Phase="Pending", Reason="", readiness=false. Elapsed: 9.653574ms
Jun  9 10:21:11.396: INFO: Pod "pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016418172s
Jun  9 10:21:13.397: INFO: Pod "pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017024655s
STEP: Saw pod success 06/09/23 10:21:13.397
Jun  9 10:21:13.397: INFO: Pod "pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567" satisfied condition "Succeeded or Failed"
Jun  9 10:21:13.403: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567 container configmap-volume-test: <nil>
STEP: delete the pod 06/09/23 10:21:13.413
Jun  9 10:21:13.436: INFO: Waiting for pod pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567 to disappear
Jun  9 10:21:13.441: INFO: Pod pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 10:21:13.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-780" for this suite. 06/09/23 10:21:13.447
------------------------------
• [4.154 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:21:09.322
    Jun  9 10:21:09.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 10:21:09.324
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:09.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:09.351
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-4bcdee33-d848-4ab9-8657-5534d2452b32 06/09/23 10:21:09.357
    STEP: Creating a pod to test consume configMaps 06/09/23 10:21:09.366
    Jun  9 10:21:09.379: INFO: Waiting up to 5m0s for pod "pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567" in namespace "configmap-780" to be "Succeeded or Failed"
    Jun  9 10:21:09.389: INFO: Pod "pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567": Phase="Pending", Reason="", readiness=false. Elapsed: 9.653574ms
    Jun  9 10:21:11.396: INFO: Pod "pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016418172s
    Jun  9 10:21:13.397: INFO: Pod "pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017024655s
    STEP: Saw pod success 06/09/23 10:21:13.397
    Jun  9 10:21:13.397: INFO: Pod "pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567" satisfied condition "Succeeded or Failed"
    Jun  9 10:21:13.403: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567 container configmap-volume-test: <nil>
    STEP: delete the pod 06/09/23 10:21:13.413
    Jun  9 10:21:13.436: INFO: Waiting for pod pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567 to disappear
    Jun  9 10:21:13.441: INFO: Pod pod-configmaps-7fe104b5-bf73-4777-ba0e-66e8b23e0567 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:21:13.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-780" for this suite. 06/09/23 10:21:13.447
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:21:13.477
Jun  9 10:21:13.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename events 06/09/23 10:21:13.478
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:13.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:13.505
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 06/09/23 10:21:13.51
STEP: get a list of Events with a label in the current namespace 06/09/23 10:21:13.537
STEP: delete a list of events 06/09/23 10:21:13.543
Jun  9 10:21:13.543: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 06/09/23 10:21:13.573
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jun  9 10:21:13.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2057" for this suite. 06/09/23 10:21:13.588
------------------------------
• [0.122 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:21:13.477
    Jun  9 10:21:13.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename events 06/09/23 10:21:13.478
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:13.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:13.505
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 06/09/23 10:21:13.51
    STEP: get a list of Events with a label in the current namespace 06/09/23 10:21:13.537
    STEP: delete a list of events 06/09/23 10:21:13.543
    Jun  9 10:21:13.543: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 06/09/23 10:21:13.573
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:21:13.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2057" for this suite. 06/09/23 10:21:13.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:21:13.599
Jun  9 10:21:13.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename security-context 06/09/23 10:21:13.6
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:13.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:13.63
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/09/23 10:21:13.635
Jun  9 10:21:13.649: INFO: Waiting up to 5m0s for pod "security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c" in namespace "security-context-2178" to be "Succeeded or Failed"
Jun  9 10:21:13.655: INFO: Pod "security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.209526ms
Jun  9 10:21:15.662: INFO: Pod "security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01293512s
Jun  9 10:21:17.663: INFO: Pod "security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014226031s
STEP: Saw pod success 06/09/23 10:21:17.663
Jun  9 10:21:17.663: INFO: Pod "security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c" satisfied condition "Succeeded or Failed"
Jun  9 10:21:17.669: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c container test-container: <nil>
STEP: delete the pod 06/09/23 10:21:17.685
Jun  9 10:21:17.713: INFO: Waiting for pod security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c to disappear
Jun  9 10:21:17.719: INFO: Pod security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun  9 10:21:17.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-2178" for this suite. 06/09/23 10:21:17.727
------------------------------
• [4.138 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:21:13.599
    Jun  9 10:21:13.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename security-context 06/09/23 10:21:13.6
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:13.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:13.63
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/09/23 10:21:13.635
    Jun  9 10:21:13.649: INFO: Waiting up to 5m0s for pod "security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c" in namespace "security-context-2178" to be "Succeeded or Failed"
    Jun  9 10:21:13.655: INFO: Pod "security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.209526ms
    Jun  9 10:21:15.662: INFO: Pod "security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01293512s
    Jun  9 10:21:17.663: INFO: Pod "security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014226031s
    STEP: Saw pod success 06/09/23 10:21:17.663
    Jun  9 10:21:17.663: INFO: Pod "security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c" satisfied condition "Succeeded or Failed"
    Jun  9 10:21:17.669: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c container test-container: <nil>
    STEP: delete the pod 06/09/23 10:21:17.685
    Jun  9 10:21:17.713: INFO: Waiting for pod security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c to disappear
    Jun  9 10:21:17.719: INFO: Pod security-context-28e6b677-aa5e-4fd8-ac39-6e146e1d202c no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:21:17.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-2178" for this suite. 06/09/23 10:21:17.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:21:17.737
Jun  9 10:21:17.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename security-context-test 06/09/23 10:21:17.738
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:17.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:17.771
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Jun  9 10:21:17.791: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-7c5b6fdd-c556-4cee-80ef-78ca346f15f1" in namespace "security-context-test-8598" to be "Succeeded or Failed"
Jun  9 10:21:17.803: INFO: Pod "busybox-readonly-false-7c5b6fdd-c556-4cee-80ef-78ca346f15f1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.390112ms
Jun  9 10:21:19.811: INFO: Pod "busybox-readonly-false-7c5b6fdd-c556-4cee-80ef-78ca346f15f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019755885s
Jun  9 10:21:21.810: INFO: Pod "busybox-readonly-false-7c5b6fdd-c556-4cee-80ef-78ca346f15f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019051622s
Jun  9 10:21:21.810: INFO: Pod "busybox-readonly-false-7c5b6fdd-c556-4cee-80ef-78ca346f15f1" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun  9 10:21:21.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8598" for this suite. 06/09/23 10:21:21.819
------------------------------
• [4.091 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:21:17.737
    Jun  9 10:21:17.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename security-context-test 06/09/23 10:21:17.738
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:17.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:17.771
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Jun  9 10:21:17.791: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-7c5b6fdd-c556-4cee-80ef-78ca346f15f1" in namespace "security-context-test-8598" to be "Succeeded or Failed"
    Jun  9 10:21:17.803: INFO: Pod "busybox-readonly-false-7c5b6fdd-c556-4cee-80ef-78ca346f15f1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.390112ms
    Jun  9 10:21:19.811: INFO: Pod "busybox-readonly-false-7c5b6fdd-c556-4cee-80ef-78ca346f15f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019755885s
    Jun  9 10:21:21.810: INFO: Pod "busybox-readonly-false-7c5b6fdd-c556-4cee-80ef-78ca346f15f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019051622s
    Jun  9 10:21:21.810: INFO: Pod "busybox-readonly-false-7c5b6fdd-c556-4cee-80ef-78ca346f15f1" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:21:21.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8598" for this suite. 06/09/23 10:21:21.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:21:21.829
Jun  9 10:21:21.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-lifecycle-hook 06/09/23 10:21:21.83
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:21.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:21.861
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 06/09/23 10:21:21.876
Jun  9 10:21:21.890: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9503" to be "running and ready"
Jun  9 10:21:21.900: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.173852ms
Jun  9 10:21:21.900: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:21:23.907: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016189937s
Jun  9 10:21:23.907: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun  9 10:21:23.907: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 06/09/23 10:21:23.913
Jun  9 10:21:23.922: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9503" to be "running and ready"
Jun  9 10:21:23.928: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.99274ms
Jun  9 10:21:23.928: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:21:25.934: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01224458s
Jun  9 10:21:25.934: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jun  9 10:21:25.934: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 06/09/23 10:21:25.94
Jun  9 10:21:25.953: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  9 10:21:25.959: INFO: Pod pod-with-prestop-http-hook still exists
Jun  9 10:21:27.959: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  9 10:21:27.972: INFO: Pod pod-with-prestop-http-hook still exists
Jun  9 10:21:29.959: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  9 10:21:29.965: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 06/09/23 10:21:29.965
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jun  9 10:21:29.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9503" for this suite. 06/09/23 10:21:29.983
------------------------------
• [SLOW TEST] [8.165 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:21:21.829
    Jun  9 10:21:21.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/09/23 10:21:21.83
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:21.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:21.861
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 06/09/23 10:21:21.876
    Jun  9 10:21:21.890: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9503" to be "running and ready"
    Jun  9 10:21:21.900: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.173852ms
    Jun  9 10:21:21.900: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:21:23.907: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016189937s
    Jun  9 10:21:23.907: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun  9 10:21:23.907: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 06/09/23 10:21:23.913
    Jun  9 10:21:23.922: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9503" to be "running and ready"
    Jun  9 10:21:23.928: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.99274ms
    Jun  9 10:21:23.928: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:21:25.934: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01224458s
    Jun  9 10:21:25.934: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jun  9 10:21:25.934: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 06/09/23 10:21:25.94
    Jun  9 10:21:25.953: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun  9 10:21:25.959: INFO: Pod pod-with-prestop-http-hook still exists
    Jun  9 10:21:27.959: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun  9 10:21:27.972: INFO: Pod pod-with-prestop-http-hook still exists
    Jun  9 10:21:29.959: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun  9 10:21:29.965: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 06/09/23 10:21:29.965
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:21:29.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9503" for this suite. 06/09/23 10:21:29.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:21:29.997
Jun  9 10:21:29.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename deployment 06/09/23 10:21:29.999
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:30.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:30.028
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jun  9 10:21:30.034: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun  9 10:21:30.053: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  9 10:21:35.060: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/09/23 10:21:35.06
Jun  9 10:21:35.060: INFO: Creating deployment "test-rolling-update-deployment"
Jun  9 10:21:35.069: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun  9 10:21:35.079: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun  9 10:21:37.092: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun  9 10:21:37.097: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 10:21:39.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 10:21:41.104: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 10:21:43.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 10:21:45.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 10:21:47.105: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 10:21:49.102: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  9 10:21:49.120: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6068  3fb8393f-d508-4675-b021-b34548f485e4 68115 1 2023-06-09 10:21:35 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-09 10:21:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 10:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d146d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-09 10:21:35 +0000 UTC,LastTransitionTime:2023-06-09 10:21:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-06-09 10:21:47 +0000 UTC,LastTransitionTime:2023-06-09 10:21:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  9 10:21:49.125: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-6068  e69e2c01-9e4f-4c1c-9ef2-c88b8719e2e8 68105 1 2023-06-09 10:21:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3fb8393f-d508-4675-b021-b34548f485e4 0xc003d14b97 0xc003d14b98}] [] [{kube-controller-manager Update apps/v1 2023-06-09 10:21:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fb8393f-d508-4675-b021-b34548f485e4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 10:21:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d14c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  9 10:21:49.125: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun  9 10:21:49.125: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6068  9a202187-5644-4760-b235-b2c4f13b33e7 68114 2 2023-06-09 10:21:30 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3fb8393f-d508-4675-b021-b34548f485e4 0xc003d14a67 0xc003d14a68}] [] [{e2e.test Update apps/v1 2023-06-09 10:21:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 10:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fb8393f-d508-4675-b021-b34548f485e4\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-09 10:21:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003d14b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  9 10:21:49.131: INFO: Pod "test-rolling-update-deployment-7549d9f46d-bxdtd" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-bxdtd test-rolling-update-deployment-7549d9f46d- deployment-6068  6d47389c-7d05-40d2-8972-03f0d315f6b0 68104 0 2023-06-09 10:21:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:db4c2d0e3a00fd3be704eaca0f97f7f88ee55ba3bc9fc5d670983edbd5cf61d8 cni.projectcalico.org/podIP:172.30.17.131/32 cni.projectcalico.org/podIPs:172.30.17.131/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d e69e2c01-9e4f-4c1c-9ef2-c88b8719e2e8 0xc003d150a7 0xc003d150a8}] [] [{calico Update v1 2023-06-09 10:21:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-09 10:21:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e69e2c01-9e4f-4c1c-9ef2-c88b8719e2e8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 10:21:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.17.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hhgjx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hhgjx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:21:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:21:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.17.131,StartTime:2023-06-09 10:21:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 10:21:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://84c388d27888dfab74ac7cd658d88d03e2f0df80c79be595d5f929bdac774a02,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.17.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  9 10:21:49.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6068" for this suite. 06/09/23 10:21:49.138
------------------------------
• [SLOW TEST] [19.152 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:21:29.997
    Jun  9 10:21:29.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename deployment 06/09/23 10:21:29.999
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:30.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:30.028
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jun  9 10:21:30.034: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jun  9 10:21:30.053: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun  9 10:21:35.060: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/09/23 10:21:35.06
    Jun  9 10:21:35.060: INFO: Creating deployment "test-rolling-update-deployment"
    Jun  9 10:21:35.069: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jun  9 10:21:35.079: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jun  9 10:21:37.092: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jun  9 10:21:37.097: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 10:21:39.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 10:21:41.104: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 10:21:43.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 10:21:45.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 10:21:47.105: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 21, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 10:21:49.102: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  9 10:21:49.120: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6068  3fb8393f-d508-4675-b021-b34548f485e4 68115 1 2023-06-09 10:21:35 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-09 10:21:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 10:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d146d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-09 10:21:35 +0000 UTC,LastTransitionTime:2023-06-09 10:21:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-06-09 10:21:47 +0000 UTC,LastTransitionTime:2023-06-09 10:21:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun  9 10:21:49.125: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-6068  e69e2c01-9e4f-4c1c-9ef2-c88b8719e2e8 68105 1 2023-06-09 10:21:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3fb8393f-d508-4675-b021-b34548f485e4 0xc003d14b97 0xc003d14b98}] [] [{kube-controller-manager Update apps/v1 2023-06-09 10:21:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fb8393f-d508-4675-b021-b34548f485e4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 10:21:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d14c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun  9 10:21:49.125: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jun  9 10:21:49.125: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6068  9a202187-5644-4760-b235-b2c4f13b33e7 68114 2 2023-06-09 10:21:30 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3fb8393f-d508-4675-b021-b34548f485e4 0xc003d14a67 0xc003d14a68}] [] [{e2e.test Update apps/v1 2023-06-09 10:21:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 10:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fb8393f-d508-4675-b021-b34548f485e4\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-09 10:21:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003d14b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun  9 10:21:49.131: INFO: Pod "test-rolling-update-deployment-7549d9f46d-bxdtd" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-bxdtd test-rolling-update-deployment-7549d9f46d- deployment-6068  6d47389c-7d05-40d2-8972-03f0d315f6b0 68104 0 2023-06-09 10:21:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:db4c2d0e3a00fd3be704eaca0f97f7f88ee55ba3bc9fc5d670983edbd5cf61d8 cni.projectcalico.org/podIP:172.30.17.131/32 cni.projectcalico.org/podIPs:172.30.17.131/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d e69e2c01-9e4f-4c1c-9ef2-c88b8719e2e8 0xc003d150a7 0xc003d150a8}] [] [{calico Update v1 2023-06-09 10:21:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-09 10:21:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e69e2c01-9e4f-4c1c-9ef2-c88b8719e2e8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 10:21:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.17.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hhgjx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hhgjx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:21:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:21:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.17.131,StartTime:2023-06-09 10:21:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 10:21:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://84c388d27888dfab74ac7cd658d88d03e2f0df80c79be595d5f929bdac774a02,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.17.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:21:49.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6068" for this suite. 06/09/23 10:21:49.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:21:49.15
Jun  9 10:21:49.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename limitrange 06/09/23 10:21:49.151
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:49.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:49.175
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 06/09/23 10:21:49.18
STEP: Setting up watch 06/09/23 10:21:49.181
STEP: Submitting a LimitRange 06/09/23 10:21:49.288
STEP: Verifying LimitRange creation was observed 06/09/23 10:21:49.297
STEP: Fetching the LimitRange to ensure it has proper values 06/09/23 10:21:49.297
Jun  9 10:21:49.302: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun  9 10:21:49.302: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 06/09/23 10:21:49.302
STEP: Ensuring Pod has resource requirements applied from LimitRange 06/09/23 10:21:49.311
Jun  9 10:21:49.320: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun  9 10:21:49.320: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 06/09/23 10:21:49.32
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 06/09/23 10:21:49.329
Jun  9 10:21:49.337: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun  9 10:21:49.337: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 06/09/23 10:21:49.337
STEP: Failing to create a Pod with more than max resources 06/09/23 10:21:49.341
STEP: Updating a LimitRange 06/09/23 10:21:49.344
STEP: Verifying LimitRange updating is effective 06/09/23 10:21:49.352
STEP: Creating a Pod with less than former min resources 06/09/23 10:21:51.37
STEP: Failing to create a Pod with more than max resources 06/09/23 10:21:51.379
STEP: Deleting a LimitRange 06/09/23 10:21:51.382
STEP: Verifying the LimitRange was deleted 06/09/23 10:21:51.397
Jun  9 10:21:56.404: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 06/09/23 10:21:56.404
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jun  9 10:21:56.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-9559" for this suite. 06/09/23 10:21:56.428
------------------------------
• [SLOW TEST] [7.287 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:21:49.15
    Jun  9 10:21:49.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename limitrange 06/09/23 10:21:49.151
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:49.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:49.175
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 06/09/23 10:21:49.18
    STEP: Setting up watch 06/09/23 10:21:49.181
    STEP: Submitting a LimitRange 06/09/23 10:21:49.288
    STEP: Verifying LimitRange creation was observed 06/09/23 10:21:49.297
    STEP: Fetching the LimitRange to ensure it has proper values 06/09/23 10:21:49.297
    Jun  9 10:21:49.302: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jun  9 10:21:49.302: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 06/09/23 10:21:49.302
    STEP: Ensuring Pod has resource requirements applied from LimitRange 06/09/23 10:21:49.311
    Jun  9 10:21:49.320: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jun  9 10:21:49.320: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 06/09/23 10:21:49.32
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 06/09/23 10:21:49.329
    Jun  9 10:21:49.337: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jun  9 10:21:49.337: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 06/09/23 10:21:49.337
    STEP: Failing to create a Pod with more than max resources 06/09/23 10:21:49.341
    STEP: Updating a LimitRange 06/09/23 10:21:49.344
    STEP: Verifying LimitRange updating is effective 06/09/23 10:21:49.352
    STEP: Creating a Pod with less than former min resources 06/09/23 10:21:51.37
    STEP: Failing to create a Pod with more than max resources 06/09/23 10:21:51.379
    STEP: Deleting a LimitRange 06/09/23 10:21:51.382
    STEP: Verifying the LimitRange was deleted 06/09/23 10:21:51.397
    Jun  9 10:21:56.404: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 06/09/23 10:21:56.404
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:21:56.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-9559" for this suite. 06/09/23 10:21:56.428
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:21:56.438
Jun  9 10:21:56.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename subpath 06/09/23 10:21:56.439
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:56.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:56.466
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/09/23 10:21:56.471
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-vlkc 06/09/23 10:21:56.486
STEP: Creating a pod to test atomic-volume-subpath 06/09/23 10:21:56.486
Jun  9 10:21:56.498: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vlkc" in namespace "subpath-3485" to be "Succeeded or Failed"
Jun  9 10:21:56.506: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.165673ms
Jun  9 10:21:58.513: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 2.014448633s
Jun  9 10:22:00.512: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 4.013045937s
Jun  9 10:22:02.513: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 6.014946664s
Jun  9 10:22:04.513: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 8.014828278s
Jun  9 10:22:06.514: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 10.015351186s
Jun  9 10:22:08.512: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 12.013463505s
Jun  9 10:22:10.516: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 14.017848751s
Jun  9 10:22:12.513: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 16.014703839s
Jun  9 10:22:14.516: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 18.017088372s
Jun  9 10:22:16.512: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 20.013760356s
Jun  9 10:22:18.513: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=false. Elapsed: 22.013990839s
Jun  9 10:22:20.512: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013247869s
STEP: Saw pod success 06/09/23 10:22:20.512
Jun  9 10:22:20.512: INFO: Pod "pod-subpath-test-configmap-vlkc" satisfied condition "Succeeded or Failed"
Jun  9 10:22:20.517: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-subpath-test-configmap-vlkc container test-container-subpath-configmap-vlkc: <nil>
STEP: delete the pod 06/09/23 10:22:20.529
Jun  9 10:22:20.550: INFO: Waiting for pod pod-subpath-test-configmap-vlkc to disappear
Jun  9 10:22:20.556: INFO: Pod pod-subpath-test-configmap-vlkc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vlkc 06/09/23 10:22:20.556
Jun  9 10:22:20.557: INFO: Deleting pod "pod-subpath-test-configmap-vlkc" in namespace "subpath-3485"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun  9 10:22:20.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3485" for this suite. 06/09/23 10:22:20.571
------------------------------
• [SLOW TEST] [24.144 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:21:56.438
    Jun  9 10:21:56.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename subpath 06/09/23 10:21:56.439
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:21:56.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:21:56.466
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/09/23 10:21:56.471
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-vlkc 06/09/23 10:21:56.486
    STEP: Creating a pod to test atomic-volume-subpath 06/09/23 10:21:56.486
    Jun  9 10:21:56.498: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vlkc" in namespace "subpath-3485" to be "Succeeded or Failed"
    Jun  9 10:21:56.506: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.165673ms
    Jun  9 10:21:58.513: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 2.014448633s
    Jun  9 10:22:00.512: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 4.013045937s
    Jun  9 10:22:02.513: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 6.014946664s
    Jun  9 10:22:04.513: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 8.014828278s
    Jun  9 10:22:06.514: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 10.015351186s
    Jun  9 10:22:08.512: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 12.013463505s
    Jun  9 10:22:10.516: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 14.017848751s
    Jun  9 10:22:12.513: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 16.014703839s
    Jun  9 10:22:14.516: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 18.017088372s
    Jun  9 10:22:16.512: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=true. Elapsed: 20.013760356s
    Jun  9 10:22:18.513: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Running", Reason="", readiness=false. Elapsed: 22.013990839s
    Jun  9 10:22:20.512: INFO: Pod "pod-subpath-test-configmap-vlkc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013247869s
    STEP: Saw pod success 06/09/23 10:22:20.512
    Jun  9 10:22:20.512: INFO: Pod "pod-subpath-test-configmap-vlkc" satisfied condition "Succeeded or Failed"
    Jun  9 10:22:20.517: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-subpath-test-configmap-vlkc container test-container-subpath-configmap-vlkc: <nil>
    STEP: delete the pod 06/09/23 10:22:20.529
    Jun  9 10:22:20.550: INFO: Waiting for pod pod-subpath-test-configmap-vlkc to disappear
    Jun  9 10:22:20.556: INFO: Pod pod-subpath-test-configmap-vlkc no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-vlkc 06/09/23 10:22:20.556
    Jun  9 10:22:20.557: INFO: Deleting pod "pod-subpath-test-configmap-vlkc" in namespace "subpath-3485"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:22:20.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3485" for this suite. 06/09/23 10:22:20.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:22:20.583
Jun  9 10:22:20.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 10:22:20.585
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:20.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:20.613
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 06/09/23 10:22:20.618
Jun  9 10:22:20.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:22:24.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:22:34.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1852" for this suite. 06/09/23 10:22:34.052
------------------------------
• [SLOW TEST] [13.483 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:22:20.583
    Jun  9 10:22:20.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 10:22:20.585
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:20.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:20.613
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 06/09/23 10:22:20.618
    Jun  9 10:22:20.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:22:24.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:22:34.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1852" for this suite. 06/09/23 10:22:34.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:22:34.067
Jun  9 10:22:34.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename proxy 06/09/23 10:22:34.069
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:34.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:34.104
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 06/09/23 10:22:34.198
STEP: creating replication controller proxy-service-vzntb in namespace proxy-3431 06/09/23 10:22:34.198
I0609 10:22:34.261986      18 runners.go:193] Created replication controller with name: proxy-service-vzntb, namespace: proxy-3431, replica count: 1
I0609 10:22:35.315532      18 runners.go:193] proxy-service-vzntb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0609 10:22:36.318364      18 runners.go:193] proxy-service-vzntb Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 10:22:36.323: INFO: setup took 2.212922105s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 06/09/23 10:22:36.323
Jun  9 10:22:36.351: INFO: (0) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 27.138041ms)
Jun  9 10:22:36.351: INFO: (0) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 26.804402ms)
Jun  9 10:22:36.351: INFO: (0) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 27.564198ms)
Jun  9 10:22:36.351: INFO: (0) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 26.795317ms)
Jun  9 10:22:36.351: INFO: (0) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 27.15941ms)
Jun  9 10:22:36.351: INFO: (0) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 26.903571ms)
Jun  9 10:22:36.355: INFO: (0) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 31.664195ms)
Jun  9 10:22:36.356: INFO: (0) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 31.93707ms)
Jun  9 10:22:36.356: INFO: (0) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 31.582565ms)
Jun  9 10:22:36.356: INFO: (0) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 31.505862ms)
Jun  9 10:22:36.356: INFO: (0) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 32.035989ms)
Jun  9 10:22:36.356: INFO: (0) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 31.597164ms)
Jun  9 10:22:36.356: INFO: (0) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 31.911889ms)
Jun  9 10:22:36.356: INFO: (0) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 32.848114ms)
Jun  9 10:22:36.358: INFO: (0) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 33.550788ms)
Jun  9 10:22:36.358: INFO: (0) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 34.41401ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 12.48117ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 12.789319ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 12.748856ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 12.942831ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 12.93513ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 12.765407ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 12.733033ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 12.948269ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 13.08806ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 12.89739ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 12.967185ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 12.758723ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 12.806427ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 13.18358ms)
Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 12.878761ms)
Jun  9 10:22:36.379: INFO: (1) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 21.111381ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 29.996462ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 28.843795ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 29.105758ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 29.902181ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 29.834156ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 29.193875ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 29.051051ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 29.324985ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 29.761114ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 29.655885ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 28.995945ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 28.82904ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 29.541233ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 28.921177ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 29.49947ms)
Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 29.33899ms)
Jun  9 10:22:36.420: INFO: (3) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 10.493825ms)
Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 10.578348ms)
Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 10.482162ms)
Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 10.972364ms)
Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 10.752748ms)
Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 10.596268ms)
Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 10.688099ms)
Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 10.677755ms)
Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 11.245749ms)
Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 10.631831ms)
Jun  9 10:22:36.424: INFO: (3) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 14.37761ms)
Jun  9 10:22:36.424: INFO: (3) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 14.288554ms)
Jun  9 10:22:36.424: INFO: (3) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 14.14725ms)
Jun  9 10:22:36.424: INFO: (3) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 14.673368ms)
Jun  9 10:22:36.424: INFO: (3) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 14.215542ms)
Jun  9 10:22:36.424: INFO: (3) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 14.257512ms)
Jun  9 10:22:36.436: INFO: (4) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 11.11129ms)
Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 12.271558ms)
Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 12.774132ms)
Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 12.872855ms)
Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 13.969977ms)
Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 13.763496ms)
Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 13.2516ms)
Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 13.96388ms)
Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 14.246035ms)
Jun  9 10:22:36.439: INFO: (4) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 13.423984ms)
Jun  9 10:22:36.439: INFO: (4) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 13.823672ms)
Jun  9 10:22:36.440: INFO: (4) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 14.65081ms)
Jun  9 10:22:36.440: INFO: (4) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 14.829847ms)
Jun  9 10:22:36.440: INFO: (4) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 14.785174ms)
Jun  9 10:22:36.440: INFO: (4) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 15.27257ms)
Jun  9 10:22:36.440: INFO: (4) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 15.345379ms)
Jun  9 10:22:36.449: INFO: (5) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 8.853742ms)
Jun  9 10:22:36.449: INFO: (5) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 8.605616ms)
Jun  9 10:22:36.449: INFO: (5) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 8.706236ms)
Jun  9 10:22:36.449: INFO: (5) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 9.147541ms)
Jun  9 10:22:36.449: INFO: (5) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 9.029231ms)
Jun  9 10:22:36.449: INFO: (5) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 8.961544ms)
Jun  9 10:22:36.449: INFO: (5) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 8.83762ms)
Jun  9 10:22:36.450: INFO: (5) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.893583ms)
Jun  9 10:22:36.450: INFO: (5) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 10.09545ms)
Jun  9 10:22:36.451: INFO: (5) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 10.236928ms)
Jun  9 10:22:36.451: INFO: (5) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 10.314831ms)
Jun  9 10:22:36.451: INFO: (5) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 10.726551ms)
Jun  9 10:22:36.452: INFO: (5) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 11.542452ms)
Jun  9 10:22:36.452: INFO: (5) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 11.740334ms)
Jun  9 10:22:36.453: INFO: (5) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 12.079337ms)
Jun  9 10:22:36.453: INFO: (5) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 12.398002ms)
Jun  9 10:22:36.458: INFO: (6) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 5.740875ms)
Jun  9 10:22:36.462: INFO: (6) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.110594ms)
Jun  9 10:22:36.462: INFO: (6) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 8.856515ms)
Jun  9 10:22:36.462: INFO: (6) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 8.988093ms)
Jun  9 10:22:36.463: INFO: (6) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.376861ms)
Jun  9 10:22:36.463: INFO: (6) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 9.493139ms)
Jun  9 10:22:36.463: INFO: (6) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 8.994008ms)
Jun  9 10:22:36.463: INFO: (6) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.801735ms)
Jun  9 10:22:36.463: INFO: (6) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 9.816644ms)
Jun  9 10:22:36.463: INFO: (6) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 9.903482ms)
Jun  9 10:22:36.463: INFO: (6) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.480808ms)
Jun  9 10:22:36.465: INFO: (6) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 11.633479ms)
Jun  9 10:22:36.467: INFO: (6) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 13.843804ms)
Jun  9 10:22:36.467: INFO: (6) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 13.743384ms)
Jun  9 10:22:36.467: INFO: (6) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 13.812035ms)
Jun  9 10:22:36.467: INFO: (6) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 14.045013ms)
Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.266129ms)
Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.09185ms)
Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.713072ms)
Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.939383ms)
Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 9.817355ms)
Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 9.23526ms)
Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 9.318107ms)
Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 10.063624ms)
Jun  9 10:22:36.478: INFO: (7) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 9.495107ms)
Jun  9 10:22:36.478: INFO: (7) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 10.030232ms)
Jun  9 10:22:36.478: INFO: (7) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 10.521108ms)
Jun  9 10:22:36.484: INFO: (7) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 16.063625ms)
Jun  9 10:22:36.484: INFO: (7) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 15.970699ms)
Jun  9 10:22:36.484: INFO: (7) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 15.627528ms)
Jun  9 10:22:36.484: INFO: (7) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 15.565816ms)
Jun  9 10:22:36.484: INFO: (7) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 15.758564ms)
Jun  9 10:22:36.492: INFO: (8) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 7.765657ms)
Jun  9 10:22:36.492: INFO: (8) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 7.702603ms)
Jun  9 10:22:36.492: INFO: (8) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 7.775697ms)
Jun  9 10:22:36.492: INFO: (8) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 7.812638ms)
Jun  9 10:22:36.493: INFO: (8) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 8.668591ms)
Jun  9 10:22:36.493: INFO: (8) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 8.268602ms)
Jun  9 10:22:36.493: INFO: (8) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 8.190918ms)
Jun  9 10:22:36.493: INFO: (8) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 8.450427ms)
Jun  9 10:22:36.493: INFO: (8) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 8.753844ms)
Jun  9 10:22:36.493: INFO: (8) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 8.310654ms)
Jun  9 10:22:36.493: INFO: (8) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 8.368984ms)
Jun  9 10:22:36.494: INFO: (8) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 9.469233ms)
Jun  9 10:22:36.496: INFO: (8) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 11.547572ms)
Jun  9 10:22:36.496: INFO: (8) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 11.523401ms)
Jun  9 10:22:36.496: INFO: (8) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 11.613858ms)
Jun  9 10:22:36.496: INFO: (8) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 11.771354ms)
Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.353379ms)
Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 9.838594ms)
Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 9.791555ms)
Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 9.98088ms)
Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 10.510143ms)
Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 9.948668ms)
Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 10.012882ms)
Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 10.122275ms)
Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 10.20016ms)
Jun  9 10:22:36.507: INFO: (9) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 10.009128ms)
Jun  9 10:22:36.511: INFO: (9) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 15.110138ms)
Jun  9 10:22:36.511: INFO: (9) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 14.629817ms)
Jun  9 10:22:36.511: INFO: (9) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 14.619276ms)
Jun  9 10:22:36.511: INFO: (9) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 15.130959ms)
Jun  9 10:22:36.511: INFO: (9) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 15.132546ms)
Jun  9 10:22:36.511: INFO: (9) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 15.077327ms)
Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 8.123555ms)
Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 8.192986ms)
Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.309948ms)
Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 8.35364ms)
Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 8.614167ms)
Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 8.963999ms)
Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 8.435304ms)
Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.491747ms)
Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 8.369091ms)
Jun  9 10:22:36.528: INFO: (10) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 15.776191ms)
Jun  9 10:22:36.528: INFO: (10) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 16.318187ms)
Jun  9 10:22:36.528: INFO: (10) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 16.283215ms)
Jun  9 10:22:36.528: INFO: (10) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 15.785955ms)
Jun  9 10:22:36.528: INFO: (10) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 16.089294ms)
Jun  9 10:22:36.528: INFO: (10) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 16.629868ms)
Jun  9 10:22:36.529: INFO: (10) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 16.89564ms)
Jun  9 10:22:36.537: INFO: (11) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.292794ms)
Jun  9 10:22:36.537: INFO: (11) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 8.129393ms)
Jun  9 10:22:36.537: INFO: (11) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 8.385534ms)
Jun  9 10:22:36.537: INFO: (11) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 8.454507ms)
Jun  9 10:22:36.537: INFO: (11) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 8.655581ms)
Jun  9 10:22:36.537: INFO: (11) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.151767ms)
Jun  9 10:22:36.537: INFO: (11) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 8.249795ms)
Jun  9 10:22:36.538: INFO: (11) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 8.665235ms)
Jun  9 10:22:36.538: INFO: (11) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 8.961524ms)
Jun  9 10:22:36.538: INFO: (11) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 8.52908ms)
Jun  9 10:22:36.538: INFO: (11) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 8.707165ms)
Jun  9 10:22:36.538: INFO: (11) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 9.431875ms)
Jun  9 10:22:36.539: INFO: (11) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 10.298972ms)
Jun  9 10:22:36.539: INFO: (11) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 10.306238ms)
Jun  9 10:22:36.539: INFO: (11) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 10.418516ms)
Jun  9 10:22:36.539: INFO: (11) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 10.494728ms)
Jun  9 10:22:36.548: INFO: (12) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.416725ms)
Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 9.158902ms)
Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.260839ms)
Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 9.34206ms)
Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.448854ms)
Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 9.541664ms)
Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 9.399498ms)
Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.678923ms)
Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 9.204008ms)
Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 9.3584ms)
Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 9.207347ms)
Jun  9 10:22:36.551: INFO: (12) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 10.518217ms)
Jun  9 10:22:36.551: INFO: (12) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 10.651854ms)
Jun  9 10:22:36.551: INFO: (12) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 11.357348ms)
Jun  9 10:22:36.551: INFO: (12) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 11.456417ms)
Jun  9 10:22:36.551: INFO: (12) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 11.51725ms)
Jun  9 10:22:36.558: INFO: (13) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 6.72093ms)
Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 8.29995ms)
Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 7.936679ms)
Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 7.905387ms)
Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 8.359548ms)
Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 8.036333ms)
Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 7.910407ms)
Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.148377ms)
Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 7.887179ms)
Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 8.375763ms)
Jun  9 10:22:36.562: INFO: (13) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 10.193352ms)
Jun  9 10:22:36.563: INFO: (13) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 11.497826ms)
Jun  9 10:22:36.563: INFO: (13) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 11.162152ms)
Jun  9 10:22:36.563: INFO: (13) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 11.577213ms)
Jun  9 10:22:36.563: INFO: (13) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 11.139887ms)
Jun  9 10:22:36.563: INFO: (13) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 11.102274ms)
Jun  9 10:22:36.569: INFO: (14) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 5.163646ms)
Jun  9 10:22:36.570: INFO: (14) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 6.743468ms)
Jun  9 10:22:36.571: INFO: (14) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 6.110183ms)
Jun  9 10:22:36.571: INFO: (14) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 6.744454ms)
Jun  9 10:22:36.571: INFO: (14) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 7.072239ms)
Jun  9 10:22:36.571: INFO: (14) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 6.726449ms)
Jun  9 10:22:36.572: INFO: (14) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 7.507123ms)
Jun  9 10:22:36.572: INFO: (14) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 7.705317ms)
Jun  9 10:22:36.572: INFO: (14) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 7.750828ms)
Jun  9 10:22:36.572: INFO: (14) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 7.839603ms)
Jun  9 10:22:36.572: INFO: (14) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 8.283584ms)
Jun  9 10:22:36.572: INFO: (14) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 8.240111ms)
Jun  9 10:22:36.573: INFO: (14) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 8.650619ms)
Jun  9 10:22:36.577: INFO: (14) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 12.199605ms)
Jun  9 10:22:36.577: INFO: (14) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 12.206488ms)
Jun  9 10:22:36.577: INFO: (14) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 12.18903ms)
Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.794296ms)
Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 9.927446ms)
Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.718903ms)
Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.495395ms)
Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 9.439777ms)
Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.65309ms)
Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 9.369994ms)
Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 10.101114ms)
Jun  9 10:22:36.588: INFO: (15) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 9.966643ms)
Jun  9 10:22:36.588: INFO: (15) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 10.661953ms)
Jun  9 10:22:36.588: INFO: (15) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 9.122093ms)
Jun  9 10:22:36.588: INFO: (15) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 10.169083ms)
Jun  9 10:22:36.588: INFO: (15) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 10.464629ms)
Jun  9 10:22:36.590: INFO: (15) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 12.563952ms)
Jun  9 10:22:36.590: INFO: (15) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 12.110303ms)
Jun  9 10:22:36.591: INFO: (15) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 12.46972ms)
Jun  9 10:22:36.603: INFO: (16) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 11.433081ms)
Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 18.272352ms)
Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 18.301033ms)
Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 18.187957ms)
Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 18.166188ms)
Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 18.232502ms)
Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 18.428031ms)
Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 18.635222ms)
Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 18.224907ms)
Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 18.401064ms)
Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 18.660743ms)
Jun  9 10:22:36.610: INFO: (16) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 18.758968ms)
Jun  9 10:22:36.612: INFO: (16) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 21.302606ms)
Jun  9 10:22:36.613: INFO: (16) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 22.053748ms)
Jun  9 10:22:36.613: INFO: (16) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 22.10985ms)
Jun  9 10:22:36.614: INFO: (16) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 22.650769ms)
Jun  9 10:22:36.623: INFO: (17) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.60458ms)
Jun  9 10:22:36.625: INFO: (17) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 11.014678ms)
Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 11.482415ms)
Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 11.426097ms)
Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 11.803292ms)
Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 11.48354ms)
Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 11.399819ms)
Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 11.562279ms)
Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 11.497641ms)
Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 11.548648ms)
Jun  9 10:22:36.630: INFO: (17) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 16.059384ms)
Jun  9 10:22:36.631: INFO: (17) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 17.171057ms)
Jun  9 10:22:36.631: INFO: (17) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 17.176796ms)
Jun  9 10:22:36.631: INFO: (17) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 17.205798ms)
Jun  9 10:22:36.631: INFO: (17) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 17.400605ms)
Jun  9 10:22:36.632: INFO: (17) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 17.450543ms)
Jun  9 10:22:36.640: INFO: (18) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 7.427381ms)
Jun  9 10:22:36.643: INFO: (18) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 11.069265ms)
Jun  9 10:22:36.643: INFO: (18) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 11.431917ms)
Jun  9 10:22:36.643: INFO: (18) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 11.059282ms)
Jun  9 10:22:36.643: INFO: (18) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 11.264478ms)
Jun  9 10:22:36.644: INFO: (18) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 11.971458ms)
Jun  9 10:22:36.644: INFO: (18) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 12.095558ms)
Jun  9 10:22:36.644: INFO: (18) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 11.945909ms)
Jun  9 10:22:36.644: INFO: (18) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 11.993475ms)
Jun  9 10:22:36.644: INFO: (18) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 11.855262ms)
Jun  9 10:22:36.644: INFO: (18) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 12.201086ms)
Jun  9 10:22:36.645: INFO: (18) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 13.594929ms)
Jun  9 10:22:36.646: INFO: (18) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 14.115844ms)
Jun  9 10:22:36.646: INFO: (18) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 14.400752ms)
Jun  9 10:22:36.647: INFO: (18) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 14.441568ms)
Jun  9 10:22:36.647: INFO: (18) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 14.214868ms)
Jun  9 10:22:36.654: INFO: (19) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 7.575521ms)
Jun  9 10:22:36.654: INFO: (19) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 7.795005ms)
Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 9.154337ms)
Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 9.475015ms)
Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 9.234251ms)
Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.222433ms)
Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 9.218442ms)
Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 9.464744ms)
Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.136086ms)
Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.516931ms)
Jun  9 10:22:36.658: INFO: (19) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 11.085617ms)
Jun  9 10:22:36.659: INFO: (19) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 11.837471ms)
Jun  9 10:22:36.659: INFO: (19) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 11.892896ms)
Jun  9 10:22:36.659: INFO: (19) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 12.12729ms)
Jun  9 10:22:36.659: INFO: (19) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 12.046649ms)
Jun  9 10:22:36.659: INFO: (19) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 12.395659ms)
STEP: deleting ReplicationController proxy-service-vzntb in namespace proxy-3431, will wait for the garbage collector to delete the pods 06/09/23 10:22:36.66
Jun  9 10:22:36.729: INFO: Deleting ReplicationController proxy-service-vzntb took: 13.111583ms
Jun  9 10:22:36.830: INFO: Terminating ReplicationController proxy-service-vzntb pods took: 101.02375ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jun  9 10:22:39.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3431" for this suite. 06/09/23 10:22:39.34
------------------------------
• [SLOW TEST] [5.283 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:22:34.067
    Jun  9 10:22:34.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename proxy 06/09/23 10:22:34.069
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:34.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:34.104
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 06/09/23 10:22:34.198
    STEP: creating replication controller proxy-service-vzntb in namespace proxy-3431 06/09/23 10:22:34.198
    I0609 10:22:34.261986      18 runners.go:193] Created replication controller with name: proxy-service-vzntb, namespace: proxy-3431, replica count: 1
    I0609 10:22:35.315532      18 runners.go:193] proxy-service-vzntb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0609 10:22:36.318364      18 runners.go:193] proxy-service-vzntb Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  9 10:22:36.323: INFO: setup took 2.212922105s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 06/09/23 10:22:36.323
    Jun  9 10:22:36.351: INFO: (0) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 27.138041ms)
    Jun  9 10:22:36.351: INFO: (0) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 26.804402ms)
    Jun  9 10:22:36.351: INFO: (0) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 27.564198ms)
    Jun  9 10:22:36.351: INFO: (0) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 26.795317ms)
    Jun  9 10:22:36.351: INFO: (0) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 27.15941ms)
    Jun  9 10:22:36.351: INFO: (0) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 26.903571ms)
    Jun  9 10:22:36.355: INFO: (0) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 31.664195ms)
    Jun  9 10:22:36.356: INFO: (0) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 31.93707ms)
    Jun  9 10:22:36.356: INFO: (0) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 31.582565ms)
    Jun  9 10:22:36.356: INFO: (0) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 31.505862ms)
    Jun  9 10:22:36.356: INFO: (0) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 32.035989ms)
    Jun  9 10:22:36.356: INFO: (0) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 31.597164ms)
    Jun  9 10:22:36.356: INFO: (0) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 31.911889ms)
    Jun  9 10:22:36.356: INFO: (0) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 32.848114ms)
    Jun  9 10:22:36.358: INFO: (0) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 33.550788ms)
    Jun  9 10:22:36.358: INFO: (0) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 34.41401ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 12.48117ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 12.789319ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 12.748856ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 12.942831ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 12.93513ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 12.765407ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 12.733033ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 12.948269ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 13.08806ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 12.89739ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 12.967185ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 12.758723ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 12.806427ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 13.18358ms)
    Jun  9 10:22:36.371: INFO: (1) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 12.878761ms)
    Jun  9 10:22:36.379: INFO: (1) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 21.111381ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 29.996462ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 28.843795ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 29.105758ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 29.902181ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 29.834156ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 29.193875ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 29.051051ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 29.324985ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 29.761114ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 29.655885ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 28.995945ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 28.82904ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 29.541233ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 28.921177ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 29.49947ms)
    Jun  9 10:22:36.409: INFO: (2) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 29.33899ms)
    Jun  9 10:22:36.420: INFO: (3) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 10.493825ms)
    Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 10.578348ms)
    Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 10.482162ms)
    Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 10.972364ms)
    Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 10.752748ms)
    Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 10.596268ms)
    Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 10.688099ms)
    Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 10.677755ms)
    Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 11.245749ms)
    Jun  9 10:22:36.421: INFO: (3) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 10.631831ms)
    Jun  9 10:22:36.424: INFO: (3) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 14.37761ms)
    Jun  9 10:22:36.424: INFO: (3) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 14.288554ms)
    Jun  9 10:22:36.424: INFO: (3) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 14.14725ms)
    Jun  9 10:22:36.424: INFO: (3) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 14.673368ms)
    Jun  9 10:22:36.424: INFO: (3) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 14.215542ms)
    Jun  9 10:22:36.424: INFO: (3) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 14.257512ms)
    Jun  9 10:22:36.436: INFO: (4) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 11.11129ms)
    Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 12.271558ms)
    Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 12.774132ms)
    Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 12.872855ms)
    Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 13.969977ms)
    Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 13.763496ms)
    Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 13.2516ms)
    Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 13.96388ms)
    Jun  9 10:22:36.438: INFO: (4) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 14.246035ms)
    Jun  9 10:22:36.439: INFO: (4) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 13.423984ms)
    Jun  9 10:22:36.439: INFO: (4) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 13.823672ms)
    Jun  9 10:22:36.440: INFO: (4) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 14.65081ms)
    Jun  9 10:22:36.440: INFO: (4) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 14.829847ms)
    Jun  9 10:22:36.440: INFO: (4) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 14.785174ms)
    Jun  9 10:22:36.440: INFO: (4) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 15.27257ms)
    Jun  9 10:22:36.440: INFO: (4) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 15.345379ms)
    Jun  9 10:22:36.449: INFO: (5) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 8.853742ms)
    Jun  9 10:22:36.449: INFO: (5) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 8.605616ms)
    Jun  9 10:22:36.449: INFO: (5) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 8.706236ms)
    Jun  9 10:22:36.449: INFO: (5) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 9.147541ms)
    Jun  9 10:22:36.449: INFO: (5) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 9.029231ms)
    Jun  9 10:22:36.449: INFO: (5) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 8.961544ms)
    Jun  9 10:22:36.449: INFO: (5) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 8.83762ms)
    Jun  9 10:22:36.450: INFO: (5) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.893583ms)
    Jun  9 10:22:36.450: INFO: (5) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 10.09545ms)
    Jun  9 10:22:36.451: INFO: (5) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 10.236928ms)
    Jun  9 10:22:36.451: INFO: (5) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 10.314831ms)
    Jun  9 10:22:36.451: INFO: (5) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 10.726551ms)
    Jun  9 10:22:36.452: INFO: (5) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 11.542452ms)
    Jun  9 10:22:36.452: INFO: (5) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 11.740334ms)
    Jun  9 10:22:36.453: INFO: (5) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 12.079337ms)
    Jun  9 10:22:36.453: INFO: (5) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 12.398002ms)
    Jun  9 10:22:36.458: INFO: (6) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 5.740875ms)
    Jun  9 10:22:36.462: INFO: (6) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.110594ms)
    Jun  9 10:22:36.462: INFO: (6) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 8.856515ms)
    Jun  9 10:22:36.462: INFO: (6) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 8.988093ms)
    Jun  9 10:22:36.463: INFO: (6) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.376861ms)
    Jun  9 10:22:36.463: INFO: (6) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 9.493139ms)
    Jun  9 10:22:36.463: INFO: (6) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 8.994008ms)
    Jun  9 10:22:36.463: INFO: (6) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.801735ms)
    Jun  9 10:22:36.463: INFO: (6) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 9.816644ms)
    Jun  9 10:22:36.463: INFO: (6) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 9.903482ms)
    Jun  9 10:22:36.463: INFO: (6) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.480808ms)
    Jun  9 10:22:36.465: INFO: (6) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 11.633479ms)
    Jun  9 10:22:36.467: INFO: (6) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 13.843804ms)
    Jun  9 10:22:36.467: INFO: (6) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 13.743384ms)
    Jun  9 10:22:36.467: INFO: (6) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 13.812035ms)
    Jun  9 10:22:36.467: INFO: (6) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 14.045013ms)
    Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.266129ms)
    Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.09185ms)
    Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.713072ms)
    Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.939383ms)
    Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 9.817355ms)
    Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 9.23526ms)
    Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 9.318107ms)
    Jun  9 10:22:36.477: INFO: (7) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 10.063624ms)
    Jun  9 10:22:36.478: INFO: (7) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 9.495107ms)
    Jun  9 10:22:36.478: INFO: (7) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 10.030232ms)
    Jun  9 10:22:36.478: INFO: (7) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 10.521108ms)
    Jun  9 10:22:36.484: INFO: (7) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 16.063625ms)
    Jun  9 10:22:36.484: INFO: (7) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 15.970699ms)
    Jun  9 10:22:36.484: INFO: (7) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 15.627528ms)
    Jun  9 10:22:36.484: INFO: (7) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 15.565816ms)
    Jun  9 10:22:36.484: INFO: (7) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 15.758564ms)
    Jun  9 10:22:36.492: INFO: (8) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 7.765657ms)
    Jun  9 10:22:36.492: INFO: (8) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 7.702603ms)
    Jun  9 10:22:36.492: INFO: (8) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 7.775697ms)
    Jun  9 10:22:36.492: INFO: (8) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 7.812638ms)
    Jun  9 10:22:36.493: INFO: (8) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 8.668591ms)
    Jun  9 10:22:36.493: INFO: (8) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 8.268602ms)
    Jun  9 10:22:36.493: INFO: (8) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 8.190918ms)
    Jun  9 10:22:36.493: INFO: (8) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 8.450427ms)
    Jun  9 10:22:36.493: INFO: (8) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 8.753844ms)
    Jun  9 10:22:36.493: INFO: (8) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 8.310654ms)
    Jun  9 10:22:36.493: INFO: (8) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 8.368984ms)
    Jun  9 10:22:36.494: INFO: (8) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 9.469233ms)
    Jun  9 10:22:36.496: INFO: (8) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 11.547572ms)
    Jun  9 10:22:36.496: INFO: (8) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 11.523401ms)
    Jun  9 10:22:36.496: INFO: (8) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 11.613858ms)
    Jun  9 10:22:36.496: INFO: (8) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 11.771354ms)
    Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.353379ms)
    Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 9.838594ms)
    Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 9.791555ms)
    Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 9.98088ms)
    Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 10.510143ms)
    Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 9.948668ms)
    Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 10.012882ms)
    Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 10.122275ms)
    Jun  9 10:22:36.506: INFO: (9) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 10.20016ms)
    Jun  9 10:22:36.507: INFO: (9) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 10.009128ms)
    Jun  9 10:22:36.511: INFO: (9) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 15.110138ms)
    Jun  9 10:22:36.511: INFO: (9) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 14.629817ms)
    Jun  9 10:22:36.511: INFO: (9) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 14.619276ms)
    Jun  9 10:22:36.511: INFO: (9) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 15.130959ms)
    Jun  9 10:22:36.511: INFO: (9) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 15.132546ms)
    Jun  9 10:22:36.511: INFO: (9) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 15.077327ms)
    Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 8.123555ms)
    Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 8.192986ms)
    Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.309948ms)
    Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 8.35364ms)
    Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 8.614167ms)
    Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 8.963999ms)
    Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 8.435304ms)
    Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.491747ms)
    Jun  9 10:22:36.520: INFO: (10) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 8.369091ms)
    Jun  9 10:22:36.528: INFO: (10) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 15.776191ms)
    Jun  9 10:22:36.528: INFO: (10) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 16.318187ms)
    Jun  9 10:22:36.528: INFO: (10) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 16.283215ms)
    Jun  9 10:22:36.528: INFO: (10) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 15.785955ms)
    Jun  9 10:22:36.528: INFO: (10) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 16.089294ms)
    Jun  9 10:22:36.528: INFO: (10) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 16.629868ms)
    Jun  9 10:22:36.529: INFO: (10) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 16.89564ms)
    Jun  9 10:22:36.537: INFO: (11) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.292794ms)
    Jun  9 10:22:36.537: INFO: (11) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 8.129393ms)
    Jun  9 10:22:36.537: INFO: (11) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 8.385534ms)
    Jun  9 10:22:36.537: INFO: (11) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 8.454507ms)
    Jun  9 10:22:36.537: INFO: (11) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 8.655581ms)
    Jun  9 10:22:36.537: INFO: (11) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.151767ms)
    Jun  9 10:22:36.537: INFO: (11) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 8.249795ms)
    Jun  9 10:22:36.538: INFO: (11) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 8.665235ms)
    Jun  9 10:22:36.538: INFO: (11) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 8.961524ms)
    Jun  9 10:22:36.538: INFO: (11) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 8.52908ms)
    Jun  9 10:22:36.538: INFO: (11) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 8.707165ms)
    Jun  9 10:22:36.538: INFO: (11) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 9.431875ms)
    Jun  9 10:22:36.539: INFO: (11) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 10.298972ms)
    Jun  9 10:22:36.539: INFO: (11) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 10.306238ms)
    Jun  9 10:22:36.539: INFO: (11) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 10.418516ms)
    Jun  9 10:22:36.539: INFO: (11) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 10.494728ms)
    Jun  9 10:22:36.548: INFO: (12) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.416725ms)
    Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 9.158902ms)
    Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.260839ms)
    Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 9.34206ms)
    Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.448854ms)
    Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 9.541664ms)
    Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 9.399498ms)
    Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.678923ms)
    Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 9.204008ms)
    Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 9.3584ms)
    Jun  9 10:22:36.549: INFO: (12) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 9.207347ms)
    Jun  9 10:22:36.551: INFO: (12) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 10.518217ms)
    Jun  9 10:22:36.551: INFO: (12) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 10.651854ms)
    Jun  9 10:22:36.551: INFO: (12) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 11.357348ms)
    Jun  9 10:22:36.551: INFO: (12) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 11.456417ms)
    Jun  9 10:22:36.551: INFO: (12) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 11.51725ms)
    Jun  9 10:22:36.558: INFO: (13) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 6.72093ms)
    Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 8.29995ms)
    Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 7.936679ms)
    Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 7.905387ms)
    Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 8.359548ms)
    Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 8.036333ms)
    Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 7.910407ms)
    Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.148377ms)
    Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 7.887179ms)
    Jun  9 10:22:36.560: INFO: (13) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 8.375763ms)
    Jun  9 10:22:36.562: INFO: (13) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 10.193352ms)
    Jun  9 10:22:36.563: INFO: (13) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 11.497826ms)
    Jun  9 10:22:36.563: INFO: (13) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 11.162152ms)
    Jun  9 10:22:36.563: INFO: (13) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 11.577213ms)
    Jun  9 10:22:36.563: INFO: (13) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 11.139887ms)
    Jun  9 10:22:36.563: INFO: (13) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 11.102274ms)
    Jun  9 10:22:36.569: INFO: (14) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 5.163646ms)
    Jun  9 10:22:36.570: INFO: (14) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 6.743468ms)
    Jun  9 10:22:36.571: INFO: (14) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 6.110183ms)
    Jun  9 10:22:36.571: INFO: (14) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 6.744454ms)
    Jun  9 10:22:36.571: INFO: (14) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 7.072239ms)
    Jun  9 10:22:36.571: INFO: (14) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 6.726449ms)
    Jun  9 10:22:36.572: INFO: (14) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 7.507123ms)
    Jun  9 10:22:36.572: INFO: (14) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 7.705317ms)
    Jun  9 10:22:36.572: INFO: (14) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 7.750828ms)
    Jun  9 10:22:36.572: INFO: (14) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 7.839603ms)
    Jun  9 10:22:36.572: INFO: (14) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 8.283584ms)
    Jun  9 10:22:36.572: INFO: (14) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 8.240111ms)
    Jun  9 10:22:36.573: INFO: (14) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 8.650619ms)
    Jun  9 10:22:36.577: INFO: (14) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 12.199605ms)
    Jun  9 10:22:36.577: INFO: (14) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 12.206488ms)
    Jun  9 10:22:36.577: INFO: (14) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 12.18903ms)
    Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.794296ms)
    Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 9.927446ms)
    Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 8.718903ms)
    Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.495395ms)
    Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 9.439777ms)
    Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.65309ms)
    Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 9.369994ms)
    Jun  9 10:22:36.587: INFO: (15) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 10.101114ms)
    Jun  9 10:22:36.588: INFO: (15) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 9.966643ms)
    Jun  9 10:22:36.588: INFO: (15) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 10.661953ms)
    Jun  9 10:22:36.588: INFO: (15) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 9.122093ms)
    Jun  9 10:22:36.588: INFO: (15) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 10.169083ms)
    Jun  9 10:22:36.588: INFO: (15) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 10.464629ms)
    Jun  9 10:22:36.590: INFO: (15) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 12.563952ms)
    Jun  9 10:22:36.590: INFO: (15) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 12.110303ms)
    Jun  9 10:22:36.591: INFO: (15) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 12.46972ms)
    Jun  9 10:22:36.603: INFO: (16) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 11.433081ms)
    Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 18.272352ms)
    Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 18.301033ms)
    Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 18.187957ms)
    Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 18.166188ms)
    Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 18.232502ms)
    Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 18.428031ms)
    Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 18.635222ms)
    Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 18.224907ms)
    Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 18.401064ms)
    Jun  9 10:22:36.609: INFO: (16) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 18.660743ms)
    Jun  9 10:22:36.610: INFO: (16) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 18.758968ms)
    Jun  9 10:22:36.612: INFO: (16) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 21.302606ms)
    Jun  9 10:22:36.613: INFO: (16) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 22.053748ms)
    Jun  9 10:22:36.613: INFO: (16) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 22.10985ms)
    Jun  9 10:22:36.614: INFO: (16) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 22.650769ms)
    Jun  9 10:22:36.623: INFO: (17) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.60458ms)
    Jun  9 10:22:36.625: INFO: (17) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 11.014678ms)
    Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 11.482415ms)
    Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 11.426097ms)
    Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 11.803292ms)
    Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 11.48354ms)
    Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 11.399819ms)
    Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 11.562279ms)
    Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 11.497641ms)
    Jun  9 10:22:36.626: INFO: (17) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 11.548648ms)
    Jun  9 10:22:36.630: INFO: (17) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 16.059384ms)
    Jun  9 10:22:36.631: INFO: (17) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 17.171057ms)
    Jun  9 10:22:36.631: INFO: (17) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 17.176796ms)
    Jun  9 10:22:36.631: INFO: (17) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 17.205798ms)
    Jun  9 10:22:36.631: INFO: (17) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 17.400605ms)
    Jun  9 10:22:36.632: INFO: (17) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 17.450543ms)
    Jun  9 10:22:36.640: INFO: (18) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 7.427381ms)
    Jun  9 10:22:36.643: INFO: (18) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 11.069265ms)
    Jun  9 10:22:36.643: INFO: (18) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 11.431917ms)
    Jun  9 10:22:36.643: INFO: (18) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 11.059282ms)
    Jun  9 10:22:36.643: INFO: (18) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 11.264478ms)
    Jun  9 10:22:36.644: INFO: (18) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 11.971458ms)
    Jun  9 10:22:36.644: INFO: (18) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 12.095558ms)
    Jun  9 10:22:36.644: INFO: (18) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 11.945909ms)
    Jun  9 10:22:36.644: INFO: (18) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 11.993475ms)
    Jun  9 10:22:36.644: INFO: (18) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 11.855262ms)
    Jun  9 10:22:36.644: INFO: (18) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 12.201086ms)
    Jun  9 10:22:36.645: INFO: (18) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 13.594929ms)
    Jun  9 10:22:36.646: INFO: (18) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 14.115844ms)
    Jun  9 10:22:36.646: INFO: (18) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 14.400752ms)
    Jun  9 10:22:36.647: INFO: (18) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 14.441568ms)
    Jun  9 10:22:36.647: INFO: (18) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 14.214868ms)
    Jun  9 10:22:36.654: INFO: (19) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:443/proxy/tlsrewritem... (200; 7.575521ms)
    Jun  9 10:22:36.654: INFO: (19) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 7.795005ms)
    Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">test<... (200; 9.154337ms)
    Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:1080/proxy/rewriteme">... (200; 9.475015ms)
    Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/: <a href="/api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5/proxy/rewriteme">test</a> (200; 9.234251ms)
    Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:160/proxy/: foo (200; 9.222433ms)
    Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:462/proxy/: tls qux (200; 9.218442ms)
    Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/https:proxy-service-vzntb-xl5c5:460/proxy/: tls baz (200; 9.464744ms)
    Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.136086ms)
    Jun  9 10:22:36.656: INFO: (19) /api/v1/namespaces/proxy-3431/pods/http:proxy-service-vzntb-xl5c5:162/proxy/: bar (200; 9.516931ms)
    Jun  9 10:22:36.658: INFO: (19) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname1/proxy/: tls baz (200; 11.085617ms)
    Jun  9 10:22:36.659: INFO: (19) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname2/proxy/: bar (200; 11.837471ms)
    Jun  9 10:22:36.659: INFO: (19) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname2/proxy/: bar (200; 11.892896ms)
    Jun  9 10:22:36.659: INFO: (19) /api/v1/namespaces/proxy-3431/services/proxy-service-vzntb:portname1/proxy/: foo (200; 12.12729ms)
    Jun  9 10:22:36.659: INFO: (19) /api/v1/namespaces/proxy-3431/services/http:proxy-service-vzntb:portname1/proxy/: foo (200; 12.046649ms)
    Jun  9 10:22:36.659: INFO: (19) /api/v1/namespaces/proxy-3431/services/https:proxy-service-vzntb:tlsportname2/proxy/: tls qux (200; 12.395659ms)
    STEP: deleting ReplicationController proxy-service-vzntb in namespace proxy-3431, will wait for the garbage collector to delete the pods 06/09/23 10:22:36.66
    Jun  9 10:22:36.729: INFO: Deleting ReplicationController proxy-service-vzntb took: 13.111583ms
    Jun  9 10:22:36.830: INFO: Terminating ReplicationController proxy-service-vzntb pods took: 101.02375ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:22:39.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3431" for this suite. 06/09/23 10:22:39.34
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:22:39.351
Jun  9 10:22:39.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 10:22:39.352
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:39.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:39.382
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 10:22:39.413
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:22:40.212
STEP: Deploying the webhook pod 06/09/23 10:22:40.223
STEP: Wait for the deployment to be ready 06/09/23 10:22:40.241
Jun  9 10:22:40.251: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 10:22:42.267
STEP: Verifying the service has paired with the endpoint 06/09/23 10:22:42.297
Jun  9 10:22:43.298: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 06/09/23 10:22:43.41
STEP: Creating a configMap that should be mutated 06/09/23 10:22:43.429
STEP: Deleting the collection of validation webhooks 06/09/23 10:22:43.464
STEP: Creating a configMap that should not be mutated 06/09/23 10:22:43.549
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:22:43.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-821" for this suite. 06/09/23 10:22:43.643
STEP: Destroying namespace "webhook-821-markers" for this suite. 06/09/23 10:22:43.661
------------------------------
• [4.328 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:22:39.351
    Jun  9 10:22:39.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 10:22:39.352
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:39.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:39.382
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 10:22:39.413
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:22:40.212
    STEP: Deploying the webhook pod 06/09/23 10:22:40.223
    STEP: Wait for the deployment to be ready 06/09/23 10:22:40.241
    Jun  9 10:22:40.251: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 10:22:42.267
    STEP: Verifying the service has paired with the endpoint 06/09/23 10:22:42.297
    Jun  9 10:22:43.298: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 06/09/23 10:22:43.41
    STEP: Creating a configMap that should be mutated 06/09/23 10:22:43.429
    STEP: Deleting the collection of validation webhooks 06/09/23 10:22:43.464
    STEP: Creating a configMap that should not be mutated 06/09/23 10:22:43.549
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:22:43.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-821" for this suite. 06/09/23 10:22:43.643
    STEP: Destroying namespace "webhook-821-markers" for this suite. 06/09/23 10:22:43.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:22:43.68
Jun  9 10:22:43.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 10:22:43.681
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:43.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:43.711
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 06/09/23 10:22:43.716
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 10:22:43.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-747" for this suite. 06/09/23 10:22:43.73
------------------------------
• [0.067 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:22:43.68
    Jun  9 10:22:43.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 10:22:43.681
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:43.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:43.711
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 06/09/23 10:22:43.716
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:22:43.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-747" for this suite. 06/09/23 10:22:43.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:22:43.75
Jun  9 10:22:43.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename resourcequota 06/09/23 10:22:43.751
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:43.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:43.779
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 06/09/23 10:22:43.785
STEP: Creating a ResourceQuota 06/09/23 10:22:48.79
STEP: Ensuring resource quota status is calculated 06/09/23 10:22:48.802
STEP: Creating a ReplicaSet 06/09/23 10:22:50.809
STEP: Ensuring resource quota status captures replicaset creation 06/09/23 10:22:50.825
STEP: Deleting a ReplicaSet 06/09/23 10:22:52.832
STEP: Ensuring resource quota status released usage 06/09/23 10:22:52.845
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  9 10:22:54.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5274" for this suite. 06/09/23 10:22:54.861
------------------------------
• [SLOW TEST] [11.131 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:22:43.75
    Jun  9 10:22:43.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename resourcequota 06/09/23 10:22:43.751
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:43.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:43.779
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 06/09/23 10:22:43.785
    STEP: Creating a ResourceQuota 06/09/23 10:22:48.79
    STEP: Ensuring resource quota status is calculated 06/09/23 10:22:48.802
    STEP: Creating a ReplicaSet 06/09/23 10:22:50.809
    STEP: Ensuring resource quota status captures replicaset creation 06/09/23 10:22:50.825
    STEP: Deleting a ReplicaSet 06/09/23 10:22:52.832
    STEP: Ensuring resource quota status released usage 06/09/23 10:22:52.845
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:22:54.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5274" for this suite. 06/09/23 10:22:54.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:22:54.884
Jun  9 10:22:54.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename resourcequota 06/09/23 10:22:54.885
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:54.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:54.928
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 06/09/23 10:22:54.933
STEP: Getting a ResourceQuota 06/09/23 10:22:54.949
STEP: Updating a ResourceQuota 06/09/23 10:22:54.964
STEP: Verifying a ResourceQuota was modified 06/09/23 10:22:54.973
STEP: Deleting a ResourceQuota 06/09/23 10:22:54.983
STEP: Verifying the deleted ResourceQuota 06/09/23 10:22:54.996
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  9 10:22:55.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5449" for this suite. 06/09/23 10:22:55.01
------------------------------
• [0.137 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:22:54.884
    Jun  9 10:22:54.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename resourcequota 06/09/23 10:22:54.885
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:54.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:54.928
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 06/09/23 10:22:54.933
    STEP: Getting a ResourceQuota 06/09/23 10:22:54.949
    STEP: Updating a ResourceQuota 06/09/23 10:22:54.964
    STEP: Verifying a ResourceQuota was modified 06/09/23 10:22:54.973
    STEP: Deleting a ResourceQuota 06/09/23 10:22:54.983
    STEP: Verifying the deleted ResourceQuota 06/09/23 10:22:54.996
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:22:55.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5449" for this suite. 06/09/23 10:22:55.01
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:22:55.021
Jun  9 10:22:55.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 10:22:55.023
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:55.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:55.055
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 06/09/23 10:22:55.064
Jun  9 10:22:55.080: INFO: Waiting up to 5m0s for pod "pod-198b5bbb-f100-475b-af91-64bd130ed9ba" in namespace "emptydir-6888" to be "Succeeded or Failed"
Jun  9 10:22:55.089: INFO: Pod "pod-198b5bbb-f100-475b-af91-64bd130ed9ba": Phase="Pending", Reason="", readiness=false. Elapsed: 9.836023ms
Jun  9 10:22:57.097: INFO: Pod "pod-198b5bbb-f100-475b-af91-64bd130ed9ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017173967s
Jun  9 10:22:59.097: INFO: Pod "pod-198b5bbb-f100-475b-af91-64bd130ed9ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017610262s
STEP: Saw pod success 06/09/23 10:22:59.097
Jun  9 10:22:59.097: INFO: Pod "pod-198b5bbb-f100-475b-af91-64bd130ed9ba" satisfied condition "Succeeded or Failed"
Jun  9 10:22:59.103: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-198b5bbb-f100-475b-af91-64bd130ed9ba container test-container: <nil>
STEP: delete the pod 06/09/23 10:22:59.113
Jun  9 10:22:59.131: INFO: Waiting for pod pod-198b5bbb-f100-475b-af91-64bd130ed9ba to disappear
Jun  9 10:22:59.138: INFO: Pod pod-198b5bbb-f100-475b-af91-64bd130ed9ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 10:22:59.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6888" for this suite. 06/09/23 10:22:59.147
------------------------------
• [4.138 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:22:55.021
    Jun  9 10:22:55.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 10:22:55.023
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:55.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:55.055
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 06/09/23 10:22:55.064
    Jun  9 10:22:55.080: INFO: Waiting up to 5m0s for pod "pod-198b5bbb-f100-475b-af91-64bd130ed9ba" in namespace "emptydir-6888" to be "Succeeded or Failed"
    Jun  9 10:22:55.089: INFO: Pod "pod-198b5bbb-f100-475b-af91-64bd130ed9ba": Phase="Pending", Reason="", readiness=false. Elapsed: 9.836023ms
    Jun  9 10:22:57.097: INFO: Pod "pod-198b5bbb-f100-475b-af91-64bd130ed9ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017173967s
    Jun  9 10:22:59.097: INFO: Pod "pod-198b5bbb-f100-475b-af91-64bd130ed9ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017610262s
    STEP: Saw pod success 06/09/23 10:22:59.097
    Jun  9 10:22:59.097: INFO: Pod "pod-198b5bbb-f100-475b-af91-64bd130ed9ba" satisfied condition "Succeeded or Failed"
    Jun  9 10:22:59.103: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-198b5bbb-f100-475b-af91-64bd130ed9ba container test-container: <nil>
    STEP: delete the pod 06/09/23 10:22:59.113
    Jun  9 10:22:59.131: INFO: Waiting for pod pod-198b5bbb-f100-475b-af91-64bd130ed9ba to disappear
    Jun  9 10:22:59.138: INFO: Pod pod-198b5bbb-f100-475b-af91-64bd130ed9ba no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:22:59.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6888" for this suite. 06/09/23 10:22:59.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:22:59.16
Jun  9 10:22:59.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-lifecycle-hook 06/09/23 10:22:59.162
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:59.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:59.188
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 06/09/23 10:22:59.202
Jun  9 10:22:59.217: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-952" to be "running and ready"
Jun  9 10:22:59.225: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030682ms
Jun  9 10:22:59.225: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:23:01.232: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015162295s
Jun  9 10:23:01.232: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun  9 10:23:01.232: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 06/09/23 10:23:01.238
Jun  9 10:23:01.271: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-952" to be "running and ready"
Jun  9 10:23:01.282: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.697384ms
Jun  9 10:23:01.282: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:23:03.290: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018645432s
Jun  9 10:23:03.290: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jun  9 10:23:03.290: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 06/09/23 10:23:03.296
STEP: delete the pod with lifecycle hook 06/09/23 10:23:03.308
Jun  9 10:23:03.322: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  9 10:23:03.329: INFO: Pod pod-with-poststart-http-hook still exists
Jun  9 10:23:05.329: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  9 10:23:05.360: INFO: Pod pod-with-poststart-http-hook still exists
Jun  9 10:23:07.329: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  9 10:23:07.336: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jun  9 10:23:07.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-952" for this suite. 06/09/23 10:23:07.345
------------------------------
• [SLOW TEST] [8.197 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:22:59.16
    Jun  9 10:22:59.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/09/23 10:22:59.162
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:22:59.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:22:59.188
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 06/09/23 10:22:59.202
    Jun  9 10:22:59.217: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-952" to be "running and ready"
    Jun  9 10:22:59.225: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030682ms
    Jun  9 10:22:59.225: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:23:01.232: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015162295s
    Jun  9 10:23:01.232: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun  9 10:23:01.232: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 06/09/23 10:23:01.238
    Jun  9 10:23:01.271: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-952" to be "running and ready"
    Jun  9 10:23:01.282: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.697384ms
    Jun  9 10:23:01.282: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:23:03.290: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018645432s
    Jun  9 10:23:03.290: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jun  9 10:23:03.290: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 06/09/23 10:23:03.296
    STEP: delete the pod with lifecycle hook 06/09/23 10:23:03.308
    Jun  9 10:23:03.322: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun  9 10:23:03.329: INFO: Pod pod-with-poststart-http-hook still exists
    Jun  9 10:23:05.329: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun  9 10:23:05.360: INFO: Pod pod-with-poststart-http-hook still exists
    Jun  9 10:23:07.329: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun  9 10:23:07.336: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:23:07.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-952" for this suite. 06/09/23 10:23:07.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:23:07.359
Jun  9 10:23:07.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename daemonsets 06/09/23 10:23:07.361
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:23:07.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:23:07.392
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Jun  9 10:23:07.428: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 06/09/23 10:23:07.438
Jun  9 10:23:07.449: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:07.449: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:07.449: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:07.456: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:23:07.456: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:23:08.468: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:08.468: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:08.468: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:08.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:23:08.476: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:23:09.464: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:09.464: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:09.465: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:09.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  9 10:23:09.475: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 06/09/23 10:23:09.525
STEP: Check that daemon pods images are updated. 06/09/23 10:23:09.542
Jun  9 10:23:09.547: INFO: Wrong image for pod: daemon-set-2ckgl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:09.547: INFO: Wrong image for pod: daemon-set-5xfdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:09.547: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:09.553: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:09.553: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:09.553: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:10.561: INFO: Wrong image for pod: daemon-set-2ckgl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:10.561: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:10.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:10.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:10.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:11.563: INFO: Wrong image for pod: daemon-set-2ckgl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:11.563: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:11.582: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:11.582: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:11.582: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:12.576: INFO: Wrong image for pod: daemon-set-2ckgl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:12.577: INFO: Pod daemon-set-9xwgz is not available
Jun  9 10:23:12.577: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:12.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:12.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:12.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:13.560: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:13.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:13.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:13.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:14.564: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:14.564: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:14.571: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:14.571: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:14.571: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:15.560: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:15.560: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:15.567: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:15.567: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:15.567: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:16.559: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:16.559: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:16.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:16.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:16.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:17.562: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:17.562: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:17.584: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:17.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:17.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:18.559: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:18.559: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:18.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:18.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:18.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:19.561: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:19.561: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:19.570: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:19.570: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:19.570: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:20.559: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:20.559: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:20.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:20.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:20.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:21.560: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:21.560: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:21.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:21.567: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:21.567: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:22.559: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:22.559: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:22.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:22.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:22.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:23.560: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:23.561: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:23.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:23.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:23.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:24.560: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:24.560: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:24.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:24.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:24.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:25.561: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:25.561: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:25.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:25.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:25.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:26.576: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:26.576: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:26.584: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:26.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:26.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:27.561: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:27.561: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:27.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:27.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:27.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:28.560: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:28.561: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:28.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:28.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:28.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:29.559: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:29.559: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:29.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:29.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:29.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:30.560: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:30.560: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:30.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:30.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:30.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:31.609: INFO: Pod daemon-set-nghfs is not available
Jun  9 10:23:31.609: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun  9 10:23:31.625: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:31.625: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:31.625: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:32.573: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:32.573: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:32.573: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:33.559: INFO: Pod daemon-set-7l5t2 is not available
Jun  9 10:23:33.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:33.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:33.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 06/09/23 10:23:33.565
Jun  9 10:23:33.573: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:33.573: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:33.573: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:33.580: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  9 10:23:33.580: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
Jun  9 10:23:34.587: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:34.587: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:34.587: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:23:34.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  9 10:23:34.593: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/09/23 10:23:34.628
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9073, will wait for the garbage collector to delete the pods 06/09/23 10:23:34.628
Jun  9 10:23:34.695: INFO: Deleting DaemonSet.extensions daemon-set took: 10.153813ms
Jun  9 10:23:34.796: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.116418ms
Jun  9 10:23:37.602: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:23:37.602: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  9 10:23:37.607: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"69160"},"items":null}

Jun  9 10:23:37.613: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"69160"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:23:37.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9073" for this suite. 06/09/23 10:23:37.646
------------------------------
• [SLOW TEST] [30.299 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:23:07.359
    Jun  9 10:23:07.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename daemonsets 06/09/23 10:23:07.361
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:23:07.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:23:07.392
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Jun  9 10:23:07.428: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 06/09/23 10:23:07.438
    Jun  9 10:23:07.449: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:07.449: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:07.449: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:07.456: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:23:07.456: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:23:08.468: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:08.468: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:08.468: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:08.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:23:08.476: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:23:09.464: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:09.464: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:09.465: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:09.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun  9 10:23:09.475: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 06/09/23 10:23:09.525
    STEP: Check that daemon pods images are updated. 06/09/23 10:23:09.542
    Jun  9 10:23:09.547: INFO: Wrong image for pod: daemon-set-2ckgl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:09.547: INFO: Wrong image for pod: daemon-set-5xfdj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:09.547: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:09.553: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:09.553: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:09.553: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:10.561: INFO: Wrong image for pod: daemon-set-2ckgl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:10.561: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:10.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:10.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:10.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:11.563: INFO: Wrong image for pod: daemon-set-2ckgl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:11.563: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:11.582: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:11.582: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:11.582: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:12.576: INFO: Wrong image for pod: daemon-set-2ckgl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:12.577: INFO: Pod daemon-set-9xwgz is not available
    Jun  9 10:23:12.577: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:12.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:12.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:12.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:13.560: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:13.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:13.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:13.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:14.564: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:14.564: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:14.571: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:14.571: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:14.571: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:15.560: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:15.560: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:15.567: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:15.567: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:15.567: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:16.559: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:16.559: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:16.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:16.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:16.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:17.562: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:17.562: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:17.584: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:17.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:17.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:18.559: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:18.559: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:18.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:18.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:18.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:19.561: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:19.561: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:19.570: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:19.570: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:19.570: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:20.559: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:20.559: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:20.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:20.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:20.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:21.560: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:21.560: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:21.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:21.567: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:21.567: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:22.559: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:22.559: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:22.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:22.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:22.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:23.560: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:23.561: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:23.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:23.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:23.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:24.560: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:24.560: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:24.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:24.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:24.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:25.561: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:25.561: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:25.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:25.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:25.569: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:26.576: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:26.576: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:26.584: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:26.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:26.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:27.561: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:27.561: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:27.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:27.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:27.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:28.560: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:28.561: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:28.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:28.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:28.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:29.559: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:29.559: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:29.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:29.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:29.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:30.560: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:30.560: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:30.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:30.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:30.568: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:31.609: INFO: Pod daemon-set-nghfs is not available
    Jun  9 10:23:31.609: INFO: Wrong image for pod: daemon-set-t5p2m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun  9 10:23:31.625: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:31.625: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:31.625: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:32.573: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:32.573: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:32.573: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:33.559: INFO: Pod daemon-set-7l5t2 is not available
    Jun  9 10:23:33.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:33.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:33.565: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 06/09/23 10:23:33.565
    Jun  9 10:23:33.573: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:33.573: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:33.573: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:33.580: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun  9 10:23:33.580: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
    Jun  9 10:23:34.587: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:34.587: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:34.587: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:23:34.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun  9 10:23:34.593: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/09/23 10:23:34.628
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9073, will wait for the garbage collector to delete the pods 06/09/23 10:23:34.628
    Jun  9 10:23:34.695: INFO: Deleting DaemonSet.extensions daemon-set took: 10.153813ms
    Jun  9 10:23:34.796: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.116418ms
    Jun  9 10:23:37.602: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:23:37.602: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun  9 10:23:37.607: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"69160"},"items":null}

    Jun  9 10:23:37.613: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"69160"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:23:37.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9073" for this suite. 06/09/23 10:23:37.646
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:23:37.658
Jun  9 10:23:37.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 10:23:37.661
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:23:37.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:23:37.746
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 06/09/23 10:23:37.753
Jun  9 10:23:37.769: INFO: Waiting up to 5m0s for pod "pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b" in namespace "emptydir-6238" to be "Succeeded or Failed"
Jun  9 10:23:37.776: INFO: Pod "pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.025529ms
Jun  9 10:23:39.783: INFO: Pod "pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01406318s
Jun  9 10:23:41.784: INFO: Pod "pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015433262s
STEP: Saw pod success 06/09/23 10:23:41.784
Jun  9 10:23:41.785: INFO: Pod "pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b" satisfied condition "Succeeded or Failed"
Jun  9 10:23:41.791: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b container test-container: <nil>
STEP: delete the pod 06/09/23 10:23:41.801
Jun  9 10:23:41.817: INFO: Waiting for pod pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b to disappear
Jun  9 10:23:41.822: INFO: Pod pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 10:23:41.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6238" for this suite. 06/09/23 10:23:41.83
------------------------------
• [4.183 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:23:37.658
    Jun  9 10:23:37.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 10:23:37.661
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:23:37.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:23:37.746
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 06/09/23 10:23:37.753
    Jun  9 10:23:37.769: INFO: Waiting up to 5m0s for pod "pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b" in namespace "emptydir-6238" to be "Succeeded or Failed"
    Jun  9 10:23:37.776: INFO: Pod "pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.025529ms
    Jun  9 10:23:39.783: INFO: Pod "pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01406318s
    Jun  9 10:23:41.784: INFO: Pod "pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015433262s
    STEP: Saw pod success 06/09/23 10:23:41.784
    Jun  9 10:23:41.785: INFO: Pod "pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b" satisfied condition "Succeeded or Failed"
    Jun  9 10:23:41.791: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b container test-container: <nil>
    STEP: delete the pod 06/09/23 10:23:41.801
    Jun  9 10:23:41.817: INFO: Waiting for pod pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b to disappear
    Jun  9 10:23:41.822: INFO: Pod pod-5345c5a4-1cfe-4bbb-ae47-c308b65d2a9b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:23:41.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6238" for this suite. 06/09/23 10:23:41.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:23:41.842
Jun  9 10:23:41.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename gc 06/09/23 10:23:41.843
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:23:41.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:23:41.868
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 06/09/23 10:23:41.88
STEP: delete the rc 06/09/23 10:23:46.967
STEP: wait for the rc to be deleted 06/09/23 10:23:46.99
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 06/09/23 10:23:51.996
STEP: Gathering metrics 06/09/23 10:24:22.109
Jun  9 10:24:22.163: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" in namespace "kube-system" to be "running and ready"
Jun  9 10:24:22.169: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh": Phase="Running", Reason="", readiness=true. Elapsed: 5.682256ms
Jun  9 10:24:22.169: INFO: The phase of Pod kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh is Running (Ready = true)
Jun  9 10:24:22.169: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" satisfied condition "running and ready"
Jun  9 10:24:22.234: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun  9 10:24:22.234: INFO: Deleting pod "simpletest.rc-27dqp" in namespace "gc-8065"
Jun  9 10:24:22.252: INFO: Deleting pod "simpletest.rc-2b9jj" in namespace "gc-8065"
Jun  9 10:24:22.269: INFO: Deleting pod "simpletest.rc-2p8ph" in namespace "gc-8065"
Jun  9 10:24:22.289: INFO: Deleting pod "simpletest.rc-2rbr2" in namespace "gc-8065"
Jun  9 10:24:22.307: INFO: Deleting pod "simpletest.rc-2wfj8" in namespace "gc-8065"
Jun  9 10:24:22.325: INFO: Deleting pod "simpletest.rc-2wvss" in namespace "gc-8065"
Jun  9 10:24:22.347: INFO: Deleting pod "simpletest.rc-44prw" in namespace "gc-8065"
Jun  9 10:24:22.365: INFO: Deleting pod "simpletest.rc-48dtj" in namespace "gc-8065"
Jun  9 10:24:22.387: INFO: Deleting pod "simpletest.rc-49c5j" in namespace "gc-8065"
Jun  9 10:24:22.412: INFO: Deleting pod "simpletest.rc-4hdgs" in namespace "gc-8065"
Jun  9 10:24:22.439: INFO: Deleting pod "simpletest.rc-4jxpd" in namespace "gc-8065"
Jun  9 10:24:22.458: INFO: Deleting pod "simpletest.rc-4qkqj" in namespace "gc-8065"
Jun  9 10:24:22.479: INFO: Deleting pod "simpletest.rc-58rzx" in namespace "gc-8065"
Jun  9 10:24:22.528: INFO: Deleting pod "simpletest.rc-5p7km" in namespace "gc-8065"
Jun  9 10:24:22.559: INFO: Deleting pod "simpletest.rc-5z9hm" in namespace "gc-8065"
Jun  9 10:24:22.585: INFO: Deleting pod "simpletest.rc-64fd4" in namespace "gc-8065"
Jun  9 10:24:22.605: INFO: Deleting pod "simpletest.rc-6hktz" in namespace "gc-8065"
Jun  9 10:24:22.624: INFO: Deleting pod "simpletest.rc-6l4vf" in namespace "gc-8065"
Jun  9 10:24:22.684: INFO: Deleting pod "simpletest.rc-79z8f" in namespace "gc-8065"
Jun  9 10:24:22.705: INFO: Deleting pod "simpletest.rc-7bwbn" in namespace "gc-8065"
Jun  9 10:24:22.728: INFO: Deleting pod "simpletest.rc-7sgh8" in namespace "gc-8065"
Jun  9 10:24:22.747: INFO: Deleting pod "simpletest.rc-7zgnr" in namespace "gc-8065"
Jun  9 10:24:22.765: INFO: Deleting pod "simpletest.rc-82mgk" in namespace "gc-8065"
Jun  9 10:24:22.827: INFO: Deleting pod "simpletest.rc-8627q" in namespace "gc-8065"
Jun  9 10:24:22.845: INFO: Deleting pod "simpletest.rc-89klh" in namespace "gc-8065"
Jun  9 10:24:22.869: INFO: Deleting pod "simpletest.rc-8sdzk" in namespace "gc-8065"
Jun  9 10:24:22.964: INFO: Deleting pod "simpletest.rc-96njj" in namespace "gc-8065"
Jun  9 10:24:22.994: INFO: Deleting pod "simpletest.rc-9fbdr" in namespace "gc-8065"
Jun  9 10:24:23.011: INFO: Deleting pod "simpletest.rc-b6bzw" in namespace "gc-8065"
Jun  9 10:24:23.028: INFO: Deleting pod "simpletest.rc-bdp5h" in namespace "gc-8065"
Jun  9 10:24:23.047: INFO: Deleting pod "simpletest.rc-bfkjb" in namespace "gc-8065"
Jun  9 10:24:23.167: INFO: Deleting pod "simpletest.rc-bv25s" in namespace "gc-8065"
Jun  9 10:24:23.224: INFO: Deleting pod "simpletest.rc-bx9bh" in namespace "gc-8065"
Jun  9 10:24:23.280: INFO: Deleting pod "simpletest.rc-bxh8x" in namespace "gc-8065"
Jun  9 10:24:23.297: INFO: Deleting pod "simpletest.rc-c7zbl" in namespace "gc-8065"
Jun  9 10:24:23.324: INFO: Deleting pod "simpletest.rc-ckv9f" in namespace "gc-8065"
Jun  9 10:24:23.345: INFO: Deleting pod "simpletest.rc-cnmxc" in namespace "gc-8065"
Jun  9 10:24:23.429: INFO: Deleting pod "simpletest.rc-f7cll" in namespace "gc-8065"
Jun  9 10:24:23.488: INFO: Deleting pod "simpletest.rc-fgf8r" in namespace "gc-8065"
Jun  9 10:24:23.524: INFO: Deleting pod "simpletest.rc-fvwvj" in namespace "gc-8065"
Jun  9 10:24:23.541: INFO: Deleting pod "simpletest.rc-g29gl" in namespace "gc-8065"
Jun  9 10:24:23.562: INFO: Deleting pod "simpletest.rc-gjp88" in namespace "gc-8065"
Jun  9 10:24:23.579: INFO: Deleting pod "simpletest.rc-hhnjk" in namespace "gc-8065"
Jun  9 10:24:23.595: INFO: Deleting pod "simpletest.rc-hrr7q" in namespace "gc-8065"
Jun  9 10:24:23.760: INFO: Deleting pod "simpletest.rc-jcsps" in namespace "gc-8065"
Jun  9 10:24:23.785: INFO: Deleting pod "simpletest.rc-jd7dq" in namespace "gc-8065"
Jun  9 10:24:23.825: INFO: Deleting pod "simpletest.rc-k47z6" in namespace "gc-8065"
Jun  9 10:24:23.845: INFO: Deleting pod "simpletest.rc-kcqx2" in namespace "gc-8065"
Jun  9 10:24:23.864: INFO: Deleting pod "simpletest.rc-kfk68" in namespace "gc-8065"
Jun  9 10:24:23.891: INFO: Deleting pod "simpletest.rc-kpvmz" in namespace "gc-8065"
Jun  9 10:24:24.034: INFO: Deleting pod "simpletest.rc-kwbp7" in namespace "gc-8065"
Jun  9 10:24:24.276: INFO: Deleting pod "simpletest.rc-kwm7h" in namespace "gc-8065"
Jun  9 10:24:24.386: INFO: Deleting pod "simpletest.rc-l78q8" in namespace "gc-8065"
Jun  9 10:24:24.474: INFO: Deleting pod "simpletest.rc-l7hf8" in namespace "gc-8065"
Jun  9 10:24:24.495: INFO: Deleting pod "simpletest.rc-l8dtn" in namespace "gc-8065"
Jun  9 10:24:24.518: INFO: Deleting pod "simpletest.rc-lh799" in namespace "gc-8065"
Jun  9 10:24:24.679: INFO: Deleting pod "simpletest.rc-lml58" in namespace "gc-8065"
Jun  9 10:24:24.794: INFO: Deleting pod "simpletest.rc-lpqk7" in namespace "gc-8065"
Jun  9 10:24:24.827: INFO: Deleting pod "simpletest.rc-m4gmb" in namespace "gc-8065"
Jun  9 10:24:24.872: INFO: Deleting pod "simpletest.rc-m7lmj" in namespace "gc-8065"
Jun  9 10:24:24.894: INFO: Deleting pod "simpletest.rc-n2fqc" in namespace "gc-8065"
Jun  9 10:24:24.915: INFO: Deleting pod "simpletest.rc-n7lrr" in namespace "gc-8065"
Jun  9 10:24:24.933: INFO: Deleting pod "simpletest.rc-n9nwq" in namespace "gc-8065"
Jun  9 10:24:25.028: INFO: Deleting pod "simpletest.rc-nm9q2" in namespace "gc-8065"
Jun  9 10:24:25.071: INFO: Deleting pod "simpletest.rc-p7l44" in namespace "gc-8065"
Jun  9 10:24:25.092: INFO: Deleting pod "simpletest.rc-phsfq" in namespace "gc-8065"
Jun  9 10:24:25.109: INFO: Deleting pod "simpletest.rc-qj74x" in namespace "gc-8065"
Jun  9 10:24:25.399: INFO: Deleting pod "simpletest.rc-qjr2t" in namespace "gc-8065"
Jun  9 10:24:25.453: INFO: Deleting pod "simpletest.rc-qk9z7" in namespace "gc-8065"
Jun  9 10:24:25.531: INFO: Deleting pod "simpletest.rc-r6qfk" in namespace "gc-8065"
Jun  9 10:24:25.557: INFO: Deleting pod "simpletest.rc-r8z9c" in namespace "gc-8065"
Jun  9 10:24:25.593: INFO: Deleting pod "simpletest.rc-r9rq5" in namespace "gc-8065"
Jun  9 10:24:25.677: INFO: Deleting pod "simpletest.rc-rpdqb" in namespace "gc-8065"
Jun  9 10:24:25.797: INFO: Deleting pod "simpletest.rc-s49s2" in namespace "gc-8065"
Jun  9 10:24:25.960: INFO: Deleting pod "simpletest.rc-s7k7r" in namespace "gc-8065"
Jun  9 10:24:26.057: INFO: Deleting pod "simpletest.rc-s8cgd" in namespace "gc-8065"
Jun  9 10:24:26.078: INFO: Deleting pod "simpletest.rc-sd8qc" in namespace "gc-8065"
Jun  9 10:24:26.144: INFO: Deleting pod "simpletest.rc-sfrls" in namespace "gc-8065"
Jun  9 10:24:26.163: INFO: Deleting pod "simpletest.rc-skm7d" in namespace "gc-8065"
Jun  9 10:24:26.182: INFO: Deleting pod "simpletest.rc-ss62n" in namespace "gc-8065"
Jun  9 10:24:26.223: INFO: Deleting pod "simpletest.rc-sxmcq" in namespace "gc-8065"
Jun  9 10:24:26.451: INFO: Deleting pod "simpletest.rc-t8vgs" in namespace "gc-8065"
Jun  9 10:24:26.601: INFO: Deleting pod "simpletest.rc-tns4h" in namespace "gc-8065"
Jun  9 10:24:26.623: INFO: Deleting pod "simpletest.rc-tqnvg" in namespace "gc-8065"
Jun  9 10:24:26.731: INFO: Deleting pod "simpletest.rc-ttzff" in namespace "gc-8065"
Jun  9 10:24:26.749: INFO: Deleting pod "simpletest.rc-vlwnf" in namespace "gc-8065"
Jun  9 10:24:26.790: INFO: Deleting pod "simpletest.rc-vm8p2" in namespace "gc-8065"
Jun  9 10:24:26.831: INFO: Deleting pod "simpletest.rc-vzpr9" in namespace "gc-8065"
Jun  9 10:24:27.183: INFO: Deleting pod "simpletest.rc-w22nh" in namespace "gc-8065"
Jun  9 10:24:27.248: INFO: Deleting pod "simpletest.rc-wc7b8" in namespace "gc-8065"
Jun  9 10:24:27.290: INFO: Deleting pod "simpletest.rc-x9bz6" in namespace "gc-8065"
Jun  9 10:24:27.333: INFO: Deleting pod "simpletest.rc-x9v5l" in namespace "gc-8065"
Jun  9 10:24:27.367: INFO: Deleting pod "simpletest.rc-xdjph" in namespace "gc-8065"
Jun  9 10:24:27.569: INFO: Deleting pod "simpletest.rc-xpcvs" in namespace "gc-8065"
Jun  9 10:24:27.592: INFO: Deleting pod "simpletest.rc-xphd5" in namespace "gc-8065"
Jun  9 10:24:27.780: INFO: Deleting pod "simpletest.rc-xtg8c" in namespace "gc-8065"
Jun  9 10:24:27.958: INFO: Deleting pod "simpletest.rc-zd865" in namespace "gc-8065"
Jun  9 10:24:28.015: INFO: Deleting pod "simpletest.rc-znhv7" in namespace "gc-8065"
Jun  9 10:24:28.060: INFO: Deleting pod "simpletest.rc-zp7kj" in namespace "gc-8065"
Jun  9 10:24:28.205: INFO: Deleting pod "simpletest.rc-zxz8x" in namespace "gc-8065"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun  9 10:24:28.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8065" for this suite. 06/09/23 10:24:28.243
------------------------------
• [SLOW TEST] [46.414 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:23:41.842
    Jun  9 10:23:41.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename gc 06/09/23 10:23:41.843
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:23:41.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:23:41.868
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 06/09/23 10:23:41.88
    STEP: delete the rc 06/09/23 10:23:46.967
    STEP: wait for the rc to be deleted 06/09/23 10:23:46.99
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 06/09/23 10:23:51.996
    STEP: Gathering metrics 06/09/23 10:24:22.109
    Jun  9 10:24:22.163: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" in namespace "kube-system" to be "running and ready"
    Jun  9 10:24:22.169: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh": Phase="Running", Reason="", readiness=true. Elapsed: 5.682256ms
    Jun  9 10:24:22.169: INFO: The phase of Pod kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh is Running (Ready = true)
    Jun  9 10:24:22.169: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" satisfied condition "running and ready"
    Jun  9 10:24:22.234: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jun  9 10:24:22.234: INFO: Deleting pod "simpletest.rc-27dqp" in namespace "gc-8065"
    Jun  9 10:24:22.252: INFO: Deleting pod "simpletest.rc-2b9jj" in namespace "gc-8065"
    Jun  9 10:24:22.269: INFO: Deleting pod "simpletest.rc-2p8ph" in namespace "gc-8065"
    Jun  9 10:24:22.289: INFO: Deleting pod "simpletest.rc-2rbr2" in namespace "gc-8065"
    Jun  9 10:24:22.307: INFO: Deleting pod "simpletest.rc-2wfj8" in namespace "gc-8065"
    Jun  9 10:24:22.325: INFO: Deleting pod "simpletest.rc-2wvss" in namespace "gc-8065"
    Jun  9 10:24:22.347: INFO: Deleting pod "simpletest.rc-44prw" in namespace "gc-8065"
    Jun  9 10:24:22.365: INFO: Deleting pod "simpletest.rc-48dtj" in namespace "gc-8065"
    Jun  9 10:24:22.387: INFO: Deleting pod "simpletest.rc-49c5j" in namespace "gc-8065"
    Jun  9 10:24:22.412: INFO: Deleting pod "simpletest.rc-4hdgs" in namespace "gc-8065"
    Jun  9 10:24:22.439: INFO: Deleting pod "simpletest.rc-4jxpd" in namespace "gc-8065"
    Jun  9 10:24:22.458: INFO: Deleting pod "simpletest.rc-4qkqj" in namespace "gc-8065"
    Jun  9 10:24:22.479: INFO: Deleting pod "simpletest.rc-58rzx" in namespace "gc-8065"
    Jun  9 10:24:22.528: INFO: Deleting pod "simpletest.rc-5p7km" in namespace "gc-8065"
    Jun  9 10:24:22.559: INFO: Deleting pod "simpletest.rc-5z9hm" in namespace "gc-8065"
    Jun  9 10:24:22.585: INFO: Deleting pod "simpletest.rc-64fd4" in namespace "gc-8065"
    Jun  9 10:24:22.605: INFO: Deleting pod "simpletest.rc-6hktz" in namespace "gc-8065"
    Jun  9 10:24:22.624: INFO: Deleting pod "simpletest.rc-6l4vf" in namespace "gc-8065"
    Jun  9 10:24:22.684: INFO: Deleting pod "simpletest.rc-79z8f" in namespace "gc-8065"
    Jun  9 10:24:22.705: INFO: Deleting pod "simpletest.rc-7bwbn" in namespace "gc-8065"
    Jun  9 10:24:22.728: INFO: Deleting pod "simpletest.rc-7sgh8" in namespace "gc-8065"
    Jun  9 10:24:22.747: INFO: Deleting pod "simpletest.rc-7zgnr" in namespace "gc-8065"
    Jun  9 10:24:22.765: INFO: Deleting pod "simpletest.rc-82mgk" in namespace "gc-8065"
    Jun  9 10:24:22.827: INFO: Deleting pod "simpletest.rc-8627q" in namespace "gc-8065"
    Jun  9 10:24:22.845: INFO: Deleting pod "simpletest.rc-89klh" in namespace "gc-8065"
    Jun  9 10:24:22.869: INFO: Deleting pod "simpletest.rc-8sdzk" in namespace "gc-8065"
    Jun  9 10:24:22.964: INFO: Deleting pod "simpletest.rc-96njj" in namespace "gc-8065"
    Jun  9 10:24:22.994: INFO: Deleting pod "simpletest.rc-9fbdr" in namespace "gc-8065"
    Jun  9 10:24:23.011: INFO: Deleting pod "simpletest.rc-b6bzw" in namespace "gc-8065"
    Jun  9 10:24:23.028: INFO: Deleting pod "simpletest.rc-bdp5h" in namespace "gc-8065"
    Jun  9 10:24:23.047: INFO: Deleting pod "simpletest.rc-bfkjb" in namespace "gc-8065"
    Jun  9 10:24:23.167: INFO: Deleting pod "simpletest.rc-bv25s" in namespace "gc-8065"
    Jun  9 10:24:23.224: INFO: Deleting pod "simpletest.rc-bx9bh" in namespace "gc-8065"
    Jun  9 10:24:23.280: INFO: Deleting pod "simpletest.rc-bxh8x" in namespace "gc-8065"
    Jun  9 10:24:23.297: INFO: Deleting pod "simpletest.rc-c7zbl" in namespace "gc-8065"
    Jun  9 10:24:23.324: INFO: Deleting pod "simpletest.rc-ckv9f" in namespace "gc-8065"
    Jun  9 10:24:23.345: INFO: Deleting pod "simpletest.rc-cnmxc" in namespace "gc-8065"
    Jun  9 10:24:23.429: INFO: Deleting pod "simpletest.rc-f7cll" in namespace "gc-8065"
    Jun  9 10:24:23.488: INFO: Deleting pod "simpletest.rc-fgf8r" in namespace "gc-8065"
    Jun  9 10:24:23.524: INFO: Deleting pod "simpletest.rc-fvwvj" in namespace "gc-8065"
    Jun  9 10:24:23.541: INFO: Deleting pod "simpletest.rc-g29gl" in namespace "gc-8065"
    Jun  9 10:24:23.562: INFO: Deleting pod "simpletest.rc-gjp88" in namespace "gc-8065"
    Jun  9 10:24:23.579: INFO: Deleting pod "simpletest.rc-hhnjk" in namespace "gc-8065"
    Jun  9 10:24:23.595: INFO: Deleting pod "simpletest.rc-hrr7q" in namespace "gc-8065"
    Jun  9 10:24:23.760: INFO: Deleting pod "simpletest.rc-jcsps" in namespace "gc-8065"
    Jun  9 10:24:23.785: INFO: Deleting pod "simpletest.rc-jd7dq" in namespace "gc-8065"
    Jun  9 10:24:23.825: INFO: Deleting pod "simpletest.rc-k47z6" in namespace "gc-8065"
    Jun  9 10:24:23.845: INFO: Deleting pod "simpletest.rc-kcqx2" in namespace "gc-8065"
    Jun  9 10:24:23.864: INFO: Deleting pod "simpletest.rc-kfk68" in namespace "gc-8065"
    Jun  9 10:24:23.891: INFO: Deleting pod "simpletest.rc-kpvmz" in namespace "gc-8065"
    Jun  9 10:24:24.034: INFO: Deleting pod "simpletest.rc-kwbp7" in namespace "gc-8065"
    Jun  9 10:24:24.276: INFO: Deleting pod "simpletest.rc-kwm7h" in namespace "gc-8065"
    Jun  9 10:24:24.386: INFO: Deleting pod "simpletest.rc-l78q8" in namespace "gc-8065"
    Jun  9 10:24:24.474: INFO: Deleting pod "simpletest.rc-l7hf8" in namespace "gc-8065"
    Jun  9 10:24:24.495: INFO: Deleting pod "simpletest.rc-l8dtn" in namespace "gc-8065"
    Jun  9 10:24:24.518: INFO: Deleting pod "simpletest.rc-lh799" in namespace "gc-8065"
    Jun  9 10:24:24.679: INFO: Deleting pod "simpletest.rc-lml58" in namespace "gc-8065"
    Jun  9 10:24:24.794: INFO: Deleting pod "simpletest.rc-lpqk7" in namespace "gc-8065"
    Jun  9 10:24:24.827: INFO: Deleting pod "simpletest.rc-m4gmb" in namespace "gc-8065"
    Jun  9 10:24:24.872: INFO: Deleting pod "simpletest.rc-m7lmj" in namespace "gc-8065"
    Jun  9 10:24:24.894: INFO: Deleting pod "simpletest.rc-n2fqc" in namespace "gc-8065"
    Jun  9 10:24:24.915: INFO: Deleting pod "simpletest.rc-n7lrr" in namespace "gc-8065"
    Jun  9 10:24:24.933: INFO: Deleting pod "simpletest.rc-n9nwq" in namespace "gc-8065"
    Jun  9 10:24:25.028: INFO: Deleting pod "simpletest.rc-nm9q2" in namespace "gc-8065"
    Jun  9 10:24:25.071: INFO: Deleting pod "simpletest.rc-p7l44" in namespace "gc-8065"
    Jun  9 10:24:25.092: INFO: Deleting pod "simpletest.rc-phsfq" in namespace "gc-8065"
    Jun  9 10:24:25.109: INFO: Deleting pod "simpletest.rc-qj74x" in namespace "gc-8065"
    Jun  9 10:24:25.399: INFO: Deleting pod "simpletest.rc-qjr2t" in namespace "gc-8065"
    Jun  9 10:24:25.453: INFO: Deleting pod "simpletest.rc-qk9z7" in namespace "gc-8065"
    Jun  9 10:24:25.531: INFO: Deleting pod "simpletest.rc-r6qfk" in namespace "gc-8065"
    Jun  9 10:24:25.557: INFO: Deleting pod "simpletest.rc-r8z9c" in namespace "gc-8065"
    Jun  9 10:24:25.593: INFO: Deleting pod "simpletest.rc-r9rq5" in namespace "gc-8065"
    Jun  9 10:24:25.677: INFO: Deleting pod "simpletest.rc-rpdqb" in namespace "gc-8065"
    Jun  9 10:24:25.797: INFO: Deleting pod "simpletest.rc-s49s2" in namespace "gc-8065"
    Jun  9 10:24:25.960: INFO: Deleting pod "simpletest.rc-s7k7r" in namespace "gc-8065"
    Jun  9 10:24:26.057: INFO: Deleting pod "simpletest.rc-s8cgd" in namespace "gc-8065"
    Jun  9 10:24:26.078: INFO: Deleting pod "simpletest.rc-sd8qc" in namespace "gc-8065"
    Jun  9 10:24:26.144: INFO: Deleting pod "simpletest.rc-sfrls" in namespace "gc-8065"
    Jun  9 10:24:26.163: INFO: Deleting pod "simpletest.rc-skm7d" in namespace "gc-8065"
    Jun  9 10:24:26.182: INFO: Deleting pod "simpletest.rc-ss62n" in namespace "gc-8065"
    Jun  9 10:24:26.223: INFO: Deleting pod "simpletest.rc-sxmcq" in namespace "gc-8065"
    Jun  9 10:24:26.451: INFO: Deleting pod "simpletest.rc-t8vgs" in namespace "gc-8065"
    Jun  9 10:24:26.601: INFO: Deleting pod "simpletest.rc-tns4h" in namespace "gc-8065"
    Jun  9 10:24:26.623: INFO: Deleting pod "simpletest.rc-tqnvg" in namespace "gc-8065"
    Jun  9 10:24:26.731: INFO: Deleting pod "simpletest.rc-ttzff" in namespace "gc-8065"
    Jun  9 10:24:26.749: INFO: Deleting pod "simpletest.rc-vlwnf" in namespace "gc-8065"
    Jun  9 10:24:26.790: INFO: Deleting pod "simpletest.rc-vm8p2" in namespace "gc-8065"
    Jun  9 10:24:26.831: INFO: Deleting pod "simpletest.rc-vzpr9" in namespace "gc-8065"
    Jun  9 10:24:27.183: INFO: Deleting pod "simpletest.rc-w22nh" in namespace "gc-8065"
    Jun  9 10:24:27.248: INFO: Deleting pod "simpletest.rc-wc7b8" in namespace "gc-8065"
    Jun  9 10:24:27.290: INFO: Deleting pod "simpletest.rc-x9bz6" in namespace "gc-8065"
    Jun  9 10:24:27.333: INFO: Deleting pod "simpletest.rc-x9v5l" in namespace "gc-8065"
    Jun  9 10:24:27.367: INFO: Deleting pod "simpletest.rc-xdjph" in namespace "gc-8065"
    Jun  9 10:24:27.569: INFO: Deleting pod "simpletest.rc-xpcvs" in namespace "gc-8065"
    Jun  9 10:24:27.592: INFO: Deleting pod "simpletest.rc-xphd5" in namespace "gc-8065"
    Jun  9 10:24:27.780: INFO: Deleting pod "simpletest.rc-xtg8c" in namespace "gc-8065"
    Jun  9 10:24:27.958: INFO: Deleting pod "simpletest.rc-zd865" in namespace "gc-8065"
    Jun  9 10:24:28.015: INFO: Deleting pod "simpletest.rc-znhv7" in namespace "gc-8065"
    Jun  9 10:24:28.060: INFO: Deleting pod "simpletest.rc-zp7kj" in namespace "gc-8065"
    Jun  9 10:24:28.205: INFO: Deleting pod "simpletest.rc-zxz8x" in namespace "gc-8065"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:24:28.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8065" for this suite. 06/09/23 10:24:28.243
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:24:28.257
Jun  9 10:24:28.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 10:24:28.272
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:24:28.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:24:28.305
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-6687 06/09/23 10:24:28.345
STEP: creating replication controller nodeport-test in namespace services-6687 06/09/23 10:24:28.446
I0609 10:24:28.541587      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6687, replica count: 2
I0609 10:24:31.597450      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 10:24:31.597: INFO: Creating new exec pod
Jun  9 10:24:31.608: INFO: Waiting up to 5m0s for pod "execpodwlpht" in namespace "services-6687" to be "running"
Jun  9 10:24:31.616: INFO: Pod "execpodwlpht": Phase="Pending", Reason="", readiness=false. Elapsed: 7.633186ms
Jun  9 10:24:33.626: INFO: Pod "execpodwlpht": Phase="Running", Reason="", readiness=true. Elapsed: 2.018091997s
Jun  9 10:24:33.626: INFO: Pod "execpodwlpht" satisfied condition "running"
Jun  9 10:24:34.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6687 exec execpodwlpht -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jun  9 10:24:34.823: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun  9 10:24:34.824: INFO: stdout: ""
Jun  9 10:24:34.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6687 exec execpodwlpht -- /bin/sh -x -c nc -v -z -w 2 10.109.83.98 80'
Jun  9 10:24:34.991: INFO: stderr: "+ nc -v -z -w 2 10.109.83.98 80\nConnection to 10.109.83.98 80 port [tcp/http] succeeded!\n"
Jun  9 10:24:34.992: INFO: stdout: ""
Jun  9 10:24:34.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6687 exec execpodwlpht -- /bin/sh -x -c nc -v -z -w 2 10.255.64.102 30136'
Jun  9 10:24:35.159: INFO: stderr: "+ nc -v -z -w 2 10.255.64.102 30136\nConnection to 10.255.64.102 30136 port [tcp/*] succeeded!\n"
Jun  9 10:24:35.159: INFO: stdout: ""
Jun  9 10:24:35.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6687 exec execpodwlpht -- /bin/sh -x -c nc -v -z -w 2 10.255.64.104 30136'
Jun  9 10:24:35.325: INFO: stderr: "+ nc -v -z -w 2 10.255.64.104 30136\nConnection to 10.255.64.104 30136 port [tcp/*] succeeded!\n"
Jun  9 10:24:35.325: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 10:24:35.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6687" for this suite. 06/09/23 10:24:35.333
------------------------------
• [SLOW TEST] [7.089 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:24:28.257
    Jun  9 10:24:28.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 10:24:28.272
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:24:28.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:24:28.305
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-6687 06/09/23 10:24:28.345
    STEP: creating replication controller nodeport-test in namespace services-6687 06/09/23 10:24:28.446
    I0609 10:24:28.541587      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6687, replica count: 2
    I0609 10:24:31.597450      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  9 10:24:31.597: INFO: Creating new exec pod
    Jun  9 10:24:31.608: INFO: Waiting up to 5m0s for pod "execpodwlpht" in namespace "services-6687" to be "running"
    Jun  9 10:24:31.616: INFO: Pod "execpodwlpht": Phase="Pending", Reason="", readiness=false. Elapsed: 7.633186ms
    Jun  9 10:24:33.626: INFO: Pod "execpodwlpht": Phase="Running", Reason="", readiness=true. Elapsed: 2.018091997s
    Jun  9 10:24:33.626: INFO: Pod "execpodwlpht" satisfied condition "running"
    Jun  9 10:24:34.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6687 exec execpodwlpht -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jun  9 10:24:34.823: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun  9 10:24:34.824: INFO: stdout: ""
    Jun  9 10:24:34.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6687 exec execpodwlpht -- /bin/sh -x -c nc -v -z -w 2 10.109.83.98 80'
    Jun  9 10:24:34.991: INFO: stderr: "+ nc -v -z -w 2 10.109.83.98 80\nConnection to 10.109.83.98 80 port [tcp/http] succeeded!\n"
    Jun  9 10:24:34.992: INFO: stdout: ""
    Jun  9 10:24:34.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6687 exec execpodwlpht -- /bin/sh -x -c nc -v -z -w 2 10.255.64.102 30136'
    Jun  9 10:24:35.159: INFO: stderr: "+ nc -v -z -w 2 10.255.64.102 30136\nConnection to 10.255.64.102 30136 port [tcp/*] succeeded!\n"
    Jun  9 10:24:35.159: INFO: stdout: ""
    Jun  9 10:24:35.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6687 exec execpodwlpht -- /bin/sh -x -c nc -v -z -w 2 10.255.64.104 30136'
    Jun  9 10:24:35.325: INFO: stderr: "+ nc -v -z -w 2 10.255.64.104 30136\nConnection to 10.255.64.104 30136 port [tcp/*] succeeded!\n"
    Jun  9 10:24:35.325: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:24:35.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6687" for this suite. 06/09/23 10:24:35.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:24:35.348
Jun  9 10:24:35.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename statefulset 06/09/23 10:24:35.349
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:24:35.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:24:35.389
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8244 06/09/23 10:24:35.393
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 06/09/23 10:24:35.401
Jun  9 10:24:35.425: INFO: Found 0 stateful pods, waiting for 3
Jun  9 10:24:45.432: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 10:24:45.432: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 10:24:45.432: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 06/09/23 10:24:45.447
Jun  9 10:24:45.477: INFO: Updating stateful set ss2
STEP: Creating a new revision 06/09/23 10:24:45.477
STEP: Not applying an update when the partition is greater than the number of replicas 06/09/23 10:24:55.501
STEP: Performing a canary update 06/09/23 10:24:55.501
Jun  9 10:24:55.529: INFO: Updating stateful set ss2
Jun  9 10:24:55.544: INFO: Waiting for Pod statefulset-8244/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 06/09/23 10:25:05.619
Jun  9 10:25:05.755: INFO: Found 2 stateful pods, waiting for 3
Jun  9 10:25:15.766: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 10:25:15.766: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 10:25:15.766: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun  9 10:25:25.763: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 10:25:25.763: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 10:25:25.763: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 06/09/23 10:25:25.776
Jun  9 10:25:25.800: INFO: Updating stateful set ss2
Jun  9 10:25:25.811: INFO: Waiting for Pod statefulset-8244/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jun  9 10:25:35.850: INFO: Updating stateful set ss2
Jun  9 10:25:35.862: INFO: Waiting for StatefulSet statefulset-8244/ss2 to complete update
Jun  9 10:25:35.862: INFO: Waiting for Pod statefulset-8244/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  9 10:25:45.879: INFO: Deleting all statefulset in ns statefulset-8244
Jun  9 10:25:45.883: INFO: Scaling statefulset ss2 to 0
Jun  9 10:25:55.910: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 10:25:55.915: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  9 10:25:55.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8244" for this suite. 06/09/23 10:25:55.945
------------------------------
• [SLOW TEST] [80.606 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:24:35.348
    Jun  9 10:24:35.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename statefulset 06/09/23 10:24:35.349
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:24:35.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:24:35.389
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8244 06/09/23 10:24:35.393
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 06/09/23 10:24:35.401
    Jun  9 10:24:35.425: INFO: Found 0 stateful pods, waiting for 3
    Jun  9 10:24:45.432: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun  9 10:24:45.432: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun  9 10:24:45.432: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 06/09/23 10:24:45.447
    Jun  9 10:24:45.477: INFO: Updating stateful set ss2
    STEP: Creating a new revision 06/09/23 10:24:45.477
    STEP: Not applying an update when the partition is greater than the number of replicas 06/09/23 10:24:55.501
    STEP: Performing a canary update 06/09/23 10:24:55.501
    Jun  9 10:24:55.529: INFO: Updating stateful set ss2
    Jun  9 10:24:55.544: INFO: Waiting for Pod statefulset-8244/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 06/09/23 10:25:05.619
    Jun  9 10:25:05.755: INFO: Found 2 stateful pods, waiting for 3
    Jun  9 10:25:15.766: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun  9 10:25:15.766: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun  9 10:25:15.766: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
    Jun  9 10:25:25.763: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun  9 10:25:25.763: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun  9 10:25:25.763: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 06/09/23 10:25:25.776
    Jun  9 10:25:25.800: INFO: Updating stateful set ss2
    Jun  9 10:25:25.811: INFO: Waiting for Pod statefulset-8244/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jun  9 10:25:35.850: INFO: Updating stateful set ss2
    Jun  9 10:25:35.862: INFO: Waiting for StatefulSet statefulset-8244/ss2 to complete update
    Jun  9 10:25:35.862: INFO: Waiting for Pod statefulset-8244/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  9 10:25:45.879: INFO: Deleting all statefulset in ns statefulset-8244
    Jun  9 10:25:45.883: INFO: Scaling statefulset ss2 to 0
    Jun  9 10:25:55.910: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  9 10:25:55.915: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:25:55.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8244" for this suite. 06/09/23 10:25:55.945
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:25:55.955
Jun  9 10:25:55.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 10:25:55.957
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:25:55.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:25:55.983
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-9572bf9e-9824-4132-9889-e37813ec9bd7 06/09/23 10:25:55.994
STEP: Creating the pod 06/09/23 10:25:56.004
Jun  9 10:25:56.017: INFO: Waiting up to 5m0s for pod "pod-configmaps-806718e6-c56e-4403-840b-d650f5b068b5" in namespace "configmap-3235" to be "running and ready"
Jun  9 10:25:56.022: INFO: Pod "pod-configmaps-806718e6-c56e-4403-840b-d650f5b068b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.910493ms
Jun  9 10:25:56.022: INFO: The phase of Pod pod-configmaps-806718e6-c56e-4403-840b-d650f5b068b5 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:25:58.031: INFO: Pod "pod-configmaps-806718e6-c56e-4403-840b-d650f5b068b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.014106018s
Jun  9 10:25:58.031: INFO: The phase of Pod pod-configmaps-806718e6-c56e-4403-840b-d650f5b068b5 is Running (Ready = true)
Jun  9 10:25:58.031: INFO: Pod "pod-configmaps-806718e6-c56e-4403-840b-d650f5b068b5" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-9572bf9e-9824-4132-9889-e37813ec9bd7 06/09/23 10:25:58.063
STEP: waiting to observe update in volume 06/09/23 10:25:58.074
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 10:26:00.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3235" for this suite. 06/09/23 10:26:00.106
------------------------------
• [4.161 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:25:55.955
    Jun  9 10:25:55.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 10:25:55.957
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:25:55.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:25:55.983
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-9572bf9e-9824-4132-9889-e37813ec9bd7 06/09/23 10:25:55.994
    STEP: Creating the pod 06/09/23 10:25:56.004
    Jun  9 10:25:56.017: INFO: Waiting up to 5m0s for pod "pod-configmaps-806718e6-c56e-4403-840b-d650f5b068b5" in namespace "configmap-3235" to be "running and ready"
    Jun  9 10:25:56.022: INFO: Pod "pod-configmaps-806718e6-c56e-4403-840b-d650f5b068b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.910493ms
    Jun  9 10:25:56.022: INFO: The phase of Pod pod-configmaps-806718e6-c56e-4403-840b-d650f5b068b5 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:25:58.031: INFO: Pod "pod-configmaps-806718e6-c56e-4403-840b-d650f5b068b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.014106018s
    Jun  9 10:25:58.031: INFO: The phase of Pod pod-configmaps-806718e6-c56e-4403-840b-d650f5b068b5 is Running (Ready = true)
    Jun  9 10:25:58.031: INFO: Pod "pod-configmaps-806718e6-c56e-4403-840b-d650f5b068b5" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-9572bf9e-9824-4132-9889-e37813ec9bd7 06/09/23 10:25:58.063
    STEP: waiting to observe update in volume 06/09/23 10:25:58.074
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:26:00.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3235" for this suite. 06/09/23 10:26:00.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:26:00.118
Jun  9 10:26:00.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename namespaces 06/09/23 10:26:00.12
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:26:00.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:26:00.147
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-s8lfc" 06/09/23 10:26:00.153
Jun  9 10:26:00.176: INFO: Namespace "e2e-ns-s8lfc-440" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-s8lfc-440" 06/09/23 10:26:00.176
Jun  9 10:26:00.191: INFO: Namespace "e2e-ns-s8lfc-440" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-s8lfc-440" 06/09/23 10:26:00.191
Jun  9 10:26:00.207: INFO: Namespace "e2e-ns-s8lfc-440" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:26:00.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6884" for this suite. 06/09/23 10:26:00.214
STEP: Destroying namespace "e2e-ns-s8lfc-440" for this suite. 06/09/23 10:26:00.225
------------------------------
• [0.121 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:26:00.118
    Jun  9 10:26:00.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename namespaces 06/09/23 10:26:00.12
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:26:00.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:26:00.147
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-s8lfc" 06/09/23 10:26:00.153
    Jun  9 10:26:00.176: INFO: Namespace "e2e-ns-s8lfc-440" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-s8lfc-440" 06/09/23 10:26:00.176
    Jun  9 10:26:00.191: INFO: Namespace "e2e-ns-s8lfc-440" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-s8lfc-440" 06/09/23 10:26:00.191
    Jun  9 10:26:00.207: INFO: Namespace "e2e-ns-s8lfc-440" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:26:00.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6884" for this suite. 06/09/23 10:26:00.214
    STEP: Destroying namespace "e2e-ns-s8lfc-440" for this suite. 06/09/23 10:26:00.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:26:00.24
Jun  9 10:26:00.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename csiinlinevolumes 06/09/23 10:26:00.241
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:26:00.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:26:00.265
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 06/09/23 10:26:00.271
STEP: getting 06/09/23 10:26:00.297
STEP: listing in namespace 06/09/23 10:26:00.31
STEP: patching 06/09/23 10:26:00.317
STEP: deleting 06/09/23 10:26:00.331
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jun  9 10:26:00.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-4872" for this suite. 06/09/23 10:26:00.358
------------------------------
• [0.128 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:26:00.24
    Jun  9 10:26:00.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename csiinlinevolumes 06/09/23 10:26:00.241
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:26:00.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:26:00.265
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 06/09/23 10:26:00.271
    STEP: getting 06/09/23 10:26:00.297
    STEP: listing in namespace 06/09/23 10:26:00.31
    STEP: patching 06/09/23 10:26:00.317
    STEP: deleting 06/09/23 10:26:00.331
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:26:00.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-4872" for this suite. 06/09/23 10:26:00.358
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:26:00.368
Jun  9 10:26:00.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 10:26:00.37
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:26:00.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:26:00.393
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 10:26:00.418
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:26:01.163
STEP: Deploying the webhook pod 06/09/23 10:26:01.258
STEP: Wait for the deployment to be ready 06/09/23 10:26:01.291
Jun  9 10:26:01.306: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 10:26:03.326
STEP: Verifying the service has paired with the endpoint 06/09/23 10:26:03.356
Jun  9 10:26:04.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 06/09/23 10:26:04.363
STEP: create a pod 06/09/23 10:26:04.392
Jun  9 10:26:04.403: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-636" to be "running"
Jun  9 10:26:04.410: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.757764ms
Jun  9 10:26:06.417: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013750156s
Jun  9 10:26:08.433: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.029406919s
Jun  9 10:26:08.433: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 06/09/23 10:26:08.433
Jun  9 10:26:08.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=webhook-636 attach --namespace=webhook-636 to-be-attached-pod -i -c=container1'
Jun  9 10:26:08.548: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:26:08.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-636" for this suite. 06/09/23 10:26:08.737
STEP: Destroying namespace "webhook-636-markers" for this suite. 06/09/23 10:26:08.75
------------------------------
• [SLOW TEST] [8.408 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:26:00.368
    Jun  9 10:26:00.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 10:26:00.37
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:26:00.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:26:00.393
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 10:26:00.418
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:26:01.163
    STEP: Deploying the webhook pod 06/09/23 10:26:01.258
    STEP: Wait for the deployment to be ready 06/09/23 10:26:01.291
    Jun  9 10:26:01.306: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 10:26:03.326
    STEP: Verifying the service has paired with the endpoint 06/09/23 10:26:03.356
    Jun  9 10:26:04.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 06/09/23 10:26:04.363
    STEP: create a pod 06/09/23 10:26:04.392
    Jun  9 10:26:04.403: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-636" to be "running"
    Jun  9 10:26:04.410: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.757764ms
    Jun  9 10:26:06.417: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013750156s
    Jun  9 10:26:08.433: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.029406919s
    Jun  9 10:26:08.433: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 06/09/23 10:26:08.433
    Jun  9 10:26:08.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=webhook-636 attach --namespace=webhook-636 to-be-attached-pod -i -c=container1'
    Jun  9 10:26:08.548: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:26:08.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-636" for this suite. 06/09/23 10:26:08.737
    STEP: Destroying namespace "webhook-636-markers" for this suite. 06/09/23 10:26:08.75
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:26:08.777
Jun  9 10:26:08.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 10:26:08.778
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:26:08.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:26:08.802
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 10:26:08.829
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:26:09.125
STEP: Deploying the webhook pod 06/09/23 10:26:09.137
STEP: Wait for the deployment to be ready 06/09/23 10:26:09.158
Jun  9 10:26:09.177: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 10:26:11.194
STEP: Verifying the service has paired with the endpoint 06/09/23 10:26:11.225
Jun  9 10:26:12.225: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 06/09/23 10:26:12.231
STEP: Updating a mutating webhook configuration's rules to not include the create operation 06/09/23 10:26:12.257
STEP: Creating a configMap that should not be mutated 06/09/23 10:26:12.267
STEP: Patching a mutating webhook configuration's rules to include the create operation 06/09/23 10:26:12.292
STEP: Creating a configMap that should be mutated 06/09/23 10:26:12.302
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:26:12.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4381" for this suite. 06/09/23 10:26:12.41
STEP: Destroying namespace "webhook-4381-markers" for this suite. 06/09/23 10:26:12.423
------------------------------
• [3.656 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:26:08.777
    Jun  9 10:26:08.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 10:26:08.778
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:26:08.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:26:08.802
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 10:26:08.829
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:26:09.125
    STEP: Deploying the webhook pod 06/09/23 10:26:09.137
    STEP: Wait for the deployment to be ready 06/09/23 10:26:09.158
    Jun  9 10:26:09.177: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 10:26:11.194
    STEP: Verifying the service has paired with the endpoint 06/09/23 10:26:11.225
    Jun  9 10:26:12.225: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 06/09/23 10:26:12.231
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 06/09/23 10:26:12.257
    STEP: Creating a configMap that should not be mutated 06/09/23 10:26:12.267
    STEP: Patching a mutating webhook configuration's rules to include the create operation 06/09/23 10:26:12.292
    STEP: Creating a configMap that should be mutated 06/09/23 10:26:12.302
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:26:12.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4381" for this suite. 06/09/23 10:26:12.41
    STEP: Destroying namespace "webhook-4381-markers" for this suite. 06/09/23 10:26:12.423
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:26:12.434
Jun  9 10:26:12.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename sysctl 06/09/23 10:26:12.436
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:26:12.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:26:12.464
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 06/09/23 10:26:12.469
STEP: Watching for error events or started pod 06/09/23 10:26:12.484
STEP: Waiting for pod completion 06/09/23 10:26:14.49
Jun  9 10:26:14.490: INFO: Waiting up to 3m0s for pod "sysctl-d71baa46-2df5-4ea1-8589-c2a9e7a66b8f" in namespace "sysctl-2274" to be "completed"
Jun  9 10:26:14.495: INFO: Pod "sysctl-d71baa46-2df5-4ea1-8589-c2a9e7a66b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.942487ms
Jun  9 10:26:16.502: INFO: Pod "sysctl-d71baa46-2df5-4ea1-8589-c2a9e7a66b8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011931174s
Jun  9 10:26:16.502: INFO: Pod "sysctl-d71baa46-2df5-4ea1-8589-c2a9e7a66b8f" satisfied condition "completed"
STEP: Checking that the pod succeeded 06/09/23 10:26:16.507
STEP: Getting logs from the pod 06/09/23 10:26:16.507
STEP: Checking that the sysctl is actually updated 06/09/23 10:26:16.517
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:26:16.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-2274" for this suite. 06/09/23 10:26:16.525
------------------------------
• [4.100 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:26:12.434
    Jun  9 10:26:12.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename sysctl 06/09/23 10:26:12.436
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:26:12.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:26:12.464
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 06/09/23 10:26:12.469
    STEP: Watching for error events or started pod 06/09/23 10:26:12.484
    STEP: Waiting for pod completion 06/09/23 10:26:14.49
    Jun  9 10:26:14.490: INFO: Waiting up to 3m0s for pod "sysctl-d71baa46-2df5-4ea1-8589-c2a9e7a66b8f" in namespace "sysctl-2274" to be "completed"
    Jun  9 10:26:14.495: INFO: Pod "sysctl-d71baa46-2df5-4ea1-8589-c2a9e7a66b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.942487ms
    Jun  9 10:26:16.502: INFO: Pod "sysctl-d71baa46-2df5-4ea1-8589-c2a9e7a66b8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011931174s
    Jun  9 10:26:16.502: INFO: Pod "sysctl-d71baa46-2df5-4ea1-8589-c2a9e7a66b8f" satisfied condition "completed"
    STEP: Checking that the pod succeeded 06/09/23 10:26:16.507
    STEP: Getting logs from the pod 06/09/23 10:26:16.507
    STEP: Checking that the sysctl is actually updated 06/09/23 10:26:16.517
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:26:16.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-2274" for this suite. 06/09/23 10:26:16.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:26:16.537
Jun  9 10:26:16.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename disruption 06/09/23 10:26:16.538
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:26:16.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:26:16.56
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 06/09/23 10:26:16.574
STEP: Updating PodDisruptionBudget status 06/09/23 10:26:16.581
STEP: Waiting for all pods to be running 06/09/23 10:26:16.591
Jun  9 10:26:16.599: INFO: running pods: 0 < 1
Jun  9 10:26:18.607: INFO: running pods: 0 < 1
STEP: locating a running pod 06/09/23 10:26:20.604
STEP: Waiting for the pdb to be processed 06/09/23 10:26:20.621
STEP: Patching PodDisruptionBudget status 06/09/23 10:26:20.634
STEP: Waiting for the pdb to be processed 06/09/23 10:26:20.654
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun  9 10:26:20.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2575" for this suite. 06/09/23 10:26:20.668
------------------------------
• [4.142 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:26:16.537
    Jun  9 10:26:16.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename disruption 06/09/23 10:26:16.538
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:26:16.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:26:16.56
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 06/09/23 10:26:16.574
    STEP: Updating PodDisruptionBudget status 06/09/23 10:26:16.581
    STEP: Waiting for all pods to be running 06/09/23 10:26:16.591
    Jun  9 10:26:16.599: INFO: running pods: 0 < 1
    Jun  9 10:26:18.607: INFO: running pods: 0 < 1
    STEP: locating a running pod 06/09/23 10:26:20.604
    STEP: Waiting for the pdb to be processed 06/09/23 10:26:20.621
    STEP: Patching PodDisruptionBudget status 06/09/23 10:26:20.634
    STEP: Waiting for the pdb to be processed 06/09/23 10:26:20.654
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:26:20.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2575" for this suite. 06/09/23 10:26:20.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:26:20.68
Jun  9 10:26:20.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename resourcequota 06/09/23 10:26:20.681
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:26:20.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:26:20.706
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-f2vsr" 06/09/23 10:26:20.715
Jun  9 10:26:20.734: INFO: Resource quota "e2e-rq-status-f2vsr" reports spec: hard cpu limit of 500m
Jun  9 10:26:20.734: INFO: Resource quota "e2e-rq-status-f2vsr" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-f2vsr" /status 06/09/23 10:26:20.734
STEP: Confirm /status for "e2e-rq-status-f2vsr" resourceQuota via watch 06/09/23 10:26:20.758
Jun  9 10:26:20.760: INFO: observed resourceQuota "e2e-rq-status-f2vsr" in namespace "resourcequota-7037" with hard status: v1.ResourceList(nil)
Jun  9 10:26:20.760: INFO: Found resourceQuota "e2e-rq-status-f2vsr" in namespace "resourcequota-7037" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jun  9 10:26:20.761: INFO: ResourceQuota "e2e-rq-status-f2vsr" /status was updated
STEP: Patching hard spec values for cpu & memory 06/09/23 10:26:20.765
Jun  9 10:26:20.773: INFO: Resource quota "e2e-rq-status-f2vsr" reports spec: hard cpu limit of 1
Jun  9 10:26:20.774: INFO: Resource quota "e2e-rq-status-f2vsr" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-f2vsr" /status 06/09/23 10:26:20.774
STEP: Confirm /status for "e2e-rq-status-f2vsr" resourceQuota via watch 06/09/23 10:26:20.781
Jun  9 10:26:20.784: INFO: observed resourceQuota "e2e-rq-status-f2vsr" in namespace "resourcequota-7037" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jun  9 10:26:20.784: INFO: Found resourceQuota "e2e-rq-status-f2vsr" in namespace "resourcequota-7037" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jun  9 10:26:20.784: INFO: ResourceQuota "e2e-rq-status-f2vsr" /status was patched
STEP: Get "e2e-rq-status-f2vsr" /status 06/09/23 10:26:20.784
Jun  9 10:26:20.790: INFO: Resourcequota "e2e-rq-status-f2vsr" reports status: hard cpu of 1
Jun  9 10:26:20.790: INFO: Resourcequota "e2e-rq-status-f2vsr" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-f2vsr" /status before checking Spec is unchanged 06/09/23 10:26:20.795
Jun  9 10:26:20.808: INFO: Resourcequota "e2e-rq-status-f2vsr" reports status: hard cpu of 2
Jun  9 10:26:20.808: INFO: Resourcequota "e2e-rq-status-f2vsr" reports status: hard memory of 2Gi
Jun  9 10:26:20.810: INFO: Found resourceQuota "e2e-rq-status-f2vsr" in namespace "resourcequota-7037" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jun  9 10:29:35.826: INFO: ResourceQuota "e2e-rq-status-f2vsr" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  9 10:29:35.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7037" for this suite. 06/09/23 10:29:35.842
------------------------------
• [SLOW TEST] [195.184 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:26:20.68
    Jun  9 10:26:20.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename resourcequota 06/09/23 10:26:20.681
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:26:20.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:26:20.706
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-f2vsr" 06/09/23 10:26:20.715
    Jun  9 10:26:20.734: INFO: Resource quota "e2e-rq-status-f2vsr" reports spec: hard cpu limit of 500m
    Jun  9 10:26:20.734: INFO: Resource quota "e2e-rq-status-f2vsr" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-f2vsr" /status 06/09/23 10:26:20.734
    STEP: Confirm /status for "e2e-rq-status-f2vsr" resourceQuota via watch 06/09/23 10:26:20.758
    Jun  9 10:26:20.760: INFO: observed resourceQuota "e2e-rq-status-f2vsr" in namespace "resourcequota-7037" with hard status: v1.ResourceList(nil)
    Jun  9 10:26:20.760: INFO: Found resourceQuota "e2e-rq-status-f2vsr" in namespace "resourcequota-7037" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jun  9 10:26:20.761: INFO: ResourceQuota "e2e-rq-status-f2vsr" /status was updated
    STEP: Patching hard spec values for cpu & memory 06/09/23 10:26:20.765
    Jun  9 10:26:20.773: INFO: Resource quota "e2e-rq-status-f2vsr" reports spec: hard cpu limit of 1
    Jun  9 10:26:20.774: INFO: Resource quota "e2e-rq-status-f2vsr" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-f2vsr" /status 06/09/23 10:26:20.774
    STEP: Confirm /status for "e2e-rq-status-f2vsr" resourceQuota via watch 06/09/23 10:26:20.781
    Jun  9 10:26:20.784: INFO: observed resourceQuota "e2e-rq-status-f2vsr" in namespace "resourcequota-7037" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jun  9 10:26:20.784: INFO: Found resourceQuota "e2e-rq-status-f2vsr" in namespace "resourcequota-7037" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jun  9 10:26:20.784: INFO: ResourceQuota "e2e-rq-status-f2vsr" /status was patched
    STEP: Get "e2e-rq-status-f2vsr" /status 06/09/23 10:26:20.784
    Jun  9 10:26:20.790: INFO: Resourcequota "e2e-rq-status-f2vsr" reports status: hard cpu of 1
    Jun  9 10:26:20.790: INFO: Resourcequota "e2e-rq-status-f2vsr" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-f2vsr" /status before checking Spec is unchanged 06/09/23 10:26:20.795
    Jun  9 10:26:20.808: INFO: Resourcequota "e2e-rq-status-f2vsr" reports status: hard cpu of 2
    Jun  9 10:26:20.808: INFO: Resourcequota "e2e-rq-status-f2vsr" reports status: hard memory of 2Gi
    Jun  9 10:26:20.810: INFO: Found resourceQuota "e2e-rq-status-f2vsr" in namespace "resourcequota-7037" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jun  9 10:29:35.826: INFO: ResourceQuota "e2e-rq-status-f2vsr" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:29:35.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7037" for this suite. 06/09/23 10:29:35.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:29:35.865
Jun  9 10:29:35.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename namespaces 06/09/23 10:29:35.866
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:29:35.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:29:35.913
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 06/09/23 10:29:35.918
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:29:35.946
STEP: Creating a pod in the namespace 06/09/23 10:29:35.951
STEP: Waiting for the pod to have running status 06/09/23 10:29:35.968
Jun  9 10:29:35.968: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8813" to be "running"
Jun  9 10:29:35.979: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.160719ms
Jun  9 10:29:37.987: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018411633s
Jun  9 10:29:37.987: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 06/09/23 10:29:37.987
STEP: Waiting for the namespace to be removed. 06/09/23 10:29:38.002
STEP: Recreating the namespace 06/09/23 10:29:49.008
STEP: Verifying there are no pods in the namespace 06/09/23 10:29:49.038
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:29:49.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4990" for this suite. 06/09/23 10:29:49.06
STEP: Destroying namespace "nsdeletetest-8813" for this suite. 06/09/23 10:29:49.074
Jun  9 10:29:49.080: INFO: Namespace nsdeletetest-8813 was already deleted
STEP: Destroying namespace "nsdeletetest-6127" for this suite. 06/09/23 10:29:49.08
------------------------------
• [SLOW TEST] [13.226 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:29:35.865
    Jun  9 10:29:35.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename namespaces 06/09/23 10:29:35.866
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:29:35.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:29:35.913
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 06/09/23 10:29:35.918
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:29:35.946
    STEP: Creating a pod in the namespace 06/09/23 10:29:35.951
    STEP: Waiting for the pod to have running status 06/09/23 10:29:35.968
    Jun  9 10:29:35.968: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8813" to be "running"
    Jun  9 10:29:35.979: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.160719ms
    Jun  9 10:29:37.987: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018411633s
    Jun  9 10:29:37.987: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 06/09/23 10:29:37.987
    STEP: Waiting for the namespace to be removed. 06/09/23 10:29:38.002
    STEP: Recreating the namespace 06/09/23 10:29:49.008
    STEP: Verifying there are no pods in the namespace 06/09/23 10:29:49.038
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:29:49.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4990" for this suite. 06/09/23 10:29:49.06
    STEP: Destroying namespace "nsdeletetest-8813" for this suite. 06/09/23 10:29:49.074
    Jun  9 10:29:49.080: INFO: Namespace nsdeletetest-8813 was already deleted
    STEP: Destroying namespace "nsdeletetest-6127" for this suite. 06/09/23 10:29:49.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:29:49.093
Jun  9 10:29:49.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:29:49.094
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:29:49.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:29:49.128
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-2f03fe3b-aeea-4f09-bdf9-050fc1127ee6 06/09/23 10:29:49.134
STEP: Creating a pod to test consume configMaps 06/09/23 10:29:49.147
Jun  9 10:29:49.165: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959" in namespace "projected-3717" to be "Succeeded or Failed"
Jun  9 10:29:49.175: INFO: Pod "pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959": Phase="Pending", Reason="", readiness=false. Elapsed: 9.676913ms
Jun  9 10:29:51.184: INFO: Pod "pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959": Phase="Running", Reason="", readiness=false. Elapsed: 2.018189684s
Jun  9 10:29:53.188: INFO: Pod "pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022803458s
STEP: Saw pod success 06/09/23 10:29:53.188
Jun  9 10:29:53.188: INFO: Pod "pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959" satisfied condition "Succeeded or Failed"
Jun  9 10:29:53.202: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959 container projected-configmap-volume-test: <nil>
STEP: delete the pod 06/09/23 10:29:53.23
Jun  9 10:29:53.251: INFO: Waiting for pod pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959 to disappear
Jun  9 10:29:53.257: INFO: Pod pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  9 10:29:53.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3717" for this suite. 06/09/23 10:29:53.267
------------------------------
• [4.185 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:29:49.093
    Jun  9 10:29:49.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:29:49.094
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:29:49.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:29:49.128
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-2f03fe3b-aeea-4f09-bdf9-050fc1127ee6 06/09/23 10:29:49.134
    STEP: Creating a pod to test consume configMaps 06/09/23 10:29:49.147
    Jun  9 10:29:49.165: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959" in namespace "projected-3717" to be "Succeeded or Failed"
    Jun  9 10:29:49.175: INFO: Pod "pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959": Phase="Pending", Reason="", readiness=false. Elapsed: 9.676913ms
    Jun  9 10:29:51.184: INFO: Pod "pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959": Phase="Running", Reason="", readiness=false. Elapsed: 2.018189684s
    Jun  9 10:29:53.188: INFO: Pod "pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022803458s
    STEP: Saw pod success 06/09/23 10:29:53.188
    Jun  9 10:29:53.188: INFO: Pod "pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959" satisfied condition "Succeeded or Failed"
    Jun  9 10:29:53.202: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 06/09/23 10:29:53.23
    Jun  9 10:29:53.251: INFO: Waiting for pod pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959 to disappear
    Jun  9 10:29:53.257: INFO: Pod pod-projected-configmaps-f2a96312-247b-4fb9-9cdd-f9e88f895959 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:29:53.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3717" for this suite. 06/09/23 10:29:53.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:29:53.279
Jun  9 10:29:53.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename proxy 06/09/23 10:29:53.28
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:29:53.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:29:53.312
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jun  9 10:29:53.317: INFO: Creating pod...
Jun  9 10:29:53.330: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4887" to be "running"
Jun  9 10:29:53.337: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.735408ms
Jun  9 10:29:55.344: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013940861s
Jun  9 10:29:55.344: INFO: Pod "agnhost" satisfied condition "running"
Jun  9 10:29:55.344: INFO: Creating service...
Jun  9 10:29:55.551: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/pods/agnhost/proxy/some/path/with/DELETE
Jun  9 10:29:55.579: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun  9 10:29:55.579: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/pods/agnhost/proxy/some/path/with/GET
Jun  9 10:29:55.587: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun  9 10:29:55.587: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/pods/agnhost/proxy/some/path/with/HEAD
Jun  9 10:29:55.594: INFO: http.Client request:HEAD | StatusCode:200
Jun  9 10:29:55.594: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/pods/agnhost/proxy/some/path/with/OPTIONS
Jun  9 10:29:55.609: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun  9 10:29:55.609: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/pods/agnhost/proxy/some/path/with/PATCH
Jun  9 10:29:55.621: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun  9 10:29:55.621: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/pods/agnhost/proxy/some/path/with/POST
Jun  9 10:29:55.633: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun  9 10:29:55.633: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/pods/agnhost/proxy/some/path/with/PUT
Jun  9 10:29:55.655: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun  9 10:29:55.655: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/services/test-service/proxy/some/path/with/DELETE
Jun  9 10:29:55.672: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun  9 10:29:55.672: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/services/test-service/proxy/some/path/with/GET
Jun  9 10:29:55.687: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun  9 10:29:55.687: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/services/test-service/proxy/some/path/with/HEAD
Jun  9 10:29:55.702: INFO: http.Client request:HEAD | StatusCode:200
Jun  9 10:29:55.702: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/services/test-service/proxy/some/path/with/OPTIONS
Jun  9 10:29:55.720: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun  9 10:29:55.720: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/services/test-service/proxy/some/path/with/PATCH
Jun  9 10:29:55.735: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun  9 10:29:55.735: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/services/test-service/proxy/some/path/with/POST
Jun  9 10:29:55.744: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun  9 10:29:55.744: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/services/test-service/proxy/some/path/with/PUT
Jun  9 10:29:55.762: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jun  9 10:29:55.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-4887" for this suite. 06/09/23 10:29:55.78
------------------------------
• [2.515 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:29:53.279
    Jun  9 10:29:53.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename proxy 06/09/23 10:29:53.28
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:29:53.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:29:53.312
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jun  9 10:29:53.317: INFO: Creating pod...
    Jun  9 10:29:53.330: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4887" to be "running"
    Jun  9 10:29:53.337: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.735408ms
    Jun  9 10:29:55.344: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013940861s
    Jun  9 10:29:55.344: INFO: Pod "agnhost" satisfied condition "running"
    Jun  9 10:29:55.344: INFO: Creating service...
    Jun  9 10:29:55.551: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/pods/agnhost/proxy/some/path/with/DELETE
    Jun  9 10:29:55.579: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun  9 10:29:55.579: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/pods/agnhost/proxy/some/path/with/GET
    Jun  9 10:29:55.587: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jun  9 10:29:55.587: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/pods/agnhost/proxy/some/path/with/HEAD
    Jun  9 10:29:55.594: INFO: http.Client request:HEAD | StatusCode:200
    Jun  9 10:29:55.594: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/pods/agnhost/proxy/some/path/with/OPTIONS
    Jun  9 10:29:55.609: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun  9 10:29:55.609: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/pods/agnhost/proxy/some/path/with/PATCH
    Jun  9 10:29:55.621: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun  9 10:29:55.621: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/pods/agnhost/proxy/some/path/with/POST
    Jun  9 10:29:55.633: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun  9 10:29:55.633: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/pods/agnhost/proxy/some/path/with/PUT
    Jun  9 10:29:55.655: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun  9 10:29:55.655: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/services/test-service/proxy/some/path/with/DELETE
    Jun  9 10:29:55.672: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun  9 10:29:55.672: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/services/test-service/proxy/some/path/with/GET
    Jun  9 10:29:55.687: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jun  9 10:29:55.687: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/services/test-service/proxy/some/path/with/HEAD
    Jun  9 10:29:55.702: INFO: http.Client request:HEAD | StatusCode:200
    Jun  9 10:29:55.702: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/services/test-service/proxy/some/path/with/OPTIONS
    Jun  9 10:29:55.720: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun  9 10:29:55.720: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/services/test-service/proxy/some/path/with/PATCH
    Jun  9 10:29:55.735: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun  9 10:29:55.735: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/services/test-service/proxy/some/path/with/POST
    Jun  9 10:29:55.744: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun  9 10:29:55.744: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4887/services/test-service/proxy/some/path/with/PUT
    Jun  9 10:29:55.762: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:29:55.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-4887" for this suite. 06/09/23 10:29:55.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:29:55.797
Jun  9 10:29:55.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 10:29:55.798
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:29:55.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:29:55.827
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-6326/configmap-test-7a50732f-fc03-4ec5-8ceb-8567b593c472 06/09/23 10:29:55.831
STEP: Creating a pod to test consume configMaps 06/09/23 10:29:55.84
Jun  9 10:29:55.856: INFO: Waiting up to 5m0s for pod "pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e" in namespace "configmap-6326" to be "Succeeded or Failed"
Jun  9 10:29:55.864: INFO: Pod "pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.531187ms
Jun  9 10:29:57.872: INFO: Pod "pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015357568s
Jun  9 10:29:59.876: INFO: Pod "pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019839014s
STEP: Saw pod success 06/09/23 10:29:59.876
Jun  9 10:29:59.876: INFO: Pod "pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e" satisfied condition "Succeeded or Failed"
Jun  9 10:29:59.883: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e container env-test: <nil>
STEP: delete the pod 06/09/23 10:29:59.898
Jun  9 10:29:59.921: INFO: Waiting for pod pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e to disappear
Jun  9 10:29:59.932: INFO: Pod pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 10:29:59.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6326" for this suite. 06/09/23 10:29:59.942
------------------------------
• [4.162 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:29:55.797
    Jun  9 10:29:55.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 10:29:55.798
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:29:55.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:29:55.827
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-6326/configmap-test-7a50732f-fc03-4ec5-8ceb-8567b593c472 06/09/23 10:29:55.831
    STEP: Creating a pod to test consume configMaps 06/09/23 10:29:55.84
    Jun  9 10:29:55.856: INFO: Waiting up to 5m0s for pod "pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e" in namespace "configmap-6326" to be "Succeeded or Failed"
    Jun  9 10:29:55.864: INFO: Pod "pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.531187ms
    Jun  9 10:29:57.872: INFO: Pod "pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015357568s
    Jun  9 10:29:59.876: INFO: Pod "pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019839014s
    STEP: Saw pod success 06/09/23 10:29:59.876
    Jun  9 10:29:59.876: INFO: Pod "pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e" satisfied condition "Succeeded or Failed"
    Jun  9 10:29:59.883: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e container env-test: <nil>
    STEP: delete the pod 06/09/23 10:29:59.898
    Jun  9 10:29:59.921: INFO: Waiting for pod pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e to disappear
    Jun  9 10:29:59.932: INFO: Pod pod-configmaps-1764b4ac-2af1-410a-a050-961230ea5c4e no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:29:59.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6326" for this suite. 06/09/23 10:29:59.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:29:59.961
Jun  9 10:29:59.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename security-context-test 06/09/23 10:29:59.962
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:29:59.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:30:00
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Jun  9 10:30:00.041: INFO: Waiting up to 5m0s for pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f" in namespace "security-context-test-9258" to be "Succeeded or Failed"
Jun  9 10:30:00.059: INFO: Pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.751834ms
Jun  9 10:30:02.071: INFO: Pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030793666s
Jun  9 10:30:04.076: INFO: Pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035644308s
Jun  9 10:30:06.068: INFO: Pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027698449s
Jun  9 10:30:08.073: INFO: Pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032632383s
Jun  9 10:30:10.067: INFO: Pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.026033453s
Jun  9 10:30:10.067: INFO: Pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun  9 10:30:10.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9258" for this suite. 06/09/23 10:30:10.08
------------------------------
• [SLOW TEST] [10.134 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:29:59.961
    Jun  9 10:29:59.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename security-context-test 06/09/23 10:29:59.962
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:29:59.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:30:00
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Jun  9 10:30:00.041: INFO: Waiting up to 5m0s for pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f" in namespace "security-context-test-9258" to be "Succeeded or Failed"
    Jun  9 10:30:00.059: INFO: Pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.751834ms
    Jun  9 10:30:02.071: INFO: Pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030793666s
    Jun  9 10:30:04.076: INFO: Pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035644308s
    Jun  9 10:30:06.068: INFO: Pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027698449s
    Jun  9 10:30:08.073: INFO: Pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032632383s
    Jun  9 10:30:10.067: INFO: Pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.026033453s
    Jun  9 10:30:10.067: INFO: Pod "busybox-user-65534-3f16a070-17f7-4eb7-9ec5-e8ee193b8f3f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:30:10.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9258" for this suite. 06/09/23 10:30:10.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:30:10.095
Jun  9 10:30:10.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:30:10.096
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:30:10.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:30:10.141
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 06/09/23 10:30:10.147
Jun  9 10:30:10.162: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7" in namespace "projected-2393" to be "Succeeded or Failed"
Jun  9 10:30:10.170: INFO: Pod "downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.514611ms
Jun  9 10:30:12.179: INFO: Pod "downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017883027s
Jun  9 10:30:14.179: INFO: Pod "downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017866929s
Jun  9 10:30:16.183: INFO: Pod "downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021783621s
STEP: Saw pod success 06/09/23 10:30:16.183
Jun  9 10:30:16.184: INFO: Pod "downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7" satisfied condition "Succeeded or Failed"
Jun  9 10:30:16.191: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7 container client-container: <nil>
STEP: delete the pod 06/09/23 10:30:16.202
Jun  9 10:30:16.227: INFO: Waiting for pod downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7 to disappear
Jun  9 10:30:16.234: INFO: Pod downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  9 10:30:16.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2393" for this suite. 06/09/23 10:30:16.244
------------------------------
• [SLOW TEST] [6.163 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:30:10.095
    Jun  9 10:30:10.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:30:10.096
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:30:10.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:30:10.141
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 06/09/23 10:30:10.147
    Jun  9 10:30:10.162: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7" in namespace "projected-2393" to be "Succeeded or Failed"
    Jun  9 10:30:10.170: INFO: Pod "downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.514611ms
    Jun  9 10:30:12.179: INFO: Pod "downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017883027s
    Jun  9 10:30:14.179: INFO: Pod "downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017866929s
    Jun  9 10:30:16.183: INFO: Pod "downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021783621s
    STEP: Saw pod success 06/09/23 10:30:16.183
    Jun  9 10:30:16.184: INFO: Pod "downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7" satisfied condition "Succeeded or Failed"
    Jun  9 10:30:16.191: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7 container client-container: <nil>
    STEP: delete the pod 06/09/23 10:30:16.202
    Jun  9 10:30:16.227: INFO: Waiting for pod downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7 to disappear
    Jun  9 10:30:16.234: INFO: Pod downwardapi-volume-4a8048ee-1d52-417e-807e-bcd4282e88e7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:30:16.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2393" for this suite. 06/09/23 10:30:16.244
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:30:16.259
Jun  9 10:30:16.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir-wrapper 06/09/23 10:30:16.261
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:30:16.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:30:16.305
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 06/09/23 10:30:16.312
STEP: Creating RC which spawns configmap-volume pods 06/09/23 10:30:17.032
Jun  9 10:30:17.150: INFO: Pod name wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c: Found 1 pods out of 5
Jun  9 10:30:22.161: INFO: Pod name wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/09/23 10:30:22.161
Jun  9 10:30:22.162: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:30:22.169: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.118012ms
Jun  9 10:30:24.185: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023024418s
Jun  9 10:30:26.180: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018864565s
Jun  9 10:30:28.192: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030637329s
Jun  9 10:30:30.197: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.035715468s
Jun  9 10:30:32.177: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb": Phase="Running", Reason="", readiness=true. Elapsed: 10.015825631s
Jun  9 10:30:32.177: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb" satisfied condition "running"
Jun  9 10:30:32.177: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-4kmrb" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:30:32.189: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-4kmrb": Phase="Running", Reason="", readiness=true. Elapsed: 11.499281ms
Jun  9 10:30:32.189: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-4kmrb" satisfied condition "running"
Jun  9 10:30:32.189: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-4slzb" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:30:32.196: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-4slzb": Phase="Running", Reason="", readiness=true. Elapsed: 6.52546ms
Jun  9 10:30:32.196: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-4slzb" satisfied condition "running"
Jun  9 10:30:32.196: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-9w6b9" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:30:32.203: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-9w6b9": Phase="Running", Reason="", readiness=true. Elapsed: 7.830365ms
Jun  9 10:30:32.203: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-9w6b9" satisfied condition "running"
Jun  9 10:30:32.204: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-rsxwk" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:30:32.211: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-rsxwk": Phase="Running", Reason="", readiness=true. Elapsed: 7.015335ms
Jun  9 10:30:32.211: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-rsxwk" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c in namespace emptydir-wrapper-2571, will wait for the garbage collector to delete the pods 06/09/23 10:30:32.211
Jun  9 10:30:32.296: INFO: Deleting ReplicationController wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c took: 26.273085ms
Jun  9 10:30:32.398: INFO: Terminating ReplicationController wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c pods took: 101.150484ms
STEP: Creating RC which spawns configmap-volume pods 06/09/23 10:30:36.108
Jun  9 10:30:36.136: INFO: Pod name wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33: Found 1 pods out of 5
Jun  9 10:30:41.225: INFO: Pod name wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/09/23 10:30:41.225
Jun  9 10:30:41.225: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:30:41.233: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.472709ms
Jun  9 10:30:43.240: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015104232s
Jun  9 10:30:45.244: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019294975s
Jun  9 10:30:47.240: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015096421s
Jun  9 10:30:49.322: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097265618s
Jun  9 10:30:51.263: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4": Phase="Running", Reason="", readiness=true. Elapsed: 10.038384791s
Jun  9 10:30:51.264: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4" satisfied condition "running"
Jun  9 10:30:51.264: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-dpcvz" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:30:51.271: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-dpcvz": Phase="Running", Reason="", readiness=true. Elapsed: 7.467126ms
Jun  9 10:30:51.271: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-dpcvz" satisfied condition "running"
Jun  9 10:30:51.271: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-qlp8k" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:30:51.285: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-qlp8k": Phase="Running", Reason="", readiness=true. Elapsed: 14.078157ms
Jun  9 10:30:51.285: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-qlp8k" satisfied condition "running"
Jun  9 10:30:51.285: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-sxxrw" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:30:51.299: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-sxxrw": Phase="Running", Reason="", readiness=true. Elapsed: 14.166973ms
Jun  9 10:30:51.299: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-sxxrw" satisfied condition "running"
Jun  9 10:30:51.299: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-x9g5v" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:30:51.312: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-x9g5v": Phase="Running", Reason="", readiness=true. Elapsed: 12.966964ms
Jun  9 10:30:51.312: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-x9g5v" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33 in namespace emptydir-wrapper-2571, will wait for the garbage collector to delete the pods 06/09/23 10:30:51.312
Jun  9 10:30:51.395: INFO: Deleting ReplicationController wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33 took: 21.32114ms
Jun  9 10:30:51.496: INFO: Terminating ReplicationController wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33 pods took: 101.155188ms
STEP: Creating RC which spawns configmap-volume pods 06/09/23 10:30:55.006
Jun  9 10:30:55.033: INFO: Pod name wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612: Found 0 pods out of 5
Jun  9 10:31:00.048: INFO: Pod name wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/09/23 10:31:00.048
Jun  9 10:31:00.048: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:31:00.056: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.615142ms
Jun  9 10:31:02.070: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021756305s
Jun  9 10:31:04.065: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017148422s
Jun  9 10:31:06.209: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.161083938s
Jun  9 10:31:08.074: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025884666s
Jun  9 10:31:10.065: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn": Phase="Running", Reason="", readiness=true. Elapsed: 10.017048166s
Jun  9 10:31:10.065: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn" satisfied condition "running"
Jun  9 10:31:10.065: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-42c4c" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:31:10.071: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-42c4c": Phase="Running", Reason="", readiness=true. Elapsed: 6.203001ms
Jun  9 10:31:10.071: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-42c4c" satisfied condition "running"
Jun  9 10:31:10.071: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-8h8jn" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:31:10.078: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-8h8jn": Phase="Running", Reason="", readiness=true. Elapsed: 6.960383ms
Jun  9 10:31:10.078: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-8h8jn" satisfied condition "running"
Jun  9 10:31:10.078: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-glh2r" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:31:10.086: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-glh2r": Phase="Running", Reason="", readiness=true. Elapsed: 8.134754ms
Jun  9 10:31:10.086: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-glh2r" satisfied condition "running"
Jun  9 10:31:10.086: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-j47m8" in namespace "emptydir-wrapper-2571" to be "running"
Jun  9 10:31:10.093: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-j47m8": Phase="Running", Reason="", readiness=true. Elapsed: 7.021918ms
Jun  9 10:31:10.094: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-j47m8" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612 in namespace emptydir-wrapper-2571, will wait for the garbage collector to delete the pods 06/09/23 10:31:10.094
Jun  9 10:31:10.175: INFO: Deleting ReplicationController wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612 took: 24.599248ms
Jun  9 10:31:10.276: INFO: Terminating ReplicationController wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612 pods took: 100.810839ms
STEP: Cleaning up the configMaps 06/09/23 10:31:13.577
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 10:31:14.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-2571" for this suite. 06/09/23 10:31:14.292
------------------------------
• [SLOW TEST] [58.045 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:30:16.259
    Jun  9 10:30:16.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir-wrapper 06/09/23 10:30:16.261
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:30:16.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:30:16.305
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 06/09/23 10:30:16.312
    STEP: Creating RC which spawns configmap-volume pods 06/09/23 10:30:17.032
    Jun  9 10:30:17.150: INFO: Pod name wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c: Found 1 pods out of 5
    Jun  9 10:30:22.161: INFO: Pod name wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/09/23 10:30:22.161
    Jun  9 10:30:22.162: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:30:22.169: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.118012ms
    Jun  9 10:30:24.185: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023024418s
    Jun  9 10:30:26.180: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018864565s
    Jun  9 10:30:28.192: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030637329s
    Jun  9 10:30:30.197: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.035715468s
    Jun  9 10:30:32.177: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb": Phase="Running", Reason="", readiness=true. Elapsed: 10.015825631s
    Jun  9 10:30:32.177: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-2qxgb" satisfied condition "running"
    Jun  9 10:30:32.177: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-4kmrb" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:30:32.189: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-4kmrb": Phase="Running", Reason="", readiness=true. Elapsed: 11.499281ms
    Jun  9 10:30:32.189: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-4kmrb" satisfied condition "running"
    Jun  9 10:30:32.189: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-4slzb" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:30:32.196: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-4slzb": Phase="Running", Reason="", readiness=true. Elapsed: 6.52546ms
    Jun  9 10:30:32.196: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-4slzb" satisfied condition "running"
    Jun  9 10:30:32.196: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-9w6b9" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:30:32.203: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-9w6b9": Phase="Running", Reason="", readiness=true. Elapsed: 7.830365ms
    Jun  9 10:30:32.203: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-9w6b9" satisfied condition "running"
    Jun  9 10:30:32.204: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-rsxwk" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:30:32.211: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-rsxwk": Phase="Running", Reason="", readiness=true. Elapsed: 7.015335ms
    Jun  9 10:30:32.211: INFO: Pod "wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c-rsxwk" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c in namespace emptydir-wrapper-2571, will wait for the garbage collector to delete the pods 06/09/23 10:30:32.211
    Jun  9 10:30:32.296: INFO: Deleting ReplicationController wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c took: 26.273085ms
    Jun  9 10:30:32.398: INFO: Terminating ReplicationController wrapped-volume-race-f296614a-d72f-4183-85fe-7aeec690633c pods took: 101.150484ms
    STEP: Creating RC which spawns configmap-volume pods 06/09/23 10:30:36.108
    Jun  9 10:30:36.136: INFO: Pod name wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33: Found 1 pods out of 5
    Jun  9 10:30:41.225: INFO: Pod name wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/09/23 10:30:41.225
    Jun  9 10:30:41.225: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:30:41.233: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.472709ms
    Jun  9 10:30:43.240: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015104232s
    Jun  9 10:30:45.244: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019294975s
    Jun  9 10:30:47.240: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015096421s
    Jun  9 10:30:49.322: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097265618s
    Jun  9 10:30:51.263: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4": Phase="Running", Reason="", readiness=true. Elapsed: 10.038384791s
    Jun  9 10:30:51.264: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-97hd4" satisfied condition "running"
    Jun  9 10:30:51.264: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-dpcvz" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:30:51.271: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-dpcvz": Phase="Running", Reason="", readiness=true. Elapsed: 7.467126ms
    Jun  9 10:30:51.271: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-dpcvz" satisfied condition "running"
    Jun  9 10:30:51.271: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-qlp8k" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:30:51.285: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-qlp8k": Phase="Running", Reason="", readiness=true. Elapsed: 14.078157ms
    Jun  9 10:30:51.285: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-qlp8k" satisfied condition "running"
    Jun  9 10:30:51.285: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-sxxrw" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:30:51.299: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-sxxrw": Phase="Running", Reason="", readiness=true. Elapsed: 14.166973ms
    Jun  9 10:30:51.299: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-sxxrw" satisfied condition "running"
    Jun  9 10:30:51.299: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-x9g5v" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:30:51.312: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-x9g5v": Phase="Running", Reason="", readiness=true. Elapsed: 12.966964ms
    Jun  9 10:30:51.312: INFO: Pod "wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33-x9g5v" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33 in namespace emptydir-wrapper-2571, will wait for the garbage collector to delete the pods 06/09/23 10:30:51.312
    Jun  9 10:30:51.395: INFO: Deleting ReplicationController wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33 took: 21.32114ms
    Jun  9 10:30:51.496: INFO: Terminating ReplicationController wrapped-volume-race-5315e891-74ff-4283-a3df-5138b3d12d33 pods took: 101.155188ms
    STEP: Creating RC which spawns configmap-volume pods 06/09/23 10:30:55.006
    Jun  9 10:30:55.033: INFO: Pod name wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612: Found 0 pods out of 5
    Jun  9 10:31:00.048: INFO: Pod name wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/09/23 10:31:00.048
    Jun  9 10:31:00.048: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:31:00.056: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.615142ms
    Jun  9 10:31:02.070: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021756305s
    Jun  9 10:31:04.065: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017148422s
    Jun  9 10:31:06.209: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.161083938s
    Jun  9 10:31:08.074: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025884666s
    Jun  9 10:31:10.065: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn": Phase="Running", Reason="", readiness=true. Elapsed: 10.017048166s
    Jun  9 10:31:10.065: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-22cnn" satisfied condition "running"
    Jun  9 10:31:10.065: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-42c4c" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:31:10.071: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-42c4c": Phase="Running", Reason="", readiness=true. Elapsed: 6.203001ms
    Jun  9 10:31:10.071: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-42c4c" satisfied condition "running"
    Jun  9 10:31:10.071: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-8h8jn" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:31:10.078: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-8h8jn": Phase="Running", Reason="", readiness=true. Elapsed: 6.960383ms
    Jun  9 10:31:10.078: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-8h8jn" satisfied condition "running"
    Jun  9 10:31:10.078: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-glh2r" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:31:10.086: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-glh2r": Phase="Running", Reason="", readiness=true. Elapsed: 8.134754ms
    Jun  9 10:31:10.086: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-glh2r" satisfied condition "running"
    Jun  9 10:31:10.086: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-j47m8" in namespace "emptydir-wrapper-2571" to be "running"
    Jun  9 10:31:10.093: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-j47m8": Phase="Running", Reason="", readiness=true. Elapsed: 7.021918ms
    Jun  9 10:31:10.094: INFO: Pod "wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612-j47m8" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612 in namespace emptydir-wrapper-2571, will wait for the garbage collector to delete the pods 06/09/23 10:31:10.094
    Jun  9 10:31:10.175: INFO: Deleting ReplicationController wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612 took: 24.599248ms
    Jun  9 10:31:10.276: INFO: Terminating ReplicationController wrapped-volume-race-d68fd05c-2a77-47d5-b883-384453a87612 pods took: 100.810839ms
    STEP: Cleaning up the configMaps 06/09/23 10:31:13.577
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:31:14.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-2571" for this suite. 06/09/23 10:31:14.292
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:31:14.305
Jun  9 10:31:14.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 10:31:14.307
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:31:14.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:31:14.339
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 06/09/23 10:31:14.346
Jun  9 10:31:14.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 create -f -'
Jun  9 10:31:15.228: INFO: stderr: ""
Jun  9 10:31:15.228: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/09/23 10:31:15.228
Jun  9 10:31:15.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  9 10:31:15.324: INFO: stderr: ""
Jun  9 10:31:15.324: INFO: stdout: "update-demo-nautilus-6nwdt update-demo-nautilus-c9js2 "
Jun  9 10:31:15.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-6nwdt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  9 10:31:15.417: INFO: stderr: ""
Jun  9 10:31:15.417: INFO: stdout: ""
Jun  9 10:31:15.417: INFO: update-demo-nautilus-6nwdt is created but not running
Jun  9 10:31:20.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  9 10:31:20.553: INFO: stderr: ""
Jun  9 10:31:20.553: INFO: stdout: "update-demo-nautilus-6nwdt update-demo-nautilus-c9js2 "
Jun  9 10:31:20.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-6nwdt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  9 10:31:20.661: INFO: stderr: ""
Jun  9 10:31:20.661: INFO: stdout: ""
Jun  9 10:31:20.661: INFO: update-demo-nautilus-6nwdt is created but not running
Jun  9 10:31:25.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  9 10:31:25.760: INFO: stderr: ""
Jun  9 10:31:25.760: INFO: stdout: "update-demo-nautilus-6nwdt update-demo-nautilus-c9js2 "
Jun  9 10:31:25.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-6nwdt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  9 10:31:25.854: INFO: stderr: ""
Jun  9 10:31:25.854: INFO: stdout: ""
Jun  9 10:31:25.854: INFO: update-demo-nautilus-6nwdt is created but not running
Jun  9 10:31:30.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  9 10:31:30.996: INFO: stderr: ""
Jun  9 10:31:30.996: INFO: stdout: "update-demo-nautilus-6nwdt update-demo-nautilus-c9js2 "
Jun  9 10:31:30.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-6nwdt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  9 10:31:31.104: INFO: stderr: ""
Jun  9 10:31:31.104: INFO: stdout: ""
Jun  9 10:31:31.104: INFO: update-demo-nautilus-6nwdt is created but not running
Jun  9 10:31:36.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  9 10:31:36.201: INFO: stderr: ""
Jun  9 10:31:36.201: INFO: stdout: "update-demo-nautilus-6nwdt update-demo-nautilus-c9js2 "
Jun  9 10:31:36.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-6nwdt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  9 10:31:36.295: INFO: stderr: ""
Jun  9 10:31:36.295: INFO: stdout: "true"
Jun  9 10:31:36.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-6nwdt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  9 10:31:36.414: INFO: stderr: ""
Jun  9 10:31:36.414: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  9 10:31:36.414: INFO: validating pod update-demo-nautilus-6nwdt
Jun  9 10:31:36.475: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 10:31:36.476: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 10:31:36.476: INFO: update-demo-nautilus-6nwdt is verified up and running
Jun  9 10:31:36.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-c9js2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  9 10:31:36.586: INFO: stderr: ""
Jun  9 10:31:36.587: INFO: stdout: "true"
Jun  9 10:31:36.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-c9js2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  9 10:31:36.679: INFO: stderr: ""
Jun  9 10:31:36.679: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  9 10:31:36.679: INFO: validating pod update-demo-nautilus-c9js2
Jun  9 10:31:36.691: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 10:31:36.691: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 10:31:36.691: INFO: update-demo-nautilus-c9js2 is verified up and running
STEP: scaling down the replication controller 06/09/23 10:31:36.691
Jun  9 10:31:36.693: INFO: scanned /root for discovery docs: <nil>
Jun  9 10:31:36.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun  9 10:31:37.843: INFO: stderr: ""
Jun  9 10:31:37.843: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/09/23 10:31:37.843
Jun  9 10:31:37.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  9 10:31:37.942: INFO: stderr: ""
Jun  9 10:31:37.942: INFO: stdout: "update-demo-nautilus-c9js2 "
Jun  9 10:31:37.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-c9js2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  9 10:31:38.027: INFO: stderr: ""
Jun  9 10:31:38.027: INFO: stdout: "true"
Jun  9 10:31:38.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-c9js2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  9 10:31:38.109: INFO: stderr: ""
Jun  9 10:31:38.110: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  9 10:31:38.110: INFO: validating pod update-demo-nautilus-c9js2
Jun  9 10:31:38.117: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 10:31:38.117: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 10:31:38.117: INFO: update-demo-nautilus-c9js2 is verified up and running
STEP: scaling up the replication controller 06/09/23 10:31:38.117
Jun  9 10:31:38.119: INFO: scanned /root for discovery docs: <nil>
Jun  9 10:31:38.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun  9 10:31:39.236: INFO: stderr: ""
Jun  9 10:31:39.236: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/09/23 10:31:39.236
Jun  9 10:31:39.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  9 10:31:39.326: INFO: stderr: ""
Jun  9 10:31:39.326: INFO: stdout: "update-demo-nautilus-c9js2 update-demo-nautilus-t7wmf "
Jun  9 10:31:39.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-c9js2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  9 10:31:39.411: INFO: stderr: ""
Jun  9 10:31:39.411: INFO: stdout: "true"
Jun  9 10:31:39.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-c9js2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  9 10:31:39.496: INFO: stderr: ""
Jun  9 10:31:39.496: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  9 10:31:39.496: INFO: validating pod update-demo-nautilus-c9js2
Jun  9 10:31:39.505: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 10:31:39.505: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 10:31:39.505: INFO: update-demo-nautilus-c9js2 is verified up and running
Jun  9 10:31:39.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-t7wmf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  9 10:31:39.603: INFO: stderr: ""
Jun  9 10:31:39.603: INFO: stdout: "true"
Jun  9 10:31:39.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-t7wmf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  9 10:31:39.697: INFO: stderr: ""
Jun  9 10:31:39.697: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  9 10:31:39.697: INFO: validating pod update-demo-nautilus-t7wmf
Jun  9 10:31:39.707: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 10:31:39.707: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 10:31:39.707: INFO: update-demo-nautilus-t7wmf is verified up and running
STEP: using delete to clean up resources 06/09/23 10:31:39.707
Jun  9 10:31:39.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 delete --grace-period=0 --force -f -'
Jun  9 10:31:39.799: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 10:31:39.799: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun  9 10:31:39.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get rc,svc -l name=update-demo --no-headers'
Jun  9 10:31:39.913: INFO: stderr: "No resources found in kubectl-235 namespace.\n"
Jun  9 10:31:39.913: INFO: stdout: ""
Jun  9 10:31:39.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  9 10:31:40.008: INFO: stderr: ""
Jun  9 10:31:40.008: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 10:31:40.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-235" for this suite. 06/09/23 10:31:40.017
------------------------------
• [SLOW TEST] [25.770 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:31:14.305
    Jun  9 10:31:14.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 10:31:14.307
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:31:14.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:31:14.339
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 06/09/23 10:31:14.346
    Jun  9 10:31:14.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 create -f -'
    Jun  9 10:31:15.228: INFO: stderr: ""
    Jun  9 10:31:15.228: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/09/23 10:31:15.228
    Jun  9 10:31:15.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  9 10:31:15.324: INFO: stderr: ""
    Jun  9 10:31:15.324: INFO: stdout: "update-demo-nautilus-6nwdt update-demo-nautilus-c9js2 "
    Jun  9 10:31:15.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-6nwdt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  9 10:31:15.417: INFO: stderr: ""
    Jun  9 10:31:15.417: INFO: stdout: ""
    Jun  9 10:31:15.417: INFO: update-demo-nautilus-6nwdt is created but not running
    Jun  9 10:31:20.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  9 10:31:20.553: INFO: stderr: ""
    Jun  9 10:31:20.553: INFO: stdout: "update-demo-nautilus-6nwdt update-demo-nautilus-c9js2 "
    Jun  9 10:31:20.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-6nwdt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  9 10:31:20.661: INFO: stderr: ""
    Jun  9 10:31:20.661: INFO: stdout: ""
    Jun  9 10:31:20.661: INFO: update-demo-nautilus-6nwdt is created but not running
    Jun  9 10:31:25.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  9 10:31:25.760: INFO: stderr: ""
    Jun  9 10:31:25.760: INFO: stdout: "update-demo-nautilus-6nwdt update-demo-nautilus-c9js2 "
    Jun  9 10:31:25.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-6nwdt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  9 10:31:25.854: INFO: stderr: ""
    Jun  9 10:31:25.854: INFO: stdout: ""
    Jun  9 10:31:25.854: INFO: update-demo-nautilus-6nwdt is created but not running
    Jun  9 10:31:30.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  9 10:31:30.996: INFO: stderr: ""
    Jun  9 10:31:30.996: INFO: stdout: "update-demo-nautilus-6nwdt update-demo-nautilus-c9js2 "
    Jun  9 10:31:30.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-6nwdt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  9 10:31:31.104: INFO: stderr: ""
    Jun  9 10:31:31.104: INFO: stdout: ""
    Jun  9 10:31:31.104: INFO: update-demo-nautilus-6nwdt is created but not running
    Jun  9 10:31:36.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  9 10:31:36.201: INFO: stderr: ""
    Jun  9 10:31:36.201: INFO: stdout: "update-demo-nautilus-6nwdt update-demo-nautilus-c9js2 "
    Jun  9 10:31:36.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-6nwdt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  9 10:31:36.295: INFO: stderr: ""
    Jun  9 10:31:36.295: INFO: stdout: "true"
    Jun  9 10:31:36.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-6nwdt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  9 10:31:36.414: INFO: stderr: ""
    Jun  9 10:31:36.414: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  9 10:31:36.414: INFO: validating pod update-demo-nautilus-6nwdt
    Jun  9 10:31:36.475: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  9 10:31:36.476: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  9 10:31:36.476: INFO: update-demo-nautilus-6nwdt is verified up and running
    Jun  9 10:31:36.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-c9js2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  9 10:31:36.586: INFO: stderr: ""
    Jun  9 10:31:36.587: INFO: stdout: "true"
    Jun  9 10:31:36.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-c9js2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  9 10:31:36.679: INFO: stderr: ""
    Jun  9 10:31:36.679: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  9 10:31:36.679: INFO: validating pod update-demo-nautilus-c9js2
    Jun  9 10:31:36.691: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  9 10:31:36.691: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  9 10:31:36.691: INFO: update-demo-nautilus-c9js2 is verified up and running
    STEP: scaling down the replication controller 06/09/23 10:31:36.691
    Jun  9 10:31:36.693: INFO: scanned /root for discovery docs: <nil>
    Jun  9 10:31:36.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jun  9 10:31:37.843: INFO: stderr: ""
    Jun  9 10:31:37.843: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/09/23 10:31:37.843
    Jun  9 10:31:37.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  9 10:31:37.942: INFO: stderr: ""
    Jun  9 10:31:37.942: INFO: stdout: "update-demo-nautilus-c9js2 "
    Jun  9 10:31:37.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-c9js2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  9 10:31:38.027: INFO: stderr: ""
    Jun  9 10:31:38.027: INFO: stdout: "true"
    Jun  9 10:31:38.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-c9js2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  9 10:31:38.109: INFO: stderr: ""
    Jun  9 10:31:38.110: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  9 10:31:38.110: INFO: validating pod update-demo-nautilus-c9js2
    Jun  9 10:31:38.117: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  9 10:31:38.117: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  9 10:31:38.117: INFO: update-demo-nautilus-c9js2 is verified up and running
    STEP: scaling up the replication controller 06/09/23 10:31:38.117
    Jun  9 10:31:38.119: INFO: scanned /root for discovery docs: <nil>
    Jun  9 10:31:38.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jun  9 10:31:39.236: INFO: stderr: ""
    Jun  9 10:31:39.236: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/09/23 10:31:39.236
    Jun  9 10:31:39.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  9 10:31:39.326: INFO: stderr: ""
    Jun  9 10:31:39.326: INFO: stdout: "update-demo-nautilus-c9js2 update-demo-nautilus-t7wmf "
    Jun  9 10:31:39.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-c9js2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  9 10:31:39.411: INFO: stderr: ""
    Jun  9 10:31:39.411: INFO: stdout: "true"
    Jun  9 10:31:39.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-c9js2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  9 10:31:39.496: INFO: stderr: ""
    Jun  9 10:31:39.496: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  9 10:31:39.496: INFO: validating pod update-demo-nautilus-c9js2
    Jun  9 10:31:39.505: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  9 10:31:39.505: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  9 10:31:39.505: INFO: update-demo-nautilus-c9js2 is verified up and running
    Jun  9 10:31:39.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-t7wmf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  9 10:31:39.603: INFO: stderr: ""
    Jun  9 10:31:39.603: INFO: stdout: "true"
    Jun  9 10:31:39.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods update-demo-nautilus-t7wmf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  9 10:31:39.697: INFO: stderr: ""
    Jun  9 10:31:39.697: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  9 10:31:39.697: INFO: validating pod update-demo-nautilus-t7wmf
    Jun  9 10:31:39.707: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  9 10:31:39.707: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  9 10:31:39.707: INFO: update-demo-nautilus-t7wmf is verified up and running
    STEP: using delete to clean up resources 06/09/23 10:31:39.707
    Jun  9 10:31:39.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 delete --grace-period=0 --force -f -'
    Jun  9 10:31:39.799: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  9 10:31:39.799: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jun  9 10:31:39.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get rc,svc -l name=update-demo --no-headers'
    Jun  9 10:31:39.913: INFO: stderr: "No resources found in kubectl-235 namespace.\n"
    Jun  9 10:31:39.913: INFO: stdout: ""
    Jun  9 10:31:39.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-235 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun  9 10:31:40.008: INFO: stderr: ""
    Jun  9 10:31:40.008: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:31:40.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-235" for this suite. 06/09/23 10:31:40.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:31:40.077
Jun  9 10:31:40.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename deployment 06/09/23 10:31:40.082
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:31:40.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:31:40.138
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 06/09/23 10:31:40.152
Jun  9 10:31:40.152: INFO: Creating simple deployment test-deployment-q9s2p
Jun  9 10:31:40.182: INFO: deployment "test-deployment-q9s2p" doesn't have the required revision set
STEP: Getting /status 06/09/23 10:31:42.213
Jun  9 10:31:42.221: INFO: Deployment test-deployment-q9s2p has Conditions: [{Available True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q9s2p-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 06/09/23 10:31:42.221
Jun  9 10:31:42.241: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 31, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 31, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 31, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 31, 40, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-q9s2p-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 06/09/23 10:31:42.241
Jun  9 10:31:42.244: INFO: Observed &Deployment event: ADDED
Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q9s2p-54bc444df"}
Jun  9 10:31:42.245: INFO: Observed &Deployment event: MODIFIED
Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q9s2p-54bc444df"}
Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  9 10:31:42.245: INFO: Observed &Deployment event: MODIFIED
Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-q9s2p-54bc444df" is progressing.}
Jun  9 10:31:42.245: INFO: Observed &Deployment event: MODIFIED
Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q9s2p-54bc444df" has successfully progressed.}
Jun  9 10:31:42.245: INFO: Observed &Deployment event: MODIFIED
Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q9s2p-54bc444df" has successfully progressed.}
Jun  9 10:31:42.245: INFO: Found Deployment test-deployment-q9s2p in namespace deployment-2735 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  9 10:31:42.245: INFO: Deployment test-deployment-q9s2p has an updated status
STEP: patching the Statefulset Status 06/09/23 10:31:42.245
Jun  9 10:31:42.245: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun  9 10:31:42.260: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 06/09/23 10:31:42.26
Jun  9 10:31:42.264: INFO: Observed &Deployment event: ADDED
Jun  9 10:31:42.264: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q9s2p-54bc444df"}
Jun  9 10:31:42.265: INFO: Observed &Deployment event: MODIFIED
Jun  9 10:31:42.267: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q9s2p-54bc444df"}
Jun  9 10:31:42.267: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  9 10:31:42.267: INFO: Observed &Deployment event: MODIFIED
Jun  9 10:31:42.267: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  9 10:31:42.267: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-q9s2p-54bc444df" is progressing.}
Jun  9 10:31:42.267: INFO: Observed &Deployment event: MODIFIED
Jun  9 10:31:42.267: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  9 10:31:42.267: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q9s2p-54bc444df" has successfully progressed.}
Jun  9 10:31:42.268: INFO: Observed &Deployment event: MODIFIED
Jun  9 10:31:42.268: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  9 10:31:42.268: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q9s2p-54bc444df" has successfully progressed.}
Jun  9 10:31:42.268: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  9 10:31:42.268: INFO: Observed &Deployment event: MODIFIED
Jun  9 10:31:42.268: INFO: Found deployment test-deployment-q9s2p in namespace deployment-2735 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jun  9 10:31:42.268: INFO: Deployment test-deployment-q9s2p has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  9 10:31:42.277: INFO: Deployment "test-deployment-q9s2p":
&Deployment{ObjectMeta:{test-deployment-q9s2p  deployment-2735  d2558105-fa32-4ce9-ba06-031e31236f77 75867 1 2023-06-09 10:31:40 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-09 10:31:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-09 10:31:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-09 10:31:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00416f238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-q9s2p-54bc444df",LastUpdateTime:2023-06-09 10:31:42 +0000 UTC,LastTransitionTime:2023-06-09 10:31:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  9 10:31:42.287: INFO: New ReplicaSet "test-deployment-q9s2p-54bc444df" of Deployment "test-deployment-q9s2p":
&ReplicaSet{ObjectMeta:{test-deployment-q9s2p-54bc444df  deployment-2735  405da27b-9728-4b12-98b6-f23a19a818d9 75857 1 2023-06-09 10:31:40 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-q9s2p d2558105-fa32-4ce9-ba06-031e31236f77 0xc00416f620 0xc00416f621}] [] [{kube-controller-manager Update apps/v1 2023-06-09 10:31:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2558105-fa32-4ce9-ba06-031e31236f77\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 10:31:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00416f6c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  9 10:31:42.296: INFO: Pod "test-deployment-q9s2p-54bc444df-d69zb" is available:
&Pod{ObjectMeta:{test-deployment-q9s2p-54bc444df-d69zb test-deployment-q9s2p-54bc444df- deployment-2735  a1fe96b1-b351-4951-967c-6f1f3c18b159 75856 0 2023-06-09 10:31:40 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:87c162e9a09f603008a65e477182d55c5b3a1fe37992e0ffb905d274b5b49d52 cni.projectcalico.org/podIP:172.27.53.87/32 cni.projectcalico.org/podIPs:172.27.53.87/32] [{apps/v1 ReplicaSet test-deployment-q9s2p-54bc444df 405da27b-9728-4b12-98b6-f23a19a818d9 0xc0048064b0 0xc0048064b1}] [] [{calico Update v1 2023-06-09 10:31:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-09 10:31:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"405da27b-9728-4b12-98b6-f23a19a818d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 10:31:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.27.53.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zbnvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zbnvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:31:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:31:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:31:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:31:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.27.53.87,StartTime:2023-06-09 10:31:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 10:31:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://948988908a57b7c53aed0104917be398327eccbbad66e75dea7ed9515227e915,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.27.53.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  9 10:31:42.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2735" for this suite. 06/09/23 10:31:42.305
------------------------------
• [2.242 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:31:40.077
    Jun  9 10:31:40.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename deployment 06/09/23 10:31:40.082
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:31:40.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:31:40.138
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 06/09/23 10:31:40.152
    Jun  9 10:31:40.152: INFO: Creating simple deployment test-deployment-q9s2p
    Jun  9 10:31:40.182: INFO: deployment "test-deployment-q9s2p" doesn't have the required revision set
    STEP: Getting /status 06/09/23 10:31:42.213
    Jun  9 10:31:42.221: INFO: Deployment test-deployment-q9s2p has Conditions: [{Available True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q9s2p-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 06/09/23 10:31:42.221
    Jun  9 10:31:42.241: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 31, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 31, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 10, 31, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 10, 31, 40, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-q9s2p-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 06/09/23 10:31:42.241
    Jun  9 10:31:42.244: INFO: Observed &Deployment event: ADDED
    Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q9s2p-54bc444df"}
    Jun  9 10:31:42.245: INFO: Observed &Deployment event: MODIFIED
    Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q9s2p-54bc444df"}
    Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun  9 10:31:42.245: INFO: Observed &Deployment event: MODIFIED
    Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-q9s2p-54bc444df" is progressing.}
    Jun  9 10:31:42.245: INFO: Observed &Deployment event: MODIFIED
    Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q9s2p-54bc444df" has successfully progressed.}
    Jun  9 10:31:42.245: INFO: Observed &Deployment event: MODIFIED
    Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun  9 10:31:42.245: INFO: Observed Deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q9s2p-54bc444df" has successfully progressed.}
    Jun  9 10:31:42.245: INFO: Found Deployment test-deployment-q9s2p in namespace deployment-2735 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun  9 10:31:42.245: INFO: Deployment test-deployment-q9s2p has an updated status
    STEP: patching the Statefulset Status 06/09/23 10:31:42.245
    Jun  9 10:31:42.245: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun  9 10:31:42.260: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 06/09/23 10:31:42.26
    Jun  9 10:31:42.264: INFO: Observed &Deployment event: ADDED
    Jun  9 10:31:42.264: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q9s2p-54bc444df"}
    Jun  9 10:31:42.265: INFO: Observed &Deployment event: MODIFIED
    Jun  9 10:31:42.267: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q9s2p-54bc444df"}
    Jun  9 10:31:42.267: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun  9 10:31:42.267: INFO: Observed &Deployment event: MODIFIED
    Jun  9 10:31:42.267: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun  9 10:31:42.267: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:40 +0000 UTC 2023-06-09 10:31:40 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-q9s2p-54bc444df" is progressing.}
    Jun  9 10:31:42.267: INFO: Observed &Deployment event: MODIFIED
    Jun  9 10:31:42.267: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun  9 10:31:42.267: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q9s2p-54bc444df" has successfully progressed.}
    Jun  9 10:31:42.268: INFO: Observed &Deployment event: MODIFIED
    Jun  9 10:31:42.268: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun  9 10:31:42.268: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-09 10:31:41 +0000 UTC 2023-06-09 10:31:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q9s2p-54bc444df" has successfully progressed.}
    Jun  9 10:31:42.268: INFO: Observed deployment test-deployment-q9s2p in namespace deployment-2735 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun  9 10:31:42.268: INFO: Observed &Deployment event: MODIFIED
    Jun  9 10:31:42.268: INFO: Found deployment test-deployment-q9s2p in namespace deployment-2735 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jun  9 10:31:42.268: INFO: Deployment test-deployment-q9s2p has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  9 10:31:42.277: INFO: Deployment "test-deployment-q9s2p":
    &Deployment{ObjectMeta:{test-deployment-q9s2p  deployment-2735  d2558105-fa32-4ce9-ba06-031e31236f77 75867 1 2023-06-09 10:31:40 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-09 10:31:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-09 10:31:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-09 10:31:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00416f238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-q9s2p-54bc444df",LastUpdateTime:2023-06-09 10:31:42 +0000 UTC,LastTransitionTime:2023-06-09 10:31:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun  9 10:31:42.287: INFO: New ReplicaSet "test-deployment-q9s2p-54bc444df" of Deployment "test-deployment-q9s2p":
    &ReplicaSet{ObjectMeta:{test-deployment-q9s2p-54bc444df  deployment-2735  405da27b-9728-4b12-98b6-f23a19a818d9 75857 1 2023-06-09 10:31:40 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-q9s2p d2558105-fa32-4ce9-ba06-031e31236f77 0xc00416f620 0xc00416f621}] [] [{kube-controller-manager Update apps/v1 2023-06-09 10:31:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2558105-fa32-4ce9-ba06-031e31236f77\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 10:31:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00416f6c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun  9 10:31:42.296: INFO: Pod "test-deployment-q9s2p-54bc444df-d69zb" is available:
    &Pod{ObjectMeta:{test-deployment-q9s2p-54bc444df-d69zb test-deployment-q9s2p-54bc444df- deployment-2735  a1fe96b1-b351-4951-967c-6f1f3c18b159 75856 0 2023-06-09 10:31:40 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:87c162e9a09f603008a65e477182d55c5b3a1fe37992e0ffb905d274b5b49d52 cni.projectcalico.org/podIP:172.27.53.87/32 cni.projectcalico.org/podIPs:172.27.53.87/32] [{apps/v1 ReplicaSet test-deployment-q9s2p-54bc444df 405da27b-9728-4b12-98b6-f23a19a818d9 0xc0048064b0 0xc0048064b1}] [] [{calico Update v1 2023-06-09 10:31:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-09 10:31:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"405da27b-9728-4b12-98b6-f23a19a818d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 10:31:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.27.53.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zbnvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zbnvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:31:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:31:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:31:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:31:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.27.53.87,StartTime:2023-06-09 10:31:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 10:31:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://948988908a57b7c53aed0104917be398327eccbbad66e75dea7ed9515227e915,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.27.53.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:31:42.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2735" for this suite. 06/09/23 10:31:42.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:31:42.321
Jun  9 10:31:42.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename gc 06/09/23 10:31:42.323
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:31:42.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:31:42.353
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 06/09/23 10:31:42.359
STEP: Wait for the Deployment to create new ReplicaSet 06/09/23 10:31:42.382
STEP: delete the deployment 06/09/23 10:31:42.505
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 06/09/23 10:31:42.524
STEP: Gathering metrics 06/09/23 10:31:43.085
Jun  9 10:31:43.147: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" in namespace "kube-system" to be "running and ready"
Jun  9 10:31:43.171: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh": Phase="Running", Reason="", readiness=true. Elapsed: 24.200921ms
Jun  9 10:31:43.171: INFO: The phase of Pod kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh is Running (Ready = true)
Jun  9 10:31:43.171: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" satisfied condition "running and ready"
Jun  9 10:31:43.241: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun  9 10:31:43.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7809" for this suite. 06/09/23 10:31:43.25
------------------------------
• [0.954 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:31:42.321
    Jun  9 10:31:42.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename gc 06/09/23 10:31:42.323
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:31:42.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:31:42.353
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 06/09/23 10:31:42.359
    STEP: Wait for the Deployment to create new ReplicaSet 06/09/23 10:31:42.382
    STEP: delete the deployment 06/09/23 10:31:42.505
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 06/09/23 10:31:42.524
    STEP: Gathering metrics 06/09/23 10:31:43.085
    Jun  9 10:31:43.147: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" in namespace "kube-system" to be "running and ready"
    Jun  9 10:31:43.171: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh": Phase="Running", Reason="", readiness=true. Elapsed: 24.200921ms
    Jun  9 10:31:43.171: INFO: The phase of Pod kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh is Running (Ready = true)
    Jun  9 10:31:43.171: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" satisfied condition "running and ready"
    Jun  9 10:31:43.241: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:31:43.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7809" for this suite. 06/09/23 10:31:43.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:31:43.276
Jun  9 10:31:43.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 10:31:43.278
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:31:43.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:31:43.329
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 06/09/23 10:31:43.334
Jun  9 10:31:43.357: INFO: Waiting up to 5m0s for pod "downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f" in namespace "downward-api-9575" to be "Succeeded or Failed"
Jun  9 10:31:43.365: INFO: Pod "downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.962007ms
Jun  9 10:31:45.372: INFO: Pod "downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014586399s
Jun  9 10:31:47.379: INFO: Pod "downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022213427s
STEP: Saw pod success 06/09/23 10:31:47.379
Jun  9 10:31:47.379: INFO: Pod "downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f" satisfied condition "Succeeded or Failed"
Jun  9 10:31:47.393: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f container dapi-container: <nil>
STEP: delete the pod 06/09/23 10:31:47.423
Jun  9 10:31:47.453: INFO: Waiting for pod downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f to disappear
Jun  9 10:31:47.467: INFO: Pod downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun  9 10:31:47.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9575" for this suite. 06/09/23 10:31:47.483
------------------------------
• [4.225 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:31:43.276
    Jun  9 10:31:43.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 10:31:43.278
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:31:43.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:31:43.329
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 06/09/23 10:31:43.334
    Jun  9 10:31:43.357: INFO: Waiting up to 5m0s for pod "downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f" in namespace "downward-api-9575" to be "Succeeded or Failed"
    Jun  9 10:31:43.365: INFO: Pod "downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.962007ms
    Jun  9 10:31:45.372: INFO: Pod "downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014586399s
    Jun  9 10:31:47.379: INFO: Pod "downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022213427s
    STEP: Saw pod success 06/09/23 10:31:47.379
    Jun  9 10:31:47.379: INFO: Pod "downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f" satisfied condition "Succeeded or Failed"
    Jun  9 10:31:47.393: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f container dapi-container: <nil>
    STEP: delete the pod 06/09/23 10:31:47.423
    Jun  9 10:31:47.453: INFO: Waiting for pod downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f to disappear
    Jun  9 10:31:47.467: INFO: Pod downward-api-fad29079-7a99-4c91-9423-2fd2b7b28e6f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:31:47.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9575" for this suite. 06/09/23 10:31:47.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:31:47.502
Jun  9 10:31:47.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename ingressclass 06/09/23 10:31:47.503
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:31:47.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:31:47.55
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 06/09/23 10:31:47.557
STEP: getting /apis/networking.k8s.io 06/09/23 10:31:47.563
STEP: getting /apis/networking.k8s.iov1 06/09/23 10:31:47.566
STEP: creating 06/09/23 10:31:47.57
STEP: getting 06/09/23 10:31:47.619
STEP: listing 06/09/23 10:31:47.64
STEP: watching 06/09/23 10:31:47.66
Jun  9 10:31:47.660: INFO: starting watch
STEP: patching 06/09/23 10:31:47.663
STEP: updating 06/09/23 10:31:47.677
Jun  9 10:31:47.691: INFO: waiting for watch events with expected annotations
Jun  9 10:31:47.692: INFO: saw patched and updated annotations
STEP: deleting 06/09/23 10:31:47.692
STEP: deleting a collection 06/09/23 10:31:47.732
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jun  9 10:31:47.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-3114" for this suite. 06/09/23 10:31:47.807
------------------------------
• [0.320 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:31:47.502
    Jun  9 10:31:47.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename ingressclass 06/09/23 10:31:47.503
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:31:47.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:31:47.55
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 06/09/23 10:31:47.557
    STEP: getting /apis/networking.k8s.io 06/09/23 10:31:47.563
    STEP: getting /apis/networking.k8s.iov1 06/09/23 10:31:47.566
    STEP: creating 06/09/23 10:31:47.57
    STEP: getting 06/09/23 10:31:47.619
    STEP: listing 06/09/23 10:31:47.64
    STEP: watching 06/09/23 10:31:47.66
    Jun  9 10:31:47.660: INFO: starting watch
    STEP: patching 06/09/23 10:31:47.663
    STEP: updating 06/09/23 10:31:47.677
    Jun  9 10:31:47.691: INFO: waiting for watch events with expected annotations
    Jun  9 10:31:47.692: INFO: saw patched and updated annotations
    STEP: deleting 06/09/23 10:31:47.692
    STEP: deleting a collection 06/09/23 10:31:47.732
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:31:47.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-3114" for this suite. 06/09/23 10:31:47.807
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:31:47.822
Jun  9 10:31:47.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:31:47.826
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:31:47.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:31:47.924
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-237c8e8c-fa55-4e29-8fd5-e0ba9ab0aaa9 06/09/23 10:31:47.929
STEP: Creating a pod to test consume configMaps 06/09/23 10:31:47.938
Jun  9 10:31:47.949: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d" in namespace "projected-3518" to be "Succeeded or Failed"
Jun  9 10:31:47.962: INFO: Pod "pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.236062ms
Jun  9 10:31:49.968: INFO: Pod "pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01889697s
Jun  9 10:31:51.978: INFO: Pod "pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028051511s
STEP: Saw pod success 06/09/23 10:31:51.978
Jun  9 10:31:51.978: INFO: Pod "pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d" satisfied condition "Succeeded or Failed"
Jun  9 10:31:51.985: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d container agnhost-container: <nil>
STEP: delete the pod 06/09/23 10:31:51.998
Jun  9 10:31:52.029: INFO: Waiting for pod pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d to disappear
Jun  9 10:31:52.035: INFO: Pod pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  9 10:31:52.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3518" for this suite. 06/09/23 10:31:52.046
------------------------------
• [4.245 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:31:47.822
    Jun  9 10:31:47.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:31:47.826
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:31:47.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:31:47.924
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-237c8e8c-fa55-4e29-8fd5-e0ba9ab0aaa9 06/09/23 10:31:47.929
    STEP: Creating a pod to test consume configMaps 06/09/23 10:31:47.938
    Jun  9 10:31:47.949: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d" in namespace "projected-3518" to be "Succeeded or Failed"
    Jun  9 10:31:47.962: INFO: Pod "pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.236062ms
    Jun  9 10:31:49.968: INFO: Pod "pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01889697s
    Jun  9 10:31:51.978: INFO: Pod "pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028051511s
    STEP: Saw pod success 06/09/23 10:31:51.978
    Jun  9 10:31:51.978: INFO: Pod "pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d" satisfied condition "Succeeded or Failed"
    Jun  9 10:31:51.985: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 10:31:51.998
    Jun  9 10:31:52.029: INFO: Waiting for pod pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d to disappear
    Jun  9 10:31:52.035: INFO: Pod pod-projected-configmaps-ef582cd5-45cd-4f2c-8fb6-b5512670119d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:31:52.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3518" for this suite. 06/09/23 10:31:52.046
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:31:52.067
Jun  9 10:31:52.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename sched-preemption 06/09/23 10:31:52.069
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:31:52.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:31:52.099
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jun  9 10:31:52.135: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  9 10:32:52.260: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 06/09/23 10:32:52.296
Jun  9 10:32:52.395: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun  9 10:32:52.423: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun  9 10:32:52.465: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun  9 10:32:52.482: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun  9 10:32:52.524: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun  9 10:32:52.549: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 06/09/23 10:32:52.549
Jun  9 10:32:52.550: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1264" to be "running"
Jun  9 10:32:52.566: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 16.297539ms
Jun  9 10:32:54.577: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.026984416s
Jun  9 10:32:54.577: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jun  9 10:32:54.577: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1264" to be "running"
Jun  9 10:32:54.583: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.384935ms
Jun  9 10:32:54.583: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jun  9 10:32:54.583: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1264" to be "running"
Jun  9 10:32:54.589: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.988729ms
Jun  9 10:32:54.589: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jun  9 10:32:54.589: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1264" to be "running"
Jun  9 10:32:54.595: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.227675ms
Jun  9 10:32:54.595: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jun  9 10:32:54.595: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1264" to be "running"
Jun  9 10:32:54.601: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.967431ms
Jun  9 10:32:54.601: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jun  9 10:32:54.601: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1264" to be "running"
Jun  9 10:32:54.607: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.311959ms
Jun  9 10:32:56.614: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.01217335s
Jun  9 10:32:56.614: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 06/09/23 10:32:56.614
Jun  9 10:32:56.626: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-1264" to be "running"
Jun  9 10:32:56.633: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.401597ms
Jun  9 10:32:58.640: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014128407s
Jun  9 10:33:00.641: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014961148s
Jun  9 10:33:02.643: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.016462104s
Jun  9 10:33:02.643: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:33:02.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1264" for this suite. 06/09/23 10:33:02.819
------------------------------
• [SLOW TEST] [70.772 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:31:52.067
    Jun  9 10:31:52.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename sched-preemption 06/09/23 10:31:52.069
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:31:52.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:31:52.099
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jun  9 10:31:52.135: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun  9 10:32:52.260: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 06/09/23 10:32:52.296
    Jun  9 10:32:52.395: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jun  9 10:32:52.423: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jun  9 10:32:52.465: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jun  9 10:32:52.482: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jun  9 10:32:52.524: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jun  9 10:32:52.549: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 06/09/23 10:32:52.549
    Jun  9 10:32:52.550: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1264" to be "running"
    Jun  9 10:32:52.566: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 16.297539ms
    Jun  9 10:32:54.577: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.026984416s
    Jun  9 10:32:54.577: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jun  9 10:32:54.577: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1264" to be "running"
    Jun  9 10:32:54.583: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.384935ms
    Jun  9 10:32:54.583: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun  9 10:32:54.583: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1264" to be "running"
    Jun  9 10:32:54.589: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.988729ms
    Jun  9 10:32:54.589: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun  9 10:32:54.589: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1264" to be "running"
    Jun  9 10:32:54.595: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.227675ms
    Jun  9 10:32:54.595: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun  9 10:32:54.595: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1264" to be "running"
    Jun  9 10:32:54.601: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.967431ms
    Jun  9 10:32:54.601: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun  9 10:32:54.601: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1264" to be "running"
    Jun  9 10:32:54.607: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.311959ms
    Jun  9 10:32:56.614: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.01217335s
    Jun  9 10:32:56.614: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 06/09/23 10:32:56.614
    Jun  9 10:32:56.626: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-1264" to be "running"
    Jun  9 10:32:56.633: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.401597ms
    Jun  9 10:32:58.640: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014128407s
    Jun  9 10:33:00.641: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014961148s
    Jun  9 10:33:02.643: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.016462104s
    Jun  9 10:33:02.643: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:33:02.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1264" for this suite. 06/09/23 10:33:02.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:33:02.84
Jun  9 10:33:02.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 10:33:02.842
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:02.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:02.943
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 06/09/23 10:33:02.948
Jun  9 10:33:02.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2248 cluster-info'
Jun  9 10:33:03.055: INFO: stderr: ""
Jun  9 10:33:03.055: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 10:33:03.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2248" for this suite. 06/09/23 10:33:03.065
------------------------------
• [0.244 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:33:02.84
    Jun  9 10:33:02.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 10:33:02.842
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:02.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:02.943
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 06/09/23 10:33:02.948
    Jun  9 10:33:02.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2248 cluster-info'
    Jun  9 10:33:03.055: INFO: stderr: ""
    Jun  9 10:33:03.055: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:33:03.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2248" for this suite. 06/09/23 10:33:03.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:33:03.086
Jun  9 10:33:03.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:33:03.087
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:03.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:03.119
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 06/09/23 10:33:03.126
Jun  9 10:33:03.145: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030" in namespace "projected-3688" to be "Succeeded or Failed"
Jun  9 10:33:03.159: INFO: Pod "downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030": Phase="Pending", Reason="", readiness=false. Elapsed: 13.767233ms
Jun  9 10:33:05.174: INFO: Pod "downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028728259s
Jun  9 10:33:07.166: INFO: Pod "downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021072369s
STEP: Saw pod success 06/09/23 10:33:07.166
Jun  9 10:33:07.167: INFO: Pod "downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030" satisfied condition "Succeeded or Failed"
Jun  9 10:33:07.174: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030 container client-container: <nil>
STEP: delete the pod 06/09/23 10:33:07.195
Jun  9 10:33:07.283: INFO: Waiting for pod downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030 to disappear
Jun  9 10:33:07.296: INFO: Pod downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  9 10:33:07.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3688" for this suite. 06/09/23 10:33:07.312
------------------------------
• [4.238 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:33:03.086
    Jun  9 10:33:03.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:33:03.087
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:03.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:03.119
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 06/09/23 10:33:03.126
    Jun  9 10:33:03.145: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030" in namespace "projected-3688" to be "Succeeded or Failed"
    Jun  9 10:33:03.159: INFO: Pod "downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030": Phase="Pending", Reason="", readiness=false. Elapsed: 13.767233ms
    Jun  9 10:33:05.174: INFO: Pod "downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028728259s
    Jun  9 10:33:07.166: INFO: Pod "downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021072369s
    STEP: Saw pod success 06/09/23 10:33:07.166
    Jun  9 10:33:07.167: INFO: Pod "downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030" satisfied condition "Succeeded or Failed"
    Jun  9 10:33:07.174: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030 container client-container: <nil>
    STEP: delete the pod 06/09/23 10:33:07.195
    Jun  9 10:33:07.283: INFO: Waiting for pod downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030 to disappear
    Jun  9 10:33:07.296: INFO: Pod downwardapi-volume-67030484-ee39-41c2-bc0c-43251309a030 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:33:07.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3688" for this suite. 06/09/23 10:33:07.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:33:07.328
Jun  9 10:33:07.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename containers 06/09/23 10:33:07.329
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:07.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:07.374
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Jun  9 10:33:07.409: INFO: Waiting up to 5m0s for pod "client-containers-503f4d11-dae4-4519-b701-f28ed90e8db5" in namespace "containers-4443" to be "running"
Jun  9 10:33:07.419: INFO: Pod "client-containers-503f4d11-dae4-4519-b701-f28ed90e8db5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.131821ms
Jun  9 10:33:09.432: INFO: Pod "client-containers-503f4d11-dae4-4519-b701-f28ed90e8db5": Phase="Running", Reason="", readiness=true. Elapsed: 2.022911055s
Jun  9 10:33:09.432: INFO: Pod "client-containers-503f4d11-dae4-4519-b701-f28ed90e8db5" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jun  9 10:33:09.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4443" for this suite. 06/09/23 10:33:09.453
------------------------------
• [2.138 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:33:07.328
    Jun  9 10:33:07.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename containers 06/09/23 10:33:07.329
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:07.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:07.374
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Jun  9 10:33:07.409: INFO: Waiting up to 5m0s for pod "client-containers-503f4d11-dae4-4519-b701-f28ed90e8db5" in namespace "containers-4443" to be "running"
    Jun  9 10:33:07.419: INFO: Pod "client-containers-503f4d11-dae4-4519-b701-f28ed90e8db5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.131821ms
    Jun  9 10:33:09.432: INFO: Pod "client-containers-503f4d11-dae4-4519-b701-f28ed90e8db5": Phase="Running", Reason="", readiness=true. Elapsed: 2.022911055s
    Jun  9 10:33:09.432: INFO: Pod "client-containers-503f4d11-dae4-4519-b701-f28ed90e8db5" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:33:09.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4443" for this suite. 06/09/23 10:33:09.453
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:33:09.466
Jun  9 10:33:09.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename daemonsets 06/09/23 10:33:09.468
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:09.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:09.5
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 06/09/23 10:33:09.561
STEP: Check that daemon pods launch on every node of the cluster. 06/09/23 10:33:09.612
Jun  9 10:33:09.624: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:09.624: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:09.624: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:09.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:33:09.641: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:33:10.668: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:10.668: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:10.668: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:10.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:33:10.685: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:33:11.692: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:11.692: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:11.692: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:11.707: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  9 10:33:11.707: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
Jun  9 10:33:12.665: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:12.665: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:12.665: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:12.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  9 10:33:12.681: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 06/09/23 10:33:12.696
Jun  9 10:33:12.765: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:12.765: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:12.765: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:12.781: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  9 10:33:12.781: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:33:13.792: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:13.792: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:13.792: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:13.802: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  9 10:33:13.802: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:33:14.798: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:14.798: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:14.798: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:14.811: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  9 10:33:14.811: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:33:15.801: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:15.801: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:15.801: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:15.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  9 10:33:15.815: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:33:16.789: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:16.789: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:16.789: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:33:16.795: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  9 10:33:16.795: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/09/23 10:33:16.801
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-212, will wait for the garbage collector to delete the pods 06/09/23 10:33:16.801
Jun  9 10:33:16.870: INFO: Deleting DaemonSet.extensions daemon-set took: 12.805065ms
Jun  9 10:33:16.972: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.903956ms
Jun  9 10:33:18.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:33:18.982: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  9 10:33:18.989: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"76823"},"items":null}

Jun  9 10:33:18.995: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"76823"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:33:19.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-212" for this suite. 06/09/23 10:33:19.041
------------------------------
• [SLOW TEST] [9.592 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:33:09.466
    Jun  9 10:33:09.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename daemonsets 06/09/23 10:33:09.468
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:09.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:09.5
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 06/09/23 10:33:09.561
    STEP: Check that daemon pods launch on every node of the cluster. 06/09/23 10:33:09.612
    Jun  9 10:33:09.624: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:09.624: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:09.624: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:09.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:33:09.641: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:33:10.668: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:10.668: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:10.668: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:10.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:33:10.685: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:33:11.692: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:11.692: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:11.692: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:11.707: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun  9 10:33:11.707: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
    Jun  9 10:33:12.665: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:12.665: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:12.665: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:12.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun  9 10:33:12.681: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 06/09/23 10:33:12.696
    Jun  9 10:33:12.765: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:12.765: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:12.765: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:12.781: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun  9 10:33:12.781: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:33:13.792: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:13.792: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:13.792: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:13.802: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun  9 10:33:13.802: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:33:14.798: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:14.798: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:14.798: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:14.811: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun  9 10:33:14.811: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:33:15.801: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:15.801: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:15.801: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:15.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun  9 10:33:15.815: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:33:16.789: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:16.789: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:16.789: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:33:16.795: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun  9 10:33:16.795: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/09/23 10:33:16.801
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-212, will wait for the garbage collector to delete the pods 06/09/23 10:33:16.801
    Jun  9 10:33:16.870: INFO: Deleting DaemonSet.extensions daemon-set took: 12.805065ms
    Jun  9 10:33:16.972: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.903956ms
    Jun  9 10:33:18.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:33:18.982: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun  9 10:33:18.989: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"76823"},"items":null}

    Jun  9 10:33:18.995: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"76823"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:33:19.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-212" for this suite. 06/09/23 10:33:19.041
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:33:19.06
Jun  9 10:33:19.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 10:33:19.065
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:19.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:19.105
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 10:33:19.196
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:33:19.791
STEP: Deploying the webhook pod 06/09/23 10:33:19.807
STEP: Wait for the deployment to be ready 06/09/23 10:33:19.868
Jun  9 10:33:19.884: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 10:33:21.907
STEP: Verifying the service has paired with the endpoint 06/09/23 10:33:21.933
Jun  9 10:33:22.934: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jun  9 10:33:22.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Registering the custom resource webhook via the AdmissionRegistration API 06/09/23 10:33:23.458
STEP: Creating a custom resource that should be denied by the webhook 06/09/23 10:33:23.507
STEP: Creating a custom resource whose deletion would be denied by the webhook 06/09/23 10:33:25.569
STEP: Updating the custom resource with disallowed data should be denied 06/09/23 10:33:25.582
STEP: Deleting the custom resource should be denied 06/09/23 10:33:25.602
STEP: Remove the offending key and value from the custom resource data 06/09/23 10:33:25.616
STEP: Deleting the updated custom resource should be successful 06/09/23 10:33:25.64
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:33:26.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2925" for this suite. 06/09/23 10:33:26.331
STEP: Destroying namespace "webhook-2925-markers" for this suite. 06/09/23 10:33:26.365
------------------------------
• [SLOW TEST] [7.348 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:33:19.06
    Jun  9 10:33:19.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 10:33:19.065
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:19.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:19.105
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 10:33:19.196
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:33:19.791
    STEP: Deploying the webhook pod 06/09/23 10:33:19.807
    STEP: Wait for the deployment to be ready 06/09/23 10:33:19.868
    Jun  9 10:33:19.884: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 10:33:21.907
    STEP: Verifying the service has paired with the endpoint 06/09/23 10:33:21.933
    Jun  9 10:33:22.934: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jun  9 10:33:22.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 06/09/23 10:33:23.458
    STEP: Creating a custom resource that should be denied by the webhook 06/09/23 10:33:23.507
    STEP: Creating a custom resource whose deletion would be denied by the webhook 06/09/23 10:33:25.569
    STEP: Updating the custom resource with disallowed data should be denied 06/09/23 10:33:25.582
    STEP: Deleting the custom resource should be denied 06/09/23 10:33:25.602
    STEP: Remove the offending key and value from the custom resource data 06/09/23 10:33:25.616
    STEP: Deleting the updated custom resource should be successful 06/09/23 10:33:25.64
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:33:26.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2925" for this suite. 06/09/23 10:33:26.331
    STEP: Destroying namespace "webhook-2925-markers" for this suite. 06/09/23 10:33:26.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:33:26.409
Jun  9 10:33:26.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-probe 06/09/23 10:33:26.41
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:26.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:26.481
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Jun  9 10:33:26.541: INFO: Waiting up to 5m0s for pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982" in namespace "container-probe-6668" to be "running and ready"
Jun  9 10:33:26.556: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Pending", Reason="", readiness=false. Elapsed: 15.06663ms
Jun  9 10:33:26.556: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:33:28.565: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 2.024520797s
Jun  9 10:33:28.565: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
Jun  9 10:33:30.563: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 4.022578046s
Jun  9 10:33:30.563: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
Jun  9 10:33:32.569: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 6.028182731s
Jun  9 10:33:32.569: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
Jun  9 10:33:34.563: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 8.021756103s
Jun  9 10:33:34.563: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
Jun  9 10:33:36.562: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 10.021748172s
Jun  9 10:33:36.563: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
Jun  9 10:33:38.564: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 12.023392121s
Jun  9 10:33:38.564: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
Jun  9 10:33:40.567: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 14.026212954s
Jun  9 10:33:40.567: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
Jun  9 10:33:42.563: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 16.021784261s
Jun  9 10:33:42.563: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
Jun  9 10:33:44.563: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 18.022103244s
Jun  9 10:33:44.563: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
Jun  9 10:33:46.563: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 20.022209902s
Jun  9 10:33:46.563: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
Jun  9 10:33:48.564: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=true. Elapsed: 22.023506212s
Jun  9 10:33:48.564: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = true)
Jun  9 10:33:48.564: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982" satisfied condition "running and ready"
Jun  9 10:33:48.575: INFO: Container started at 2023-06-09 10:33:27 +0000 UTC, pod became ready at 2023-06-09 10:33:46 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  9 10:33:48.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6668" for this suite. 06/09/23 10:33:48.585
------------------------------
• [SLOW TEST] [22.190 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:33:26.409
    Jun  9 10:33:26.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-probe 06/09/23 10:33:26.41
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:26.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:26.481
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Jun  9 10:33:26.541: INFO: Waiting up to 5m0s for pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982" in namespace "container-probe-6668" to be "running and ready"
    Jun  9 10:33:26.556: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Pending", Reason="", readiness=false. Elapsed: 15.06663ms
    Jun  9 10:33:26.556: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:33:28.565: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 2.024520797s
    Jun  9 10:33:28.565: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
    Jun  9 10:33:30.563: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 4.022578046s
    Jun  9 10:33:30.563: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
    Jun  9 10:33:32.569: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 6.028182731s
    Jun  9 10:33:32.569: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
    Jun  9 10:33:34.563: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 8.021756103s
    Jun  9 10:33:34.563: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
    Jun  9 10:33:36.562: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 10.021748172s
    Jun  9 10:33:36.563: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
    Jun  9 10:33:38.564: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 12.023392121s
    Jun  9 10:33:38.564: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
    Jun  9 10:33:40.567: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 14.026212954s
    Jun  9 10:33:40.567: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
    Jun  9 10:33:42.563: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 16.021784261s
    Jun  9 10:33:42.563: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
    Jun  9 10:33:44.563: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 18.022103244s
    Jun  9 10:33:44.563: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
    Jun  9 10:33:46.563: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=false. Elapsed: 20.022209902s
    Jun  9 10:33:46.563: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = false)
    Jun  9 10:33:48.564: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982": Phase="Running", Reason="", readiness=true. Elapsed: 22.023506212s
    Jun  9 10:33:48.564: INFO: The phase of Pod test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982 is Running (Ready = true)
    Jun  9 10:33:48.564: INFO: Pod "test-webserver-76ee6dab-ef68-4c63-a8c0-291c68a2a982" satisfied condition "running and ready"
    Jun  9 10:33:48.575: INFO: Container started at 2023-06-09 10:33:27 +0000 UTC, pod became ready at 2023-06-09 10:33:46 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:33:48.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6668" for this suite. 06/09/23 10:33:48.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:33:48.6
Jun  9 10:33:48.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename custom-resource-definition 06/09/23 10:33:48.604
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:48.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:48.647
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jun  9 10:33:48.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:33:55.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2472" for this suite. 06/09/23 10:33:55.426
------------------------------
• [SLOW TEST] [6.872 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:33:48.6
    Jun  9 10:33:48.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename custom-resource-definition 06/09/23 10:33:48.604
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:48.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:48.647
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jun  9 10:33:48.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:33:55.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2472" for this suite. 06/09/23 10:33:55.426
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:33:55.473
Jun  9 10:33:55.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename init-container 06/09/23 10:33:55.474
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:55.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:55.532
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 06/09/23 10:33:55.544
Jun  9 10:33:55.544: INFO: PodSpec: initContainers in spec.initContainers
Jun  9 10:34:38.289: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-dfc49101-f2e2-4800-9fc8-f9534cc006b5", GenerateName:"", Namespace:"init-container-7348", SelfLink:"", UID:"97d96c7e-7d97-4a70-a3f3-36541f1958c5", ResourceVersion:"77390", Generation:0, CreationTimestamp:time.Date(2023, time.June, 9, 10, 33, 55, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"544773582"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"1c140672517b376385efeeab637dbc0cb0372269ce16c8f6ac81668d3d5fc8f4", "cni.projectcalico.org/podIP":"172.27.53.97/32", "cni.projectcalico.org/podIPs":"172.27.53.97/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 9, 10, 33, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013958d8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 9, 10, 33, 56, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001395908), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 9, 10, 34, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001395938), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-4fh9k", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00474a7a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4fh9k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4fh9k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4fh9k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00263c628), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"sks-test-v1-26.4-workergroup-qdprq", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0007c8930), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00263cba0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00263cbc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00263cbc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00263cbcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0010ead20), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 9, 10, 33, 55, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 9, 10, 33, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 9, 10, 33, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 9, 10, 33, 55, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.255.64.103", PodIP:"172.27.53.97", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.27.53.97"}}, StartTime:time.Date(2023, time.June, 9, 10, 33, 55, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0007c8a10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0007c8a80)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://ad9726d05e7b8234c81aaf96abcbb92348cf8a2015af4046d44243c948dce3c1", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00474a820), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00474a800), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00263cc4f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:34:38.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7348" for this suite. 06/09/23 10:34:38.303
------------------------------
• [SLOW TEST] [42.847 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:33:55.473
    Jun  9 10:33:55.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename init-container 06/09/23 10:33:55.474
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:33:55.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:33:55.532
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 06/09/23 10:33:55.544
    Jun  9 10:33:55.544: INFO: PodSpec: initContainers in spec.initContainers
    Jun  9 10:34:38.289: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-dfc49101-f2e2-4800-9fc8-f9534cc006b5", GenerateName:"", Namespace:"init-container-7348", SelfLink:"", UID:"97d96c7e-7d97-4a70-a3f3-36541f1958c5", ResourceVersion:"77390", Generation:0, CreationTimestamp:time.Date(2023, time.June, 9, 10, 33, 55, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"544773582"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"1c140672517b376385efeeab637dbc0cb0372269ce16c8f6ac81668d3d5fc8f4", "cni.projectcalico.org/podIP":"172.27.53.97/32", "cni.projectcalico.org/podIPs":"172.27.53.97/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 9, 10, 33, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013958d8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 9, 10, 33, 56, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001395908), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 9, 10, 34, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001395938), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-4fh9k", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00474a7a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4fh9k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4fh9k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4fh9k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00263c628), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"sks-test-v1-26.4-workergroup-qdprq", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0007c8930), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00263cba0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00263cbc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00263cbc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00263cbcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0010ead20), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 9, 10, 33, 55, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 9, 10, 33, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 9, 10, 33, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 9, 10, 33, 55, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.255.64.103", PodIP:"172.27.53.97", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.27.53.97"}}, StartTime:time.Date(2023, time.June, 9, 10, 33, 55, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0007c8a10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0007c8a80)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://ad9726d05e7b8234c81aaf96abcbb92348cf8a2015af4046d44243c948dce3c1", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00474a820), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00474a800), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00263cc4f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:34:38.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7348" for this suite. 06/09/23 10:34:38.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:34:38.324
Jun  9 10:34:38.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 10:34:38.326
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:34:38.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:34:38.362
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-8064 06/09/23 10:34:38.372
STEP: creating service affinity-clusterip in namespace services-8064 06/09/23 10:34:38.372
STEP: creating replication controller affinity-clusterip in namespace services-8064 06/09/23 10:34:38.424
I0609 10:34:38.492760      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8064, replica count: 3
I0609 10:34:41.543529      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 10:34:41.557: INFO: Creating new exec pod
Jun  9 10:34:41.572: INFO: Waiting up to 5m0s for pod "execpod-affinity572nq" in namespace "services-8064" to be "running"
Jun  9 10:34:41.597: INFO: Pod "execpod-affinity572nq": Phase="Pending", Reason="", readiness=false. Elapsed: 25.076778ms
Jun  9 10:34:43.622: INFO: Pod "execpod-affinity572nq": Phase="Running", Reason="", readiness=true. Elapsed: 2.049948819s
Jun  9 10:34:43.622: INFO: Pod "execpod-affinity572nq" satisfied condition "running"
Jun  9 10:34:44.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-8064 exec execpod-affinity572nq -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jun  9 10:34:44.837: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun  9 10:34:44.837: INFO: stdout: ""
Jun  9 10:34:44.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-8064 exec execpod-affinity572nq -- /bin/sh -x -c nc -v -z -w 2 10.101.122.138 80'
Jun  9 10:34:45.072: INFO: stderr: "+ nc -v -z -w 2 10.101.122.138 80\nConnection to 10.101.122.138 80 port [tcp/http] succeeded!\n"
Jun  9 10:34:45.072: INFO: stdout: ""
Jun  9 10:34:45.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-8064 exec execpod-affinity572nq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.122.138:80/ ; done'
Jun  9 10:34:45.350: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n"
Jun  9 10:34:45.350: INFO: stdout: "\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d"
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
Jun  9 10:34:45.350: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-8064, will wait for the garbage collector to delete the pods 06/09/23 10:34:45.445
Jun  9 10:34:45.744: INFO: Deleting ReplicationController affinity-clusterip took: 242.737749ms
Jun  9 10:34:46.146: INFO: Terminating ReplicationController affinity-clusterip pods took: 401.643988ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 10:34:49.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8064" for this suite. 06/09/23 10:34:49.409
------------------------------
• [SLOW TEST] [11.132 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:34:38.324
    Jun  9 10:34:38.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 10:34:38.326
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:34:38.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:34:38.362
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-8064 06/09/23 10:34:38.372
    STEP: creating service affinity-clusterip in namespace services-8064 06/09/23 10:34:38.372
    STEP: creating replication controller affinity-clusterip in namespace services-8064 06/09/23 10:34:38.424
    I0609 10:34:38.492760      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8064, replica count: 3
    I0609 10:34:41.543529      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  9 10:34:41.557: INFO: Creating new exec pod
    Jun  9 10:34:41.572: INFO: Waiting up to 5m0s for pod "execpod-affinity572nq" in namespace "services-8064" to be "running"
    Jun  9 10:34:41.597: INFO: Pod "execpod-affinity572nq": Phase="Pending", Reason="", readiness=false. Elapsed: 25.076778ms
    Jun  9 10:34:43.622: INFO: Pod "execpod-affinity572nq": Phase="Running", Reason="", readiness=true. Elapsed: 2.049948819s
    Jun  9 10:34:43.622: INFO: Pod "execpod-affinity572nq" satisfied condition "running"
    Jun  9 10:34:44.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-8064 exec execpod-affinity572nq -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jun  9 10:34:44.837: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jun  9 10:34:44.837: INFO: stdout: ""
    Jun  9 10:34:44.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-8064 exec execpod-affinity572nq -- /bin/sh -x -c nc -v -z -w 2 10.101.122.138 80'
    Jun  9 10:34:45.072: INFO: stderr: "+ nc -v -z -w 2 10.101.122.138 80\nConnection to 10.101.122.138 80 port [tcp/http] succeeded!\n"
    Jun  9 10:34:45.072: INFO: stdout: ""
    Jun  9 10:34:45.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-8064 exec execpod-affinity572nq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.122.138:80/ ; done'
    Jun  9 10:34:45.350: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.122.138:80/\n"
    Jun  9 10:34:45.350: INFO: stdout: "\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d\naffinity-clusterip-4527d"
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Received response from host: affinity-clusterip-4527d
    Jun  9 10:34:45.350: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-8064, will wait for the garbage collector to delete the pods 06/09/23 10:34:45.445
    Jun  9 10:34:45.744: INFO: Deleting ReplicationController affinity-clusterip took: 242.737749ms
    Jun  9 10:34:46.146: INFO: Terminating ReplicationController affinity-clusterip pods took: 401.643988ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:34:49.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8064" for this suite. 06/09/23 10:34:49.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:34:49.46
Jun  9 10:34:49.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename endpointslice 06/09/23 10:34:49.461
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:34:49.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:34:49.785
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jun  9 10:34:50.005: INFO: Endpoints addresses: [10.255.64.105 10.255.64.106 10.255.64.107] , ports: [6443]
Jun  9 10:34:50.005: INFO: EndpointSlices addresses: [10.255.64.105 10.255.64.106 10.255.64.107] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jun  9 10:34:50.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4165" for this suite. 06/09/23 10:34:50.022
------------------------------
• [0.777 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:34:49.46
    Jun  9 10:34:49.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename endpointslice 06/09/23 10:34:49.461
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:34:49.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:34:49.785
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jun  9 10:34:50.005: INFO: Endpoints addresses: [10.255.64.105 10.255.64.106 10.255.64.107] , ports: [6443]
    Jun  9 10:34:50.005: INFO: EndpointSlices addresses: [10.255.64.105 10.255.64.106 10.255.64.107] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:34:50.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4165" for this suite. 06/09/23 10:34:50.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:34:50.239
Jun  9 10:34:50.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename replication-controller 06/09/23 10:34:50.24
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:34:50.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:34:50.479
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8 06/09/23 10:34:50.49
Jun  9 10:34:50.531: INFO: Pod name my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8: Found 1 pods out of 1
Jun  9 10:34:50.531: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8" are running
Jun  9 10:34:50.531: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8-tzs9k" in namespace "replication-controller-6839" to be "running"
Jun  9 10:34:50.554: INFO: Pod "my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8-tzs9k": Phase="Pending", Reason="", readiness=false. Elapsed: 23.079697ms
Jun  9 10:34:52.705: INFO: Pod "my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8-tzs9k": Phase="Running", Reason="", readiness=true. Elapsed: 2.174316235s
Jun  9 10:34:52.705: INFO: Pod "my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8-tzs9k" satisfied condition "running"
Jun  9 10:34:52.705: INFO: Pod "my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8-tzs9k" is running (conditions: [])
Jun  9 10:34:52.705: INFO: Trying to dial the pod
Jun  9 10:34:57.744: INFO: Controller my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8: Got expected result from replica 1 [my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8-tzs9k]: "my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8-tzs9k", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun  9 10:34:57.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6839" for this suite. 06/09/23 10:34:57.754
------------------------------
• [SLOW TEST] [7.527 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:34:50.239
    Jun  9 10:34:50.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename replication-controller 06/09/23 10:34:50.24
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:34:50.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:34:50.479
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8 06/09/23 10:34:50.49
    Jun  9 10:34:50.531: INFO: Pod name my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8: Found 1 pods out of 1
    Jun  9 10:34:50.531: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8" are running
    Jun  9 10:34:50.531: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8-tzs9k" in namespace "replication-controller-6839" to be "running"
    Jun  9 10:34:50.554: INFO: Pod "my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8-tzs9k": Phase="Pending", Reason="", readiness=false. Elapsed: 23.079697ms
    Jun  9 10:34:52.705: INFO: Pod "my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8-tzs9k": Phase="Running", Reason="", readiness=true. Elapsed: 2.174316235s
    Jun  9 10:34:52.705: INFO: Pod "my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8-tzs9k" satisfied condition "running"
    Jun  9 10:34:52.705: INFO: Pod "my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8-tzs9k" is running (conditions: [])
    Jun  9 10:34:52.705: INFO: Trying to dial the pod
    Jun  9 10:34:57.744: INFO: Controller my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8: Got expected result from replica 1 [my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8-tzs9k]: "my-hostname-basic-5da3a5e1-75fc-4b3a-be9f-f4919caadda8-tzs9k", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:34:57.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6839" for this suite. 06/09/23 10:34:57.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:34:57.768
Jun  9 10:34:57.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename endpointslice 06/09/23 10:34:57.77
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:34:57.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:34:57.814
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 06/09/23 10:35:03.228
STEP: referencing matching pods with named port 06/09/23 10:35:08.243
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 06/09/23 10:35:13.255
STEP: recreating EndpointSlices after they've been deleted 06/09/23 10:35:18.276
Jun  9 10:35:18.321: INFO: EndpointSlice for Service endpointslice-6724/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jun  9 10:35:28.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6724" for this suite. 06/09/23 10:35:28.355
------------------------------
• [SLOW TEST] [30.605 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:34:57.768
    Jun  9 10:34:57.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename endpointslice 06/09/23 10:34:57.77
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:34:57.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:34:57.814
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 06/09/23 10:35:03.228
    STEP: referencing matching pods with named port 06/09/23 10:35:08.243
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 06/09/23 10:35:13.255
    STEP: recreating EndpointSlices after they've been deleted 06/09/23 10:35:18.276
    Jun  9 10:35:18.321: INFO: EndpointSlice for Service endpointslice-6724/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:35:28.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6724" for this suite. 06/09/23 10:35:28.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:35:28.376
Jun  9 10:35:28.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename sched-preemption 06/09/23 10:35:28.378
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:35:28.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:35:28.433
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jun  9 10:35:28.632: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  9 10:36:28.726: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:36:28.735
Jun  9 10:36:28.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename sched-preemption-path 06/09/23 10:36:28.738
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:36:28.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:36:28.795
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 06/09/23 10:36:28.802
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/09/23 10:36:28.802
Jun  9 10:36:28.826: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6145" to be "running"
Jun  9 10:36:28.833: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.923707ms
Jun  9 10:36:30.843: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016968986s
Jun  9 10:36:30.843: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/09/23 10:36:30.853
Jun  9 10:36:30.881: INFO: found a healthy node: sks-test-v1-26.4-workergroup-qdprq
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Jun  9 10:36:37.021: INFO: pods created so far: [1 1 1]
Jun  9 10:36:37.021: INFO: length of pods created so far: 3
Jun  9 10:36:39.044: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jun  9 10:36:46.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:36:46.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-6145" for this suite. 06/09/23 10:36:46.278
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8289" for this suite. 06/09/23 10:36:46.29
------------------------------
• [SLOW TEST] [77.928 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:35:28.376
    Jun  9 10:35:28.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename sched-preemption 06/09/23 10:35:28.378
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:35:28.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:35:28.433
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jun  9 10:35:28.632: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun  9 10:36:28.726: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:36:28.735
    Jun  9 10:36:28.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename sched-preemption-path 06/09/23 10:36:28.738
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:36:28.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:36:28.795
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 06/09/23 10:36:28.802
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/09/23 10:36:28.802
    Jun  9 10:36:28.826: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6145" to be "running"
    Jun  9 10:36:28.833: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.923707ms
    Jun  9 10:36:30.843: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016968986s
    Jun  9 10:36:30.843: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/09/23 10:36:30.853
    Jun  9 10:36:30.881: INFO: found a healthy node: sks-test-v1-26.4-workergroup-qdprq
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Jun  9 10:36:37.021: INFO: pods created so far: [1 1 1]
    Jun  9 10:36:37.021: INFO: length of pods created so far: 3
    Jun  9 10:36:39.044: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:36:46.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:36:46.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-6145" for this suite. 06/09/23 10:36:46.278
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8289" for this suite. 06/09/23 10:36:46.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:36:46.304
Jun  9 10:36:46.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename daemonsets 06/09/23 10:36:46.308
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:36:46.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:36:46.344
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 06/09/23 10:36:46.388
STEP: Check that daemon pods launch on every node of the cluster. 06/09/23 10:36:46.399
Jun  9 10:36:46.413: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:46.413: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:46.413: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:46.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:36:46.422: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:36:47.434: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:47.434: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:47.434: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:47.440: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:36:47.440: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:36:48.434: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:48.434: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:48.434: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:48.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  9 10:36:48.460: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
Jun  9 10:36:49.432: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:49.432: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:49.432: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:49.438: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  9 10:36:49.438: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 06/09/23 10:36:49.445
Jun  9 10:36:49.481: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:49.481: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:49.482: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:49.501: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  9 10:36:49.501: INFO: Node sks-test-v1-26.4-workergroup-qdprq is running 0 daemon pod, expected 1
Jun  9 10:36:50.517: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:50.517: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:50.517: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:50.525: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  9 10:36:50.525: INFO: Node sks-test-v1-26.4-workergroup-qdprq is running 0 daemon pod, expected 1
Jun  9 10:36:51.531: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:51.532: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:51.532: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:51.548: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  9 10:36:51.548: INFO: Node sks-test-v1-26.4-workergroup-qdprq is running 0 daemon pod, expected 1
Jun  9 10:36:52.524: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:52.524: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:52.524: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:36:52.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  9 10:36:52.537: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 06/09/23 10:36:52.537
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/09/23 10:36:52.562
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4761, will wait for the garbage collector to delete the pods 06/09/23 10:36:52.562
Jun  9 10:36:52.662: INFO: Deleting DaemonSet.extensions daemon-set took: 31.055375ms
Jun  9 10:36:52.762: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.345977ms
Jun  9 10:36:54.872: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:36:54.872: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  9 10:36:54.878: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"78571"},"items":null}

Jun  9 10:36:54.884: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"78571"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:36:54.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4761" for this suite. 06/09/23 10:36:54.945
------------------------------
• [SLOW TEST] [8.658 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:36:46.304
    Jun  9 10:36:46.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename daemonsets 06/09/23 10:36:46.308
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:36:46.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:36:46.344
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 06/09/23 10:36:46.388
    STEP: Check that daemon pods launch on every node of the cluster. 06/09/23 10:36:46.399
    Jun  9 10:36:46.413: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:46.413: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:46.413: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:46.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:36:46.422: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:36:47.434: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:47.434: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:47.434: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:47.440: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:36:47.440: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:36:48.434: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:48.434: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:48.434: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:48.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun  9 10:36:48.460: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
    Jun  9 10:36:49.432: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:49.432: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:49.432: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:49.438: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun  9 10:36:49.438: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 06/09/23 10:36:49.445
    Jun  9 10:36:49.481: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:49.481: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:49.482: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:49.501: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun  9 10:36:49.501: INFO: Node sks-test-v1-26.4-workergroup-qdprq is running 0 daemon pod, expected 1
    Jun  9 10:36:50.517: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:50.517: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:50.517: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:50.525: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun  9 10:36:50.525: INFO: Node sks-test-v1-26.4-workergroup-qdprq is running 0 daemon pod, expected 1
    Jun  9 10:36:51.531: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:51.532: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:51.532: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:51.548: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun  9 10:36:51.548: INFO: Node sks-test-v1-26.4-workergroup-qdprq is running 0 daemon pod, expected 1
    Jun  9 10:36:52.524: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:52.524: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:52.524: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:36:52.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun  9 10:36:52.537: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 06/09/23 10:36:52.537
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/09/23 10:36:52.562
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4761, will wait for the garbage collector to delete the pods 06/09/23 10:36:52.562
    Jun  9 10:36:52.662: INFO: Deleting DaemonSet.extensions daemon-set took: 31.055375ms
    Jun  9 10:36:52.762: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.345977ms
    Jun  9 10:36:54.872: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:36:54.872: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun  9 10:36:54.878: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"78571"},"items":null}

    Jun  9 10:36:54.884: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"78571"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:36:54.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4761" for this suite. 06/09/23 10:36:54.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:36:54.966
Jun  9 10:36:54.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 10:36:54.968
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:36:55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:36:55.008
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 06/09/23 10:36:55.014
Jun  9 10:36:55.048: INFO: Waiting up to 5m0s for pod "pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce" in namespace "emptydir-5049" to be "Succeeded or Failed"
Jun  9 10:36:55.056: INFO: Pod "pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033115ms
Jun  9 10:36:57.071: INFO: Pod "pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023143915s
Jun  9 10:36:59.080: INFO: Pod "pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031904206s
STEP: Saw pod success 06/09/23 10:36:59.08
Jun  9 10:36:59.080: INFO: Pod "pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce" satisfied condition "Succeeded or Failed"
Jun  9 10:36:59.108: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce container test-container: <nil>
STEP: delete the pod 06/09/23 10:36:59.159
Jun  9 10:36:59.224: INFO: Waiting for pod pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce to disappear
Jun  9 10:36:59.238: INFO: Pod pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 10:36:59.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5049" for this suite. 06/09/23 10:36:59.248
------------------------------
• [4.314 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:36:54.966
    Jun  9 10:36:54.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 10:36:54.968
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:36:55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:36:55.008
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 06/09/23 10:36:55.014
    Jun  9 10:36:55.048: INFO: Waiting up to 5m0s for pod "pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce" in namespace "emptydir-5049" to be "Succeeded or Failed"
    Jun  9 10:36:55.056: INFO: Pod "pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033115ms
    Jun  9 10:36:57.071: INFO: Pod "pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023143915s
    Jun  9 10:36:59.080: INFO: Pod "pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031904206s
    STEP: Saw pod success 06/09/23 10:36:59.08
    Jun  9 10:36:59.080: INFO: Pod "pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce" satisfied condition "Succeeded or Failed"
    Jun  9 10:36:59.108: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce container test-container: <nil>
    STEP: delete the pod 06/09/23 10:36:59.159
    Jun  9 10:36:59.224: INFO: Waiting for pod pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce to disappear
    Jun  9 10:36:59.238: INFO: Pod pod-8e06d71e-8b9f-449a-9efc-3c16230f70ce no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:36:59.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5049" for this suite. 06/09/23 10:36:59.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:36:59.281
Jun  9 10:36:59.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:36:59.282
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:36:59.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:36:59.519
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 06/09/23 10:36:59.525
Jun  9 10:36:59.556: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf" in namespace "projected-5333" to be "Succeeded or Failed"
Jun  9 10:36:59.577: INFO: Pod "downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf": Phase="Pending", Reason="", readiness=false. Elapsed: 20.914785ms
Jun  9 10:37:01.598: INFO: Pod "downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041940829s
Jun  9 10:37:03.588: INFO: Pod "downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031673373s
Jun  9 10:37:05.585: INFO: Pod "downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028809117s
STEP: Saw pod success 06/09/23 10:37:05.585
Jun  9 10:37:05.585: INFO: Pod "downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf" satisfied condition "Succeeded or Failed"
Jun  9 10:37:05.591: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf container client-container: <nil>
STEP: delete the pod 06/09/23 10:37:05.611
Jun  9 10:37:05.699: INFO: Waiting for pod downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf to disappear
Jun  9 10:37:05.708: INFO: Pod downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  9 10:37:05.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5333" for this suite. 06/09/23 10:37:05.72
------------------------------
• [SLOW TEST] [6.459 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:36:59.281
    Jun  9 10:36:59.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:36:59.282
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:36:59.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:36:59.519
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 06/09/23 10:36:59.525
    Jun  9 10:36:59.556: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf" in namespace "projected-5333" to be "Succeeded or Failed"
    Jun  9 10:36:59.577: INFO: Pod "downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf": Phase="Pending", Reason="", readiness=false. Elapsed: 20.914785ms
    Jun  9 10:37:01.598: INFO: Pod "downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041940829s
    Jun  9 10:37:03.588: INFO: Pod "downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031673373s
    Jun  9 10:37:05.585: INFO: Pod "downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028809117s
    STEP: Saw pod success 06/09/23 10:37:05.585
    Jun  9 10:37:05.585: INFO: Pod "downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf" satisfied condition "Succeeded or Failed"
    Jun  9 10:37:05.591: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf container client-container: <nil>
    STEP: delete the pod 06/09/23 10:37:05.611
    Jun  9 10:37:05.699: INFO: Waiting for pod downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf to disappear
    Jun  9 10:37:05.708: INFO: Pod downwardapi-volume-9708ec00-b4eb-4341-8c3d-ecaf90fbbebf no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:37:05.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5333" for this suite. 06/09/23 10:37:05.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:37:05.741
Jun  9 10:37:05.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename containers 06/09/23 10:37:05.742
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:05.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:05.795
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 06/09/23 10:37:05.804
Jun  9 10:37:05.833: INFO: Waiting up to 5m0s for pod "client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec" in namespace "containers-6101" to be "Succeeded or Failed"
Jun  9 10:37:05.847: INFO: Pod "client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec": Phase="Pending", Reason="", readiness=false. Elapsed: 14.563426ms
Jun  9 10:37:07.862: INFO: Pod "client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029861804s
Jun  9 10:37:09.857: INFO: Pod "client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024257928s
STEP: Saw pod success 06/09/23 10:37:09.857
Jun  9 10:37:09.857: INFO: Pod "client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec" satisfied condition "Succeeded or Failed"
Jun  9 10:37:09.868: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec container agnhost-container: <nil>
STEP: delete the pod 06/09/23 10:37:09.881
Jun  9 10:37:09.994: INFO: Waiting for pod client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec to disappear
Jun  9 10:37:10.000: INFO: Pod client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jun  9 10:37:10.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6101" for this suite. 06/09/23 10:37:10.009
------------------------------
• [4.311 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:37:05.741
    Jun  9 10:37:05.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename containers 06/09/23 10:37:05.742
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:05.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:05.795
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 06/09/23 10:37:05.804
    Jun  9 10:37:05.833: INFO: Waiting up to 5m0s for pod "client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec" in namespace "containers-6101" to be "Succeeded or Failed"
    Jun  9 10:37:05.847: INFO: Pod "client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec": Phase="Pending", Reason="", readiness=false. Elapsed: 14.563426ms
    Jun  9 10:37:07.862: INFO: Pod "client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029861804s
    Jun  9 10:37:09.857: INFO: Pod "client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024257928s
    STEP: Saw pod success 06/09/23 10:37:09.857
    Jun  9 10:37:09.857: INFO: Pod "client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec" satisfied condition "Succeeded or Failed"
    Jun  9 10:37:09.868: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 10:37:09.881
    Jun  9 10:37:09.994: INFO: Waiting for pod client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec to disappear
    Jun  9 10:37:10.000: INFO: Pod client-containers-d8677a63-8b2e-4b69-9dde-593740d659ec no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:37:10.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6101" for this suite. 06/09/23 10:37:10.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:37:10.052
Jun  9 10:37:10.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename cronjob 06/09/23 10:37:10.053
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:10.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:10.115
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 06/09/23 10:37:10.12
STEP: creating 06/09/23 10:37:10.12
STEP: getting 06/09/23 10:37:10.161
STEP: listing 06/09/23 10:37:10.175
STEP: watching 06/09/23 10:37:10.181
Jun  9 10:37:10.182: INFO: starting watch
STEP: cluster-wide listing 06/09/23 10:37:10.184
STEP: cluster-wide watching 06/09/23 10:37:10.198
Jun  9 10:37:10.198: INFO: starting watch
STEP: patching 06/09/23 10:37:10.2
STEP: updating 06/09/23 10:37:10.219
Jun  9 10:37:10.242: INFO: waiting for watch events with expected annotations
Jun  9 10:37:10.242: INFO: saw patched and updated annotations
STEP: patching /status 06/09/23 10:37:10.242
STEP: updating /status 06/09/23 10:37:10.256
STEP: get /status 06/09/23 10:37:10.281
STEP: deleting 06/09/23 10:37:10.292
STEP: deleting a collection 06/09/23 10:37:10.412
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun  9 10:37:10.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3015" for this suite. 06/09/23 10:37:10.445
------------------------------
• [0.411 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:37:10.052
    Jun  9 10:37:10.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename cronjob 06/09/23 10:37:10.053
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:10.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:10.115
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 06/09/23 10:37:10.12
    STEP: creating 06/09/23 10:37:10.12
    STEP: getting 06/09/23 10:37:10.161
    STEP: listing 06/09/23 10:37:10.175
    STEP: watching 06/09/23 10:37:10.181
    Jun  9 10:37:10.182: INFO: starting watch
    STEP: cluster-wide listing 06/09/23 10:37:10.184
    STEP: cluster-wide watching 06/09/23 10:37:10.198
    Jun  9 10:37:10.198: INFO: starting watch
    STEP: patching 06/09/23 10:37:10.2
    STEP: updating 06/09/23 10:37:10.219
    Jun  9 10:37:10.242: INFO: waiting for watch events with expected annotations
    Jun  9 10:37:10.242: INFO: saw patched and updated annotations
    STEP: patching /status 06/09/23 10:37:10.242
    STEP: updating /status 06/09/23 10:37:10.256
    STEP: get /status 06/09/23 10:37:10.281
    STEP: deleting 06/09/23 10:37:10.292
    STEP: deleting a collection 06/09/23 10:37:10.412
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:37:10.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3015" for this suite. 06/09/23 10:37:10.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:37:10.465
Jun  9 10:37:10.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubelet-test 06/09/23 10:37:10.466
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:10.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:10.534
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun  9 10:37:10.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5452" for this suite. 06/09/23 10:37:10.661
------------------------------
• [0.211 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:37:10.465
    Jun  9 10:37:10.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubelet-test 06/09/23 10:37:10.466
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:10.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:10.534
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:37:10.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5452" for this suite. 06/09/23 10:37:10.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:37:10.684
Jun  9 10:37:10.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename resourcequota 06/09/23 10:37:10.685
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:10.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:10.723
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 06/09/23 10:37:10.729
STEP: Counting existing ResourceQuota 06/09/23 10:37:15.742
STEP: Creating a ResourceQuota 06/09/23 10:37:20.752
STEP: Ensuring resource quota status is calculated 06/09/23 10:37:20.768
STEP: Creating a Secret 06/09/23 10:37:22.775
STEP: Ensuring resource quota status captures secret creation 06/09/23 10:37:22.818
STEP: Deleting a secret 06/09/23 10:37:24.829
STEP: Ensuring resource quota status released usage 06/09/23 10:37:24.852
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  9 10:37:26.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6995" for this suite. 06/09/23 10:37:26.88
------------------------------
• [SLOW TEST] [16.214 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:37:10.684
    Jun  9 10:37:10.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename resourcequota 06/09/23 10:37:10.685
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:10.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:10.723
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 06/09/23 10:37:10.729
    STEP: Counting existing ResourceQuota 06/09/23 10:37:15.742
    STEP: Creating a ResourceQuota 06/09/23 10:37:20.752
    STEP: Ensuring resource quota status is calculated 06/09/23 10:37:20.768
    STEP: Creating a Secret 06/09/23 10:37:22.775
    STEP: Ensuring resource quota status captures secret creation 06/09/23 10:37:22.818
    STEP: Deleting a secret 06/09/23 10:37:24.829
    STEP: Ensuring resource quota status released usage 06/09/23 10:37:24.852
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:37:26.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6995" for this suite. 06/09/23 10:37:26.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:37:26.899
Jun  9 10:37:26.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename custom-resource-definition 06/09/23 10:37:26.903
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:26.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:26.958
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jun  9 10:37:26.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:37:28.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-759" for this suite. 06/09/23 10:37:28.035
------------------------------
• [1.165 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:37:26.899
    Jun  9 10:37:26.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename custom-resource-definition 06/09/23 10:37:26.903
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:26.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:26.958
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jun  9 10:37:26.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:37:28.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-759" for this suite. 06/09/23 10:37:28.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:37:28.065
Jun  9 10:37:28.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename ephemeral-containers-test 06/09/23 10:37:28.068
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:28.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:28.122
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 06/09/23 10:37:28.128
Jun  9 10:37:28.154: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9089" to be "running and ready"
Jun  9 10:37:28.164: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.530672ms
Jun  9 10:37:28.164: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:37:30.172: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018389753s
Jun  9 10:37:30.172: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jun  9 10:37:30.172: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 06/09/23 10:37:30.181
Jun  9 10:37:30.206: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9089" to be "container debugger running"
Jun  9 10:37:30.212: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.359302ms
Jun  9 10:37:32.221: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015160375s
Jun  9 10:37:34.224: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018058199s
Jun  9 10:37:34.224: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 06/09/23 10:37:34.224
Jun  9 10:37:34.224: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9089 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:37:34.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:37:34.225: INFO: ExecWithOptions: Clientset creation
Jun  9 10:37:34.226: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-9089/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jun  9 10:37:34.318: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:37:34.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-9089" for this suite. 06/09/23 10:37:34.345
------------------------------
• [SLOW TEST] [6.299 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:37:28.065
    Jun  9 10:37:28.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename ephemeral-containers-test 06/09/23 10:37:28.068
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:28.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:28.122
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 06/09/23 10:37:28.128
    Jun  9 10:37:28.154: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9089" to be "running and ready"
    Jun  9 10:37:28.164: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.530672ms
    Jun  9 10:37:28.164: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:37:30.172: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018389753s
    Jun  9 10:37:30.172: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jun  9 10:37:30.172: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 06/09/23 10:37:30.181
    Jun  9 10:37:30.206: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9089" to be "container debugger running"
    Jun  9 10:37:30.212: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.359302ms
    Jun  9 10:37:32.221: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015160375s
    Jun  9 10:37:34.224: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018058199s
    Jun  9 10:37:34.224: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 06/09/23 10:37:34.224
    Jun  9 10:37:34.224: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9089 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:37:34.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:37:34.225: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:37:34.226: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-9089/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jun  9 10:37:34.318: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:37:34.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-9089" for this suite. 06/09/23 10:37:34.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:37:34.365
Jun  9 10:37:34.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pods 06/09/23 10:37:34.366
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:34.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:34.405
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 06/09/23 10:37:34.413
STEP: submitting the pod to kubernetes 06/09/23 10:37:34.413
Jun  9 10:37:34.432: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399" in namespace "pods-5177" to be "running and ready"
Jun  9 10:37:34.443: INFO: Pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399": Phase="Pending", Reason="", readiness=false. Elapsed: 11.002535ms
Jun  9 10:37:34.443: INFO: The phase of Pod pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:37:36.456: INFO: Pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024580866s
Jun  9 10:37:36.456: INFO: The phase of Pod pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:37:38.455: INFO: Pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399": Phase="Running", Reason="", readiness=true. Elapsed: 4.023165177s
Jun  9 10:37:38.455: INFO: The phase of Pod pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399 is Running (Ready = true)
Jun  9 10:37:38.455: INFO: Pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 06/09/23 10:37:38.461
STEP: updating the pod 06/09/23 10:37:38.468
Jun  9 10:37:39.083: INFO: Successfully updated pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399"
Jun  9 10:37:39.083: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399" in namespace "pods-5177" to be "terminated with reason DeadlineExceeded"
Jun  9 10:37:39.111: INFO: Pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399": Phase="Running", Reason="", readiness=true. Elapsed: 27.784229ms
Jun  9 10:37:41.130: INFO: Pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.046921523s
Jun  9 10:37:41.130: INFO: Pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  9 10:37:41.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5177" for this suite. 06/09/23 10:37:41.148
------------------------------
• [SLOW TEST] [6.819 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:37:34.365
    Jun  9 10:37:34.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pods 06/09/23 10:37:34.366
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:34.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:34.405
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 06/09/23 10:37:34.413
    STEP: submitting the pod to kubernetes 06/09/23 10:37:34.413
    Jun  9 10:37:34.432: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399" in namespace "pods-5177" to be "running and ready"
    Jun  9 10:37:34.443: INFO: Pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399": Phase="Pending", Reason="", readiness=false. Elapsed: 11.002535ms
    Jun  9 10:37:34.443: INFO: The phase of Pod pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:37:36.456: INFO: Pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024580866s
    Jun  9 10:37:36.456: INFO: The phase of Pod pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:37:38.455: INFO: Pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399": Phase="Running", Reason="", readiness=true. Elapsed: 4.023165177s
    Jun  9 10:37:38.455: INFO: The phase of Pod pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399 is Running (Ready = true)
    Jun  9 10:37:38.455: INFO: Pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 06/09/23 10:37:38.461
    STEP: updating the pod 06/09/23 10:37:38.468
    Jun  9 10:37:39.083: INFO: Successfully updated pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399"
    Jun  9 10:37:39.083: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399" in namespace "pods-5177" to be "terminated with reason DeadlineExceeded"
    Jun  9 10:37:39.111: INFO: Pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399": Phase="Running", Reason="", readiness=true. Elapsed: 27.784229ms
    Jun  9 10:37:41.130: INFO: Pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.046921523s
    Jun  9 10:37:41.130: INFO: Pod "pod-update-activedeadlineseconds-cebd8203-df4d-4ed9-a38a-784cf81a1399" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:37:41.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5177" for this suite. 06/09/23 10:37:41.148
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:37:41.185
Jun  9 10:37:41.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 10:37:41.186
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:41.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:41.243
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 06/09/23 10:37:41.25
Jun  9 10:37:41.279: INFO: Waiting up to 5m0s for pod "pod-0a55c756-368f-4b81-92ac-894922f86511" in namespace "emptydir-8307" to be "Succeeded or Failed"
Jun  9 10:37:41.294: INFO: Pod "pod-0a55c756-368f-4b81-92ac-894922f86511": Phase="Pending", Reason="", readiness=false. Elapsed: 15.203991ms
Jun  9 10:37:43.303: INFO: Pod "pod-0a55c756-368f-4b81-92ac-894922f86511": Phase="Running", Reason="", readiness=true. Elapsed: 2.024686149s
Jun  9 10:37:45.305: INFO: Pod "pod-0a55c756-368f-4b81-92ac-894922f86511": Phase="Running", Reason="", readiness=false. Elapsed: 4.02611686s
Jun  9 10:37:47.345: INFO: Pod "pod-0a55c756-368f-4b81-92ac-894922f86511": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066680374s
STEP: Saw pod success 06/09/23 10:37:47.345
Jun  9 10:37:47.346: INFO: Pod "pod-0a55c756-368f-4b81-92ac-894922f86511" satisfied condition "Succeeded or Failed"
Jun  9 10:37:47.361: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-0a55c756-368f-4b81-92ac-894922f86511 container test-container: <nil>
STEP: delete the pod 06/09/23 10:37:47.404
Jun  9 10:37:47.632: INFO: Waiting for pod pod-0a55c756-368f-4b81-92ac-894922f86511 to disappear
Jun  9 10:37:47.644: INFO: Pod pod-0a55c756-368f-4b81-92ac-894922f86511 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 10:37:47.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8307" for this suite. 06/09/23 10:37:47.654
------------------------------
• [SLOW TEST] [6.496 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:37:41.185
    Jun  9 10:37:41.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 10:37:41.186
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:41.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:41.243
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 06/09/23 10:37:41.25
    Jun  9 10:37:41.279: INFO: Waiting up to 5m0s for pod "pod-0a55c756-368f-4b81-92ac-894922f86511" in namespace "emptydir-8307" to be "Succeeded or Failed"
    Jun  9 10:37:41.294: INFO: Pod "pod-0a55c756-368f-4b81-92ac-894922f86511": Phase="Pending", Reason="", readiness=false. Elapsed: 15.203991ms
    Jun  9 10:37:43.303: INFO: Pod "pod-0a55c756-368f-4b81-92ac-894922f86511": Phase="Running", Reason="", readiness=true. Elapsed: 2.024686149s
    Jun  9 10:37:45.305: INFO: Pod "pod-0a55c756-368f-4b81-92ac-894922f86511": Phase="Running", Reason="", readiness=false. Elapsed: 4.02611686s
    Jun  9 10:37:47.345: INFO: Pod "pod-0a55c756-368f-4b81-92ac-894922f86511": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066680374s
    STEP: Saw pod success 06/09/23 10:37:47.345
    Jun  9 10:37:47.346: INFO: Pod "pod-0a55c756-368f-4b81-92ac-894922f86511" satisfied condition "Succeeded or Failed"
    Jun  9 10:37:47.361: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-0a55c756-368f-4b81-92ac-894922f86511 container test-container: <nil>
    STEP: delete the pod 06/09/23 10:37:47.404
    Jun  9 10:37:47.632: INFO: Waiting for pod pod-0a55c756-368f-4b81-92ac-894922f86511 to disappear
    Jun  9 10:37:47.644: INFO: Pod pod-0a55c756-368f-4b81-92ac-894922f86511 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:37:47.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8307" for this suite. 06/09/23 10:37:47.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:37:47.682
Jun  9 10:37:47.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename namespaces 06/09/23 10:37:47.683
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:47.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:47.724
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 06/09/23 10:37:47.737
Jun  9 10:37:47.746: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 06/09/23 10:37:47.746
Jun  9 10:37:47.767: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 06/09/23 10:37:47.767
Jun  9 10:37:47.824: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:37:47.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2338" for this suite. 06/09/23 10:37:47.837
------------------------------
• [0.177 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:37:47.682
    Jun  9 10:37:47.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename namespaces 06/09/23 10:37:47.683
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:47.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:47.724
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 06/09/23 10:37:47.737
    Jun  9 10:37:47.746: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 06/09/23 10:37:47.746
    Jun  9 10:37:47.767: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 06/09/23 10:37:47.767
    Jun  9 10:37:47.824: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:37:47.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2338" for this suite. 06/09/23 10:37:47.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:37:47.86
Jun  9 10:37:47.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 10:37:47.862
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:47.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:47.94
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 06/09/23 10:37:47.946
Jun  9 10:37:47.977: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b" in namespace "downward-api-9084" to be "Succeeded or Failed"
Jun  9 10:37:47.991: INFO: Pod "downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.456681ms
Jun  9 10:37:50.006: INFO: Pod "downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028146484s
Jun  9 10:37:52.000: INFO: Pod "downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022559369s
STEP: Saw pod success 06/09/23 10:37:52
Jun  9 10:37:52.000: INFO: Pod "downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b" satisfied condition "Succeeded or Failed"
Jun  9 10:37:52.007: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b container client-container: <nil>
STEP: delete the pod 06/09/23 10:37:52.022
Jun  9 10:37:52.042: INFO: Waiting for pod downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b to disappear
Jun  9 10:37:52.048: INFO: Pod downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  9 10:37:52.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9084" for this suite. 06/09/23 10:37:52.057
------------------------------
• [4.217 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:37:47.86
    Jun  9 10:37:47.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 10:37:47.862
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:47.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:47.94
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 06/09/23 10:37:47.946
    Jun  9 10:37:47.977: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b" in namespace "downward-api-9084" to be "Succeeded or Failed"
    Jun  9 10:37:47.991: INFO: Pod "downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.456681ms
    Jun  9 10:37:50.006: INFO: Pod "downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028146484s
    Jun  9 10:37:52.000: INFO: Pod "downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022559369s
    STEP: Saw pod success 06/09/23 10:37:52
    Jun  9 10:37:52.000: INFO: Pod "downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b" satisfied condition "Succeeded or Failed"
    Jun  9 10:37:52.007: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b container client-container: <nil>
    STEP: delete the pod 06/09/23 10:37:52.022
    Jun  9 10:37:52.042: INFO: Waiting for pod downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b to disappear
    Jun  9 10:37:52.048: INFO: Pod downwardapi-volume-fed83649-167d-4557-95d8-a08804ad738b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:37:52.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9084" for this suite. 06/09/23 10:37:52.057
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:37:52.078
Jun  9 10:37:52.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename replicaset 06/09/23 10:37:52.08
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:52.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:52.123
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jun  9 10:37:52.155: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  9 10:37:57.175: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/09/23 10:37:57.175
STEP: Scaling up "test-rs" replicaset  06/09/23 10:37:57.175
Jun  9 10:37:57.221: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 06/09/23 10:37:57.221
W0609 10:37:57.649712      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun  9 10:37:57.653: INFO: observed ReplicaSet test-rs in namespace replicaset-7855 with ReadyReplicas 1, AvailableReplicas 1
Jun  9 10:37:57.787: INFO: observed ReplicaSet test-rs in namespace replicaset-7855 with ReadyReplicas 1, AvailableReplicas 1
Jun  9 10:37:57.787: INFO: observed ReplicaSet test-rs in namespace replicaset-7855 with ReadyReplicas 1, AvailableReplicas 1
Jun  9 10:37:59.752: INFO: observed ReplicaSet test-rs in namespace replicaset-7855 with ReadyReplicas 2, AvailableReplicas 2
Jun  9 10:37:59.777: INFO: observed Replicaset test-rs in namespace replicaset-7855 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun  9 10:37:59.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7855" for this suite. 06/09/23 10:37:59.792
------------------------------
• [SLOW TEST] [7.746 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:37:52.078
    Jun  9 10:37:52.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename replicaset 06/09/23 10:37:52.08
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:52.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:52.123
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jun  9 10:37:52.155: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun  9 10:37:57.175: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/09/23 10:37:57.175
    STEP: Scaling up "test-rs" replicaset  06/09/23 10:37:57.175
    Jun  9 10:37:57.221: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 06/09/23 10:37:57.221
    W0609 10:37:57.649712      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun  9 10:37:57.653: INFO: observed ReplicaSet test-rs in namespace replicaset-7855 with ReadyReplicas 1, AvailableReplicas 1
    Jun  9 10:37:57.787: INFO: observed ReplicaSet test-rs in namespace replicaset-7855 with ReadyReplicas 1, AvailableReplicas 1
    Jun  9 10:37:57.787: INFO: observed ReplicaSet test-rs in namespace replicaset-7855 with ReadyReplicas 1, AvailableReplicas 1
    Jun  9 10:37:59.752: INFO: observed ReplicaSet test-rs in namespace replicaset-7855 with ReadyReplicas 2, AvailableReplicas 2
    Jun  9 10:37:59.777: INFO: observed Replicaset test-rs in namespace replicaset-7855 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:37:59.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7855" for this suite. 06/09/23 10:37:59.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:37:59.825
Jun  9 10:37:59.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename replication-controller 06/09/23 10:37:59.827
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:59.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:59.881
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 06/09/23 10:37:59.888
Jun  9 10:37:59.941: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-3856" to be "running and ready"
Jun  9 10:37:59.949: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 8.132356ms
Jun  9 10:37:59.949: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:38:01.957: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.01594109s
Jun  9 10:38:01.957: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jun  9 10:38:01.957: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 06/09/23 10:38:01.965
STEP: Then the orphan pod is adopted 06/09/23 10:38:01.982
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun  9 10:38:01.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3856" for this suite. 06/09/23 10:38:02.008
------------------------------
• [2.198 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:37:59.825
    Jun  9 10:37:59.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename replication-controller 06/09/23 10:37:59.827
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:37:59.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:37:59.881
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 06/09/23 10:37:59.888
    Jun  9 10:37:59.941: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-3856" to be "running and ready"
    Jun  9 10:37:59.949: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 8.132356ms
    Jun  9 10:37:59.949: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:38:01.957: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.01594109s
    Jun  9 10:38:01.957: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jun  9 10:38:01.957: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 06/09/23 10:38:01.965
    STEP: Then the orphan pod is adopted 06/09/23 10:38:01.982
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:38:01.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3856" for this suite. 06/09/23 10:38:02.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:38:02.025
Jun  9 10:38:02.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubelet-test 06/09/23 10:38:02.026
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:02.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:02.072
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun  9 10:38:06.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5680" for this suite. 06/09/23 10:38:06.139
------------------------------
• [4.151 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:38:02.025
    Jun  9 10:38:02.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubelet-test 06/09/23 10:38:02.026
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:02.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:02.072
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:38:06.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5680" for this suite. 06/09/23 10:38:06.139
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:38:06.177
Jun  9 10:38:06.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubelet-test 06/09/23 10:38:06.178
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:06.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:06.251
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jun  9 10:38:06.276: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs667fc1a5-837d-468b-b15b-1e4037d481a2" in namespace "kubelet-test-2628" to be "running and ready"
Jun  9 10:38:06.304: INFO: Pod "busybox-readonly-fs667fc1a5-837d-468b-b15b-1e4037d481a2": Phase="Pending", Reason="", readiness=false. Elapsed: 28.466646ms
Jun  9 10:38:06.304: INFO: The phase of Pod busybox-readonly-fs667fc1a5-837d-468b-b15b-1e4037d481a2 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:38:08.311: INFO: Pod "busybox-readonly-fs667fc1a5-837d-468b-b15b-1e4037d481a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.035604532s
Jun  9 10:38:08.311: INFO: The phase of Pod busybox-readonly-fs667fc1a5-837d-468b-b15b-1e4037d481a2 is Running (Ready = true)
Jun  9 10:38:08.311: INFO: Pod "busybox-readonly-fs667fc1a5-837d-468b-b15b-1e4037d481a2" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun  9 10:38:08.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2628" for this suite. 06/09/23 10:38:08.337
------------------------------
• [2.182 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:38:06.177
    Jun  9 10:38:06.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubelet-test 06/09/23 10:38:06.178
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:06.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:06.251
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jun  9 10:38:06.276: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs667fc1a5-837d-468b-b15b-1e4037d481a2" in namespace "kubelet-test-2628" to be "running and ready"
    Jun  9 10:38:06.304: INFO: Pod "busybox-readonly-fs667fc1a5-837d-468b-b15b-1e4037d481a2": Phase="Pending", Reason="", readiness=false. Elapsed: 28.466646ms
    Jun  9 10:38:06.304: INFO: The phase of Pod busybox-readonly-fs667fc1a5-837d-468b-b15b-1e4037d481a2 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:38:08.311: INFO: Pod "busybox-readonly-fs667fc1a5-837d-468b-b15b-1e4037d481a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.035604532s
    Jun  9 10:38:08.311: INFO: The phase of Pod busybox-readonly-fs667fc1a5-837d-468b-b15b-1e4037d481a2 is Running (Ready = true)
    Jun  9 10:38:08.311: INFO: Pod "busybox-readonly-fs667fc1a5-837d-468b-b15b-1e4037d481a2" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:38:08.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2628" for this suite. 06/09/23 10:38:08.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:38:08.36
Jun  9 10:38:08.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename resourcequota 06/09/23 10:38:08.362
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:08.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:08.536
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 06/09/23 10:38:08.546
STEP: Getting a ResourceQuota 06/09/23 10:38:08.646
STEP: Listing all ResourceQuotas with LabelSelector 06/09/23 10:38:08.663
STEP: Patching the ResourceQuota 06/09/23 10:38:08.67
STEP: Deleting a Collection of ResourceQuotas 06/09/23 10:38:08.799
STEP: Verifying the deleted ResourceQuota 06/09/23 10:38:08.826
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  9 10:38:08.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7307" for this suite. 06/09/23 10:38:08.877
------------------------------
• [0.535 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:38:08.36
    Jun  9 10:38:08.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename resourcequota 06/09/23 10:38:08.362
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:08.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:08.536
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 06/09/23 10:38:08.546
    STEP: Getting a ResourceQuota 06/09/23 10:38:08.646
    STEP: Listing all ResourceQuotas with LabelSelector 06/09/23 10:38:08.663
    STEP: Patching the ResourceQuota 06/09/23 10:38:08.67
    STEP: Deleting a Collection of ResourceQuotas 06/09/23 10:38:08.799
    STEP: Verifying the deleted ResourceQuota 06/09/23 10:38:08.826
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:38:08.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7307" for this suite. 06/09/23 10:38:08.877
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:38:08.896
Jun  9 10:38:08.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 10:38:08.898
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:08.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:08.963
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 06/09/23 10:38:08.969
Jun  9 10:38:09.002: INFO: Waiting up to 5m0s for pod "pod-33ac80c1-f581-4e35-8f34-a924382be581" in namespace "emptydir-7868" to be "Succeeded or Failed"
Jun  9 10:38:09.013: INFO: Pod "pod-33ac80c1-f581-4e35-8f34-a924382be581": Phase="Pending", Reason="", readiness=false. Elapsed: 10.754227ms
Jun  9 10:38:11.032: INFO: Pod "pod-33ac80c1-f581-4e35-8f34-a924382be581": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030215169s
Jun  9 10:38:13.021: INFO: Pod "pod-33ac80c1-f581-4e35-8f34-a924382be581": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018994739s
STEP: Saw pod success 06/09/23 10:38:13.021
Jun  9 10:38:13.021: INFO: Pod "pod-33ac80c1-f581-4e35-8f34-a924382be581" satisfied condition "Succeeded or Failed"
Jun  9 10:38:13.029: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-33ac80c1-f581-4e35-8f34-a924382be581 container test-container: <nil>
STEP: delete the pod 06/09/23 10:38:13.044
Jun  9 10:38:13.079: INFO: Waiting for pod pod-33ac80c1-f581-4e35-8f34-a924382be581 to disappear
Jun  9 10:38:13.086: INFO: Pod pod-33ac80c1-f581-4e35-8f34-a924382be581 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 10:38:13.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7868" for this suite. 06/09/23 10:38:13.1
------------------------------
• [4.216 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:38:08.896
    Jun  9 10:38:08.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 10:38:08.898
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:08.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:08.963
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 06/09/23 10:38:08.969
    Jun  9 10:38:09.002: INFO: Waiting up to 5m0s for pod "pod-33ac80c1-f581-4e35-8f34-a924382be581" in namespace "emptydir-7868" to be "Succeeded or Failed"
    Jun  9 10:38:09.013: INFO: Pod "pod-33ac80c1-f581-4e35-8f34-a924382be581": Phase="Pending", Reason="", readiness=false. Elapsed: 10.754227ms
    Jun  9 10:38:11.032: INFO: Pod "pod-33ac80c1-f581-4e35-8f34-a924382be581": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030215169s
    Jun  9 10:38:13.021: INFO: Pod "pod-33ac80c1-f581-4e35-8f34-a924382be581": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018994739s
    STEP: Saw pod success 06/09/23 10:38:13.021
    Jun  9 10:38:13.021: INFO: Pod "pod-33ac80c1-f581-4e35-8f34-a924382be581" satisfied condition "Succeeded or Failed"
    Jun  9 10:38:13.029: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-33ac80c1-f581-4e35-8f34-a924382be581 container test-container: <nil>
    STEP: delete the pod 06/09/23 10:38:13.044
    Jun  9 10:38:13.079: INFO: Waiting for pod pod-33ac80c1-f581-4e35-8f34-a924382be581 to disappear
    Jun  9 10:38:13.086: INFO: Pod pod-33ac80c1-f581-4e35-8f34-a924382be581 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:38:13.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7868" for this suite. 06/09/23 10:38:13.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:38:13.118
Jun  9 10:38:13.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-runtime 06/09/23 10:38:13.12
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:13.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:13.156
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 06/09/23 10:38:13.179
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 06/09/23 10:38:34.382
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 06/09/23 10:38:34.389
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 06/09/23 10:38:34.403
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 06/09/23 10:38:34.403
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 06/09/23 10:38:34.453
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 06/09/23 10:38:37.492
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 06/09/23 10:38:39.518
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 06/09/23 10:38:39.53
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 06/09/23 10:38:39.53
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 06/09/23 10:38:39.942
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 06/09/23 10:38:40.972
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 06/09/23 10:38:45.015
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 06/09/23 10:38:45.029
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 06/09/23 10:38:45.03
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun  9 10:38:45.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2257" for this suite. 06/09/23 10:38:45.106
------------------------------
• [SLOW TEST] [32.035 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:38:13.118
    Jun  9 10:38:13.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-runtime 06/09/23 10:38:13.12
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:13.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:13.156
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 06/09/23 10:38:13.179
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 06/09/23 10:38:34.382
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 06/09/23 10:38:34.389
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 06/09/23 10:38:34.403
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 06/09/23 10:38:34.403
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 06/09/23 10:38:34.453
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 06/09/23 10:38:37.492
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 06/09/23 10:38:39.518
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 06/09/23 10:38:39.53
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 06/09/23 10:38:39.53
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 06/09/23 10:38:39.942
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 06/09/23 10:38:40.972
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 06/09/23 10:38:45.015
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 06/09/23 10:38:45.029
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 06/09/23 10:38:45.03
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:38:45.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2257" for this suite. 06/09/23 10:38:45.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:38:45.158
Jun  9 10:38:45.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename discovery 06/09/23 10:38:45.159
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:45.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:45.261
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 06/09/23 10:38:45.27
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jun  9 10:38:45.573: INFO: Checking APIGroup: apiregistration.k8s.io
Jun  9 10:38:45.577: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun  9 10:38:45.577: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jun  9 10:38:45.577: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun  9 10:38:45.577: INFO: Checking APIGroup: apps
Jun  9 10:38:45.578: INFO: PreferredVersion.GroupVersion: apps/v1
Jun  9 10:38:45.579: INFO: Versions found [{apps/v1 v1}]
Jun  9 10:38:45.579: INFO: apps/v1 matches apps/v1
Jun  9 10:38:45.579: INFO: Checking APIGroup: events.k8s.io
Jun  9 10:38:45.580: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun  9 10:38:45.580: INFO: Versions found [{events.k8s.io/v1 v1}]
Jun  9 10:38:45.580: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun  9 10:38:45.580: INFO: Checking APIGroup: authentication.k8s.io
Jun  9 10:38:45.581: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun  9 10:38:45.581: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jun  9 10:38:45.581: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun  9 10:38:45.581: INFO: Checking APIGroup: authorization.k8s.io
Jun  9 10:38:45.583: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun  9 10:38:45.583: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jun  9 10:38:45.583: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun  9 10:38:45.583: INFO: Checking APIGroup: autoscaling
Jun  9 10:38:45.584: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jun  9 10:38:45.584: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jun  9 10:38:45.584: INFO: autoscaling/v2 matches autoscaling/v2
Jun  9 10:38:45.584: INFO: Checking APIGroup: batch
Jun  9 10:38:45.586: INFO: PreferredVersion.GroupVersion: batch/v1
Jun  9 10:38:45.586: INFO: Versions found [{batch/v1 v1}]
Jun  9 10:38:45.586: INFO: batch/v1 matches batch/v1
Jun  9 10:38:45.586: INFO: Checking APIGroup: certificates.k8s.io
Jun  9 10:38:45.587: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun  9 10:38:45.587: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jun  9 10:38:45.587: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun  9 10:38:45.587: INFO: Checking APIGroup: networking.k8s.io
Jun  9 10:38:45.590: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun  9 10:38:45.590: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jun  9 10:38:45.590: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun  9 10:38:45.590: INFO: Checking APIGroup: policy
Jun  9 10:38:45.592: INFO: PreferredVersion.GroupVersion: policy/v1
Jun  9 10:38:45.592: INFO: Versions found [{policy/v1 v1}]
Jun  9 10:38:45.592: INFO: policy/v1 matches policy/v1
Jun  9 10:38:45.592: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun  9 10:38:45.593: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun  9 10:38:45.593: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jun  9 10:38:45.593: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun  9 10:38:45.593: INFO: Checking APIGroup: storage.k8s.io
Jun  9 10:38:45.596: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun  9 10:38:45.596: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun  9 10:38:45.596: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun  9 10:38:45.596: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun  9 10:38:45.598: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun  9 10:38:45.598: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jun  9 10:38:45.598: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun  9 10:38:45.598: INFO: Checking APIGroup: apiextensions.k8s.io
Jun  9 10:38:45.606: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun  9 10:38:45.607: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jun  9 10:38:45.607: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun  9 10:38:45.607: INFO: Checking APIGroup: scheduling.k8s.io
Jun  9 10:38:45.609: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun  9 10:38:45.609: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jun  9 10:38:45.609: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun  9 10:38:45.609: INFO: Checking APIGroup: coordination.k8s.io
Jun  9 10:38:45.611: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun  9 10:38:45.611: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jun  9 10:38:45.611: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun  9 10:38:45.611: INFO: Checking APIGroup: node.k8s.io
Jun  9 10:38:45.613: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jun  9 10:38:45.613: INFO: Versions found [{node.k8s.io/v1 v1}]
Jun  9 10:38:45.613: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jun  9 10:38:45.613: INFO: Checking APIGroup: discovery.k8s.io
Jun  9 10:38:45.615: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jun  9 10:38:45.615: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jun  9 10:38:45.615: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jun  9 10:38:45.615: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jun  9 10:38:45.617: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jun  9 10:38:45.617: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jun  9 10:38:45.617: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Jun  9 10:38:45.617: INFO: Checking APIGroup: projectcalico.org
Jun  9 10:38:45.619: INFO: PreferredVersion.GroupVersion: projectcalico.org/v3
Jun  9 10:38:45.619: INFO: Versions found [{projectcalico.org/v3 v3}]
Jun  9 10:38:45.620: INFO: projectcalico.org/v3 matches projectcalico.org/v3
Jun  9 10:38:45.620: INFO: Checking APIGroup: crd.projectcalico.org
Jun  9 10:38:45.623: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jun  9 10:38:45.623: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jun  9 10:38:45.623: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jun  9 10:38:45.623: INFO: Checking APIGroup: operator.tigera.io
Jun  9 10:38:45.625: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Jun  9 10:38:45.625: INFO: Versions found [{operator.tigera.io/v1 v1}]
Jun  9 10:38:45.625: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Jun  9 10:38:45.625: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jun  9 10:38:45.627: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jun  9 10:38:45.627: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jun  9 10:38:45.627: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jun  9 10:38:45.627: INFO: Checking APIGroup: internal.packaging.carvel.dev
Jun  9 10:38:45.629: INFO: PreferredVersion.GroupVersion: internal.packaging.carvel.dev/v1alpha1
Jun  9 10:38:45.629: INFO: Versions found [{internal.packaging.carvel.dev/v1alpha1 v1alpha1}]
Jun  9 10:38:45.629: INFO: internal.packaging.carvel.dev/v1alpha1 matches internal.packaging.carvel.dev/v1alpha1
Jun  9 10:38:45.629: INFO: Checking APIGroup: kappctrl.k14s.io
Jun  9 10:38:45.630: INFO: PreferredVersion.GroupVersion: kappctrl.k14s.io/v1alpha1
Jun  9 10:38:45.630: INFO: Versions found [{kappctrl.k14s.io/v1alpha1 v1alpha1}]
Jun  9 10:38:45.630: INFO: kappctrl.k14s.io/v1alpha1 matches kappctrl.k14s.io/v1alpha1
Jun  9 10:38:45.630: INFO: Checking APIGroup: packaging.carvel.dev
Jun  9 10:38:45.632: INFO: PreferredVersion.GroupVersion: packaging.carvel.dev/v1alpha1
Jun  9 10:38:45.632: INFO: Versions found [{packaging.carvel.dev/v1alpha1 v1alpha1}]
Jun  9 10:38:45.632: INFO: packaging.carvel.dev/v1alpha1 matches packaging.carvel.dev/v1alpha1
Jun  9 10:38:45.632: INFO: Checking APIGroup: data.packaging.carvel.dev
Jun  9 10:38:45.634: INFO: PreferredVersion.GroupVersion: data.packaging.carvel.dev/v1alpha1
Jun  9 10:38:45.634: INFO: Versions found [{data.packaging.carvel.dev/v1alpha1 v1alpha1}]
Jun  9 10:38:45.634: INFO: data.packaging.carvel.dev/v1alpha1 matches data.packaging.carvel.dev/v1alpha1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jun  9 10:38:45.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-9037" for this suite. 06/09/23 10:38:45.643
------------------------------
• [0.501 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:38:45.158
    Jun  9 10:38:45.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename discovery 06/09/23 10:38:45.159
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:45.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:45.261
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 06/09/23 10:38:45.27
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jun  9 10:38:45.573: INFO: Checking APIGroup: apiregistration.k8s.io
    Jun  9 10:38:45.577: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jun  9 10:38:45.577: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jun  9 10:38:45.577: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jun  9 10:38:45.577: INFO: Checking APIGroup: apps
    Jun  9 10:38:45.578: INFO: PreferredVersion.GroupVersion: apps/v1
    Jun  9 10:38:45.579: INFO: Versions found [{apps/v1 v1}]
    Jun  9 10:38:45.579: INFO: apps/v1 matches apps/v1
    Jun  9 10:38:45.579: INFO: Checking APIGroup: events.k8s.io
    Jun  9 10:38:45.580: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jun  9 10:38:45.580: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jun  9 10:38:45.580: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jun  9 10:38:45.580: INFO: Checking APIGroup: authentication.k8s.io
    Jun  9 10:38:45.581: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jun  9 10:38:45.581: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jun  9 10:38:45.581: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jun  9 10:38:45.581: INFO: Checking APIGroup: authorization.k8s.io
    Jun  9 10:38:45.583: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jun  9 10:38:45.583: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jun  9 10:38:45.583: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jun  9 10:38:45.583: INFO: Checking APIGroup: autoscaling
    Jun  9 10:38:45.584: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jun  9 10:38:45.584: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jun  9 10:38:45.584: INFO: autoscaling/v2 matches autoscaling/v2
    Jun  9 10:38:45.584: INFO: Checking APIGroup: batch
    Jun  9 10:38:45.586: INFO: PreferredVersion.GroupVersion: batch/v1
    Jun  9 10:38:45.586: INFO: Versions found [{batch/v1 v1}]
    Jun  9 10:38:45.586: INFO: batch/v1 matches batch/v1
    Jun  9 10:38:45.586: INFO: Checking APIGroup: certificates.k8s.io
    Jun  9 10:38:45.587: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jun  9 10:38:45.587: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jun  9 10:38:45.587: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jun  9 10:38:45.587: INFO: Checking APIGroup: networking.k8s.io
    Jun  9 10:38:45.590: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jun  9 10:38:45.590: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jun  9 10:38:45.590: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jun  9 10:38:45.590: INFO: Checking APIGroup: policy
    Jun  9 10:38:45.592: INFO: PreferredVersion.GroupVersion: policy/v1
    Jun  9 10:38:45.592: INFO: Versions found [{policy/v1 v1}]
    Jun  9 10:38:45.592: INFO: policy/v1 matches policy/v1
    Jun  9 10:38:45.592: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jun  9 10:38:45.593: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jun  9 10:38:45.593: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jun  9 10:38:45.593: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jun  9 10:38:45.593: INFO: Checking APIGroup: storage.k8s.io
    Jun  9 10:38:45.596: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jun  9 10:38:45.596: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jun  9 10:38:45.596: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jun  9 10:38:45.596: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jun  9 10:38:45.598: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jun  9 10:38:45.598: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jun  9 10:38:45.598: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jun  9 10:38:45.598: INFO: Checking APIGroup: apiextensions.k8s.io
    Jun  9 10:38:45.606: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jun  9 10:38:45.607: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jun  9 10:38:45.607: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jun  9 10:38:45.607: INFO: Checking APIGroup: scheduling.k8s.io
    Jun  9 10:38:45.609: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jun  9 10:38:45.609: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jun  9 10:38:45.609: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jun  9 10:38:45.609: INFO: Checking APIGroup: coordination.k8s.io
    Jun  9 10:38:45.611: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jun  9 10:38:45.611: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jun  9 10:38:45.611: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jun  9 10:38:45.611: INFO: Checking APIGroup: node.k8s.io
    Jun  9 10:38:45.613: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jun  9 10:38:45.613: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jun  9 10:38:45.613: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jun  9 10:38:45.613: INFO: Checking APIGroup: discovery.k8s.io
    Jun  9 10:38:45.615: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jun  9 10:38:45.615: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jun  9 10:38:45.615: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jun  9 10:38:45.615: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jun  9 10:38:45.617: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jun  9 10:38:45.617: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jun  9 10:38:45.617: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Jun  9 10:38:45.617: INFO: Checking APIGroup: projectcalico.org
    Jun  9 10:38:45.619: INFO: PreferredVersion.GroupVersion: projectcalico.org/v3
    Jun  9 10:38:45.619: INFO: Versions found [{projectcalico.org/v3 v3}]
    Jun  9 10:38:45.620: INFO: projectcalico.org/v3 matches projectcalico.org/v3
    Jun  9 10:38:45.620: INFO: Checking APIGroup: crd.projectcalico.org
    Jun  9 10:38:45.623: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jun  9 10:38:45.623: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jun  9 10:38:45.623: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jun  9 10:38:45.623: INFO: Checking APIGroup: operator.tigera.io
    Jun  9 10:38:45.625: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
    Jun  9 10:38:45.625: INFO: Versions found [{operator.tigera.io/v1 v1}]
    Jun  9 10:38:45.625: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
    Jun  9 10:38:45.625: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jun  9 10:38:45.627: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jun  9 10:38:45.627: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    Jun  9 10:38:45.627: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Jun  9 10:38:45.627: INFO: Checking APIGroup: internal.packaging.carvel.dev
    Jun  9 10:38:45.629: INFO: PreferredVersion.GroupVersion: internal.packaging.carvel.dev/v1alpha1
    Jun  9 10:38:45.629: INFO: Versions found [{internal.packaging.carvel.dev/v1alpha1 v1alpha1}]
    Jun  9 10:38:45.629: INFO: internal.packaging.carvel.dev/v1alpha1 matches internal.packaging.carvel.dev/v1alpha1
    Jun  9 10:38:45.629: INFO: Checking APIGroup: kappctrl.k14s.io
    Jun  9 10:38:45.630: INFO: PreferredVersion.GroupVersion: kappctrl.k14s.io/v1alpha1
    Jun  9 10:38:45.630: INFO: Versions found [{kappctrl.k14s.io/v1alpha1 v1alpha1}]
    Jun  9 10:38:45.630: INFO: kappctrl.k14s.io/v1alpha1 matches kappctrl.k14s.io/v1alpha1
    Jun  9 10:38:45.630: INFO: Checking APIGroup: packaging.carvel.dev
    Jun  9 10:38:45.632: INFO: PreferredVersion.GroupVersion: packaging.carvel.dev/v1alpha1
    Jun  9 10:38:45.632: INFO: Versions found [{packaging.carvel.dev/v1alpha1 v1alpha1}]
    Jun  9 10:38:45.632: INFO: packaging.carvel.dev/v1alpha1 matches packaging.carvel.dev/v1alpha1
    Jun  9 10:38:45.632: INFO: Checking APIGroup: data.packaging.carvel.dev
    Jun  9 10:38:45.634: INFO: PreferredVersion.GroupVersion: data.packaging.carvel.dev/v1alpha1
    Jun  9 10:38:45.634: INFO: Versions found [{data.packaging.carvel.dev/v1alpha1 v1alpha1}]
    Jun  9 10:38:45.634: INFO: data.packaging.carvel.dev/v1alpha1 matches data.packaging.carvel.dev/v1alpha1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:38:45.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-9037" for this suite. 06/09/23 10:38:45.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:38:45.66
Jun  9 10:38:45.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename replicaset 06/09/23 10:38:45.661
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:45.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:45.702
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 06/09/23 10:38:45.709
Jun  9 10:38:45.728: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9326" to be "running and ready"
Jun  9 10:38:45.737: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.831312ms
Jun  9 10:38:45.737: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:38:47.744: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.016252429s
Jun  9 10:38:47.744: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jun  9 10:38:47.744: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 06/09/23 10:38:47.751
STEP: Then the orphan pod is adopted 06/09/23 10:38:47.765
STEP: When the matched label of one of its pods change 06/09/23 10:38:47.781
Jun  9 10:38:47.796: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 06/09/23 10:38:47.82
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun  9 10:38:48.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9326" for this suite. 06/09/23 10:38:48.845
------------------------------
• [3.228 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:38:45.66
    Jun  9 10:38:45.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename replicaset 06/09/23 10:38:45.661
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:45.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:45.702
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 06/09/23 10:38:45.709
    Jun  9 10:38:45.728: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9326" to be "running and ready"
    Jun  9 10:38:45.737: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.831312ms
    Jun  9 10:38:45.737: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:38:47.744: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.016252429s
    Jun  9 10:38:47.744: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jun  9 10:38:47.744: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 06/09/23 10:38:47.751
    STEP: Then the orphan pod is adopted 06/09/23 10:38:47.765
    STEP: When the matched label of one of its pods change 06/09/23 10:38:47.781
    Jun  9 10:38:47.796: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 06/09/23 10:38:47.82
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:38:48.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9326" for this suite. 06/09/23 10:38:48.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:38:48.889
Jun  9 10:38:48.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:38:48.891
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:48.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:48.935
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-aa33872f-648a-4889-8d04-0ba53d024419 06/09/23 10:38:48.941
STEP: Creating a pod to test consume secrets 06/09/23 10:38:48.952
Jun  9 10:38:48.967: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c" in namespace "projected-7517" to be "Succeeded or Failed"
Jun  9 10:38:48.976: INFO: Pod "pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.053003ms
Jun  9 10:38:50.990: INFO: Pod "pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022484596s
Jun  9 10:38:52.986: INFO: Pod "pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019303608s
STEP: Saw pod success 06/09/23 10:38:52.986
Jun  9 10:38:52.987: INFO: Pod "pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c" satisfied condition "Succeeded or Failed"
Jun  9 10:38:52.994: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c container projected-secret-volume-test: <nil>
STEP: delete the pod 06/09/23 10:38:53.008
Jun  9 10:38:53.029: INFO: Waiting for pod pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c to disappear
Jun  9 10:38:53.036: INFO: Pod pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun  9 10:38:53.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7517" for this suite. 06/09/23 10:38:53.049
------------------------------
• [4.184 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:38:48.889
    Jun  9 10:38:48.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:38:48.891
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:48.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:48.935
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-aa33872f-648a-4889-8d04-0ba53d024419 06/09/23 10:38:48.941
    STEP: Creating a pod to test consume secrets 06/09/23 10:38:48.952
    Jun  9 10:38:48.967: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c" in namespace "projected-7517" to be "Succeeded or Failed"
    Jun  9 10:38:48.976: INFO: Pod "pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.053003ms
    Jun  9 10:38:50.990: INFO: Pod "pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022484596s
    Jun  9 10:38:52.986: INFO: Pod "pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019303608s
    STEP: Saw pod success 06/09/23 10:38:52.986
    Jun  9 10:38:52.987: INFO: Pod "pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c" satisfied condition "Succeeded or Failed"
    Jun  9 10:38:52.994: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/09/23 10:38:53.008
    Jun  9 10:38:53.029: INFO: Waiting for pod pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c to disappear
    Jun  9 10:38:53.036: INFO: Pod pod-projected-secrets-82c23db6-47f9-47ee-a6d6-df4e3541bb0c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:38:53.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7517" for this suite. 06/09/23 10:38:53.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:38:53.075
Jun  9 10:38:53.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename cronjob 06/09/23 10:38:53.076
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:53.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:53.106
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 06/09/23 10:38:53.112
STEP: Ensuring more than one job is running at a time 06/09/23 10:38:53.119
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 06/09/23 10:40:01.177
STEP: Removing cronjob 06/09/23 10:40:01.183
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun  9 10:40:01.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7331" for this suite. 06/09/23 10:40:01.257
------------------------------
• [SLOW TEST] [68.443 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:38:53.075
    Jun  9 10:38:53.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename cronjob 06/09/23 10:38:53.076
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:38:53.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:38:53.106
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 06/09/23 10:38:53.112
    STEP: Ensuring more than one job is running at a time 06/09/23 10:38:53.119
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 06/09/23 10:40:01.177
    STEP: Removing cronjob 06/09/23 10:40:01.183
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:40:01.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7331" for this suite. 06/09/23 10:40:01.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:40:01.519
Jun  9 10:40:01.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 10:40:01.522
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:01.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:01.858
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-4431 06/09/23 10:40:01.865
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4431 to expose endpoints map[] 06/09/23 10:40:01.95
Jun  9 10:40:01.970: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jun  9 10:40:03.026: INFO: successfully validated that service multi-endpoint-test in namespace services-4431 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4431 06/09/23 10:40:03.026
Jun  9 10:40:03.048: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4431" to be "running and ready"
Jun  9 10:40:03.059: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.843789ms
Jun  9 10:40:03.059: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:40:05.143: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.094779948s
Jun  9 10:40:05.143: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun  9 10:40:05.143: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4431 to expose endpoints map[pod1:[100]] 06/09/23 10:40:05.335
Jun  9 10:40:05.369: INFO: successfully validated that service multi-endpoint-test in namespace services-4431 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4431 06/09/23 10:40:05.369
Jun  9 10:40:05.484: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4431" to be "running and ready"
Jun  9 10:40:05.505: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.434902ms
Jun  9 10:40:05.505: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:40:07.523: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.038882305s
Jun  9 10:40:07.523: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun  9 10:40:07.523: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4431 to expose endpoints map[pod1:[100] pod2:[101]] 06/09/23 10:40:07.531
Jun  9 10:40:07.580: INFO: successfully validated that service multi-endpoint-test in namespace services-4431 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 06/09/23 10:40:07.58
Jun  9 10:40:07.580: INFO: Creating new exec pod
Jun  9 10:40:07.593: INFO: Waiting up to 5m0s for pod "execpodng2lj" in namespace "services-4431" to be "running"
Jun  9 10:40:07.609: INFO: Pod "execpodng2lj": Phase="Pending", Reason="", readiness=false. Elapsed: 16.036425ms
Jun  9 10:40:09.617: INFO: Pod "execpodng2lj": Phase="Running", Reason="", readiness=true. Elapsed: 2.023840909s
Jun  9 10:40:09.617: INFO: Pod "execpodng2lj" satisfied condition "running"
Jun  9 10:40:10.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-4431 exec execpodng2lj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jun  9 10:40:10.832: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jun  9 10:40:10.832: INFO: stdout: ""
Jun  9 10:40:10.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-4431 exec execpodng2lj -- /bin/sh -x -c nc -v -z -w 2 10.99.132.78 80'
Jun  9 10:40:11.021: INFO: stderr: "+ nc -v -z -w 2 10.99.132.78 80\nConnection to 10.99.132.78 80 port [tcp/http] succeeded!\n"
Jun  9 10:40:11.021: INFO: stdout: ""
Jun  9 10:40:11.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-4431 exec execpodng2lj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jun  9 10:40:11.222: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jun  9 10:40:11.222: INFO: stdout: ""
Jun  9 10:40:11.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-4431 exec execpodng2lj -- /bin/sh -x -c nc -v -z -w 2 10.99.132.78 81'
Jun  9 10:40:11.434: INFO: stderr: "+ nc -v -z -w 2 10.99.132.78 81\nConnection to 10.99.132.78 81 port [tcp/*] succeeded!\n"
Jun  9 10:40:11.434: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-4431 06/09/23 10:40:11.434
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4431 to expose endpoints map[pod2:[101]] 06/09/23 10:40:11.632
Jun  9 10:40:11.676: INFO: successfully validated that service multi-endpoint-test in namespace services-4431 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4431 06/09/23 10:40:11.676
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4431 to expose endpoints map[] 06/09/23 10:40:11.744
Jun  9 10:40:11.785: INFO: successfully validated that service multi-endpoint-test in namespace services-4431 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 10:40:11.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4431" for this suite. 06/09/23 10:40:11.888
------------------------------
• [SLOW TEST] [10.386 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:40:01.519
    Jun  9 10:40:01.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 10:40:01.522
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:01.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:01.858
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-4431 06/09/23 10:40:01.865
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4431 to expose endpoints map[] 06/09/23 10:40:01.95
    Jun  9 10:40:01.970: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jun  9 10:40:03.026: INFO: successfully validated that service multi-endpoint-test in namespace services-4431 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4431 06/09/23 10:40:03.026
    Jun  9 10:40:03.048: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4431" to be "running and ready"
    Jun  9 10:40:03.059: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.843789ms
    Jun  9 10:40:03.059: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:40:05.143: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.094779948s
    Jun  9 10:40:05.143: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun  9 10:40:05.143: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4431 to expose endpoints map[pod1:[100]] 06/09/23 10:40:05.335
    Jun  9 10:40:05.369: INFO: successfully validated that service multi-endpoint-test in namespace services-4431 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-4431 06/09/23 10:40:05.369
    Jun  9 10:40:05.484: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4431" to be "running and ready"
    Jun  9 10:40:05.505: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.434902ms
    Jun  9 10:40:05.505: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:40:07.523: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.038882305s
    Jun  9 10:40:07.523: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun  9 10:40:07.523: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4431 to expose endpoints map[pod1:[100] pod2:[101]] 06/09/23 10:40:07.531
    Jun  9 10:40:07.580: INFO: successfully validated that service multi-endpoint-test in namespace services-4431 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 06/09/23 10:40:07.58
    Jun  9 10:40:07.580: INFO: Creating new exec pod
    Jun  9 10:40:07.593: INFO: Waiting up to 5m0s for pod "execpodng2lj" in namespace "services-4431" to be "running"
    Jun  9 10:40:07.609: INFO: Pod "execpodng2lj": Phase="Pending", Reason="", readiness=false. Elapsed: 16.036425ms
    Jun  9 10:40:09.617: INFO: Pod "execpodng2lj": Phase="Running", Reason="", readiness=true. Elapsed: 2.023840909s
    Jun  9 10:40:09.617: INFO: Pod "execpodng2lj" satisfied condition "running"
    Jun  9 10:40:10.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-4431 exec execpodng2lj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jun  9 10:40:10.832: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jun  9 10:40:10.832: INFO: stdout: ""
    Jun  9 10:40:10.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-4431 exec execpodng2lj -- /bin/sh -x -c nc -v -z -w 2 10.99.132.78 80'
    Jun  9 10:40:11.021: INFO: stderr: "+ nc -v -z -w 2 10.99.132.78 80\nConnection to 10.99.132.78 80 port [tcp/http] succeeded!\n"
    Jun  9 10:40:11.021: INFO: stdout: ""
    Jun  9 10:40:11.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-4431 exec execpodng2lj -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jun  9 10:40:11.222: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jun  9 10:40:11.222: INFO: stdout: ""
    Jun  9 10:40:11.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-4431 exec execpodng2lj -- /bin/sh -x -c nc -v -z -w 2 10.99.132.78 81'
    Jun  9 10:40:11.434: INFO: stderr: "+ nc -v -z -w 2 10.99.132.78 81\nConnection to 10.99.132.78 81 port [tcp/*] succeeded!\n"
    Jun  9 10:40:11.434: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-4431 06/09/23 10:40:11.434
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4431 to expose endpoints map[pod2:[101]] 06/09/23 10:40:11.632
    Jun  9 10:40:11.676: INFO: successfully validated that service multi-endpoint-test in namespace services-4431 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-4431 06/09/23 10:40:11.676
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4431 to expose endpoints map[] 06/09/23 10:40:11.744
    Jun  9 10:40:11.785: INFO: successfully validated that service multi-endpoint-test in namespace services-4431 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:40:11.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4431" for this suite. 06/09/23 10:40:11.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:40:11.927
Jun  9 10:40:11.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename replicaset 06/09/23 10:40:11.928
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:11.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:11.993
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 06/09/23 10:40:11.999
Jun  9 10:40:12.022: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  9 10:40:17.033: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/09/23 10:40:17.033
STEP: getting scale subresource 06/09/23 10:40:17.033
STEP: updating a scale subresource 06/09/23 10:40:17.122
STEP: verifying the replicaset Spec.Replicas was modified 06/09/23 10:40:17.145
STEP: Patch a scale subresource 06/09/23 10:40:17.156
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun  9 10:40:17.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-872" for this suite. 06/09/23 10:40:17.218
------------------------------
• [SLOW TEST] [5.310 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:40:11.927
    Jun  9 10:40:11.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename replicaset 06/09/23 10:40:11.928
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:11.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:11.993
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 06/09/23 10:40:11.999
    Jun  9 10:40:12.022: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun  9 10:40:17.033: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/09/23 10:40:17.033
    STEP: getting scale subresource 06/09/23 10:40:17.033
    STEP: updating a scale subresource 06/09/23 10:40:17.122
    STEP: verifying the replicaset Spec.Replicas was modified 06/09/23 10:40:17.145
    STEP: Patch a scale subresource 06/09/23 10:40:17.156
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:40:17.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-872" for this suite. 06/09/23 10:40:17.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:40:17.238
Jun  9 10:40:17.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename secrets 06/09/23 10:40:17.239
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:17.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:17.266
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-88051bd5-d8fc-4617-9cf0-85fb9816c723 06/09/23 10:40:17.273
STEP: Creating a pod to test consume secrets 06/09/23 10:40:17.3
Jun  9 10:40:17.322: INFO: Waiting up to 5m0s for pod "pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f" in namespace "secrets-6281" to be "Succeeded or Failed"
Jun  9 10:40:17.338: INFO: Pod "pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.312256ms
Jun  9 10:40:19.346: INFO: Pod "pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02405028s
Jun  9 10:40:21.351: INFO: Pod "pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029053828s
STEP: Saw pod success 06/09/23 10:40:21.351
Jun  9 10:40:21.351: INFO: Pod "pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f" satisfied condition "Succeeded or Failed"
Jun  9 10:40:21.358: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f container secret-volume-test: <nil>
STEP: delete the pod 06/09/23 10:40:21.39
Jun  9 10:40:21.440: INFO: Waiting for pod pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f to disappear
Jun  9 10:40:21.445: INFO: Pod pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  9 10:40:21.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6281" for this suite. 06/09/23 10:40:21.457
------------------------------
• [4.231 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:40:17.238
    Jun  9 10:40:17.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename secrets 06/09/23 10:40:17.239
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:17.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:17.266
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-88051bd5-d8fc-4617-9cf0-85fb9816c723 06/09/23 10:40:17.273
    STEP: Creating a pod to test consume secrets 06/09/23 10:40:17.3
    Jun  9 10:40:17.322: INFO: Waiting up to 5m0s for pod "pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f" in namespace "secrets-6281" to be "Succeeded or Failed"
    Jun  9 10:40:17.338: INFO: Pod "pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.312256ms
    Jun  9 10:40:19.346: INFO: Pod "pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02405028s
    Jun  9 10:40:21.351: INFO: Pod "pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029053828s
    STEP: Saw pod success 06/09/23 10:40:21.351
    Jun  9 10:40:21.351: INFO: Pod "pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f" satisfied condition "Succeeded or Failed"
    Jun  9 10:40:21.358: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f container secret-volume-test: <nil>
    STEP: delete the pod 06/09/23 10:40:21.39
    Jun  9 10:40:21.440: INFO: Waiting for pod pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f to disappear
    Jun  9 10:40:21.445: INFO: Pod pod-secrets-e80349f7-8edb-4a8d-a456-64bccc4ade0f no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:40:21.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6281" for this suite. 06/09/23 10:40:21.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:40:21.47
Jun  9 10:40:21.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 10:40:21.473
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:21.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:21.676
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-4925275b-5e93-4b32-8d8e-cfbdcf32d391 06/09/23 10:40:21.688
STEP: Creating a pod to test consume configMaps 06/09/23 10:40:21.707
Jun  9 10:40:21.744: INFO: Waiting up to 5m0s for pod "pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994" in namespace "configmap-1262" to be "Succeeded or Failed"
Jun  9 10:40:21.757: INFO: Pod "pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994": Phase="Pending", Reason="", readiness=false. Elapsed: 12.928763ms
Jun  9 10:40:23.764: INFO: Pod "pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020199553s
Jun  9 10:40:25.765: INFO: Pod "pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021454311s
STEP: Saw pod success 06/09/23 10:40:25.765
Jun  9 10:40:25.766: INFO: Pod "pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994" satisfied condition "Succeeded or Failed"
Jun  9 10:40:25.780: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994 container agnhost-container: <nil>
STEP: delete the pod 06/09/23 10:40:25.792
Jun  9 10:40:25.819: INFO: Waiting for pod pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994 to disappear
Jun  9 10:40:25.829: INFO: Pod pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 10:40:25.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1262" for this suite. 06/09/23 10:40:25.838
------------------------------
• [4.380 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:40:21.47
    Jun  9 10:40:21.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 10:40:21.473
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:21.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:21.676
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-4925275b-5e93-4b32-8d8e-cfbdcf32d391 06/09/23 10:40:21.688
    STEP: Creating a pod to test consume configMaps 06/09/23 10:40:21.707
    Jun  9 10:40:21.744: INFO: Waiting up to 5m0s for pod "pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994" in namespace "configmap-1262" to be "Succeeded or Failed"
    Jun  9 10:40:21.757: INFO: Pod "pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994": Phase="Pending", Reason="", readiness=false. Elapsed: 12.928763ms
    Jun  9 10:40:23.764: INFO: Pod "pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020199553s
    Jun  9 10:40:25.765: INFO: Pod "pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021454311s
    STEP: Saw pod success 06/09/23 10:40:25.765
    Jun  9 10:40:25.766: INFO: Pod "pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994" satisfied condition "Succeeded or Failed"
    Jun  9 10:40:25.780: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994 container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 10:40:25.792
    Jun  9 10:40:25.819: INFO: Waiting for pod pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994 to disappear
    Jun  9 10:40:25.829: INFO: Pod pod-configmaps-d30b2a6d-f1fe-4fed-9a9d-068d6fc4c994 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:40:25.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1262" for this suite. 06/09/23 10:40:25.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:40:25.852
Jun  9 10:40:25.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 10:40:25.855
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:25.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:25.892
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-1c496715-7f39-405e-82ac-f61537439b08 06/09/23 10:40:25.905
STEP: Creating configMap with name cm-test-opt-upd-9e49321b-f663-4bb2-bd85-587c95fe29be 06/09/23 10:40:25.915
STEP: Creating the pod 06/09/23 10:40:25.924
Jun  9 10:40:25.949: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce2c77de-c56b-4b25-bc1b-742f6853f289" in namespace "configmap-951" to be "running and ready"
Jun  9 10:40:25.962: INFO: Pod "pod-configmaps-ce2c77de-c56b-4b25-bc1b-742f6853f289": Phase="Pending", Reason="", readiness=false. Elapsed: 12.733975ms
Jun  9 10:40:25.962: INFO: The phase of Pod pod-configmaps-ce2c77de-c56b-4b25-bc1b-742f6853f289 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:40:27.970: INFO: Pod "pod-configmaps-ce2c77de-c56b-4b25-bc1b-742f6853f289": Phase="Running", Reason="", readiness=true. Elapsed: 2.021182301s
Jun  9 10:40:27.970: INFO: The phase of Pod pod-configmaps-ce2c77de-c56b-4b25-bc1b-742f6853f289 is Running (Ready = true)
Jun  9 10:40:27.970: INFO: Pod "pod-configmaps-ce2c77de-c56b-4b25-bc1b-742f6853f289" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-1c496715-7f39-405e-82ac-f61537439b08 06/09/23 10:40:28.029
STEP: Updating configmap cm-test-opt-upd-9e49321b-f663-4bb2-bd85-587c95fe29be 06/09/23 10:40:28.042
STEP: Creating configMap with name cm-test-opt-create-be1e9b37-9b64-413b-b998-021295057994 06/09/23 10:40:28.052
STEP: waiting to observe update in volume 06/09/23 10:40:28.063
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 10:40:30.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-951" for this suite. 06/09/23 10:40:30.121
------------------------------
• [4.286 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:40:25.852
    Jun  9 10:40:25.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 10:40:25.855
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:25.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:25.892
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-1c496715-7f39-405e-82ac-f61537439b08 06/09/23 10:40:25.905
    STEP: Creating configMap with name cm-test-opt-upd-9e49321b-f663-4bb2-bd85-587c95fe29be 06/09/23 10:40:25.915
    STEP: Creating the pod 06/09/23 10:40:25.924
    Jun  9 10:40:25.949: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce2c77de-c56b-4b25-bc1b-742f6853f289" in namespace "configmap-951" to be "running and ready"
    Jun  9 10:40:25.962: INFO: Pod "pod-configmaps-ce2c77de-c56b-4b25-bc1b-742f6853f289": Phase="Pending", Reason="", readiness=false. Elapsed: 12.733975ms
    Jun  9 10:40:25.962: INFO: The phase of Pod pod-configmaps-ce2c77de-c56b-4b25-bc1b-742f6853f289 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:40:27.970: INFO: Pod "pod-configmaps-ce2c77de-c56b-4b25-bc1b-742f6853f289": Phase="Running", Reason="", readiness=true. Elapsed: 2.021182301s
    Jun  9 10:40:27.970: INFO: The phase of Pod pod-configmaps-ce2c77de-c56b-4b25-bc1b-742f6853f289 is Running (Ready = true)
    Jun  9 10:40:27.970: INFO: Pod "pod-configmaps-ce2c77de-c56b-4b25-bc1b-742f6853f289" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-1c496715-7f39-405e-82ac-f61537439b08 06/09/23 10:40:28.029
    STEP: Updating configmap cm-test-opt-upd-9e49321b-f663-4bb2-bd85-587c95fe29be 06/09/23 10:40:28.042
    STEP: Creating configMap with name cm-test-opt-create-be1e9b37-9b64-413b-b998-021295057994 06/09/23 10:40:28.052
    STEP: waiting to observe update in volume 06/09/23 10:40:28.063
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:40:30.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-951" for this suite. 06/09/23 10:40:30.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:40:30.142
Jun  9 10:40:30.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 10:40:30.143
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:30.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:30.175
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 10:40:30.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7509" for this suite. 06/09/23 10:40:30.267
------------------------------
• [0.137 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:40:30.142
    Jun  9 10:40:30.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 10:40:30.143
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:30.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:30.175
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:40:30.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7509" for this suite. 06/09/23 10:40:30.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:40:30.28
Jun  9 10:40:30.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pod-network-test 06/09/23 10:40:30.28
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:30.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:30.315
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-7969 06/09/23 10:40:30.322
STEP: creating a selector 06/09/23 10:40:30.322
STEP: Creating the service pods in kubernetes 06/09/23 10:40:30.322
Jun  9 10:40:30.322: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  9 10:40:30.388: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7969" to be "running and ready"
Jun  9 10:40:30.408: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.736982ms
Jun  9 10:40:30.408: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:40:32.417: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.028343065s
Jun  9 10:40:32.417: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:40:34.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.027353555s
Jun  9 10:40:34.416: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:40:36.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.027028981s
Jun  9 10:40:36.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:40:38.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.026308346s
Jun  9 10:40:38.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:40:40.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.026879742s
Jun  9 10:40:40.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:40:42.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.027113563s
Jun  9 10:40:42.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:40:44.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.027966379s
Jun  9 10:40:44.416: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:40:46.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.027112436s
Jun  9 10:40:46.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:40:48.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.027821375s
Jun  9 10:40:48.416: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:40:50.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.027100959s
Jun  9 10:40:50.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:40:52.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.02806261s
Jun  9 10:40:52.416: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun  9 10:40:52.416: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun  9 10:40:52.422: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7969" to be "running and ready"
Jun  9 10:40:52.436: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.037161ms
Jun  9 10:40:52.436: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun  9 10:40:52.436: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun  9 10:40:52.447: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7969" to be "running and ready"
Jun  9 10:40:52.456: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 9.275741ms
Jun  9 10:40:52.456: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun  9 10:40:52.456: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/09/23 10:40:52.467
Jun  9 10:40:52.491: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7969" to be "running"
Jun  9 10:40:52.506: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.266751ms
Jun  9 10:40:54.518: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027256072s
Jun  9 10:40:56.513: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.021783597s
Jun  9 10:40:56.513: INFO: Pod "test-container-pod" satisfied condition "running"
Jun  9 10:40:56.520: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun  9 10:40:56.520: INFO: Breadth first check of 172.26.90.28 on host 10.255.64.104...
Jun  9 10:40:56.526: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.27.53.115:9080/dial?request=hostname&protocol=http&host=172.26.90.28&port=8083&tries=1'] Namespace:pod-network-test-7969 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:40:56.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:40:56.527: INFO: ExecWithOptions: Clientset creation
Jun  9 10:40:56.527: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7969/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.27.53.115%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.26.90.28%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  9 10:40:56.614: INFO: Waiting for responses: map[]
Jun  9 10:40:56.614: INFO: reached 172.26.90.28 after 0/1 tries
Jun  9 10:40:56.615: INFO: Breadth first check of 172.30.17.180 on host 10.255.64.102...
Jun  9 10:40:56.622: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.27.53.115:9080/dial?request=hostname&protocol=http&host=172.30.17.180&port=8083&tries=1'] Namespace:pod-network-test-7969 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:40:56.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:40:56.623: INFO: ExecWithOptions: Clientset creation
Jun  9 10:40:56.623: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7969/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.27.53.115%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.17.180%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  9 10:40:56.735: INFO: Waiting for responses: map[]
Jun  9 10:40:56.735: INFO: reached 172.30.17.180 after 0/1 tries
Jun  9 10:40:56.735: INFO: Breadth first check of 172.27.53.112 on host 10.255.64.103...
Jun  9 10:40:56.741: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.27.53.115:9080/dial?request=hostname&protocol=http&host=172.27.53.112&port=8083&tries=1'] Namespace:pod-network-test-7969 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:40:56.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:40:56.742: INFO: ExecWithOptions: Clientset creation
Jun  9 10:40:56.742: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7969/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.27.53.115%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.27.53.112%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  9 10:40:56.818: INFO: Waiting for responses: map[]
Jun  9 10:40:56.818: INFO: reached 172.27.53.112 after 0/1 tries
Jun  9 10:40:56.818: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jun  9 10:40:56.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7969" for this suite. 06/09/23 10:40:56.83
------------------------------
• [SLOW TEST] [26.562 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:40:30.28
    Jun  9 10:40:30.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pod-network-test 06/09/23 10:40:30.28
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:30.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:30.315
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-7969 06/09/23 10:40:30.322
    STEP: creating a selector 06/09/23 10:40:30.322
    STEP: Creating the service pods in kubernetes 06/09/23 10:40:30.322
    Jun  9 10:40:30.322: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun  9 10:40:30.388: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7969" to be "running and ready"
    Jun  9 10:40:30.408: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.736982ms
    Jun  9 10:40:30.408: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:40:32.417: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.028343065s
    Jun  9 10:40:32.417: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:40:34.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.027353555s
    Jun  9 10:40:34.416: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:40:36.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.027028981s
    Jun  9 10:40:36.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:40:38.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.026308346s
    Jun  9 10:40:38.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:40:40.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.026879742s
    Jun  9 10:40:40.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:40:42.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.027113563s
    Jun  9 10:40:42.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:40:44.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.027966379s
    Jun  9 10:40:44.416: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:40:46.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.027112436s
    Jun  9 10:40:46.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:40:48.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.027821375s
    Jun  9 10:40:48.416: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:40:50.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.027100959s
    Jun  9 10:40:50.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:40:52.416: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.02806261s
    Jun  9 10:40:52.416: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun  9 10:40:52.416: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun  9 10:40:52.422: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7969" to be "running and ready"
    Jun  9 10:40:52.436: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.037161ms
    Jun  9 10:40:52.436: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun  9 10:40:52.436: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun  9 10:40:52.447: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7969" to be "running and ready"
    Jun  9 10:40:52.456: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 9.275741ms
    Jun  9 10:40:52.456: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun  9 10:40:52.456: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/09/23 10:40:52.467
    Jun  9 10:40:52.491: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7969" to be "running"
    Jun  9 10:40:52.506: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.266751ms
    Jun  9 10:40:54.518: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027256072s
    Jun  9 10:40:56.513: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.021783597s
    Jun  9 10:40:56.513: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun  9 10:40:56.520: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun  9 10:40:56.520: INFO: Breadth first check of 172.26.90.28 on host 10.255.64.104...
    Jun  9 10:40:56.526: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.27.53.115:9080/dial?request=hostname&protocol=http&host=172.26.90.28&port=8083&tries=1'] Namespace:pod-network-test-7969 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:40:56.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:40:56.527: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:40:56.527: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7969/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.27.53.115%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.26.90.28%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  9 10:40:56.614: INFO: Waiting for responses: map[]
    Jun  9 10:40:56.614: INFO: reached 172.26.90.28 after 0/1 tries
    Jun  9 10:40:56.615: INFO: Breadth first check of 172.30.17.180 on host 10.255.64.102...
    Jun  9 10:40:56.622: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.27.53.115:9080/dial?request=hostname&protocol=http&host=172.30.17.180&port=8083&tries=1'] Namespace:pod-network-test-7969 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:40:56.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:40:56.623: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:40:56.623: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7969/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.27.53.115%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.17.180%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  9 10:40:56.735: INFO: Waiting for responses: map[]
    Jun  9 10:40:56.735: INFO: reached 172.30.17.180 after 0/1 tries
    Jun  9 10:40:56.735: INFO: Breadth first check of 172.27.53.112 on host 10.255.64.103...
    Jun  9 10:40:56.741: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.27.53.115:9080/dial?request=hostname&protocol=http&host=172.27.53.112&port=8083&tries=1'] Namespace:pod-network-test-7969 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:40:56.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:40:56.742: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:40:56.742: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7969/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.27.53.115%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.27.53.112%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  9 10:40:56.818: INFO: Waiting for responses: map[]
    Jun  9 10:40:56.818: INFO: reached 172.27.53.112 after 0/1 tries
    Jun  9 10:40:56.818: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:40:56.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7969" for this suite. 06/09/23 10:40:56.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:40:56.846
Jun  9 10:40:56.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename dns 06/09/23 10:40:56.847
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:56.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:56.875
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 06/09/23 10:40:56.879
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 06/09/23 10:40:56.879
STEP: creating a pod to probe DNS 06/09/23 10:40:56.879
STEP: submitting the pod to kubernetes 06/09/23 10:40:56.879
Jun  9 10:40:56.898: INFO: Waiting up to 15m0s for pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b" in namespace "dns-5202" to be "running"
Jun  9 10:40:56.907: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.346587ms
Jun  9 10:40:58.915: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016662465s
Jun  9 10:41:00.913: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01478458s
Jun  9 10:41:02.913: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014803662s
Jun  9 10:41:04.919: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021060455s
Jun  9 10:41:06.914: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015715893s
Jun  9 10:41:08.916: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.018095543s
Jun  9 10:41:10.914: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.016140482s
Jun  9 10:41:12.916: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.017363155s
Jun  9 10:41:14.916: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.017401309s
Jun  9 10:41:16.916: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.017874521s
Jun  9 10:41:18.916: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.018089328s
Jun  9 10:41:20.913: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.014868638s
Jun  9 10:41:22.913: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 26.015162858s
Jun  9 10:41:24.918: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Running", Reason="", readiness=true. Elapsed: 28.020067775s
Jun  9 10:41:24.918: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b" satisfied condition "running"
STEP: retrieving the pod 06/09/23 10:41:24.918
STEP: looking for the results for each expected name from probers 06/09/23 10:41:24.927
Jun  9 10:41:24.983: INFO: DNS probes using dns-5202/dns-test-10ca15df-fda9-490e-9999-93bb1505298b succeeded

STEP: deleting the pod 06/09/23 10:41:24.983
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  9 10:41:25.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5202" for this suite. 06/09/23 10:41:25.013
------------------------------
• [SLOW TEST] [28.179 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:40:56.846
    Jun  9 10:40:56.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename dns 06/09/23 10:40:56.847
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:40:56.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:40:56.875
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     06/09/23 10:40:56.879
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     06/09/23 10:40:56.879
    STEP: creating a pod to probe DNS 06/09/23 10:40:56.879
    STEP: submitting the pod to kubernetes 06/09/23 10:40:56.879
    Jun  9 10:40:56.898: INFO: Waiting up to 15m0s for pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b" in namespace "dns-5202" to be "running"
    Jun  9 10:40:56.907: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.346587ms
    Jun  9 10:40:58.915: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016662465s
    Jun  9 10:41:00.913: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01478458s
    Jun  9 10:41:02.913: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014803662s
    Jun  9 10:41:04.919: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021060455s
    Jun  9 10:41:06.914: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015715893s
    Jun  9 10:41:08.916: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.018095543s
    Jun  9 10:41:10.914: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.016140482s
    Jun  9 10:41:12.916: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.017363155s
    Jun  9 10:41:14.916: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.017401309s
    Jun  9 10:41:16.916: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.017874521s
    Jun  9 10:41:18.916: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.018089328s
    Jun  9 10:41:20.913: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.014868638s
    Jun  9 10:41:22.913: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Pending", Reason="", readiness=false. Elapsed: 26.015162858s
    Jun  9 10:41:24.918: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b": Phase="Running", Reason="", readiness=true. Elapsed: 28.020067775s
    Jun  9 10:41:24.918: INFO: Pod "dns-test-10ca15df-fda9-490e-9999-93bb1505298b" satisfied condition "running"
    STEP: retrieving the pod 06/09/23 10:41:24.918
    STEP: looking for the results for each expected name from probers 06/09/23 10:41:24.927
    Jun  9 10:41:24.983: INFO: DNS probes using dns-5202/dns-test-10ca15df-fda9-490e-9999-93bb1505298b succeeded

    STEP: deleting the pod 06/09/23 10:41:24.983
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:41:25.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5202" for this suite. 06/09/23 10:41:25.013
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:41:25.026
Jun  9 10:41:25.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 10:41:25.027
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:25.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:25.057
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 06/09/23 10:41:25.062
Jun  9 10:41:25.076: INFO: Waiting up to 5m0s for pod "pod-4a58b1bd-1054-43db-824d-d41052d0aaa0" in namespace "emptydir-6336" to be "Succeeded or Failed"
Jun  9 10:41:25.084: INFO: Pod "pod-4a58b1bd-1054-43db-824d-d41052d0aaa0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.884572ms
Jun  9 10:41:27.091: INFO: Pod "pod-4a58b1bd-1054-43db-824d-d41052d0aaa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014769741s
Jun  9 10:41:29.095: INFO: Pod "pod-4a58b1bd-1054-43db-824d-d41052d0aaa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018261441s
STEP: Saw pod success 06/09/23 10:41:29.095
Jun  9 10:41:29.095: INFO: Pod "pod-4a58b1bd-1054-43db-824d-d41052d0aaa0" satisfied condition "Succeeded or Failed"
Jun  9 10:41:29.102: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-4a58b1bd-1054-43db-824d-d41052d0aaa0 container test-container: <nil>
STEP: delete the pod 06/09/23 10:41:29.115
Jun  9 10:41:29.224: INFO: Waiting for pod pod-4a58b1bd-1054-43db-824d-d41052d0aaa0 to disappear
Jun  9 10:41:29.230: INFO: Pod pod-4a58b1bd-1054-43db-824d-d41052d0aaa0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 10:41:29.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6336" for this suite. 06/09/23 10:41:29.239
------------------------------
• [4.225 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:41:25.026
    Jun  9 10:41:25.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 10:41:25.027
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:25.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:25.057
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 06/09/23 10:41:25.062
    Jun  9 10:41:25.076: INFO: Waiting up to 5m0s for pod "pod-4a58b1bd-1054-43db-824d-d41052d0aaa0" in namespace "emptydir-6336" to be "Succeeded or Failed"
    Jun  9 10:41:25.084: INFO: Pod "pod-4a58b1bd-1054-43db-824d-d41052d0aaa0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.884572ms
    Jun  9 10:41:27.091: INFO: Pod "pod-4a58b1bd-1054-43db-824d-d41052d0aaa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014769741s
    Jun  9 10:41:29.095: INFO: Pod "pod-4a58b1bd-1054-43db-824d-d41052d0aaa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018261441s
    STEP: Saw pod success 06/09/23 10:41:29.095
    Jun  9 10:41:29.095: INFO: Pod "pod-4a58b1bd-1054-43db-824d-d41052d0aaa0" satisfied condition "Succeeded or Failed"
    Jun  9 10:41:29.102: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-4a58b1bd-1054-43db-824d-d41052d0aaa0 container test-container: <nil>
    STEP: delete the pod 06/09/23 10:41:29.115
    Jun  9 10:41:29.224: INFO: Waiting for pod pod-4a58b1bd-1054-43db-824d-d41052d0aaa0 to disappear
    Jun  9 10:41:29.230: INFO: Pod pod-4a58b1bd-1054-43db-824d-d41052d0aaa0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:41:29.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6336" for this suite. 06/09/23 10:41:29.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:41:29.253
Jun  9 10:41:29.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 10:41:29.255
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:29.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:29.292
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 10:41:29.424
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:41:29.919
STEP: Deploying the webhook pod 06/09/23 10:41:29.999
STEP: Wait for the deployment to be ready 06/09/23 10:41:30.024
Jun  9 10:41:30.066: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 10:41:32.098
STEP: Verifying the service has paired with the endpoint 06/09/23 10:41:32.161
Jun  9 10:41:33.161: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 06/09/23 10:41:33.169
STEP: create a namespace for the webhook 06/09/23 10:41:33.195
STEP: create a configmap should be unconditionally rejected by the webhook 06/09/23 10:41:33.213
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:41:33.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6813" for this suite. 06/09/23 10:41:33.441
STEP: Destroying namespace "webhook-6813-markers" for this suite. 06/09/23 10:41:33.46
------------------------------
• [4.226 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:41:29.253
    Jun  9 10:41:29.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 10:41:29.255
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:29.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:29.292
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 10:41:29.424
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:41:29.919
    STEP: Deploying the webhook pod 06/09/23 10:41:29.999
    STEP: Wait for the deployment to be ready 06/09/23 10:41:30.024
    Jun  9 10:41:30.066: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 10:41:32.098
    STEP: Verifying the service has paired with the endpoint 06/09/23 10:41:32.161
    Jun  9 10:41:33.161: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 06/09/23 10:41:33.169
    STEP: create a namespace for the webhook 06/09/23 10:41:33.195
    STEP: create a configmap should be unconditionally rejected by the webhook 06/09/23 10:41:33.213
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:41:33.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6813" for this suite. 06/09/23 10:41:33.441
    STEP: Destroying namespace "webhook-6813-markers" for this suite. 06/09/23 10:41:33.46
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:41:33.48
Jun  9 10:41:33.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:41:33.481
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:33.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:33.546
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-593c862c-a23b-47f5-84db-ad5a5a14645e 06/09/23 10:41:33.553
STEP: Creating a pod to test consume secrets 06/09/23 10:41:33.562
Jun  9 10:41:33.583: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185" in namespace "projected-3653" to be "Succeeded or Failed"
Jun  9 10:41:33.592: INFO: Pod "pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185": Phase="Pending", Reason="", readiness=false. Elapsed: 8.624854ms
Jun  9 10:41:35.599: INFO: Pod "pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185": Phase="Running", Reason="", readiness=true. Elapsed: 2.016009885s
Jun  9 10:41:37.601: INFO: Pod "pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185": Phase="Running", Reason="", readiness=false. Elapsed: 4.017112068s
Jun  9 10:41:39.599: INFO: Pod "pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016022286s
STEP: Saw pod success 06/09/23 10:41:39.6
Jun  9 10:41:39.600: INFO: Pod "pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185" satisfied condition "Succeeded or Failed"
Jun  9 10:41:39.611: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/09/23 10:41:39.625
Jun  9 10:41:39.648: INFO: Waiting for pod pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185 to disappear
Jun  9 10:41:39.656: INFO: Pod pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun  9 10:41:39.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3653" for this suite. 06/09/23 10:41:39.669
------------------------------
• [SLOW TEST] [6.207 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:41:33.48
    Jun  9 10:41:33.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:41:33.481
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:33.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:33.546
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-593c862c-a23b-47f5-84db-ad5a5a14645e 06/09/23 10:41:33.553
    STEP: Creating a pod to test consume secrets 06/09/23 10:41:33.562
    Jun  9 10:41:33.583: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185" in namespace "projected-3653" to be "Succeeded or Failed"
    Jun  9 10:41:33.592: INFO: Pod "pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185": Phase="Pending", Reason="", readiness=false. Elapsed: 8.624854ms
    Jun  9 10:41:35.599: INFO: Pod "pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185": Phase="Running", Reason="", readiness=true. Elapsed: 2.016009885s
    Jun  9 10:41:37.601: INFO: Pod "pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185": Phase="Running", Reason="", readiness=false. Elapsed: 4.017112068s
    Jun  9 10:41:39.599: INFO: Pod "pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016022286s
    STEP: Saw pod success 06/09/23 10:41:39.6
    Jun  9 10:41:39.600: INFO: Pod "pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185" satisfied condition "Succeeded or Failed"
    Jun  9 10:41:39.611: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/09/23 10:41:39.625
    Jun  9 10:41:39.648: INFO: Waiting for pod pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185 to disappear
    Jun  9 10:41:39.656: INFO: Pod pod-projected-secrets-ff4642d5-49dd-4f96-ae3c-1dd674267185 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:41:39.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3653" for this suite. 06/09/23 10:41:39.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:41:39.689
Jun  9 10:41:39.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename csiinlinevolumes 06/09/23 10:41:39.69
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:39.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:39.731
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 06/09/23 10:41:39.737
STEP: getting 06/09/23 10:41:39.78
STEP: listing 06/09/23 10:41:39.795
STEP: deleting 06/09/23 10:41:39.803
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jun  9 10:41:39.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-8313" for this suite. 06/09/23 10:41:39.848
------------------------------
• [0.189 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:41:39.689
    Jun  9 10:41:39.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename csiinlinevolumes 06/09/23 10:41:39.69
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:39.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:39.731
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 06/09/23 10:41:39.737
    STEP: getting 06/09/23 10:41:39.78
    STEP: listing 06/09/23 10:41:39.795
    STEP: deleting 06/09/23 10:41:39.803
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:41:39.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-8313" for this suite. 06/09/23 10:41:39.848
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:41:39.879
Jun  9 10:41:39.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename crd-webhook 06/09/23 10:41:39.881
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:39.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:39.971
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 06/09/23 10:41:39.976
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/09/23 10:41:40.268
STEP: Deploying the custom resource conversion webhook pod 06/09/23 10:41:40.276
STEP: Wait for the deployment to be ready 06/09/23 10:41:40.294
Jun  9 10:41:40.311: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 10:41:42.425
STEP: Verifying the service has paired with the endpoint 06/09/23 10:41:42.477
Jun  9 10:41:43.477: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jun  9 10:41:43.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Creating a v1 custom resource 06/09/23 10:41:46.11
STEP: Create a v2 custom resource 06/09/23 10:41:46.141
STEP: List CRs in v1 06/09/23 10:41:46.209
STEP: List CRs in v2 06/09/23 10:41:46.22
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:41:46.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-9039" for this suite. 06/09/23 10:41:46.876
------------------------------
• [SLOW TEST] [7.009 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:41:39.879
    Jun  9 10:41:39.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename crd-webhook 06/09/23 10:41:39.881
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:39.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:39.971
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 06/09/23 10:41:39.976
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/09/23 10:41:40.268
    STEP: Deploying the custom resource conversion webhook pod 06/09/23 10:41:40.276
    STEP: Wait for the deployment to be ready 06/09/23 10:41:40.294
    Jun  9 10:41:40.311: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 10:41:42.425
    STEP: Verifying the service has paired with the endpoint 06/09/23 10:41:42.477
    Jun  9 10:41:43.477: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jun  9 10:41:43.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Creating a v1 custom resource 06/09/23 10:41:46.11
    STEP: Create a v2 custom resource 06/09/23 10:41:46.141
    STEP: List CRs in v1 06/09/23 10:41:46.209
    STEP: List CRs in v2 06/09/23 10:41:46.22
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:41:46.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-9039" for this suite. 06/09/23 10:41:46.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:41:46.888
Jun  9 10:41:46.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pods 06/09/23 10:41:46.889
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:46.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:46.969
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jun  9 10:41:46.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: creating the pod 06/09/23 10:41:46.982
STEP: submitting the pod to kubernetes 06/09/23 10:41:46.982
Jun  9 10:41:46.997: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-114f1764-3dd1-4dd3-b65f-9baa53980ed4" in namespace "pods-7251" to be "running and ready"
Jun  9 10:41:47.010: INFO: Pod "pod-logs-websocket-114f1764-3dd1-4dd3-b65f-9baa53980ed4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.558158ms
Jun  9 10:41:47.010: INFO: The phase of Pod pod-logs-websocket-114f1764-3dd1-4dd3-b65f-9baa53980ed4 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:41:49.016: INFO: Pod "pod-logs-websocket-114f1764-3dd1-4dd3-b65f-9baa53980ed4": Phase="Running", Reason="", readiness=true. Elapsed: 2.019249552s
Jun  9 10:41:49.016: INFO: The phase of Pod pod-logs-websocket-114f1764-3dd1-4dd3-b65f-9baa53980ed4 is Running (Ready = true)
Jun  9 10:41:49.016: INFO: Pod "pod-logs-websocket-114f1764-3dd1-4dd3-b65f-9baa53980ed4" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  9 10:41:49.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7251" for this suite. 06/09/23 10:41:49.055
------------------------------
• [2.180 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:41:46.888
    Jun  9 10:41:46.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pods 06/09/23 10:41:46.889
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:46.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:46.969
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jun  9 10:41:46.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: creating the pod 06/09/23 10:41:46.982
    STEP: submitting the pod to kubernetes 06/09/23 10:41:46.982
    Jun  9 10:41:46.997: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-114f1764-3dd1-4dd3-b65f-9baa53980ed4" in namespace "pods-7251" to be "running and ready"
    Jun  9 10:41:47.010: INFO: Pod "pod-logs-websocket-114f1764-3dd1-4dd3-b65f-9baa53980ed4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.558158ms
    Jun  9 10:41:47.010: INFO: The phase of Pod pod-logs-websocket-114f1764-3dd1-4dd3-b65f-9baa53980ed4 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:41:49.016: INFO: Pod "pod-logs-websocket-114f1764-3dd1-4dd3-b65f-9baa53980ed4": Phase="Running", Reason="", readiness=true. Elapsed: 2.019249552s
    Jun  9 10:41:49.016: INFO: The phase of Pod pod-logs-websocket-114f1764-3dd1-4dd3-b65f-9baa53980ed4 is Running (Ready = true)
    Jun  9 10:41:49.016: INFO: Pod "pod-logs-websocket-114f1764-3dd1-4dd3-b65f-9baa53980ed4" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:41:49.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7251" for this suite. 06/09/23 10:41:49.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:41:49.07
Jun  9 10:41:49.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-lifecycle-hook 06/09/23 10:41:49.072
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:49.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:49.104
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 06/09/23 10:41:49.116
Jun  9 10:41:49.129: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-219" to be "running and ready"
Jun  9 10:41:49.135: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.154359ms
Jun  9 10:41:49.135: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:41:51.143: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013814871s
Jun  9 10:41:51.143: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun  9 10:41:51.143: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 06/09/23 10:41:51.149
Jun  9 10:41:51.159: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-219" to be "running and ready"
Jun  9 10:41:51.165: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.193762ms
Jun  9 10:41:51.166: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:41:53.174: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014868736s
Jun  9 10:41:53.174: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jun  9 10:41:53.174: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 06/09/23 10:41:53.193
Jun  9 10:41:53.204: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  9 10:41:53.213: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  9 10:41:55.213: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  9 10:41:55.220: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  9 10:41:57.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  9 10:41:57.222: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 06/09/23 10:41:57.222
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jun  9 10:41:57.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-219" for this suite. 06/09/23 10:41:57.258
------------------------------
• [SLOW TEST] [8.208 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:41:49.07
    Jun  9 10:41:49.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/09/23 10:41:49.072
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:49.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:49.104
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 06/09/23 10:41:49.116
    Jun  9 10:41:49.129: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-219" to be "running and ready"
    Jun  9 10:41:49.135: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.154359ms
    Jun  9 10:41:49.135: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:41:51.143: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013814871s
    Jun  9 10:41:51.143: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun  9 10:41:51.143: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 06/09/23 10:41:51.149
    Jun  9 10:41:51.159: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-219" to be "running and ready"
    Jun  9 10:41:51.165: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.193762ms
    Jun  9 10:41:51.166: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:41:53.174: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014868736s
    Jun  9 10:41:53.174: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jun  9 10:41:53.174: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 06/09/23 10:41:53.193
    Jun  9 10:41:53.204: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun  9 10:41:53.213: INFO: Pod pod-with-prestop-exec-hook still exists
    Jun  9 10:41:55.213: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun  9 10:41:55.220: INFO: Pod pod-with-prestop-exec-hook still exists
    Jun  9 10:41:57.214: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun  9 10:41:57.222: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 06/09/23 10:41:57.222
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:41:57.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-219" for this suite. 06/09/23 10:41:57.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:41:57.279
Jun  9 10:41:57.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 10:41:57.281
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:57.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:57.311
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/09/23 10:41:57.317
Jun  9 10:41:57.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-9621 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Jun  9 10:41:57.435: INFO: stderr: ""
Jun  9 10:41:57.435: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 06/09/23 10:41:57.435
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jun  9 10:41:57.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-9621 delete pods e2e-test-httpd-pod'
Jun  9 10:41:59.896: INFO: stderr: ""
Jun  9 10:41:59.896: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 10:41:59.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9621" for this suite. 06/09/23 10:41:59.913
------------------------------
• [2.655 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:41:57.279
    Jun  9 10:41:57.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 10:41:57.281
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:57.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:57.311
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/09/23 10:41:57.317
    Jun  9 10:41:57.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-9621 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Jun  9 10:41:57.435: INFO: stderr: ""
    Jun  9 10:41:57.435: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 06/09/23 10:41:57.435
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jun  9 10:41:57.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-9621 delete pods e2e-test-httpd-pod'
    Jun  9 10:41:59.896: INFO: stderr: ""
    Jun  9 10:41:59.896: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:41:59.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9621" for this suite. 06/09/23 10:41:59.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:41:59.935
Jun  9 10:41:59.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 10:41:59.936
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:59.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:59.971
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5650 06/09/23 10:41:59.977
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/09/23 10:42:00.013
STEP: creating service externalsvc in namespace services-5650 06/09/23 10:42:00.014
STEP: creating replication controller externalsvc in namespace services-5650 06/09/23 10:42:00.069
I0609 10:42:00.096396      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5650, replica count: 2
I0609 10:42:03.150029      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 06/09/23 10:42:03.156
Jun  9 10:42:03.206: INFO: Creating new exec pod
Jun  9 10:42:03.222: INFO: Waiting up to 5m0s for pod "execpodldh47" in namespace "services-5650" to be "running"
Jun  9 10:42:03.232: INFO: Pod "execpodldh47": Phase="Pending", Reason="", readiness=false. Elapsed: 10.032875ms
Jun  9 10:42:05.237: INFO: Pod "execpodldh47": Phase="Running", Reason="", readiness=true. Elapsed: 2.015612805s
Jun  9 10:42:05.237: INFO: Pod "execpodldh47" satisfied condition "running"
Jun  9 10:42:05.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-5650 exec execpodldh47 -- /bin/sh -x -c nslookup clusterip-service.services-5650.svc.cluster.local'
Jun  9 10:42:05.452: INFO: stderr: "+ nslookup clusterip-service.services-5650.svc.cluster.local\n"
Jun  9 10:42:05.452: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-5650.svc.cluster.local\tcanonical name = externalsvc.services-5650.svc.cluster.local.\nName:\texternalsvc.services-5650.svc.cluster.local\nAddress: 10.104.56.73\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5650, will wait for the garbage collector to delete the pods 06/09/23 10:42:05.452
Jun  9 10:42:05.529: INFO: Deleting ReplicationController externalsvc took: 13.078153ms
Jun  9 10:42:05.630: INFO: Terminating ReplicationController externalsvc pods took: 100.557579ms
Jun  9 10:42:08.991: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 10:42:09.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5650" for this suite. 06/09/23 10:42:09.072
------------------------------
• [SLOW TEST] [9.149 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:41:59.935
    Jun  9 10:41:59.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 10:41:59.936
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:41:59.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:41:59.971
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5650 06/09/23 10:41:59.977
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/09/23 10:42:00.013
    STEP: creating service externalsvc in namespace services-5650 06/09/23 10:42:00.014
    STEP: creating replication controller externalsvc in namespace services-5650 06/09/23 10:42:00.069
    I0609 10:42:00.096396      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5650, replica count: 2
    I0609 10:42:03.150029      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 06/09/23 10:42:03.156
    Jun  9 10:42:03.206: INFO: Creating new exec pod
    Jun  9 10:42:03.222: INFO: Waiting up to 5m0s for pod "execpodldh47" in namespace "services-5650" to be "running"
    Jun  9 10:42:03.232: INFO: Pod "execpodldh47": Phase="Pending", Reason="", readiness=false. Elapsed: 10.032875ms
    Jun  9 10:42:05.237: INFO: Pod "execpodldh47": Phase="Running", Reason="", readiness=true. Elapsed: 2.015612805s
    Jun  9 10:42:05.237: INFO: Pod "execpodldh47" satisfied condition "running"
    Jun  9 10:42:05.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-5650 exec execpodldh47 -- /bin/sh -x -c nslookup clusterip-service.services-5650.svc.cluster.local'
    Jun  9 10:42:05.452: INFO: stderr: "+ nslookup clusterip-service.services-5650.svc.cluster.local\n"
    Jun  9 10:42:05.452: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-5650.svc.cluster.local\tcanonical name = externalsvc.services-5650.svc.cluster.local.\nName:\texternalsvc.services-5650.svc.cluster.local\nAddress: 10.104.56.73\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-5650, will wait for the garbage collector to delete the pods 06/09/23 10:42:05.452
    Jun  9 10:42:05.529: INFO: Deleting ReplicationController externalsvc took: 13.078153ms
    Jun  9 10:42:05.630: INFO: Terminating ReplicationController externalsvc pods took: 100.557579ms
    Jun  9 10:42:08.991: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:42:09.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5650" for this suite. 06/09/23 10:42:09.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:42:09.085
Jun  9 10:42:09.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename job 06/09/23 10:42:09.086
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:42:09.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:42:09.117
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 06/09/23 10:42:09.121
STEP: Ensuring job reaches completions 06/09/23 10:42:09.13
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun  9 10:42:25.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-862" for this suite. 06/09/23 10:42:25.152
------------------------------
• [SLOW TEST] [16.078 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:42:09.085
    Jun  9 10:42:09.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename job 06/09/23 10:42:09.086
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:42:09.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:42:09.117
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 06/09/23 10:42:09.121
    STEP: Ensuring job reaches completions 06/09/23 10:42:09.13
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:42:25.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-862" for this suite. 06/09/23 10:42:25.152
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:42:25.165
Jun  9 10:42:25.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename security-context 06/09/23 10:42:25.167
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:42:25.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:42:25.195
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/09/23 10:42:25.199
Jun  9 10:42:25.213: INFO: Waiting up to 5m0s for pod "security-context-89df77a2-9765-48ee-9281-e04204cc0ccc" in namespace "security-context-3045" to be "Succeeded or Failed"
Jun  9 10:42:25.222: INFO: Pod "security-context-89df77a2-9765-48ee-9281-e04204cc0ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.362123ms
Jun  9 10:42:27.229: INFO: Pod "security-context-89df77a2-9765-48ee-9281-e04204cc0ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015809859s
Jun  9 10:42:29.231: INFO: Pod "security-context-89df77a2-9765-48ee-9281-e04204cc0ccc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017170895s
STEP: Saw pod success 06/09/23 10:42:29.231
Jun  9 10:42:29.231: INFO: Pod "security-context-89df77a2-9765-48ee-9281-e04204cc0ccc" satisfied condition "Succeeded or Failed"
Jun  9 10:42:29.237: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod security-context-89df77a2-9765-48ee-9281-e04204cc0ccc container test-container: <nil>
STEP: delete the pod 06/09/23 10:42:29.266
Jun  9 10:42:29.285: INFO: Waiting for pod security-context-89df77a2-9765-48ee-9281-e04204cc0ccc to disappear
Jun  9 10:42:29.291: INFO: Pod security-context-89df77a2-9765-48ee-9281-e04204cc0ccc no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun  9 10:42:29.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-3045" for this suite. 06/09/23 10:42:29.303
------------------------------
• [4.157 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:42:25.165
    Jun  9 10:42:25.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename security-context 06/09/23 10:42:25.167
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:42:25.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:42:25.195
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/09/23 10:42:25.199
    Jun  9 10:42:25.213: INFO: Waiting up to 5m0s for pod "security-context-89df77a2-9765-48ee-9281-e04204cc0ccc" in namespace "security-context-3045" to be "Succeeded or Failed"
    Jun  9 10:42:25.222: INFO: Pod "security-context-89df77a2-9765-48ee-9281-e04204cc0ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.362123ms
    Jun  9 10:42:27.229: INFO: Pod "security-context-89df77a2-9765-48ee-9281-e04204cc0ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015809859s
    Jun  9 10:42:29.231: INFO: Pod "security-context-89df77a2-9765-48ee-9281-e04204cc0ccc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017170895s
    STEP: Saw pod success 06/09/23 10:42:29.231
    Jun  9 10:42:29.231: INFO: Pod "security-context-89df77a2-9765-48ee-9281-e04204cc0ccc" satisfied condition "Succeeded or Failed"
    Jun  9 10:42:29.237: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod security-context-89df77a2-9765-48ee-9281-e04204cc0ccc container test-container: <nil>
    STEP: delete the pod 06/09/23 10:42:29.266
    Jun  9 10:42:29.285: INFO: Waiting for pod security-context-89df77a2-9765-48ee-9281-e04204cc0ccc to disappear
    Jun  9 10:42:29.291: INFO: Pod security-context-89df77a2-9765-48ee-9281-e04204cc0ccc no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:42:29.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-3045" for this suite. 06/09/23 10:42:29.303
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:42:29.322
Jun  9 10:42:29.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename namespaces 06/09/23 10:42:29.325
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:42:29.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:42:29.356
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-3457" 06/09/23 10:42:29.362
Jun  9 10:42:29.381: INFO: Namespace "namespaces-3457" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"1e3b8b3b-6336-49ab-9439-1cfe1adcddf6", "kubernetes.io/metadata.name":"namespaces-3457", "namespaces-3457":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:42:29.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3457" for this suite. 06/09/23 10:42:29.391
------------------------------
• [0.084 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:42:29.322
    Jun  9 10:42:29.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename namespaces 06/09/23 10:42:29.325
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:42:29.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:42:29.356
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-3457" 06/09/23 10:42:29.362
    Jun  9 10:42:29.381: INFO: Namespace "namespaces-3457" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"1e3b8b3b-6336-49ab-9439-1cfe1adcddf6", "kubernetes.io/metadata.name":"namespaces-3457", "namespaces-3457":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:42:29.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3457" for this suite. 06/09/23 10:42:29.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:42:29.408
Jun  9 10:42:29.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename secrets 06/09/23 10:42:29.41
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:42:29.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:42:29.453
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-5779/secret-test-2cf01cbd-5bdf-4171-9c7f-4c5b9733546f 06/09/23 10:42:29.461
STEP: Creating a pod to test consume secrets 06/09/23 10:42:29.469
Jun  9 10:42:29.484: INFO: Waiting up to 5m0s for pod "pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9" in namespace "secrets-5779" to be "Succeeded or Failed"
Jun  9 10:42:29.493: INFO: Pod "pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.667033ms
Jun  9 10:42:31.505: INFO: Pod "pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020940904s
Jun  9 10:42:33.503: INFO: Pod "pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018323063s
STEP: Saw pod success 06/09/23 10:42:33.503
Jun  9 10:42:33.503: INFO: Pod "pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9" satisfied condition "Succeeded or Failed"
Jun  9 10:42:33.512: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9 container env-test: <nil>
STEP: delete the pod 06/09/23 10:42:33.528
Jun  9 10:42:33.565: INFO: Waiting for pod pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9 to disappear
Jun  9 10:42:33.573: INFO: Pod pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  9 10:42:33.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5779" for this suite. 06/09/23 10:42:33.582
------------------------------
• [4.190 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:42:29.408
    Jun  9 10:42:29.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename secrets 06/09/23 10:42:29.41
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:42:29.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:42:29.453
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-5779/secret-test-2cf01cbd-5bdf-4171-9c7f-4c5b9733546f 06/09/23 10:42:29.461
    STEP: Creating a pod to test consume secrets 06/09/23 10:42:29.469
    Jun  9 10:42:29.484: INFO: Waiting up to 5m0s for pod "pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9" in namespace "secrets-5779" to be "Succeeded or Failed"
    Jun  9 10:42:29.493: INFO: Pod "pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.667033ms
    Jun  9 10:42:31.505: INFO: Pod "pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020940904s
    Jun  9 10:42:33.503: INFO: Pod "pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018323063s
    STEP: Saw pod success 06/09/23 10:42:33.503
    Jun  9 10:42:33.503: INFO: Pod "pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9" satisfied condition "Succeeded or Failed"
    Jun  9 10:42:33.512: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9 container env-test: <nil>
    STEP: delete the pod 06/09/23 10:42:33.528
    Jun  9 10:42:33.565: INFO: Waiting for pod pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9 to disappear
    Jun  9 10:42:33.573: INFO: Pod pod-configmaps-22f53c78-ae3f-4b7e-a0fb-bb3fd29bd6a9 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:42:33.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5779" for this suite. 06/09/23 10:42:33.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:42:33.6
Jun  9 10:42:33.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename sched-pred 06/09/23 10:42:33.603
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:42:33.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:42:33.678
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jun  9 10:42:33.684: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  9 10:42:33.700: INFO: Waiting for terminating namespaces to be deleted...
Jun  9 10:42:33.707: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-4hkw9 before test
Jun  9 10:42:33.724: INFO: calico-kube-controllers-75b856575b-4vmmr from calico-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 10:42:33.724: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun  9 10:42:33.724: INFO: calico-node-7bzns from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
Jun  9 10:42:33.724: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 10:42:33.724: INFO: csi-node-driver-qw6d7 from calico-system started at 2023-06-09 06:03:15 +0000 UTC (2 container statuses recorded)
Jun  9 10:42:33.724: INFO: 	Container calico-csi ready: true, restart count 0
Jun  9 10:42:33.724: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun  9 10:42:33.724: INFO: coredns-5c4bbd897-mndk4 from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 10:42:33.724: INFO: 	Container coredns ready: true, restart count 0
Jun  9 10:42:33.724: INFO: coredns-5c4bbd897-vswmt from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 10:42:33.724: INFO: 	Container coredns ready: true, restart count 0
Jun  9 10:42:33.724: INFO: kube-proxy-f8glt from kube-system started at 2023-06-09 06:01:45 +0000 UTC (1 container statuses recorded)
Jun  9 10:42:33.724: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 10:42:33.724: INFO: snapshot-controller-df74f7b6c-pdcpl from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 10:42:33.724: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun  9 10:42:33.724: INFO: smtx-elf-csi-driver-node-plugin-dbgrm from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
Jun  9 10:42:33.724: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 10:42:33.724: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  9 10:42:33.724: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 10:42:33.724: INFO: tigera-operator-5948566997-9t6st from sks-system started at 2023-06-09 06:02:52 +0000 UTC (1 container statuses recorded)
Jun  9 10:42:33.724: INFO: 	Container tigera-operator ready: true, restart count 0
Jun  9 10:42:33.724: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-4r82f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 10:42:33.724: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 10:42:33.724: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 10:42:33.724: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-q5bjm before test
Jun  9 10:42:33.758: INFO: calico-node-vpfwq from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
Jun  9 10:42:33.758: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 10:42:33.758: INFO: calico-typha-866dbffd5c-r6nf8 from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
Jun  9 10:42:33.758: INFO: 	Container calico-typha ready: true, restart count 0
Jun  9 10:42:33.758: INFO: csi-node-driver-k2xdl from calico-system started at 2023-06-09 06:04:17 +0000 UTC (2 container statuses recorded)
Jun  9 10:42:33.758: INFO: 	Container calico-csi ready: true, restart count 0
Jun  9 10:42:33.758: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun  9 10:42:33.758: INFO: kube-proxy-b8r9m from kube-system started at 2023-06-09 06:02:23 +0000 UTC (1 container statuses recorded)
Jun  9 10:42:33.758: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 10:42:33.758: INFO: smtx-elf-csi-driver-controller-plugin-7bb788b57-vhsgd from sks-system started at 2023-06-09 06:04:22 +0000 UTC (6 container statuses recorded)
Jun  9 10:42:33.758: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  9 10:42:33.758: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 10:42:33.758: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun  9 10:42:33.758: INFO: 	Container csi-resizer ready: true, restart count 0
Jun  9 10:42:33.758: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jun  9 10:42:33.758: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 10:42:33.758: INFO: smtx-elf-csi-driver-node-plugin-cs9pp from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
Jun  9 10:42:33.758: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 10:42:33.758: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  9 10:42:33.758: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 10:42:33.758: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-crx45 from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 10:42:33.758: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 10:42:33.758: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 10:42:33.758: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-qdprq before test
Jun  9 10:42:33.773: INFO: calico-node-5cjkc from calico-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
Jun  9 10:42:33.773: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 10:42:33.773: INFO: calico-typha-866dbffd5c-n5p5b from calico-system started at 2023-06-09 06:03:07 +0000 UTC (1 container statuses recorded)
Jun  9 10:42:33.773: INFO: 	Container calico-typha ready: true, restart count 0
Jun  9 10:42:33.773: INFO: csi-node-driver-6q224 from calico-system started at 2023-06-09 06:03:44 +0000 UTC (2 container statuses recorded)
Jun  9 10:42:33.773: INFO: 	Container calico-csi ready: true, restart count 0
Jun  9 10:42:33.773: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun  9 10:42:33.773: INFO: kube-proxy-gpttb from kube-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
Jun  9 10:42:33.773: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 10:42:33.773: INFO: smtx-elf-csi-driver-node-plugin-9plgj from sks-system started at 2023-06-09 06:04:22 +0000 UTC (3 container statuses recorded)
Jun  9 10:42:33.773: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 10:42:33.773: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  9 10:42:33.773: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 10:42:33.773: INFO: sonobuoy from sonobuoy started at 2023-06-09 10:18:23 +0000 UTC (1 container statuses recorded)
Jun  9 10:42:33.773: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  9 10:42:33.773: INFO: sonobuoy-e2e-job-09e3c2818710427f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 10:42:33.773: INFO: 	Container e2e ready: true, restart count 0
Jun  9 10:42:33.773: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 10:42:33.773: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-f89bf from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 10:42:33.773: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 10:42:33.773: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 06/09/23 10:42:33.773
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1766f79a9755352d], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling..] 06/09/23 10:42:33.846
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:42:34.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8064" for this suite. 06/09/23 10:42:34.858
------------------------------
• [1.281 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:42:33.6
    Jun  9 10:42:33.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename sched-pred 06/09/23 10:42:33.603
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:42:33.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:42:33.678
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jun  9 10:42:33.684: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun  9 10:42:33.700: INFO: Waiting for terminating namespaces to be deleted...
    Jun  9 10:42:33.707: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-4hkw9 before test
    Jun  9 10:42:33.724: INFO: calico-kube-controllers-75b856575b-4vmmr from calico-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 10:42:33.724: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jun  9 10:42:33.724: INFO: calico-node-7bzns from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
    Jun  9 10:42:33.724: INFO: 	Container calico-node ready: true, restart count 0
    Jun  9 10:42:33.724: INFO: csi-node-driver-qw6d7 from calico-system started at 2023-06-09 06:03:15 +0000 UTC (2 container statuses recorded)
    Jun  9 10:42:33.724: INFO: 	Container calico-csi ready: true, restart count 0
    Jun  9 10:42:33.724: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun  9 10:42:33.724: INFO: coredns-5c4bbd897-mndk4 from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 10:42:33.724: INFO: 	Container coredns ready: true, restart count 0
    Jun  9 10:42:33.724: INFO: coredns-5c4bbd897-vswmt from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 10:42:33.724: INFO: 	Container coredns ready: true, restart count 0
    Jun  9 10:42:33.724: INFO: kube-proxy-f8glt from kube-system started at 2023-06-09 06:01:45 +0000 UTC (1 container statuses recorded)
    Jun  9 10:42:33.724: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  9 10:42:33.724: INFO: snapshot-controller-df74f7b6c-pdcpl from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 10:42:33.724: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun  9 10:42:33.724: INFO: smtx-elf-csi-driver-node-plugin-dbgrm from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
    Jun  9 10:42:33.724: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 10:42:33.724: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun  9 10:42:33.724: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 10:42:33.724: INFO: tigera-operator-5948566997-9t6st from sks-system started at 2023-06-09 06:02:52 +0000 UTC (1 container statuses recorded)
    Jun  9 10:42:33.724: INFO: 	Container tigera-operator ready: true, restart count 0
    Jun  9 10:42:33.724: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-4r82f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 10:42:33.724: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 10:42:33.724: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  9 10:42:33.724: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-q5bjm before test
    Jun  9 10:42:33.758: INFO: calico-node-vpfwq from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
    Jun  9 10:42:33.758: INFO: 	Container calico-node ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: calico-typha-866dbffd5c-r6nf8 from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
    Jun  9 10:42:33.758: INFO: 	Container calico-typha ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: csi-node-driver-k2xdl from calico-system started at 2023-06-09 06:04:17 +0000 UTC (2 container statuses recorded)
    Jun  9 10:42:33.758: INFO: 	Container calico-csi ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: kube-proxy-b8r9m from kube-system started at 2023-06-09 06:02:23 +0000 UTC (1 container statuses recorded)
    Jun  9 10:42:33.758: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: smtx-elf-csi-driver-controller-plugin-7bb788b57-vhsgd from sks-system started at 2023-06-09 06:04:22 +0000 UTC (6 container statuses recorded)
    Jun  9 10:42:33.758: INFO: 	Container csi-attacher ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: 	Container csi-resizer ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: smtx-elf-csi-driver-node-plugin-cs9pp from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
    Jun  9 10:42:33.758: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-crx45 from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 10:42:33.758: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  9 10:42:33.758: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-qdprq before test
    Jun  9 10:42:33.773: INFO: calico-node-5cjkc from calico-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
    Jun  9 10:42:33.773: INFO: 	Container calico-node ready: true, restart count 0
    Jun  9 10:42:33.773: INFO: calico-typha-866dbffd5c-n5p5b from calico-system started at 2023-06-09 06:03:07 +0000 UTC (1 container statuses recorded)
    Jun  9 10:42:33.773: INFO: 	Container calico-typha ready: true, restart count 0
    Jun  9 10:42:33.773: INFO: csi-node-driver-6q224 from calico-system started at 2023-06-09 06:03:44 +0000 UTC (2 container statuses recorded)
    Jun  9 10:42:33.773: INFO: 	Container calico-csi ready: true, restart count 0
    Jun  9 10:42:33.773: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun  9 10:42:33.773: INFO: kube-proxy-gpttb from kube-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
    Jun  9 10:42:33.773: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  9 10:42:33.773: INFO: smtx-elf-csi-driver-node-plugin-9plgj from sks-system started at 2023-06-09 06:04:22 +0000 UTC (3 container statuses recorded)
    Jun  9 10:42:33.773: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 10:42:33.773: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun  9 10:42:33.773: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 10:42:33.773: INFO: sonobuoy from sonobuoy started at 2023-06-09 10:18:23 +0000 UTC (1 container statuses recorded)
    Jun  9 10:42:33.773: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun  9 10:42:33.773: INFO: sonobuoy-e2e-job-09e3c2818710427f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 10:42:33.773: INFO: 	Container e2e ready: true, restart count 0
    Jun  9 10:42:33.773: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 10:42:33.773: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-f89bf from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 10:42:33.773: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 10:42:33.773: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 06/09/23 10:42:33.773
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1766f79a9755352d], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling..] 06/09/23 10:42:33.846
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:42:34.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8064" for this suite. 06/09/23 10:42:34.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:42:34.883
Jun  9 10:42:34.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename taint-single-pod 06/09/23 10:42:34.884
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:42:34.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:42:34.915
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jun  9 10:42:34.920: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  9 10:43:34.981: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jun  9 10:43:34.990: INFO: Starting informer...
STEP: Starting pod... 06/09/23 10:43:34.99
Jun  9 10:43:35.218: INFO: Pod is running on sks-test-v1-26.4-workergroup-qdprq. Tainting Node
STEP: Trying to apply a taint on the Node 06/09/23 10:43:35.218
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/09/23 10:43:35.249
STEP: Waiting short time to make sure Pod is queued for deletion 06/09/23 10:43:35.254
Jun  9 10:43:35.254: INFO: Pod wasn't evicted. Proceeding
Jun  9 10:43:35.254: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/09/23 10:43:35.291
STEP: Waiting some time to make sure that toleration time passed. 06/09/23 10:43:35.305
Jun  9 10:44:50.306: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:44:50.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-8592" for this suite. 06/09/23 10:44:50.316
------------------------------
• [SLOW TEST] [135.444 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:42:34.883
    Jun  9 10:42:34.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename taint-single-pod 06/09/23 10:42:34.884
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:42:34.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:42:34.915
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jun  9 10:42:34.920: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun  9 10:43:34.981: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jun  9 10:43:34.990: INFO: Starting informer...
    STEP: Starting pod... 06/09/23 10:43:34.99
    Jun  9 10:43:35.218: INFO: Pod is running on sks-test-v1-26.4-workergroup-qdprq. Tainting Node
    STEP: Trying to apply a taint on the Node 06/09/23 10:43:35.218
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/09/23 10:43:35.249
    STEP: Waiting short time to make sure Pod is queued for deletion 06/09/23 10:43:35.254
    Jun  9 10:43:35.254: INFO: Pod wasn't evicted. Proceeding
    Jun  9 10:43:35.254: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/09/23 10:43:35.291
    STEP: Waiting some time to make sure that toleration time passed. 06/09/23 10:43:35.305
    Jun  9 10:44:50.306: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:44:50.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-8592" for this suite. 06/09/23 10:44:50.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:44:50.33
Jun  9 10:44:50.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename server-version 06/09/23 10:44:50.331
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:44:50.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:44:50.36
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 06/09/23 10:44:50.366
STEP: Confirm major version 06/09/23 10:44:50.368
Jun  9 10:44:50.369: INFO: Major version: 1
STEP: Confirm minor version 06/09/23 10:44:50.369
Jun  9 10:44:50.369: INFO: cleanMinorVersion: 26
Jun  9 10:44:50.369: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jun  9 10:44:50.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-3606" for this suite. 06/09/23 10:44:50.377
------------------------------
• [0.060 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:44:50.33
    Jun  9 10:44:50.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename server-version 06/09/23 10:44:50.331
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:44:50.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:44:50.36
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 06/09/23 10:44:50.366
    STEP: Confirm major version 06/09/23 10:44:50.368
    Jun  9 10:44:50.369: INFO: Major version: 1
    STEP: Confirm minor version 06/09/23 10:44:50.369
    Jun  9 10:44:50.369: INFO: cleanMinorVersion: 26
    Jun  9 10:44:50.369: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:44:50.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-3606" for this suite. 06/09/23 10:44:50.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:44:50.39
Jun  9 10:44:50.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename security-context-test 06/09/23 10:44:50.392
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:44:50.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:44:50.422
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Jun  9 10:44:50.447: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-08fb2c2f-3b78-4df8-b625-74277c25a5bc" in namespace "security-context-test-7690" to be "Succeeded or Failed"
Jun  9 10:44:50.460: INFO: Pod "busybox-privileged-false-08fb2c2f-3b78-4df8-b625-74277c25a5bc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.998557ms
Jun  9 10:44:52.471: INFO: Pod "busybox-privileged-false-08fb2c2f-3b78-4df8-b625-74277c25a5bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023471391s
Jun  9 10:44:54.469: INFO: Pod "busybox-privileged-false-08fb2c2f-3b78-4df8-b625-74277c25a5bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022200622s
Jun  9 10:44:54.469: INFO: Pod "busybox-privileged-false-08fb2c2f-3b78-4df8-b625-74277c25a5bc" satisfied condition "Succeeded or Failed"
Jun  9 10:44:54.495: INFO: Got logs for pod "busybox-privileged-false-08fb2c2f-3b78-4df8-b625-74277c25a5bc": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun  9 10:44:54.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7690" for this suite. 06/09/23 10:44:54.505
------------------------------
• [4.127 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:44:50.39
    Jun  9 10:44:50.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename security-context-test 06/09/23 10:44:50.392
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:44:50.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:44:50.422
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Jun  9 10:44:50.447: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-08fb2c2f-3b78-4df8-b625-74277c25a5bc" in namespace "security-context-test-7690" to be "Succeeded or Failed"
    Jun  9 10:44:50.460: INFO: Pod "busybox-privileged-false-08fb2c2f-3b78-4df8-b625-74277c25a5bc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.998557ms
    Jun  9 10:44:52.471: INFO: Pod "busybox-privileged-false-08fb2c2f-3b78-4df8-b625-74277c25a5bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023471391s
    Jun  9 10:44:54.469: INFO: Pod "busybox-privileged-false-08fb2c2f-3b78-4df8-b625-74277c25a5bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022200622s
    Jun  9 10:44:54.469: INFO: Pod "busybox-privileged-false-08fb2c2f-3b78-4df8-b625-74277c25a5bc" satisfied condition "Succeeded or Failed"
    Jun  9 10:44:54.495: INFO: Got logs for pod "busybox-privileged-false-08fb2c2f-3b78-4df8-b625-74277c25a5bc": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:44:54.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7690" for this suite. 06/09/23 10:44:54.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:44:54.518
Jun  9 10:44:54.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename var-expansion 06/09/23 10:44:54.519
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:44:54.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:44:54.546
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 06/09/23 10:44:54.553
Jun  9 10:44:54.600: INFO: Waiting up to 5m0s for pod "var-expansion-7563017d-e60f-4aea-86dd-202c226c8053" in namespace "var-expansion-9826" to be "Succeeded or Failed"
Jun  9 10:44:54.631: INFO: Pod "var-expansion-7563017d-e60f-4aea-86dd-202c226c8053": Phase="Pending", Reason="", readiness=false. Elapsed: 30.200315ms
Jun  9 10:44:56.642: INFO: Pod "var-expansion-7563017d-e60f-4aea-86dd-202c226c8053": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041287384s
Jun  9 10:44:58.639: INFO: Pod "var-expansion-7563017d-e60f-4aea-86dd-202c226c8053": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038628271s
STEP: Saw pod success 06/09/23 10:44:58.639
Jun  9 10:44:58.639: INFO: Pod "var-expansion-7563017d-e60f-4aea-86dd-202c226c8053" satisfied condition "Succeeded or Failed"
Jun  9 10:44:58.647: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod var-expansion-7563017d-e60f-4aea-86dd-202c226c8053 container dapi-container: <nil>
STEP: delete the pod 06/09/23 10:44:58.678
Jun  9 10:44:58.696: INFO: Waiting for pod var-expansion-7563017d-e60f-4aea-86dd-202c226c8053 to disappear
Jun  9 10:44:58.702: INFO: Pod var-expansion-7563017d-e60f-4aea-86dd-202c226c8053 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  9 10:44:58.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9826" for this suite. 06/09/23 10:44:58.712
------------------------------
• [4.205 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:44:54.518
    Jun  9 10:44:54.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename var-expansion 06/09/23 10:44:54.519
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:44:54.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:44:54.546
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 06/09/23 10:44:54.553
    Jun  9 10:44:54.600: INFO: Waiting up to 5m0s for pod "var-expansion-7563017d-e60f-4aea-86dd-202c226c8053" in namespace "var-expansion-9826" to be "Succeeded or Failed"
    Jun  9 10:44:54.631: INFO: Pod "var-expansion-7563017d-e60f-4aea-86dd-202c226c8053": Phase="Pending", Reason="", readiness=false. Elapsed: 30.200315ms
    Jun  9 10:44:56.642: INFO: Pod "var-expansion-7563017d-e60f-4aea-86dd-202c226c8053": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041287384s
    Jun  9 10:44:58.639: INFO: Pod "var-expansion-7563017d-e60f-4aea-86dd-202c226c8053": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038628271s
    STEP: Saw pod success 06/09/23 10:44:58.639
    Jun  9 10:44:58.639: INFO: Pod "var-expansion-7563017d-e60f-4aea-86dd-202c226c8053" satisfied condition "Succeeded or Failed"
    Jun  9 10:44:58.647: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod var-expansion-7563017d-e60f-4aea-86dd-202c226c8053 container dapi-container: <nil>
    STEP: delete the pod 06/09/23 10:44:58.678
    Jun  9 10:44:58.696: INFO: Waiting for pod var-expansion-7563017d-e60f-4aea-86dd-202c226c8053 to disappear
    Jun  9 10:44:58.702: INFO: Pod var-expansion-7563017d-e60f-4aea-86dd-202c226c8053 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:44:58.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9826" for this suite. 06/09/23 10:44:58.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:44:58.724
Jun  9 10:44:58.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:44:58.726
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:44:58.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:44:58.762
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-89d32e0b-0b42-465d-841c-6379faa84aea 06/09/23 10:44:58.769
STEP: Creating a pod to test consume secrets 06/09/23 10:44:58.779
Jun  9 10:44:58.794: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce" in namespace "projected-3591" to be "Succeeded or Failed"
Jun  9 10:44:58.801: INFO: Pod "pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce": Phase="Pending", Reason="", readiness=false. Elapsed: 6.506868ms
Jun  9 10:45:00.808: INFO: Pod "pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014058142s
Jun  9 10:45:02.809: INFO: Pod "pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014797283s
STEP: Saw pod success 06/09/23 10:45:02.809
Jun  9 10:45:02.809: INFO: Pod "pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce" satisfied condition "Succeeded or Failed"
Jun  9 10:45:02.817: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce container projected-secret-volume-test: <nil>
STEP: delete the pod 06/09/23 10:45:02.83
Jun  9 10:45:02.854: INFO: Waiting for pod pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce to disappear
Jun  9 10:45:02.863: INFO: Pod pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun  9 10:45:02.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3591" for this suite. 06/09/23 10:45:02.873
------------------------------
• [4.168 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:44:58.724
    Jun  9 10:44:58.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:44:58.726
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:44:58.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:44:58.762
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-89d32e0b-0b42-465d-841c-6379faa84aea 06/09/23 10:44:58.769
    STEP: Creating a pod to test consume secrets 06/09/23 10:44:58.779
    Jun  9 10:44:58.794: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce" in namespace "projected-3591" to be "Succeeded or Failed"
    Jun  9 10:44:58.801: INFO: Pod "pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce": Phase="Pending", Reason="", readiness=false. Elapsed: 6.506868ms
    Jun  9 10:45:00.808: INFO: Pod "pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014058142s
    Jun  9 10:45:02.809: INFO: Pod "pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014797283s
    STEP: Saw pod success 06/09/23 10:45:02.809
    Jun  9 10:45:02.809: INFO: Pod "pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce" satisfied condition "Succeeded or Failed"
    Jun  9 10:45:02.817: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/09/23 10:45:02.83
    Jun  9 10:45:02.854: INFO: Waiting for pod pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce to disappear
    Jun  9 10:45:02.863: INFO: Pod pod-projected-secrets-7c46296f-c36a-458b-8ec8-c79bee5d84ce no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:45:02.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3591" for this suite. 06/09/23 10:45:02.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:45:02.894
Jun  9 10:45:02.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename job 06/09/23 10:45:02.895
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:02.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:02.927
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 06/09/23 10:45:02.94
STEP: Patching the Job 06/09/23 10:45:02.952
STEP: Watching for Job to be patched 06/09/23 10:45:02.979
Jun  9 10:45:02.983: INFO: Event ADDED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt] and annotations: map[batch.kubernetes.io/job-tracking:]
Jun  9 10:45:02.984: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt] and annotations: map[batch.kubernetes.io/job-tracking:]
Jun  9 10:45:02.984: INFO: Event MODIFIED found for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 06/09/23 10:45:02.984
STEP: Watching for Job to be updated 06/09/23 10:45:03.006
Jun  9 10:45:03.013: INFO: Event MODIFIED found for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun  9 10:45:03.013: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 06/09/23 10:45:03.013
Jun  9 10:45:03.020: INFO: Job: e2e-k8cxt as labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched]
STEP: Waiting for job to complete 06/09/23 10:45:03.02
STEP: Delete a job collection with a labelselector 06/09/23 10:45:13.027
STEP: Watching for Job to be deleted 06/09/23 10:45:13.042
Jun  9 10:45:13.046: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun  9 10:45:13.046: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun  9 10:45:13.046: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun  9 10:45:13.046: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun  9 10:45:13.046: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun  9 10:45:13.046: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun  9 10:45:13.046: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun  9 10:45:13.047: INFO: Event DELETED found for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 06/09/23 10:45:13.047
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun  9 10:45:13.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5951" for this suite. 06/09/23 10:45:13.065
------------------------------
• [SLOW TEST] [10.202 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:45:02.894
    Jun  9 10:45:02.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename job 06/09/23 10:45:02.895
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:02.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:02.927
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 06/09/23 10:45:02.94
    STEP: Patching the Job 06/09/23 10:45:02.952
    STEP: Watching for Job to be patched 06/09/23 10:45:02.979
    Jun  9 10:45:02.983: INFO: Event ADDED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jun  9 10:45:02.984: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jun  9 10:45:02.984: INFO: Event MODIFIED found for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 06/09/23 10:45:02.984
    STEP: Watching for Job to be updated 06/09/23 10:45:03.006
    Jun  9 10:45:03.013: INFO: Event MODIFIED found for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun  9 10:45:03.013: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 06/09/23 10:45:03.013
    Jun  9 10:45:03.020: INFO: Job: e2e-k8cxt as labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched]
    STEP: Waiting for job to complete 06/09/23 10:45:03.02
    STEP: Delete a job collection with a labelselector 06/09/23 10:45:13.027
    STEP: Watching for Job to be deleted 06/09/23 10:45:13.042
    Jun  9 10:45:13.046: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun  9 10:45:13.046: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun  9 10:45:13.046: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun  9 10:45:13.046: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun  9 10:45:13.046: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun  9 10:45:13.046: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun  9 10:45:13.046: INFO: Event MODIFIED observed for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun  9 10:45:13.047: INFO: Event DELETED found for Job e2e-k8cxt in namespace job-5951 with labels: map[e2e-job-label:e2e-k8cxt e2e-k8cxt:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 06/09/23 10:45:13.047
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:45:13.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5951" for this suite. 06/09/23 10:45:13.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:45:13.1
Jun  9 10:45:13.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-runtime 06/09/23 10:45:13.101
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:13.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:13.143
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 06/09/23 10:45:13.15
STEP: wait for the container to reach Succeeded 06/09/23 10:45:13.168
STEP: get the container status 06/09/23 10:45:17.213
STEP: the container should be terminated 06/09/23 10:45:17.219
STEP: the termination message should be set 06/09/23 10:45:17.22
Jun  9 10:45:17.220: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 06/09/23 10:45:17.22
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun  9 10:45:17.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6746" for this suite. 06/09/23 10:45:17.254
------------------------------
• [4.166 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:45:13.1
    Jun  9 10:45:13.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-runtime 06/09/23 10:45:13.101
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:13.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:13.143
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 06/09/23 10:45:13.15
    STEP: wait for the container to reach Succeeded 06/09/23 10:45:13.168
    STEP: get the container status 06/09/23 10:45:17.213
    STEP: the container should be terminated 06/09/23 10:45:17.219
    STEP: the termination message should be set 06/09/23 10:45:17.22
    Jun  9 10:45:17.220: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 06/09/23 10:45:17.22
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:45:17.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6746" for this suite. 06/09/23 10:45:17.254
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:45:17.267
Jun  9 10:45:17.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubelet-test 06/09/23 10:45:17.269
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:17.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:17.302
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 06/09/23 10:45:17.325
Jun  9 10:45:17.325: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases9fc8209a-7d48-42af-bd81-810412197748" in namespace "kubelet-test-1110" to be "completed"
Jun  9 10:45:17.332: INFO: Pod "agnhost-host-aliases9fc8209a-7d48-42af-bd81-810412197748": Phase="Pending", Reason="", readiness=false. Elapsed: 6.995519ms
Jun  9 10:45:19.338: INFO: Pod "agnhost-host-aliases9fc8209a-7d48-42af-bd81-810412197748": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013360475s
Jun  9 10:45:21.340: INFO: Pod "agnhost-host-aliases9fc8209a-7d48-42af-bd81-810412197748": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014708696s
Jun  9 10:45:21.340: INFO: Pod "agnhost-host-aliases9fc8209a-7d48-42af-bd81-810412197748" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun  9 10:45:21.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1110" for this suite. 06/09/23 10:45:21.362
------------------------------
• [4.108 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:45:17.267
    Jun  9 10:45:17.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubelet-test 06/09/23 10:45:17.269
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:17.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:17.302
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 06/09/23 10:45:17.325
    Jun  9 10:45:17.325: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases9fc8209a-7d48-42af-bd81-810412197748" in namespace "kubelet-test-1110" to be "completed"
    Jun  9 10:45:17.332: INFO: Pod "agnhost-host-aliases9fc8209a-7d48-42af-bd81-810412197748": Phase="Pending", Reason="", readiness=false. Elapsed: 6.995519ms
    Jun  9 10:45:19.338: INFO: Pod "agnhost-host-aliases9fc8209a-7d48-42af-bd81-810412197748": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013360475s
    Jun  9 10:45:21.340: INFO: Pod "agnhost-host-aliases9fc8209a-7d48-42af-bd81-810412197748": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014708696s
    Jun  9 10:45:21.340: INFO: Pod "agnhost-host-aliases9fc8209a-7d48-42af-bd81-810412197748" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:45:21.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1110" for this suite. 06/09/23 10:45:21.362
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:45:21.376
Jun  9 10:45:21.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename daemonsets 06/09/23 10:45:21.377
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:21.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:21.412
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Jun  9 10:45:21.458: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 06/09/23 10:45:21.466
Jun  9 10:45:21.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:45:21.475: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 06/09/23 10:45:21.475
Jun  9 10:45:21.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:45:21.508: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
Jun  9 10:45:22.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:45:22.514: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
Jun  9 10:45:23.515: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:45:23.515: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 06/09/23 10:45:23.521
Jun  9 10:45:23.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:45:23.546: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jun  9 10:45:24.601: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:45:24.601: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 06/09/23 10:45:24.601
Jun  9 10:45:24.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:45:24.643: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
Jun  9 10:45:25.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:45:25.653: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
Jun  9 10:45:26.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:45:26.657: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
Jun  9 10:45:27.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:45:27.650: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
Jun  9 10:45:28.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  9 10:45:28.649: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/09/23 10:45:28.658
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2708, will wait for the garbage collector to delete the pods 06/09/23 10:45:28.659
Jun  9 10:45:28.725: INFO: Deleting DaemonSet.extensions daemon-set took: 11.028984ms
Jun  9 10:45:28.826: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.650408ms
Jun  9 10:45:31.333: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:45:31.333: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  9 10:45:31.339: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"83147"},"items":null}

Jun  9 10:45:31.343: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"83147"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:45:31.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2708" for this suite. 06/09/23 10:45:31.399
------------------------------
• [SLOW TEST] [10.036 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:45:21.376
    Jun  9 10:45:21.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename daemonsets 06/09/23 10:45:21.377
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:21.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:21.412
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Jun  9 10:45:21.458: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 06/09/23 10:45:21.466
    Jun  9 10:45:21.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:45:21.475: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 06/09/23 10:45:21.475
    Jun  9 10:45:21.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:45:21.508: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
    Jun  9 10:45:22.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:45:22.514: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
    Jun  9 10:45:23.515: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:45:23.515: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 06/09/23 10:45:23.521
    Jun  9 10:45:23.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:45:23.546: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jun  9 10:45:24.601: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:45:24.601: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 06/09/23 10:45:24.601
    Jun  9 10:45:24.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:45:24.643: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
    Jun  9 10:45:25.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:45:25.653: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
    Jun  9 10:45:26.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:45:26.657: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
    Jun  9 10:45:27.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:45:27.650: INFO: Node sks-test-v1-26.4-workergroup-q5bjm is running 0 daemon pod, expected 1
    Jun  9 10:45:28.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun  9 10:45:28.649: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/09/23 10:45:28.658
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2708, will wait for the garbage collector to delete the pods 06/09/23 10:45:28.659
    Jun  9 10:45:28.725: INFO: Deleting DaemonSet.extensions daemon-set took: 11.028984ms
    Jun  9 10:45:28.826: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.650408ms
    Jun  9 10:45:31.333: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:45:31.333: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun  9 10:45:31.339: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"83147"},"items":null}

    Jun  9 10:45:31.343: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"83147"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:45:31.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2708" for this suite. 06/09/23 10:45:31.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:45:31.412
Jun  9 10:45:31.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 10:45:31.413
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:31.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:31.443
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 06/09/23 10:45:31.449
Jun  9 10:45:31.462: INFO: Waiting up to 5m0s for pod "pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8" in namespace "emptydir-3132" to be "Succeeded or Failed"
Jun  9 10:45:31.470: INFO: Pod "pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.158485ms
Jun  9 10:45:33.479: INFO: Pod "pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017393636s
Jun  9 10:45:35.479: INFO: Pod "pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016832246s
STEP: Saw pod success 06/09/23 10:45:35.479
Jun  9 10:45:35.479: INFO: Pod "pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8" satisfied condition "Succeeded or Failed"
Jun  9 10:45:35.488: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8 container test-container: <nil>
STEP: delete the pod 06/09/23 10:45:35.5
Jun  9 10:45:35.516: INFO: Waiting for pod pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8 to disappear
Jun  9 10:45:35.523: INFO: Pod pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 10:45:35.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3132" for this suite. 06/09/23 10:45:35.531
------------------------------
• [4.137 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:45:31.412
    Jun  9 10:45:31.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 10:45:31.413
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:31.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:31.443
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 06/09/23 10:45:31.449
    Jun  9 10:45:31.462: INFO: Waiting up to 5m0s for pod "pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8" in namespace "emptydir-3132" to be "Succeeded or Failed"
    Jun  9 10:45:31.470: INFO: Pod "pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.158485ms
    Jun  9 10:45:33.479: INFO: Pod "pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017393636s
    Jun  9 10:45:35.479: INFO: Pod "pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016832246s
    STEP: Saw pod success 06/09/23 10:45:35.479
    Jun  9 10:45:35.479: INFO: Pod "pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8" satisfied condition "Succeeded or Failed"
    Jun  9 10:45:35.488: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8 container test-container: <nil>
    STEP: delete the pod 06/09/23 10:45:35.5
    Jun  9 10:45:35.516: INFO: Waiting for pod pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8 to disappear
    Jun  9 10:45:35.523: INFO: Pod pod-a9ba3f36-cdbe-4db0-99d7-b1476bd417a8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:45:35.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3132" for this suite. 06/09/23 10:45:35.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:45:35.55
Jun  9 10:45:35.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename deployment 06/09/23 10:45:35.551
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:35.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:35.578
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jun  9 10:45:35.599: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun  9 10:45:40.610: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/09/23 10:45:40.61
Jun  9 10:45:40.610: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 06/09/23 10:45:40.638
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  9 10:45:42.678: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7140  ee851df2-1740-4358-8493-3e84bf9ef811 83294 1 2023-06-09 10:45:40 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-09 10:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 10:45:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00275b928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-09 10:45:40 +0000 UTC,LastTransitionTime:2023-06-09 10:45:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-06-09 10:45:42 +0000 UTC,LastTransitionTime:2023-06-09 10:45:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  9 10:45:42.686: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-7140  e8e1a97a-0f1e-4e20-8927-3add10a30dbb 83284 1 2023-06-09 10:45:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment ee851df2-1740-4358-8493-3e84bf9ef811 0xc004806547 0xc004806548}] [] [{kube-controller-manager Update apps/v1 2023-06-09 10:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee851df2-1740-4358-8493-3e84bf9ef811\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 10:45:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048065f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  9 10:45:42.703: INFO: Pod "test-cleanup-deployment-7698ff6f6b-wnnfh" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-wnnfh test-cleanup-deployment-7698ff6f6b- deployment-7140  daf72ac3-f9a1-4934-b414-6806ea1aba8c 83283 0 2023-06-09 10:45:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[cni.projectcalico.org/containerID:96185fb7351b55b3ff6e0d7e3f9fed2015703d1a314f1e180768892da7b91ef2 cni.projectcalico.org/podIP:172.26.90.22/32 cni.projectcalico.org/podIPs:172.26.90.22/32] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b e8e1a97a-0f1e-4e20-8927-3add10a30dbb 0xc0048069d7 0xc0048069d8}] [] [{kube-controller-manager Update v1 2023-06-09 10:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8e1a97a-0f1e-4e20-8927-3add10a30dbb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 10:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 10:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.90.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wqgwf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wqgwf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.26.90.22,StartTime:2023-06-09 10:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 10:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://93c9f88760717f08953a866921b77d72bdafde4c6d385e5ef60c9526087fde16,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.90.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  9 10:45:42.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7140" for this suite. 06/09/23 10:45:42.727
------------------------------
• [SLOW TEST] [7.196 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:45:35.55
    Jun  9 10:45:35.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename deployment 06/09/23 10:45:35.551
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:35.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:35.578
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jun  9 10:45:35.599: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jun  9 10:45:40.610: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/09/23 10:45:40.61
    Jun  9 10:45:40.610: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 06/09/23 10:45:40.638
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  9 10:45:42.678: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7140  ee851df2-1740-4358-8493-3e84bf9ef811 83294 1 2023-06-09 10:45:40 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-09 10:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 10:45:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00275b928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-09 10:45:40 +0000 UTC,LastTransitionTime:2023-06-09 10:45:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-06-09 10:45:42 +0000 UTC,LastTransitionTime:2023-06-09 10:45:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun  9 10:45:42.686: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-7140  e8e1a97a-0f1e-4e20-8927-3add10a30dbb 83284 1 2023-06-09 10:45:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment ee851df2-1740-4358-8493-3e84bf9ef811 0xc004806547 0xc004806548}] [] [{kube-controller-manager Update apps/v1 2023-06-09 10:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee851df2-1740-4358-8493-3e84bf9ef811\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 10:45:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048065f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun  9 10:45:42.703: INFO: Pod "test-cleanup-deployment-7698ff6f6b-wnnfh" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-wnnfh test-cleanup-deployment-7698ff6f6b- deployment-7140  daf72ac3-f9a1-4934-b414-6806ea1aba8c 83283 0 2023-06-09 10:45:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[cni.projectcalico.org/containerID:96185fb7351b55b3ff6e0d7e3f9fed2015703d1a314f1e180768892da7b91ef2 cni.projectcalico.org/podIP:172.26.90.22/32 cni.projectcalico.org/podIPs:172.26.90.22/32] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b e8e1a97a-0f1e-4e20-8927-3add10a30dbb 0xc0048069d7 0xc0048069d8}] [] [{kube-controller-manager Update v1 2023-06-09 10:45:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8e1a97a-0f1e-4e20-8927-3add10a30dbb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 10:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 10:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.90.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wqgwf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wqgwf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:45:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 10:45:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.26.90.22,StartTime:2023-06-09 10:45:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 10:45:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://93c9f88760717f08953a866921b77d72bdafde4c6d385e5ef60c9526087fde16,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.90.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:45:42.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7140" for this suite. 06/09/23 10:45:42.727
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:45:42.746
Jun  9 10:45:42.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename replication-controller 06/09/23 10:45:42.748
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:42.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:42.784
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 06/09/23 10:45:42.792
STEP: When the matched label of one of its pods change 06/09/23 10:45:42.804
Jun  9 10:45:42.810: INFO: Pod name pod-release: Found 0 pods out of 1
Jun  9 10:45:47.825: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 06/09/23 10:45:47.842
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun  9 10:45:47.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1182" for this suite. 06/09/23 10:45:47.863
------------------------------
• [SLOW TEST] [5.136 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:45:42.746
    Jun  9 10:45:42.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename replication-controller 06/09/23 10:45:42.748
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:42.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:42.784
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 06/09/23 10:45:42.792
    STEP: When the matched label of one of its pods change 06/09/23 10:45:42.804
    Jun  9 10:45:42.810: INFO: Pod name pod-release: Found 0 pods out of 1
    Jun  9 10:45:47.825: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 06/09/23 10:45:47.842
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:45:47.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1182" for this suite. 06/09/23 10:45:47.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:45:47.883
Jun  9 10:45:47.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename secrets 06/09/23 10:45:47.884
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:47.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:47.932
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-fa9b7064-41b3-4166-950e-2fe454472727 06/09/23 10:45:47.937
STEP: Creating a pod to test consume secrets 06/09/23 10:45:47.945
Jun  9 10:45:47.958: INFO: Waiting up to 5m0s for pod "pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915" in namespace "secrets-7434" to be "Succeeded or Failed"
Jun  9 10:45:47.967: INFO: Pod "pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915": Phase="Pending", Reason="", readiness=false. Elapsed: 9.028742ms
Jun  9 10:45:49.978: INFO: Pod "pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019686203s
Jun  9 10:45:51.981: INFO: Pod "pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022141741s
STEP: Saw pod success 06/09/23 10:45:51.981
Jun  9 10:45:51.981: INFO: Pod "pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915" satisfied condition "Succeeded or Failed"
Jun  9 10:45:51.989: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915 container secret-volume-test: <nil>
STEP: delete the pod 06/09/23 10:45:52.02
Jun  9 10:45:52.055: INFO: Waiting for pod pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915 to disappear
Jun  9 10:45:52.062: INFO: Pod pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  9 10:45:52.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7434" for this suite. 06/09/23 10:45:52.071
------------------------------
• [4.206 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:45:47.883
    Jun  9 10:45:47.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename secrets 06/09/23 10:45:47.884
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:47.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:47.932
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-fa9b7064-41b3-4166-950e-2fe454472727 06/09/23 10:45:47.937
    STEP: Creating a pod to test consume secrets 06/09/23 10:45:47.945
    Jun  9 10:45:47.958: INFO: Waiting up to 5m0s for pod "pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915" in namespace "secrets-7434" to be "Succeeded or Failed"
    Jun  9 10:45:47.967: INFO: Pod "pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915": Phase="Pending", Reason="", readiness=false. Elapsed: 9.028742ms
    Jun  9 10:45:49.978: INFO: Pod "pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019686203s
    Jun  9 10:45:51.981: INFO: Pod "pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022141741s
    STEP: Saw pod success 06/09/23 10:45:51.981
    Jun  9 10:45:51.981: INFO: Pod "pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915" satisfied condition "Succeeded or Failed"
    Jun  9 10:45:51.989: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915 container secret-volume-test: <nil>
    STEP: delete the pod 06/09/23 10:45:52.02
    Jun  9 10:45:52.055: INFO: Waiting for pod pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915 to disappear
    Jun  9 10:45:52.062: INFO: Pod pod-secrets-7f13b421-e6af-4286-a828-16e806e0d915 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:45:52.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7434" for this suite. 06/09/23 10:45:52.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:45:52.089
Jun  9 10:45:52.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 10:45:52.09
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:52.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:52.115
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 06/09/23 10:45:52.121
Jun  9 10:45:52.133: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0" in namespace "downward-api-9090" to be "Succeeded or Failed"
Jun  9 10:45:52.139: INFO: Pod "downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.699135ms
Jun  9 10:45:54.146: INFO: Pod "downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013149542s
Jun  9 10:45:56.148: INFO: Pod "downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014849655s
STEP: Saw pod success 06/09/23 10:45:56.148
Jun  9 10:45:56.148: INFO: Pod "downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0" satisfied condition "Succeeded or Failed"
Jun  9 10:45:56.156: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0 container client-container: <nil>
STEP: delete the pod 06/09/23 10:45:56.169
Jun  9 10:45:56.188: INFO: Waiting for pod downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0 to disappear
Jun  9 10:45:56.194: INFO: Pod downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  9 10:45:56.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9090" for this suite. 06/09/23 10:45:56.206
------------------------------
• [4.129 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:45:52.089
    Jun  9 10:45:52.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 10:45:52.09
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:52.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:52.115
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 06/09/23 10:45:52.121
    Jun  9 10:45:52.133: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0" in namespace "downward-api-9090" to be "Succeeded or Failed"
    Jun  9 10:45:52.139: INFO: Pod "downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.699135ms
    Jun  9 10:45:54.146: INFO: Pod "downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013149542s
    Jun  9 10:45:56.148: INFO: Pod "downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014849655s
    STEP: Saw pod success 06/09/23 10:45:56.148
    Jun  9 10:45:56.148: INFO: Pod "downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0" satisfied condition "Succeeded or Failed"
    Jun  9 10:45:56.156: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0 container client-container: <nil>
    STEP: delete the pod 06/09/23 10:45:56.169
    Jun  9 10:45:56.188: INFO: Waiting for pod downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0 to disappear
    Jun  9 10:45:56.194: INFO: Pod downwardapi-volume-8f744611-958a-484c-a0ba-8bcc96eb18d0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:45:56.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9090" for this suite. 06/09/23 10:45:56.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:45:56.224
Jun  9 10:45:56.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename sched-pred 06/09/23 10:45:56.225
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:56.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:56.253
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jun  9 10:45:56.257: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  9 10:45:56.276: INFO: Waiting for terminating namespaces to be deleted...
Jun  9 10:45:56.282: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-4hkw9 before test
Jun  9 10:45:56.299: INFO: calico-kube-controllers-75b856575b-4vmmr from calico-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 10:45:56.299: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun  9 10:45:56.299: INFO: calico-node-7bzns from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
Jun  9 10:45:56.299: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 10:45:56.299: INFO: csi-node-driver-qw6d7 from calico-system started at 2023-06-09 06:03:15 +0000 UTC (2 container statuses recorded)
Jun  9 10:45:56.299: INFO: 	Container calico-csi ready: true, restart count 0
Jun  9 10:45:56.299: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun  9 10:45:56.299: INFO: coredns-5c4bbd897-mndk4 from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 10:45:56.299: INFO: 	Container coredns ready: true, restart count 0
Jun  9 10:45:56.299: INFO: coredns-5c4bbd897-vswmt from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 10:45:56.299: INFO: 	Container coredns ready: true, restart count 0
Jun  9 10:45:56.299: INFO: kube-proxy-f8glt from kube-system started at 2023-06-09 06:01:45 +0000 UTC (1 container statuses recorded)
Jun  9 10:45:56.299: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 10:45:56.299: INFO: snapshot-controller-df74f7b6c-pdcpl from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 10:45:56.299: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun  9 10:45:56.299: INFO: smtx-elf-csi-driver-node-plugin-dbgrm from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
Jun  9 10:45:56.299: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 10:45:56.299: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  9 10:45:56.299: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 10:45:56.299: INFO: tigera-operator-5948566997-9t6st from sks-system started at 2023-06-09 06:02:52 +0000 UTC (1 container statuses recorded)
Jun  9 10:45:56.299: INFO: 	Container tigera-operator ready: true, restart count 0
Jun  9 10:45:56.299: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-4r82f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 10:45:56.299: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 10:45:56.299: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 10:45:56.299: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-q5bjm before test
Jun  9 10:45:56.312: INFO: calico-node-vpfwq from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
Jun  9 10:45:56.312: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 10:45:56.312: INFO: calico-typha-866dbffd5c-r6nf8 from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
Jun  9 10:45:56.312: INFO: 	Container calico-typha ready: true, restart count 0
Jun  9 10:45:56.312: INFO: csi-node-driver-k2xdl from calico-system started at 2023-06-09 06:04:17 +0000 UTC (2 container statuses recorded)
Jun  9 10:45:56.312: INFO: 	Container calico-csi ready: true, restart count 0
Jun  9 10:45:56.312: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun  9 10:45:56.312: INFO: kube-proxy-b8r9m from kube-system started at 2023-06-09 06:02:23 +0000 UTC (1 container statuses recorded)
Jun  9 10:45:56.312: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 10:45:56.312: INFO: smtx-elf-csi-driver-controller-plugin-7bb788b57-vhsgd from sks-system started at 2023-06-09 06:04:22 +0000 UTC (6 container statuses recorded)
Jun  9 10:45:56.312: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  9 10:45:56.312: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 10:45:56.312: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun  9 10:45:56.312: INFO: 	Container csi-resizer ready: true, restart count 0
Jun  9 10:45:56.312: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jun  9 10:45:56.312: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 10:45:56.312: INFO: smtx-elf-csi-driver-node-plugin-cs9pp from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
Jun  9 10:45:56.312: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 10:45:56.312: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  9 10:45:56.312: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 10:45:56.312: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-crx45 from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 10:45:56.312: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 10:45:56.312: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 10:45:56.312: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-qdprq before test
Jun  9 10:45:56.327: INFO: calico-node-5cjkc from calico-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
Jun  9 10:45:56.327: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 10:45:56.327: INFO: calico-typha-866dbffd5c-n5p5b from calico-system started at 2023-06-09 06:03:07 +0000 UTC (1 container statuses recorded)
Jun  9 10:45:56.327: INFO: 	Container calico-typha ready: true, restart count 0
Jun  9 10:45:56.327: INFO: csi-node-driver-jw49b from calico-system started at 2023-06-09 10:43:36 +0000 UTC (2 container statuses recorded)
Jun  9 10:45:56.327: INFO: 	Container calico-csi ready: true, restart count 0
Jun  9 10:45:56.327: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun  9 10:45:56.327: INFO: kube-proxy-gpttb from kube-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
Jun  9 10:45:56.327: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 10:45:56.327: INFO: smtx-elf-csi-driver-node-plugin-9plgj from sks-system started at 2023-06-09 06:04:22 +0000 UTC (3 container statuses recorded)
Jun  9 10:45:56.327: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 10:45:56.327: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  9 10:45:56.327: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 10:45:56.327: INFO: sonobuoy from sonobuoy started at 2023-06-09 10:18:23 +0000 UTC (1 container statuses recorded)
Jun  9 10:45:56.327: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  9 10:45:56.327: INFO: sonobuoy-e2e-job-09e3c2818710427f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 10:45:56.327: INFO: 	Container e2e ready: true, restart count 0
Jun  9 10:45:56.327: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 10:45:56.327: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-f89bf from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 10:45:56.327: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 10:45:56.327: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node sks-test-v1-26.4-workergroup-4hkw9 06/09/23 10:45:56.383
STEP: verifying the node has the label node sks-test-v1-26.4-workergroup-q5bjm 06/09/23 10:45:56.402
STEP: verifying the node has the label node sks-test-v1-26.4-workergroup-qdprq 06/09/23 10:45:56.424
Jun  9 10:45:56.462: INFO: Pod calico-kube-controllers-75b856575b-4vmmr requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-4hkw9
Jun  9 10:45:56.462: INFO: Pod calico-node-5cjkc requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-qdprq
Jun  9 10:45:56.462: INFO: Pod calico-node-7bzns requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-4hkw9
Jun  9 10:45:56.462: INFO: Pod calico-node-vpfwq requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-q5bjm
Jun  9 10:45:56.462: INFO: Pod calico-typha-866dbffd5c-n5p5b requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-qdprq
Jun  9 10:45:56.462: INFO: Pod calico-typha-866dbffd5c-r6nf8 requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-q5bjm
Jun  9 10:45:56.462: INFO: Pod csi-node-driver-jw49b requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-qdprq
Jun  9 10:45:56.462: INFO: Pod csi-node-driver-k2xdl requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-q5bjm
Jun  9 10:45:56.462: INFO: Pod csi-node-driver-qw6d7 requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-4hkw9
Jun  9 10:45:56.462: INFO: Pod coredns-5c4bbd897-mndk4 requesting resource cpu=100m on Node sks-test-v1-26.4-workergroup-4hkw9
Jun  9 10:45:56.462: INFO: Pod coredns-5c4bbd897-vswmt requesting resource cpu=100m on Node sks-test-v1-26.4-workergroup-4hkw9
Jun  9 10:45:56.462: INFO: Pod kube-proxy-b8r9m requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-q5bjm
Jun  9 10:45:56.462: INFO: Pod kube-proxy-f8glt requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-4hkw9
Jun  9 10:45:56.462: INFO: Pod kube-proxy-gpttb requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-qdprq
Jun  9 10:45:56.462: INFO: Pod snapshot-controller-df74f7b6c-pdcpl requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-4hkw9
Jun  9 10:45:56.462: INFO: Pod smtx-elf-csi-driver-controller-plugin-7bb788b57-vhsgd requesting resource cpu=510m on Node sks-test-v1-26.4-workergroup-q5bjm
Jun  9 10:45:56.462: INFO: Pod smtx-elf-csi-driver-node-plugin-9plgj requesting resource cpu=210m on Node sks-test-v1-26.4-workergroup-qdprq
Jun  9 10:45:56.462: INFO: Pod smtx-elf-csi-driver-node-plugin-cs9pp requesting resource cpu=210m on Node sks-test-v1-26.4-workergroup-q5bjm
Jun  9 10:45:56.462: INFO: Pod smtx-elf-csi-driver-node-plugin-dbgrm requesting resource cpu=210m on Node sks-test-v1-26.4-workergroup-4hkw9
Jun  9 10:45:56.462: INFO: Pod tigera-operator-5948566997-9t6st requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-4hkw9
Jun  9 10:45:56.462: INFO: Pod sonobuoy requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-qdprq
Jun  9 10:45:56.462: INFO: Pod sonobuoy-e2e-job-09e3c2818710427f requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-qdprq
Jun  9 10:45:56.462: INFO: Pod sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-4r82f requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-4hkw9
Jun  9 10:45:56.462: INFO: Pod sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-crx45 requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-q5bjm
Jun  9 10:45:56.462: INFO: Pod sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-f89bf requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-qdprq
STEP: Starting Pods to consume most of the cluster CPU. 06/09/23 10:45:56.462
Jun  9 10:45:56.462: INFO: Creating a pod which consumes cpu=1113m on Node sks-test-v1-26.4-workergroup-4hkw9
Jun  9 10:45:56.478: INFO: Creating a pod which consumes cpu=896m on Node sks-test-v1-26.4-workergroup-q5bjm
Jun  9 10:45:56.497: INFO: Creating a pod which consumes cpu=1253m on Node sks-test-v1-26.4-workergroup-qdprq
Jun  9 10:45:56.511: INFO: Waiting up to 5m0s for pod "filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1" in namespace "sched-pred-670" to be "running"
Jun  9 10:45:56.524: INFO: Pod "filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.375749ms
Jun  9 10:45:58.531: INFO: Pod "filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.020182232s
Jun  9 10:45:58.531: INFO: Pod "filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1" satisfied condition "running"
Jun  9 10:45:58.531: INFO: Waiting up to 5m0s for pod "filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a" in namespace "sched-pred-670" to be "running"
Jun  9 10:45:58.536: INFO: Pod "filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a": Phase="Running", Reason="", readiness=true. Elapsed: 5.201824ms
Jun  9 10:45:58.536: INFO: Pod "filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a" satisfied condition "running"
Jun  9 10:45:58.536: INFO: Waiting up to 5m0s for pod "filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55" in namespace "sched-pred-670" to be "running"
Jun  9 10:45:58.543: INFO: Pod "filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55": Phase="Running", Reason="", readiness=true. Elapsed: 6.70306ms
Jun  9 10:45:58.543: INFO: Pod "filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 06/09/23 10:45:58.543
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55.1766f7c9c877f717], Reason = [Scheduled], Message = [Successfully assigned sched-pred-670/filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55 to sks-test-v1-26.4-workergroup-qdprq] 06/09/23 10:45:58.55
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55.1766f7c9f9f6816c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/09/23 10:45:58.55
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55.1766f7c9fc578e91], Reason = [Created], Message = [Created container filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55] 06/09/23 10:45:58.55
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55.1766f7ca05082179], Reason = [Started], Message = [Started container filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55] 06/09/23 10:45:58.551
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a.1766f7c9c7051d6e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-670/filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a to sks-test-v1-26.4-workergroup-q5bjm] 06/09/23 10:45:58.551
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a.1766f7c9f686067e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/09/23 10:45:58.551
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a.1766f7c9f936b6fc], Reason = [Created], Message = [Created container filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a] 06/09/23 10:45:58.551
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a.1766f7ca027ade53], Reason = [Started], Message = [Started container filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a] 06/09/23 10:45:58.551
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1.1766f7c9c638005d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-670/filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1 to sks-test-v1-26.4-workergroup-4hkw9] 06/09/23 10:45:58.551
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1.1766f7c9f863a10c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/09/23 10:45:58.551
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1.1766f7c9fb975a1d], Reason = [Created], Message = [Created container filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1] 06/09/23 10:45:58.551
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1.1766f7ca0306271b], Reason = [Started], Message = [Started container filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1] 06/09/23 10:45:58.551
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1766f7ca42143d70], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 06/09/23 10:45:58.573
STEP: removing the label node off the node sks-test-v1-26.4-workergroup-4hkw9 06/09/23 10:45:59.573
STEP: verifying the node doesn't have the label node 06/09/23 10:45:59.597
STEP: removing the label node off the node sks-test-v1-26.4-workergroup-q5bjm 06/09/23 10:45:59.606
STEP: verifying the node doesn't have the label node 06/09/23 10:45:59.629
STEP: removing the label node off the node sks-test-v1-26.4-workergroup-qdprq 06/09/23 10:45:59.635
STEP: verifying the node doesn't have the label node 06/09/23 10:45:59.661
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:45:59.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-670" for this suite. 06/09/23 10:45:59.678
------------------------------
• [3.468 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:45:56.224
    Jun  9 10:45:56.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename sched-pred 06/09/23 10:45:56.225
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:56.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:56.253
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jun  9 10:45:56.257: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun  9 10:45:56.276: INFO: Waiting for terminating namespaces to be deleted...
    Jun  9 10:45:56.282: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-4hkw9 before test
    Jun  9 10:45:56.299: INFO: calico-kube-controllers-75b856575b-4vmmr from calico-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 10:45:56.299: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jun  9 10:45:56.299: INFO: calico-node-7bzns from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
    Jun  9 10:45:56.299: INFO: 	Container calico-node ready: true, restart count 0
    Jun  9 10:45:56.299: INFO: csi-node-driver-qw6d7 from calico-system started at 2023-06-09 06:03:15 +0000 UTC (2 container statuses recorded)
    Jun  9 10:45:56.299: INFO: 	Container calico-csi ready: true, restart count 0
    Jun  9 10:45:56.299: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun  9 10:45:56.299: INFO: coredns-5c4bbd897-mndk4 from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 10:45:56.299: INFO: 	Container coredns ready: true, restart count 0
    Jun  9 10:45:56.299: INFO: coredns-5c4bbd897-vswmt from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 10:45:56.299: INFO: 	Container coredns ready: true, restart count 0
    Jun  9 10:45:56.299: INFO: kube-proxy-f8glt from kube-system started at 2023-06-09 06:01:45 +0000 UTC (1 container statuses recorded)
    Jun  9 10:45:56.299: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  9 10:45:56.299: INFO: snapshot-controller-df74f7b6c-pdcpl from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 10:45:56.299: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun  9 10:45:56.299: INFO: smtx-elf-csi-driver-node-plugin-dbgrm from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
    Jun  9 10:45:56.299: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 10:45:56.299: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun  9 10:45:56.299: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 10:45:56.299: INFO: tigera-operator-5948566997-9t6st from sks-system started at 2023-06-09 06:02:52 +0000 UTC (1 container statuses recorded)
    Jun  9 10:45:56.299: INFO: 	Container tigera-operator ready: true, restart count 0
    Jun  9 10:45:56.299: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-4r82f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 10:45:56.299: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 10:45:56.299: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  9 10:45:56.299: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-q5bjm before test
    Jun  9 10:45:56.312: INFO: calico-node-vpfwq from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
    Jun  9 10:45:56.312: INFO: 	Container calico-node ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: calico-typha-866dbffd5c-r6nf8 from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
    Jun  9 10:45:56.312: INFO: 	Container calico-typha ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: csi-node-driver-k2xdl from calico-system started at 2023-06-09 06:04:17 +0000 UTC (2 container statuses recorded)
    Jun  9 10:45:56.312: INFO: 	Container calico-csi ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: kube-proxy-b8r9m from kube-system started at 2023-06-09 06:02:23 +0000 UTC (1 container statuses recorded)
    Jun  9 10:45:56.312: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: smtx-elf-csi-driver-controller-plugin-7bb788b57-vhsgd from sks-system started at 2023-06-09 06:04:22 +0000 UTC (6 container statuses recorded)
    Jun  9 10:45:56.312: INFO: 	Container csi-attacher ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: 	Container csi-resizer ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: smtx-elf-csi-driver-node-plugin-cs9pp from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
    Jun  9 10:45:56.312: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-crx45 from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 10:45:56.312: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  9 10:45:56.312: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-qdprq before test
    Jun  9 10:45:56.327: INFO: calico-node-5cjkc from calico-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
    Jun  9 10:45:56.327: INFO: 	Container calico-node ready: true, restart count 0
    Jun  9 10:45:56.327: INFO: calico-typha-866dbffd5c-n5p5b from calico-system started at 2023-06-09 06:03:07 +0000 UTC (1 container statuses recorded)
    Jun  9 10:45:56.327: INFO: 	Container calico-typha ready: true, restart count 0
    Jun  9 10:45:56.327: INFO: csi-node-driver-jw49b from calico-system started at 2023-06-09 10:43:36 +0000 UTC (2 container statuses recorded)
    Jun  9 10:45:56.327: INFO: 	Container calico-csi ready: true, restart count 0
    Jun  9 10:45:56.327: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun  9 10:45:56.327: INFO: kube-proxy-gpttb from kube-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
    Jun  9 10:45:56.327: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  9 10:45:56.327: INFO: smtx-elf-csi-driver-node-plugin-9plgj from sks-system started at 2023-06-09 06:04:22 +0000 UTC (3 container statuses recorded)
    Jun  9 10:45:56.327: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 10:45:56.327: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun  9 10:45:56.327: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 10:45:56.327: INFO: sonobuoy from sonobuoy started at 2023-06-09 10:18:23 +0000 UTC (1 container statuses recorded)
    Jun  9 10:45:56.327: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun  9 10:45:56.327: INFO: sonobuoy-e2e-job-09e3c2818710427f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 10:45:56.327: INFO: 	Container e2e ready: true, restart count 0
    Jun  9 10:45:56.327: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 10:45:56.327: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-f89bf from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 10:45:56.327: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 10:45:56.327: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node sks-test-v1-26.4-workergroup-4hkw9 06/09/23 10:45:56.383
    STEP: verifying the node has the label node sks-test-v1-26.4-workergroup-q5bjm 06/09/23 10:45:56.402
    STEP: verifying the node has the label node sks-test-v1-26.4-workergroup-qdprq 06/09/23 10:45:56.424
    Jun  9 10:45:56.462: INFO: Pod calico-kube-controllers-75b856575b-4vmmr requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-4hkw9
    Jun  9 10:45:56.462: INFO: Pod calico-node-5cjkc requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-qdprq
    Jun  9 10:45:56.462: INFO: Pod calico-node-7bzns requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-4hkw9
    Jun  9 10:45:56.462: INFO: Pod calico-node-vpfwq requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-q5bjm
    Jun  9 10:45:56.462: INFO: Pod calico-typha-866dbffd5c-n5p5b requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-qdprq
    Jun  9 10:45:56.462: INFO: Pod calico-typha-866dbffd5c-r6nf8 requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-q5bjm
    Jun  9 10:45:56.462: INFO: Pod csi-node-driver-jw49b requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-qdprq
    Jun  9 10:45:56.462: INFO: Pod csi-node-driver-k2xdl requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-q5bjm
    Jun  9 10:45:56.462: INFO: Pod csi-node-driver-qw6d7 requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-4hkw9
    Jun  9 10:45:56.462: INFO: Pod coredns-5c4bbd897-mndk4 requesting resource cpu=100m on Node sks-test-v1-26.4-workergroup-4hkw9
    Jun  9 10:45:56.462: INFO: Pod coredns-5c4bbd897-vswmt requesting resource cpu=100m on Node sks-test-v1-26.4-workergroup-4hkw9
    Jun  9 10:45:56.462: INFO: Pod kube-proxy-b8r9m requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-q5bjm
    Jun  9 10:45:56.462: INFO: Pod kube-proxy-f8glt requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-4hkw9
    Jun  9 10:45:56.462: INFO: Pod kube-proxy-gpttb requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-qdprq
    Jun  9 10:45:56.462: INFO: Pod snapshot-controller-df74f7b6c-pdcpl requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-4hkw9
    Jun  9 10:45:56.462: INFO: Pod smtx-elf-csi-driver-controller-plugin-7bb788b57-vhsgd requesting resource cpu=510m on Node sks-test-v1-26.4-workergroup-q5bjm
    Jun  9 10:45:56.462: INFO: Pod smtx-elf-csi-driver-node-plugin-9plgj requesting resource cpu=210m on Node sks-test-v1-26.4-workergroup-qdprq
    Jun  9 10:45:56.462: INFO: Pod smtx-elf-csi-driver-node-plugin-cs9pp requesting resource cpu=210m on Node sks-test-v1-26.4-workergroup-q5bjm
    Jun  9 10:45:56.462: INFO: Pod smtx-elf-csi-driver-node-plugin-dbgrm requesting resource cpu=210m on Node sks-test-v1-26.4-workergroup-4hkw9
    Jun  9 10:45:56.462: INFO: Pod tigera-operator-5948566997-9t6st requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-4hkw9
    Jun  9 10:45:56.462: INFO: Pod sonobuoy requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-qdprq
    Jun  9 10:45:56.462: INFO: Pod sonobuoy-e2e-job-09e3c2818710427f requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-qdprq
    Jun  9 10:45:56.462: INFO: Pod sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-4r82f requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-4hkw9
    Jun  9 10:45:56.462: INFO: Pod sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-crx45 requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-q5bjm
    Jun  9 10:45:56.462: INFO: Pod sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-f89bf requesting resource cpu=0m on Node sks-test-v1-26.4-workergroup-qdprq
    STEP: Starting Pods to consume most of the cluster CPU. 06/09/23 10:45:56.462
    Jun  9 10:45:56.462: INFO: Creating a pod which consumes cpu=1113m on Node sks-test-v1-26.4-workergroup-4hkw9
    Jun  9 10:45:56.478: INFO: Creating a pod which consumes cpu=896m on Node sks-test-v1-26.4-workergroup-q5bjm
    Jun  9 10:45:56.497: INFO: Creating a pod which consumes cpu=1253m on Node sks-test-v1-26.4-workergroup-qdprq
    Jun  9 10:45:56.511: INFO: Waiting up to 5m0s for pod "filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1" in namespace "sched-pred-670" to be "running"
    Jun  9 10:45:56.524: INFO: Pod "filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.375749ms
    Jun  9 10:45:58.531: INFO: Pod "filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.020182232s
    Jun  9 10:45:58.531: INFO: Pod "filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1" satisfied condition "running"
    Jun  9 10:45:58.531: INFO: Waiting up to 5m0s for pod "filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a" in namespace "sched-pred-670" to be "running"
    Jun  9 10:45:58.536: INFO: Pod "filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a": Phase="Running", Reason="", readiness=true. Elapsed: 5.201824ms
    Jun  9 10:45:58.536: INFO: Pod "filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a" satisfied condition "running"
    Jun  9 10:45:58.536: INFO: Waiting up to 5m0s for pod "filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55" in namespace "sched-pred-670" to be "running"
    Jun  9 10:45:58.543: INFO: Pod "filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55": Phase="Running", Reason="", readiness=true. Elapsed: 6.70306ms
    Jun  9 10:45:58.543: INFO: Pod "filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 06/09/23 10:45:58.543
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55.1766f7c9c877f717], Reason = [Scheduled], Message = [Successfully assigned sched-pred-670/filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55 to sks-test-v1-26.4-workergroup-qdprq] 06/09/23 10:45:58.55
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55.1766f7c9f9f6816c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/09/23 10:45:58.55
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55.1766f7c9fc578e91], Reason = [Created], Message = [Created container filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55] 06/09/23 10:45:58.55
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55.1766f7ca05082179], Reason = [Started], Message = [Started container filler-pod-0195a38c-d0e5-4382-8dc7-d296d6b2ce55] 06/09/23 10:45:58.551
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a.1766f7c9c7051d6e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-670/filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a to sks-test-v1-26.4-workergroup-q5bjm] 06/09/23 10:45:58.551
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a.1766f7c9f686067e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/09/23 10:45:58.551
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a.1766f7c9f936b6fc], Reason = [Created], Message = [Created container filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a] 06/09/23 10:45:58.551
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a.1766f7ca027ade53], Reason = [Started], Message = [Started container filler-pod-2c732fd7-ef56-4d5f-a4f4-abe9af86084a] 06/09/23 10:45:58.551
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1.1766f7c9c638005d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-670/filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1 to sks-test-v1-26.4-workergroup-4hkw9] 06/09/23 10:45:58.551
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1.1766f7c9f863a10c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/09/23 10:45:58.551
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1.1766f7c9fb975a1d], Reason = [Created], Message = [Created container filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1] 06/09/23 10:45:58.551
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1.1766f7ca0306271b], Reason = [Started], Message = [Started container filler-pod-564d7767-1f94-4c9f-afc4-dad972f4b8f1] 06/09/23 10:45:58.551
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1766f7ca42143d70], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 06/09/23 10:45:58.573
    STEP: removing the label node off the node sks-test-v1-26.4-workergroup-4hkw9 06/09/23 10:45:59.573
    STEP: verifying the node doesn't have the label node 06/09/23 10:45:59.597
    STEP: removing the label node off the node sks-test-v1-26.4-workergroup-q5bjm 06/09/23 10:45:59.606
    STEP: verifying the node doesn't have the label node 06/09/23 10:45:59.629
    STEP: removing the label node off the node sks-test-v1-26.4-workergroup-qdprq 06/09/23 10:45:59.635
    STEP: verifying the node doesn't have the label node 06/09/23 10:45:59.661
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:45:59.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-670" for this suite. 06/09/23 10:45:59.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:45:59.693
Jun  9 10:45:59.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 10:45:59.693
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:59.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:59.747
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 06/09/23 10:45:59.756
Jun  9 10:45:59.773: INFO: Waiting up to 5m0s for pod "pod-7e871800-3870-496a-9587-9315c1732157" in namespace "emptydir-3855" to be "Succeeded or Failed"
Jun  9 10:45:59.782: INFO: Pod "pod-7e871800-3870-496a-9587-9315c1732157": Phase="Pending", Reason="", readiness=false. Elapsed: 8.971128ms
Jun  9 10:46:01.788: INFO: Pod "pod-7e871800-3870-496a-9587-9315c1732157": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014860927s
Jun  9 10:46:03.790: INFO: Pod "pod-7e871800-3870-496a-9587-9315c1732157": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016958043s
STEP: Saw pod success 06/09/23 10:46:03.79
Jun  9 10:46:03.790: INFO: Pod "pod-7e871800-3870-496a-9587-9315c1732157" satisfied condition "Succeeded or Failed"
Jun  9 10:46:03.797: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-7e871800-3870-496a-9587-9315c1732157 container test-container: <nil>
STEP: delete the pod 06/09/23 10:46:03.808
Jun  9 10:46:03.826: INFO: Waiting for pod pod-7e871800-3870-496a-9587-9315c1732157 to disappear
Jun  9 10:46:03.832: INFO: Pod pod-7e871800-3870-496a-9587-9315c1732157 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 10:46:03.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3855" for this suite. 06/09/23 10:46:03.841
------------------------------
• [4.160 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:45:59.693
    Jun  9 10:45:59.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 10:45:59.693
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:45:59.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:45:59.747
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 06/09/23 10:45:59.756
    Jun  9 10:45:59.773: INFO: Waiting up to 5m0s for pod "pod-7e871800-3870-496a-9587-9315c1732157" in namespace "emptydir-3855" to be "Succeeded or Failed"
    Jun  9 10:45:59.782: INFO: Pod "pod-7e871800-3870-496a-9587-9315c1732157": Phase="Pending", Reason="", readiness=false. Elapsed: 8.971128ms
    Jun  9 10:46:01.788: INFO: Pod "pod-7e871800-3870-496a-9587-9315c1732157": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014860927s
    Jun  9 10:46:03.790: INFO: Pod "pod-7e871800-3870-496a-9587-9315c1732157": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016958043s
    STEP: Saw pod success 06/09/23 10:46:03.79
    Jun  9 10:46:03.790: INFO: Pod "pod-7e871800-3870-496a-9587-9315c1732157" satisfied condition "Succeeded or Failed"
    Jun  9 10:46:03.797: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-7e871800-3870-496a-9587-9315c1732157 container test-container: <nil>
    STEP: delete the pod 06/09/23 10:46:03.808
    Jun  9 10:46:03.826: INFO: Waiting for pod pod-7e871800-3870-496a-9587-9315c1732157 to disappear
    Jun  9 10:46:03.832: INFO: Pod pod-7e871800-3870-496a-9587-9315c1732157 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:46:03.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3855" for this suite. 06/09/23 10:46:03.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:46:03.854
Jun  9 10:46:03.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename init-container 06/09/23 10:46:03.855
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:46:03.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:46:03.882
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 06/09/23 10:46:03.887
Jun  9 10:46:03.887: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:46:08.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3377" for this suite. 06/09/23 10:46:08.409
------------------------------
• [4.581 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:46:03.854
    Jun  9 10:46:03.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename init-container 06/09/23 10:46:03.855
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:46:03.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:46:03.882
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 06/09/23 10:46:03.887
    Jun  9 10:46:03.887: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:46:08.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3377" for this suite. 06/09/23 10:46:08.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:46:08.436
Jun  9 10:46:08.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename replication-controller 06/09/23 10:46:08.438
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:46:08.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:46:08.473
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jun  9 10:46:08.481: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 06/09/23 10:46:08.509
STEP: Checking rc "condition-test" has the desired failure condition set 06/09/23 10:46:08.52
STEP: Scaling down rc "condition-test" to satisfy pod quota 06/09/23 10:46:09.531
Jun  9 10:46:09.547: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 06/09/23 10:46:09.547
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun  9 10:46:10.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7203" for this suite. 06/09/23 10:46:10.575
------------------------------
• [2.159 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:46:08.436
    Jun  9 10:46:08.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename replication-controller 06/09/23 10:46:08.438
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:46:08.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:46:08.473
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jun  9 10:46:08.481: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 06/09/23 10:46:08.509
    STEP: Checking rc "condition-test" has the desired failure condition set 06/09/23 10:46:08.52
    STEP: Scaling down rc "condition-test" to satisfy pod quota 06/09/23 10:46:09.531
    Jun  9 10:46:09.547: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 06/09/23 10:46:09.547
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:46:10.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7203" for this suite. 06/09/23 10:46:10.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:46:10.597
Jun  9 10:46:10.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pods 06/09/23 10:46:10.599
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:46:10.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:46:10.629
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 06/09/23 10:46:10.635
Jun  9 10:46:10.647: INFO: Waiting up to 5m0s for pod "pod-hostip-19f93184-c68c-48e0-9deb-a1d62dcbb72f" in namespace "pods-5180" to be "running and ready"
Jun  9 10:46:10.657: INFO: Pod "pod-hostip-19f93184-c68c-48e0-9deb-a1d62dcbb72f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.019911ms
Jun  9 10:46:10.657: INFO: The phase of Pod pod-hostip-19f93184-c68c-48e0-9deb-a1d62dcbb72f is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:46:12.663: INFO: Pod "pod-hostip-19f93184-c68c-48e0-9deb-a1d62dcbb72f": Phase="Running", Reason="", readiness=true. Elapsed: 2.015795668s
Jun  9 10:46:12.663: INFO: The phase of Pod pod-hostip-19f93184-c68c-48e0-9deb-a1d62dcbb72f is Running (Ready = true)
Jun  9 10:46:12.663: INFO: Pod "pod-hostip-19f93184-c68c-48e0-9deb-a1d62dcbb72f" satisfied condition "running and ready"
Jun  9 10:46:12.681: INFO: Pod pod-hostip-19f93184-c68c-48e0-9deb-a1d62dcbb72f has hostIP: 10.255.64.103
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  9 10:46:12.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5180" for this suite. 06/09/23 10:46:12.7
------------------------------
• [2.122 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:46:10.597
    Jun  9 10:46:10.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pods 06/09/23 10:46:10.599
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:46:10.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:46:10.629
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 06/09/23 10:46:10.635
    Jun  9 10:46:10.647: INFO: Waiting up to 5m0s for pod "pod-hostip-19f93184-c68c-48e0-9deb-a1d62dcbb72f" in namespace "pods-5180" to be "running and ready"
    Jun  9 10:46:10.657: INFO: Pod "pod-hostip-19f93184-c68c-48e0-9deb-a1d62dcbb72f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.019911ms
    Jun  9 10:46:10.657: INFO: The phase of Pod pod-hostip-19f93184-c68c-48e0-9deb-a1d62dcbb72f is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:46:12.663: INFO: Pod "pod-hostip-19f93184-c68c-48e0-9deb-a1d62dcbb72f": Phase="Running", Reason="", readiness=true. Elapsed: 2.015795668s
    Jun  9 10:46:12.663: INFO: The phase of Pod pod-hostip-19f93184-c68c-48e0-9deb-a1d62dcbb72f is Running (Ready = true)
    Jun  9 10:46:12.663: INFO: Pod "pod-hostip-19f93184-c68c-48e0-9deb-a1d62dcbb72f" satisfied condition "running and ready"
    Jun  9 10:46:12.681: INFO: Pod pod-hostip-19f93184-c68c-48e0-9deb-a1d62dcbb72f has hostIP: 10.255.64.103
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:46:12.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5180" for this suite. 06/09/23 10:46:12.7
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:46:12.72
Jun  9 10:46:12.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename disruption 06/09/23 10:46:12.722
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:46:12.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:46:12.762
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 06/09/23 10:46:12.766
STEP: Waiting for the pdb to be processed 06/09/23 10:46:12.776
STEP: First trying to evict a pod which shouldn't be evictable 06/09/23 10:46:14.808
STEP: Waiting for all pods to be running 06/09/23 10:46:14.808
Jun  9 10:46:14.815: INFO: pods: 0 < 3
STEP: locating a running pod 06/09/23 10:46:16.823
STEP: Updating the pdb to allow a pod to be evicted 06/09/23 10:46:16.847
STEP: Waiting for the pdb to be processed 06/09/23 10:46:16.864
STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/09/23 10:46:16.874
STEP: Waiting for all pods to be running 06/09/23 10:46:16.874
STEP: Waiting for the pdb to observed all healthy pods 06/09/23 10:46:16.882
STEP: Patching the pdb to disallow a pod to be evicted 06/09/23 10:46:16.927
STEP: Waiting for the pdb to be processed 06/09/23 10:46:16.954
STEP: Waiting for all pods to be running 06/09/23 10:46:16.963
Jun  9 10:46:16.970: INFO: running pods: 2 < 3
STEP: locating a running pod 06/09/23 10:46:19.016
STEP: Deleting the pdb to allow a pod to be evicted 06/09/23 10:46:19.031
STEP: Waiting for the pdb to be deleted 06/09/23 10:46:19.046
STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/09/23 10:46:19.053
STEP: Waiting for all pods to be running 06/09/23 10:46:19.053
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun  9 10:46:19.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3223" for this suite. 06/09/23 10:46:19.095
------------------------------
• [SLOW TEST] [6.400 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:46:12.72
    Jun  9 10:46:12.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename disruption 06/09/23 10:46:12.722
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:46:12.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:46:12.762
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 06/09/23 10:46:12.766
    STEP: Waiting for the pdb to be processed 06/09/23 10:46:12.776
    STEP: First trying to evict a pod which shouldn't be evictable 06/09/23 10:46:14.808
    STEP: Waiting for all pods to be running 06/09/23 10:46:14.808
    Jun  9 10:46:14.815: INFO: pods: 0 < 3
    STEP: locating a running pod 06/09/23 10:46:16.823
    STEP: Updating the pdb to allow a pod to be evicted 06/09/23 10:46:16.847
    STEP: Waiting for the pdb to be processed 06/09/23 10:46:16.864
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/09/23 10:46:16.874
    STEP: Waiting for all pods to be running 06/09/23 10:46:16.874
    STEP: Waiting for the pdb to observed all healthy pods 06/09/23 10:46:16.882
    STEP: Patching the pdb to disallow a pod to be evicted 06/09/23 10:46:16.927
    STEP: Waiting for the pdb to be processed 06/09/23 10:46:16.954
    STEP: Waiting for all pods to be running 06/09/23 10:46:16.963
    Jun  9 10:46:16.970: INFO: running pods: 2 < 3
    STEP: locating a running pod 06/09/23 10:46:19.016
    STEP: Deleting the pdb to allow a pod to be evicted 06/09/23 10:46:19.031
    STEP: Waiting for the pdb to be deleted 06/09/23 10:46:19.046
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/09/23 10:46:19.053
    STEP: Waiting for all pods to be running 06/09/23 10:46:19.053
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:46:19.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3223" for this suite. 06/09/23 10:46:19.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:46:19.12
Jun  9 10:46:19.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename statefulset 06/09/23 10:46:19.122
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:46:19.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:46:19.176
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-166 06/09/23 10:46:19.184
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-166 06/09/23 10:46:19.21
Jun  9 10:46:19.242: INFO: Found 0 stateful pods, waiting for 1
Jun  9 10:46:29.263: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 06/09/23 10:46:29.284
STEP: Getting /status 06/09/23 10:46:29.32
Jun  9 10:46:29.328: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 06/09/23 10:46:29.329
Jun  9 10:46:29.364: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 06/09/23 10:46:29.364
Jun  9 10:46:29.369: INFO: Observed &StatefulSet event: ADDED
Jun  9 10:46:29.369: INFO: Found Statefulset ss in namespace statefulset-166 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  9 10:46:29.369: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 06/09/23 10:46:29.369
Jun  9 10:46:29.369: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun  9 10:46:29.385: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 06/09/23 10:46:29.385
Jun  9 10:46:29.389: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  9 10:46:29.389: INFO: Deleting all statefulset in ns statefulset-166
Jun  9 10:46:29.394: INFO: Scaling statefulset ss to 0
Jun  9 10:46:39.424: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 10:46:39.435: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  9 10:46:39.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-166" for this suite. 06/09/23 10:46:39.47
------------------------------
• [SLOW TEST] [20.364 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:46:19.12
    Jun  9 10:46:19.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename statefulset 06/09/23 10:46:19.122
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:46:19.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:46:19.176
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-166 06/09/23 10:46:19.184
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-166 06/09/23 10:46:19.21
    Jun  9 10:46:19.242: INFO: Found 0 stateful pods, waiting for 1
    Jun  9 10:46:29.263: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 06/09/23 10:46:29.284
    STEP: Getting /status 06/09/23 10:46:29.32
    Jun  9 10:46:29.328: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 06/09/23 10:46:29.329
    Jun  9 10:46:29.364: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 06/09/23 10:46:29.364
    Jun  9 10:46:29.369: INFO: Observed &StatefulSet event: ADDED
    Jun  9 10:46:29.369: INFO: Found Statefulset ss in namespace statefulset-166 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun  9 10:46:29.369: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 06/09/23 10:46:29.369
    Jun  9 10:46:29.369: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun  9 10:46:29.385: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 06/09/23 10:46:29.385
    Jun  9 10:46:29.389: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  9 10:46:29.389: INFO: Deleting all statefulset in ns statefulset-166
    Jun  9 10:46:29.394: INFO: Scaling statefulset ss to 0
    Jun  9 10:46:39.424: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  9 10:46:39.435: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:46:39.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-166" for this suite. 06/09/23 10:46:39.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:46:39.487
Jun  9 10:46:39.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename statefulset 06/09/23 10:46:39.49
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:46:39.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:46:39.521
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6319 06/09/23 10:46:39.528
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-6319 06/09/23 10:46:39.547
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6319 06/09/23 10:46:39.556
Jun  9 10:46:39.597: INFO: Found 0 stateful pods, waiting for 1
Jun  9 10:46:49.605: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 06/09/23 10:46:49.605
Jun  9 10:46:49.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-6319 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 10:46:49.827: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 10:46:49.827: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 10:46:49.828: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 10:46:49.835: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun  9 10:46:59.852: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 10:46:59.852: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 10:46:59.959: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Jun  9 10:46:59.959: INFO: ss-0  sks-test-v1-26.4-workergroup-qdprq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:39 +0000 UTC  }]
Jun  9 10:46:59.959: INFO: ss-1                                      Pending         []
Jun  9 10:46:59.959: INFO: 
Jun  9 10:46:59.959: INFO: StatefulSet ss has not reached scale 3, at 2
Jun  9 10:47:00.969: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.970458516s
Jun  9 10:47:01.982: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.960632009s
Jun  9 10:47:02.997: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.948004842s
Jun  9 10:47:04.005: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.932970783s
Jun  9 10:47:05.014: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.925541604s
Jun  9 10:47:06.028: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.916039428s
Jun  9 10:47:07.038: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.902095952s
Jun  9 10:47:08.045: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.892414377s
Jun  9 10:47:09.054: INFO: Verifying statefulset ss doesn't scale past 3 for another 885.238538ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6319 06/09/23 10:47:10.055
Jun  9 10:47:10.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-6319 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 10:47:10.225: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  9 10:47:10.225: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 10:47:10.225: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 10:47:10.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-6319 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 10:47:10.409: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun  9 10:47:10.409: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 10:47:10.409: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 10:47:10.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-6319 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 10:47:10.585: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun  9 10:47:10.585: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 10:47:10.585: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 10:47:10.596: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jun  9 10:47:20.604: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 10:47:20.604: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 10:47:20.604: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 06/09/23 10:47:20.604
Jun  9 10:47:20.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-6319 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 10:47:20.793: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 10:47:20.793: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 10:47:20.793: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 10:47:20.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-6319 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 10:47:21.057: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 10:47:21.058: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 10:47:21.058: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 10:47:21.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-6319 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 10:47:21.239: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 10:47:21.239: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 10:47:21.239: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 10:47:21.239: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 10:47:21.245: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun  9 10:47:31.265: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 10:47:31.265: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 10:47:31.265: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 10:47:31.294: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Jun  9 10:47:31.294: INFO: ss-0  sks-test-v1-26.4-workergroup-qdprq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:39 +0000 UTC  }]
Jun  9 10:47:31.294: INFO: ss-1  sks-test-v1-26.4-workergroup-4hkw9  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  }]
Jun  9 10:47:31.294: INFO: ss-2  sks-test-v1-26.4-workergroup-q5bjm  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  }]
Jun  9 10:47:31.294: INFO: 
Jun  9 10:47:31.294: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  9 10:47:32.301: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Jun  9 10:47:32.301: INFO: ss-1  sks-test-v1-26.4-workergroup-4hkw9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  }]
Jun  9 10:47:32.301: INFO: ss-2  sks-test-v1-26.4-workergroup-q5bjm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  }]
Jun  9 10:47:32.301: INFO: 
Jun  9 10:47:32.301: INFO: StatefulSet ss has not reached scale 0, at 2
Jun  9 10:47:33.308: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.982669335s
Jun  9 10:47:34.315: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.975220927s
Jun  9 10:47:35.321: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.968707012s
Jun  9 10:47:36.329: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.960554133s
Jun  9 10:47:37.336: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.954446387s
Jun  9 10:47:38.343: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.946587989s
Jun  9 10:47:39.349: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.940327949s
Jun  9 10:47:40.356: INFO: Verifying statefulset ss doesn't scale past 0 for another 934.167758ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6319 06/09/23 10:47:41.357
Jun  9 10:47:41.362: INFO: Scaling statefulset ss to 0
Jun  9 10:47:41.379: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  9 10:47:41.384: INFO: Deleting all statefulset in ns statefulset-6319
Jun  9 10:47:41.389: INFO: Scaling statefulset ss to 0
Jun  9 10:47:41.405: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 10:47:41.410: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  9 10:47:41.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6319" for this suite. 06/09/23 10:47:41.436
------------------------------
• [SLOW TEST] [61.959 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:46:39.487
    Jun  9 10:46:39.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename statefulset 06/09/23 10:46:39.49
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:46:39.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:46:39.521
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6319 06/09/23 10:46:39.528
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-6319 06/09/23 10:46:39.547
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6319 06/09/23 10:46:39.556
    Jun  9 10:46:39.597: INFO: Found 0 stateful pods, waiting for 1
    Jun  9 10:46:49.605: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 06/09/23 10:46:49.605
    Jun  9 10:46:49.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-6319 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  9 10:46:49.827: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  9 10:46:49.827: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  9 10:46:49.828: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  9 10:46:49.835: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jun  9 10:46:59.852: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun  9 10:46:59.852: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  9 10:46:59.959: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
    Jun  9 10:46:59.959: INFO: ss-0  sks-test-v1-26.4-workergroup-qdprq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:39 +0000 UTC  }]
    Jun  9 10:46:59.959: INFO: ss-1                                      Pending         []
    Jun  9 10:46:59.959: INFO: 
    Jun  9 10:46:59.959: INFO: StatefulSet ss has not reached scale 3, at 2
    Jun  9 10:47:00.969: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.970458516s
    Jun  9 10:47:01.982: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.960632009s
    Jun  9 10:47:02.997: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.948004842s
    Jun  9 10:47:04.005: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.932970783s
    Jun  9 10:47:05.014: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.925541604s
    Jun  9 10:47:06.028: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.916039428s
    Jun  9 10:47:07.038: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.902095952s
    Jun  9 10:47:08.045: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.892414377s
    Jun  9 10:47:09.054: INFO: Verifying statefulset ss doesn't scale past 3 for another 885.238538ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6319 06/09/23 10:47:10.055
    Jun  9 10:47:10.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-6319 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  9 10:47:10.225: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun  9 10:47:10.225: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  9 10:47:10.225: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  9 10:47:10.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-6319 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  9 10:47:10.409: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jun  9 10:47:10.409: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  9 10:47:10.409: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  9 10:47:10.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-6319 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  9 10:47:10.585: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jun  9 10:47:10.585: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  9 10:47:10.585: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  9 10:47:10.596: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jun  9 10:47:20.604: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun  9 10:47:20.604: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun  9 10:47:20.604: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 06/09/23 10:47:20.604
    Jun  9 10:47:20.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-6319 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  9 10:47:20.793: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  9 10:47:20.793: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  9 10:47:20.793: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  9 10:47:20.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-6319 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  9 10:47:21.057: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  9 10:47:21.058: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  9 10:47:21.058: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  9 10:47:21.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-6319 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  9 10:47:21.239: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  9 10:47:21.239: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  9 10:47:21.239: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  9 10:47:21.239: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  9 10:47:21.245: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jun  9 10:47:31.265: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun  9 10:47:31.265: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jun  9 10:47:31.265: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jun  9 10:47:31.294: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
    Jun  9 10:47:31.294: INFO: ss-0  sks-test-v1-26.4-workergroup-qdprq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:39 +0000 UTC  }]
    Jun  9 10:47:31.294: INFO: ss-1  sks-test-v1-26.4-workergroup-4hkw9  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  }]
    Jun  9 10:47:31.294: INFO: ss-2  sks-test-v1-26.4-workergroup-q5bjm  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  }]
    Jun  9 10:47:31.294: INFO: 
    Jun  9 10:47:31.294: INFO: StatefulSet ss has not reached scale 0, at 3
    Jun  9 10:47:32.301: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
    Jun  9 10:47:32.301: INFO: ss-1  sks-test-v1-26.4-workergroup-4hkw9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  }]
    Jun  9 10:47:32.301: INFO: ss-2  sks-test-v1-26.4-workergroup-q5bjm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:47:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:46:59 +0000 UTC  }]
    Jun  9 10:47:32.301: INFO: 
    Jun  9 10:47:32.301: INFO: StatefulSet ss has not reached scale 0, at 2
    Jun  9 10:47:33.308: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.982669335s
    Jun  9 10:47:34.315: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.975220927s
    Jun  9 10:47:35.321: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.968707012s
    Jun  9 10:47:36.329: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.960554133s
    Jun  9 10:47:37.336: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.954446387s
    Jun  9 10:47:38.343: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.946587989s
    Jun  9 10:47:39.349: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.940327949s
    Jun  9 10:47:40.356: INFO: Verifying statefulset ss doesn't scale past 0 for another 934.167758ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6319 06/09/23 10:47:41.357
    Jun  9 10:47:41.362: INFO: Scaling statefulset ss to 0
    Jun  9 10:47:41.379: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  9 10:47:41.384: INFO: Deleting all statefulset in ns statefulset-6319
    Jun  9 10:47:41.389: INFO: Scaling statefulset ss to 0
    Jun  9 10:47:41.405: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  9 10:47:41.410: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:47:41.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6319" for this suite. 06/09/23 10:47:41.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:47:41.448
Jun  9 10:47:41.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename subpath 06/09/23 10:47:41.45
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:47:41.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:47:41.475
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/09/23 10:47:41.48
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-gf79 06/09/23 10:47:41.5
STEP: Creating a pod to test atomic-volume-subpath 06/09/23 10:47:41.5
Jun  9 10:47:41.515: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-gf79" in namespace "subpath-8549" to be "Succeeded or Failed"
Jun  9 10:47:41.524: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Pending", Reason="", readiness=false. Elapsed: 8.974628ms
Jun  9 10:47:43.533: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 2.017801943s
Jun  9 10:47:45.531: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 4.015708833s
Jun  9 10:47:47.630: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 6.114542368s
Jun  9 10:47:49.532: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 8.016345557s
Jun  9 10:47:51.534: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 10.018398415s
Jun  9 10:47:53.533: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 12.01743748s
Jun  9 10:47:55.531: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 14.015669933s
Jun  9 10:47:57.534: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 16.018934293s
Jun  9 10:47:59.531: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 18.015576384s
Jun  9 10:48:01.535: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 20.019654959s
Jun  9 10:48:03.534: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=false. Elapsed: 22.018791544s
Jun  9 10:48:05.532: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016307912s
STEP: Saw pod success 06/09/23 10:48:05.532
Jun  9 10:48:05.532: INFO: Pod "pod-subpath-test-projected-gf79" satisfied condition "Succeeded or Failed"
Jun  9 10:48:05.539: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-subpath-test-projected-gf79 container test-container-subpath-projected-gf79: <nil>
STEP: delete the pod 06/09/23 10:48:05.567
Jun  9 10:48:05.648: INFO: Waiting for pod pod-subpath-test-projected-gf79 to disappear
Jun  9 10:48:05.654: INFO: Pod pod-subpath-test-projected-gf79 no longer exists
STEP: Deleting pod pod-subpath-test-projected-gf79 06/09/23 10:48:05.655
Jun  9 10:48:05.655: INFO: Deleting pod "pod-subpath-test-projected-gf79" in namespace "subpath-8549"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun  9 10:48:05.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8549" for this suite. 06/09/23 10:48:05.669
------------------------------
• [SLOW TEST] [24.246 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:47:41.448
    Jun  9 10:47:41.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename subpath 06/09/23 10:47:41.45
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:47:41.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:47:41.475
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/09/23 10:47:41.48
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-gf79 06/09/23 10:47:41.5
    STEP: Creating a pod to test atomic-volume-subpath 06/09/23 10:47:41.5
    Jun  9 10:47:41.515: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-gf79" in namespace "subpath-8549" to be "Succeeded or Failed"
    Jun  9 10:47:41.524: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Pending", Reason="", readiness=false. Elapsed: 8.974628ms
    Jun  9 10:47:43.533: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 2.017801943s
    Jun  9 10:47:45.531: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 4.015708833s
    Jun  9 10:47:47.630: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 6.114542368s
    Jun  9 10:47:49.532: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 8.016345557s
    Jun  9 10:47:51.534: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 10.018398415s
    Jun  9 10:47:53.533: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 12.01743748s
    Jun  9 10:47:55.531: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 14.015669933s
    Jun  9 10:47:57.534: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 16.018934293s
    Jun  9 10:47:59.531: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 18.015576384s
    Jun  9 10:48:01.535: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=true. Elapsed: 20.019654959s
    Jun  9 10:48:03.534: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Running", Reason="", readiness=false. Elapsed: 22.018791544s
    Jun  9 10:48:05.532: INFO: Pod "pod-subpath-test-projected-gf79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016307912s
    STEP: Saw pod success 06/09/23 10:48:05.532
    Jun  9 10:48:05.532: INFO: Pod "pod-subpath-test-projected-gf79" satisfied condition "Succeeded or Failed"
    Jun  9 10:48:05.539: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-subpath-test-projected-gf79 container test-container-subpath-projected-gf79: <nil>
    STEP: delete the pod 06/09/23 10:48:05.567
    Jun  9 10:48:05.648: INFO: Waiting for pod pod-subpath-test-projected-gf79 to disappear
    Jun  9 10:48:05.654: INFO: Pod pod-subpath-test-projected-gf79 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-gf79 06/09/23 10:48:05.655
    Jun  9 10:48:05.655: INFO: Deleting pod "pod-subpath-test-projected-gf79" in namespace "subpath-8549"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:48:05.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8549" for this suite. 06/09/23 10:48:05.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:48:05.695
Jun  9 10:48:05.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename namespaces 06/09/23 10:48:05.697
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:05.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:05.782
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 06/09/23 10:48:05.786
STEP: patching the Namespace 06/09/23 10:48:05.813
STEP: get the Namespace and ensuring it has the label 06/09/23 10:48:05.824
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:48:05.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3183" for this suite. 06/09/23 10:48:05.846
STEP: Destroying namespace "nspatchtest-6325dc02-4782-4fa5-aadb-830e248e032c-7178" for this suite. 06/09/23 10:48:05.857
------------------------------
• [0.174 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:48:05.695
    Jun  9 10:48:05.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename namespaces 06/09/23 10:48:05.697
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:05.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:05.782
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 06/09/23 10:48:05.786
    STEP: patching the Namespace 06/09/23 10:48:05.813
    STEP: get the Namespace and ensuring it has the label 06/09/23 10:48:05.824
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:48:05.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3183" for this suite. 06/09/23 10:48:05.846
    STEP: Destroying namespace "nspatchtest-6325dc02-4782-4fa5-aadb-830e248e032c-7178" for this suite. 06/09/23 10:48:05.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:48:05.87
Jun  9 10:48:05.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename svcaccounts 06/09/23 10:48:05.871
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:05.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:05.905
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  06/09/23 10:48:05.91
Jun  9 10:48:05.927: INFO: Waiting up to 5m0s for pod "test-pod-af74350d-0932-4bc8-8330-de5ad102a2de" in namespace "svcaccounts-5384" to be "Succeeded or Failed"
Jun  9 10:48:05.932: INFO: Pod "test-pod-af74350d-0932-4bc8-8330-de5ad102a2de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.630391ms
Jun  9 10:48:07.941: INFO: Pod "test-pod-af74350d-0932-4bc8-8330-de5ad102a2de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014198805s
Jun  9 10:48:09.940: INFO: Pod "test-pod-af74350d-0932-4bc8-8330-de5ad102a2de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013034217s
STEP: Saw pod success 06/09/23 10:48:09.94
Jun  9 10:48:09.940: INFO: Pod "test-pod-af74350d-0932-4bc8-8330-de5ad102a2de" satisfied condition "Succeeded or Failed"
Jun  9 10:48:09.960: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod test-pod-af74350d-0932-4bc8-8330-de5ad102a2de container agnhost-container: <nil>
STEP: delete the pod 06/09/23 10:48:09.985
Jun  9 10:48:10.009: INFO: Waiting for pod test-pod-af74350d-0932-4bc8-8330-de5ad102a2de to disappear
Jun  9 10:48:10.018: INFO: Pod test-pod-af74350d-0932-4bc8-8330-de5ad102a2de no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun  9 10:48:10.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5384" for this suite. 06/09/23 10:48:10.028
------------------------------
• [4.169 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:48:05.87
    Jun  9 10:48:05.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename svcaccounts 06/09/23 10:48:05.871
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:05.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:05.905
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  06/09/23 10:48:05.91
    Jun  9 10:48:05.927: INFO: Waiting up to 5m0s for pod "test-pod-af74350d-0932-4bc8-8330-de5ad102a2de" in namespace "svcaccounts-5384" to be "Succeeded or Failed"
    Jun  9 10:48:05.932: INFO: Pod "test-pod-af74350d-0932-4bc8-8330-de5ad102a2de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.630391ms
    Jun  9 10:48:07.941: INFO: Pod "test-pod-af74350d-0932-4bc8-8330-de5ad102a2de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014198805s
    Jun  9 10:48:09.940: INFO: Pod "test-pod-af74350d-0932-4bc8-8330-de5ad102a2de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013034217s
    STEP: Saw pod success 06/09/23 10:48:09.94
    Jun  9 10:48:09.940: INFO: Pod "test-pod-af74350d-0932-4bc8-8330-de5ad102a2de" satisfied condition "Succeeded or Failed"
    Jun  9 10:48:09.960: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod test-pod-af74350d-0932-4bc8-8330-de5ad102a2de container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 10:48:09.985
    Jun  9 10:48:10.009: INFO: Waiting for pod test-pod-af74350d-0932-4bc8-8330-de5ad102a2de to disappear
    Jun  9 10:48:10.018: INFO: Pod test-pod-af74350d-0932-4bc8-8330-de5ad102a2de no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:48:10.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5384" for this suite. 06/09/23 10:48:10.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:48:10.04
Jun  9 10:48:10.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 10:48:10.041
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:10.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:10.071
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 06/09/23 10:48:10.084
STEP: watching for the Service to be added 06/09/23 10:48:10.117
Jun  9 10:48:10.121: INFO: Found Service test-service-grlpm in namespace services-4603 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jun  9 10:48:10.121: INFO: Service test-service-grlpm created
STEP: Getting /status 06/09/23 10:48:10.121
Jun  9 10:48:10.131: INFO: Service test-service-grlpm has LoadBalancer: {[]}
STEP: patching the ServiceStatus 06/09/23 10:48:10.131
STEP: watching for the Service to be patched 06/09/23 10:48:10.148
Jun  9 10:48:10.151: INFO: observed Service test-service-grlpm in namespace services-4603 with annotations: map[] & LoadBalancer: {[]}
Jun  9 10:48:10.151: INFO: Found Service test-service-grlpm in namespace services-4603 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jun  9 10:48:10.151: INFO: Service test-service-grlpm has service status patched
STEP: updating the ServiceStatus 06/09/23 10:48:10.151
Jun  9 10:48:10.172: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 06/09/23 10:48:10.172
Jun  9 10:48:10.176: INFO: Observed Service test-service-grlpm in namespace services-4603 with annotations: map[] & Conditions: {[]}
Jun  9 10:48:10.176: INFO: Observed event: &Service{ObjectMeta:{test-service-grlpm  services-4603  38c214a3-6f63-4615-be18-fff55cd48f67 84828 0 2023-06-09 10:48:10 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-09 10:48:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-09 10:48:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.100.150.94,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.100.150.94],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jun  9 10:48:10.177: INFO: Found Service test-service-grlpm in namespace services-4603 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  9 10:48:10.177: INFO: Service test-service-grlpm has service status updated
STEP: patching the service 06/09/23 10:48:10.177
STEP: watching for the Service to be patched 06/09/23 10:48:10.204
Jun  9 10:48:10.207: INFO: observed Service test-service-grlpm in namespace services-4603 with labels: map[test-service-static:true]
Jun  9 10:48:10.207: INFO: observed Service test-service-grlpm in namespace services-4603 with labels: map[test-service-static:true]
Jun  9 10:48:10.207: INFO: observed Service test-service-grlpm in namespace services-4603 with labels: map[test-service-static:true]
Jun  9 10:48:10.207: INFO: Found Service test-service-grlpm in namespace services-4603 with labels: map[test-service:patched test-service-static:true]
Jun  9 10:48:10.207: INFO: Service test-service-grlpm patched
STEP: deleting the service 06/09/23 10:48:10.207
STEP: watching for the Service to be deleted 06/09/23 10:48:10.248
Jun  9 10:48:10.254: INFO: Observed event: ADDED
Jun  9 10:48:10.254: INFO: Observed event: MODIFIED
Jun  9 10:48:10.254: INFO: Observed event: MODIFIED
Jun  9 10:48:10.254: INFO: Observed event: MODIFIED
Jun  9 10:48:10.254: INFO: Found Service test-service-grlpm in namespace services-4603 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jun  9 10:48:10.254: INFO: Service test-service-grlpm deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 10:48:10.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4603" for this suite. 06/09/23 10:48:10.267
------------------------------
• [0.245 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:48:10.04
    Jun  9 10:48:10.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 10:48:10.041
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:10.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:10.071
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 06/09/23 10:48:10.084
    STEP: watching for the Service to be added 06/09/23 10:48:10.117
    Jun  9 10:48:10.121: INFO: Found Service test-service-grlpm in namespace services-4603 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jun  9 10:48:10.121: INFO: Service test-service-grlpm created
    STEP: Getting /status 06/09/23 10:48:10.121
    Jun  9 10:48:10.131: INFO: Service test-service-grlpm has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 06/09/23 10:48:10.131
    STEP: watching for the Service to be patched 06/09/23 10:48:10.148
    Jun  9 10:48:10.151: INFO: observed Service test-service-grlpm in namespace services-4603 with annotations: map[] & LoadBalancer: {[]}
    Jun  9 10:48:10.151: INFO: Found Service test-service-grlpm in namespace services-4603 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jun  9 10:48:10.151: INFO: Service test-service-grlpm has service status patched
    STEP: updating the ServiceStatus 06/09/23 10:48:10.151
    Jun  9 10:48:10.172: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 06/09/23 10:48:10.172
    Jun  9 10:48:10.176: INFO: Observed Service test-service-grlpm in namespace services-4603 with annotations: map[] & Conditions: {[]}
    Jun  9 10:48:10.176: INFO: Observed event: &Service{ObjectMeta:{test-service-grlpm  services-4603  38c214a3-6f63-4615-be18-fff55cd48f67 84828 0 2023-06-09 10:48:10 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-09 10:48:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-09 10:48:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.100.150.94,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.100.150.94],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jun  9 10:48:10.177: INFO: Found Service test-service-grlpm in namespace services-4603 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun  9 10:48:10.177: INFO: Service test-service-grlpm has service status updated
    STEP: patching the service 06/09/23 10:48:10.177
    STEP: watching for the Service to be patched 06/09/23 10:48:10.204
    Jun  9 10:48:10.207: INFO: observed Service test-service-grlpm in namespace services-4603 with labels: map[test-service-static:true]
    Jun  9 10:48:10.207: INFO: observed Service test-service-grlpm in namespace services-4603 with labels: map[test-service-static:true]
    Jun  9 10:48:10.207: INFO: observed Service test-service-grlpm in namespace services-4603 with labels: map[test-service-static:true]
    Jun  9 10:48:10.207: INFO: Found Service test-service-grlpm in namespace services-4603 with labels: map[test-service:patched test-service-static:true]
    Jun  9 10:48:10.207: INFO: Service test-service-grlpm patched
    STEP: deleting the service 06/09/23 10:48:10.207
    STEP: watching for the Service to be deleted 06/09/23 10:48:10.248
    Jun  9 10:48:10.254: INFO: Observed event: ADDED
    Jun  9 10:48:10.254: INFO: Observed event: MODIFIED
    Jun  9 10:48:10.254: INFO: Observed event: MODIFIED
    Jun  9 10:48:10.254: INFO: Observed event: MODIFIED
    Jun  9 10:48:10.254: INFO: Found Service test-service-grlpm in namespace services-4603 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jun  9 10:48:10.254: INFO: Service test-service-grlpm deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:48:10.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4603" for this suite. 06/09/23 10:48:10.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:48:10.284
Jun  9 10:48:10.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 10:48:10.287
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:10.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:10.325
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 06/09/23 10:48:10.331
STEP: fetching the ConfigMap 06/09/23 10:48:10.341
STEP: patching the ConfigMap 06/09/23 10:48:10.348
STEP: listing all ConfigMaps in all namespaces with a label selector 06/09/23 10:48:10.363
STEP: deleting the ConfigMap by collection with a label selector 06/09/23 10:48:10.373
STEP: listing all ConfigMaps in test namespace 06/09/23 10:48:10.396
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 10:48:10.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8231" for this suite. 06/09/23 10:48:10.412
------------------------------
• [0.143 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:48:10.284
    Jun  9 10:48:10.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 10:48:10.287
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:10.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:10.325
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 06/09/23 10:48:10.331
    STEP: fetching the ConfigMap 06/09/23 10:48:10.341
    STEP: patching the ConfigMap 06/09/23 10:48:10.348
    STEP: listing all ConfigMaps in all namespaces with a label selector 06/09/23 10:48:10.363
    STEP: deleting the ConfigMap by collection with a label selector 06/09/23 10:48:10.373
    STEP: listing all ConfigMaps in test namespace 06/09/23 10:48:10.396
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:48:10.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8231" for this suite. 06/09/23 10:48:10.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:48:10.429
Jun  9 10:48:10.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pods 06/09/23 10:48:10.43
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:10.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:10.478
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 06/09/23 10:48:10.498
STEP: watching for Pod to be ready 06/09/23 10:48:10.516
Jun  9 10:48:10.519: INFO: observed Pod pod-test in namespace pods-6080 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jun  9 10:48:10.547: INFO: observed Pod pod-test in namespace pods-6080 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC  }]
Jun  9 10:48:10.574: INFO: observed Pod pod-test in namespace pods-6080 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC  }]
Jun  9 10:48:11.147: INFO: observed Pod pod-test in namespace pods-6080 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC  }]
Jun  9 10:48:12.076: INFO: Found Pod pod-test in namespace pods-6080 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 06/09/23 10:48:12.084
STEP: getting the Pod and ensuring that it's patched 06/09/23 10:48:12.101
STEP: replacing the Pod's status Ready condition to False 06/09/23 10:48:12.109
STEP: check the Pod again to ensure its Ready conditions are False 06/09/23 10:48:12.128
STEP: deleting the Pod via a Collection with a LabelSelector 06/09/23 10:48:12.128
STEP: watching for the Pod to be deleted 06/09/23 10:48:12.144
Jun  9 10:48:12.147: INFO: observed event type MODIFIED
Jun  9 10:48:14.160: INFO: observed event type MODIFIED
Jun  9 10:48:14.448: INFO: observed event type MODIFIED
Jun  9 10:48:15.083: INFO: observed event type MODIFIED
Jun  9 10:48:15.094: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  9 10:48:15.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6080" for this suite. 06/09/23 10:48:15.116
------------------------------
• [4.698 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:48:10.429
    Jun  9 10:48:10.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pods 06/09/23 10:48:10.43
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:10.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:10.478
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 06/09/23 10:48:10.498
    STEP: watching for Pod to be ready 06/09/23 10:48:10.516
    Jun  9 10:48:10.519: INFO: observed Pod pod-test in namespace pods-6080 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jun  9 10:48:10.547: INFO: observed Pod pod-test in namespace pods-6080 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC  }]
    Jun  9 10:48:10.574: INFO: observed Pod pod-test in namespace pods-6080 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC  }]
    Jun  9 10:48:11.147: INFO: observed Pod pod-test in namespace pods-6080 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC  }]
    Jun  9 10:48:12.076: INFO: Found Pod pod-test in namespace pods-6080 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:48:10 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 06/09/23 10:48:12.084
    STEP: getting the Pod and ensuring that it's patched 06/09/23 10:48:12.101
    STEP: replacing the Pod's status Ready condition to False 06/09/23 10:48:12.109
    STEP: check the Pod again to ensure its Ready conditions are False 06/09/23 10:48:12.128
    STEP: deleting the Pod via a Collection with a LabelSelector 06/09/23 10:48:12.128
    STEP: watching for the Pod to be deleted 06/09/23 10:48:12.144
    Jun  9 10:48:12.147: INFO: observed event type MODIFIED
    Jun  9 10:48:14.160: INFO: observed event type MODIFIED
    Jun  9 10:48:14.448: INFO: observed event type MODIFIED
    Jun  9 10:48:15.083: INFO: observed event type MODIFIED
    Jun  9 10:48:15.094: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:48:15.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6080" for this suite. 06/09/23 10:48:15.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:48:15.129
Jun  9 10:48:15.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 10:48:15.13
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:15.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:15.161
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-1926 06/09/23 10:48:15.167
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1926 to expose endpoints map[] 06/09/23 10:48:15.219
Jun  9 10:48:15.238: INFO: successfully validated that service endpoint-test2 in namespace services-1926 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1926 06/09/23 10:48:15.238
Jun  9 10:48:15.263: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1926" to be "running and ready"
Jun  9 10:48:15.270: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.019443ms
Jun  9 10:48:15.270: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:48:17.276: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012723676s
Jun  9 10:48:17.276: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun  9 10:48:17.276: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1926 to expose endpoints map[pod1:[80]] 06/09/23 10:48:17.281
Jun  9 10:48:17.299: INFO: successfully validated that service endpoint-test2 in namespace services-1926 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 06/09/23 10:48:17.299
Jun  9 10:48:17.299: INFO: Creating new exec pod
Jun  9 10:48:17.307: INFO: Waiting up to 5m0s for pod "execpod7sbns" in namespace "services-1926" to be "running"
Jun  9 10:48:17.315: INFO: Pod "execpod7sbns": Phase="Pending", Reason="", readiness=false. Elapsed: 7.987394ms
Jun  9 10:48:19.324: INFO: Pod "execpod7sbns": Phase="Running", Reason="", readiness=true. Elapsed: 2.016592886s
Jun  9 10:48:19.324: INFO: Pod "execpod7sbns" satisfied condition "running"
Jun  9 10:48:20.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-1926 exec execpod7sbns -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jun  9 10:48:20.495: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun  9 10:48:20.495: INFO: stdout: ""
Jun  9 10:48:20.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-1926 exec execpod7sbns -- /bin/sh -x -c nc -v -z -w 2 10.108.35.79 80'
Jun  9 10:48:20.652: INFO: stderr: "+ nc -v -z -w 2 10.108.35.79 80\nConnection to 10.108.35.79 80 port [tcp/http] succeeded!\n"
Jun  9 10:48:20.652: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-1926 06/09/23 10:48:20.652
Jun  9 10:48:20.666: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1926" to be "running and ready"
Jun  9 10:48:20.677: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.285849ms
Jun  9 10:48:20.677: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:48:22.684: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018506632s
Jun  9 10:48:22.684: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun  9 10:48:22.684: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1926 to expose endpoints map[pod1:[80] pod2:[80]] 06/09/23 10:48:22.69
Jun  9 10:48:22.719: INFO: successfully validated that service endpoint-test2 in namespace services-1926 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 06/09/23 10:48:22.719
Jun  9 10:48:23.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-1926 exec execpod7sbns -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jun  9 10:48:23.890: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun  9 10:48:23.890: INFO: stdout: ""
Jun  9 10:48:23.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-1926 exec execpod7sbns -- /bin/sh -x -c nc -v -z -w 2 10.108.35.79 80'
Jun  9 10:48:24.048: INFO: stderr: "+ nc -v -z -w 2 10.108.35.79 80\nConnection to 10.108.35.79 80 port [tcp/http] succeeded!\n"
Jun  9 10:48:24.048: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-1926 06/09/23 10:48:24.048
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1926 to expose endpoints map[pod2:[80]] 06/09/23 10:48:24.074
Jun  9 10:48:24.106: INFO: successfully validated that service endpoint-test2 in namespace services-1926 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 06/09/23 10:48:24.106
Jun  9 10:48:25.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-1926 exec execpod7sbns -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jun  9 10:48:25.279: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun  9 10:48:25.279: INFO: stdout: ""
Jun  9 10:48:25.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-1926 exec execpod7sbns -- /bin/sh -x -c nc -v -z -w 2 10.108.35.79 80'
Jun  9 10:48:25.453: INFO: stderr: "+ nc -v -z -w 2 10.108.35.79 80\nConnection to 10.108.35.79 80 port [tcp/http] succeeded!\n"
Jun  9 10:48:25.453: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-1926 06/09/23 10:48:25.453
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1926 to expose endpoints map[] 06/09/23 10:48:25.473
Jun  9 10:48:25.491: INFO: successfully validated that service endpoint-test2 in namespace services-1926 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 10:48:25.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1926" for this suite. 06/09/23 10:48:25.557
------------------------------
• [SLOW TEST] [10.440 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:48:15.129
    Jun  9 10:48:15.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 10:48:15.13
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:15.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:15.161
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-1926 06/09/23 10:48:15.167
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1926 to expose endpoints map[] 06/09/23 10:48:15.219
    Jun  9 10:48:15.238: INFO: successfully validated that service endpoint-test2 in namespace services-1926 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1926 06/09/23 10:48:15.238
    Jun  9 10:48:15.263: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1926" to be "running and ready"
    Jun  9 10:48:15.270: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.019443ms
    Jun  9 10:48:15.270: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:48:17.276: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012723676s
    Jun  9 10:48:17.276: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun  9 10:48:17.276: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1926 to expose endpoints map[pod1:[80]] 06/09/23 10:48:17.281
    Jun  9 10:48:17.299: INFO: successfully validated that service endpoint-test2 in namespace services-1926 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 06/09/23 10:48:17.299
    Jun  9 10:48:17.299: INFO: Creating new exec pod
    Jun  9 10:48:17.307: INFO: Waiting up to 5m0s for pod "execpod7sbns" in namespace "services-1926" to be "running"
    Jun  9 10:48:17.315: INFO: Pod "execpod7sbns": Phase="Pending", Reason="", readiness=false. Elapsed: 7.987394ms
    Jun  9 10:48:19.324: INFO: Pod "execpod7sbns": Phase="Running", Reason="", readiness=true. Elapsed: 2.016592886s
    Jun  9 10:48:19.324: INFO: Pod "execpod7sbns" satisfied condition "running"
    Jun  9 10:48:20.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-1926 exec execpod7sbns -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jun  9 10:48:20.495: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun  9 10:48:20.495: INFO: stdout: ""
    Jun  9 10:48:20.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-1926 exec execpod7sbns -- /bin/sh -x -c nc -v -z -w 2 10.108.35.79 80'
    Jun  9 10:48:20.652: INFO: stderr: "+ nc -v -z -w 2 10.108.35.79 80\nConnection to 10.108.35.79 80 port [tcp/http] succeeded!\n"
    Jun  9 10:48:20.652: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-1926 06/09/23 10:48:20.652
    Jun  9 10:48:20.666: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1926" to be "running and ready"
    Jun  9 10:48:20.677: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.285849ms
    Jun  9 10:48:20.677: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:48:22.684: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018506632s
    Jun  9 10:48:22.684: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun  9 10:48:22.684: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1926 to expose endpoints map[pod1:[80] pod2:[80]] 06/09/23 10:48:22.69
    Jun  9 10:48:22.719: INFO: successfully validated that service endpoint-test2 in namespace services-1926 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 06/09/23 10:48:22.719
    Jun  9 10:48:23.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-1926 exec execpod7sbns -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jun  9 10:48:23.890: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun  9 10:48:23.890: INFO: stdout: ""
    Jun  9 10:48:23.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-1926 exec execpod7sbns -- /bin/sh -x -c nc -v -z -w 2 10.108.35.79 80'
    Jun  9 10:48:24.048: INFO: stderr: "+ nc -v -z -w 2 10.108.35.79 80\nConnection to 10.108.35.79 80 port [tcp/http] succeeded!\n"
    Jun  9 10:48:24.048: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-1926 06/09/23 10:48:24.048
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1926 to expose endpoints map[pod2:[80]] 06/09/23 10:48:24.074
    Jun  9 10:48:24.106: INFO: successfully validated that service endpoint-test2 in namespace services-1926 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 06/09/23 10:48:24.106
    Jun  9 10:48:25.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-1926 exec execpod7sbns -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jun  9 10:48:25.279: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun  9 10:48:25.279: INFO: stdout: ""
    Jun  9 10:48:25.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-1926 exec execpod7sbns -- /bin/sh -x -c nc -v -z -w 2 10.108.35.79 80'
    Jun  9 10:48:25.453: INFO: stderr: "+ nc -v -z -w 2 10.108.35.79 80\nConnection to 10.108.35.79 80 port [tcp/http] succeeded!\n"
    Jun  9 10:48:25.453: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-1926 06/09/23 10:48:25.453
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1926 to expose endpoints map[] 06/09/23 10:48:25.473
    Jun  9 10:48:25.491: INFO: successfully validated that service endpoint-test2 in namespace services-1926 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:48:25.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1926" for this suite. 06/09/23 10:48:25.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:48:25.569
Jun  9 10:48:25.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:48:25.571
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:25.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:25.63
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-053e1456-5651-48d9-965b-3d8cbeb8b540 06/09/23 10:48:25.637
STEP: Creating a pod to test consume configMaps 06/09/23 10:48:25.647
Jun  9 10:48:25.664: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74" in namespace "projected-6733" to be "Succeeded or Failed"
Jun  9 10:48:25.674: INFO: Pod "pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74": Phase="Pending", Reason="", readiness=false. Elapsed: 10.479391ms
Jun  9 10:48:27.682: INFO: Pod "pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018067731s
Jun  9 10:48:29.682: INFO: Pod "pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018193919s
STEP: Saw pod success 06/09/23 10:48:29.682
Jun  9 10:48:29.682: INFO: Pod "pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74" satisfied condition "Succeeded or Failed"
Jun  9 10:48:29.689: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74 container agnhost-container: <nil>
STEP: delete the pod 06/09/23 10:48:29.712
Jun  9 10:48:29.733: INFO: Waiting for pod pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74 to disappear
Jun  9 10:48:29.740: INFO: Pod pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  9 10:48:29.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6733" for this suite. 06/09/23 10:48:29.75
------------------------------
• [4.193 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:48:25.569
    Jun  9 10:48:25.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:48:25.571
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:25.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:25.63
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-053e1456-5651-48d9-965b-3d8cbeb8b540 06/09/23 10:48:25.637
    STEP: Creating a pod to test consume configMaps 06/09/23 10:48:25.647
    Jun  9 10:48:25.664: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74" in namespace "projected-6733" to be "Succeeded or Failed"
    Jun  9 10:48:25.674: INFO: Pod "pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74": Phase="Pending", Reason="", readiness=false. Elapsed: 10.479391ms
    Jun  9 10:48:27.682: INFO: Pod "pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018067731s
    Jun  9 10:48:29.682: INFO: Pod "pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018193919s
    STEP: Saw pod success 06/09/23 10:48:29.682
    Jun  9 10:48:29.682: INFO: Pod "pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74" satisfied condition "Succeeded or Failed"
    Jun  9 10:48:29.689: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74 container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 10:48:29.712
    Jun  9 10:48:29.733: INFO: Waiting for pod pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74 to disappear
    Jun  9 10:48:29.740: INFO: Pod pod-projected-configmaps-de46dc36-c963-465b-a378-019c6bccfd74 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:48:29.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6733" for this suite. 06/09/23 10:48:29.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:48:29.765
Jun  9 10:48:29.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename watch 06/09/23 10:48:29.766
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:29.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:29.797
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 06/09/23 10:48:29.802
STEP: modifying the configmap once 06/09/23 10:48:29.81
STEP: modifying the configmap a second time 06/09/23 10:48:29.825
STEP: deleting the configmap 06/09/23 10:48:29.838
STEP: creating a watch on configmaps from the resource version returned by the first update 06/09/23 10:48:29.847
STEP: Expecting to observe notifications for all changes to the configmap after the first update 06/09/23 10:48:29.85
Jun  9 10:48:29.850: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4704  af01713a-8acf-4a09-b956-6fff50c0dba3 85093 0 2023-06-09 10:48:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-09 10:48:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  9 10:48:29.851: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4704  af01713a-8acf-4a09-b956-6fff50c0dba3 85094 0 2023-06-09 10:48:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-09 10:48:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun  9 10:48:29.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4704" for this suite. 06/09/23 10:48:29.858
------------------------------
• [0.113 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:48:29.765
    Jun  9 10:48:29.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename watch 06/09/23 10:48:29.766
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:29.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:29.797
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 06/09/23 10:48:29.802
    STEP: modifying the configmap once 06/09/23 10:48:29.81
    STEP: modifying the configmap a second time 06/09/23 10:48:29.825
    STEP: deleting the configmap 06/09/23 10:48:29.838
    STEP: creating a watch on configmaps from the resource version returned by the first update 06/09/23 10:48:29.847
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 06/09/23 10:48:29.85
    Jun  9 10:48:29.850: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4704  af01713a-8acf-4a09-b956-6fff50c0dba3 85093 0 2023-06-09 10:48:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-09 10:48:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  9 10:48:29.851: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4704  af01713a-8acf-4a09-b956-6fff50c0dba3 85094 0 2023-06-09 10:48:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-09 10:48:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:48:29.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4704" for this suite. 06/09/23 10:48:29.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:48:29.88
Jun  9 10:48:29.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 10:48:29.882
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:29.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:29.913
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 06/09/23 10:48:29.918
Jun  9 10:48:29.933: INFO: Waiting up to 5m0s for pod "pod-138e726b-70ec-492a-8077-fa47e9dcf8a5" in namespace "emptydir-8025" to be "Succeeded or Failed"
Jun  9 10:48:29.941: INFO: Pod "pod-138e726b-70ec-492a-8077-fa47e9dcf8a5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.109758ms
Jun  9 10:48:31.948: INFO: Pod "pod-138e726b-70ec-492a-8077-fa47e9dcf8a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014937934s
Jun  9 10:48:33.948: INFO: Pod "pod-138e726b-70ec-492a-8077-fa47e9dcf8a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014665212s
STEP: Saw pod success 06/09/23 10:48:33.948
Jun  9 10:48:33.948: INFO: Pod "pod-138e726b-70ec-492a-8077-fa47e9dcf8a5" satisfied condition "Succeeded or Failed"
Jun  9 10:48:33.953: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-138e726b-70ec-492a-8077-fa47e9dcf8a5 container test-container: <nil>
STEP: delete the pod 06/09/23 10:48:33.963
Jun  9 10:48:33.978: INFO: Waiting for pod pod-138e726b-70ec-492a-8077-fa47e9dcf8a5 to disappear
Jun  9 10:48:33.983: INFO: Pod pod-138e726b-70ec-492a-8077-fa47e9dcf8a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 10:48:33.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8025" for this suite. 06/09/23 10:48:33.992
------------------------------
• [4.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:48:29.88
    Jun  9 10:48:29.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 10:48:29.882
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:29.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:29.913
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 06/09/23 10:48:29.918
    Jun  9 10:48:29.933: INFO: Waiting up to 5m0s for pod "pod-138e726b-70ec-492a-8077-fa47e9dcf8a5" in namespace "emptydir-8025" to be "Succeeded or Failed"
    Jun  9 10:48:29.941: INFO: Pod "pod-138e726b-70ec-492a-8077-fa47e9dcf8a5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.109758ms
    Jun  9 10:48:31.948: INFO: Pod "pod-138e726b-70ec-492a-8077-fa47e9dcf8a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014937934s
    Jun  9 10:48:33.948: INFO: Pod "pod-138e726b-70ec-492a-8077-fa47e9dcf8a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014665212s
    STEP: Saw pod success 06/09/23 10:48:33.948
    Jun  9 10:48:33.948: INFO: Pod "pod-138e726b-70ec-492a-8077-fa47e9dcf8a5" satisfied condition "Succeeded or Failed"
    Jun  9 10:48:33.953: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-138e726b-70ec-492a-8077-fa47e9dcf8a5 container test-container: <nil>
    STEP: delete the pod 06/09/23 10:48:33.963
    Jun  9 10:48:33.978: INFO: Waiting for pod pod-138e726b-70ec-492a-8077-fa47e9dcf8a5 to disappear
    Jun  9 10:48:33.983: INFO: Pod pod-138e726b-70ec-492a-8077-fa47e9dcf8a5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:48:33.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8025" for this suite. 06/09/23 10:48:33.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:48:34.002
Jun  9 10:48:34.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename endpointslice 06/09/23 10:48:34.003
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:34.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:34.03
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jun  9 10:48:34.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8896" for this suite. 06/09/23 10:48:34.132
------------------------------
• [0.141 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:48:34.002
    Jun  9 10:48:34.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename endpointslice 06/09/23 10:48:34.003
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:34.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:34.03
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:48:34.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8896" for this suite. 06/09/23 10:48:34.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:48:34.144
Jun  9 10:48:34.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename subpath 06/09/23 10:48:34.145
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:34.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:34.174
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/09/23 10:48:34.18
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-nhph 06/09/23 10:48:34.195
STEP: Creating a pod to test atomic-volume-subpath 06/09/23 10:48:34.195
Jun  9 10:48:34.220: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nhph" in namespace "subpath-9066" to be "Succeeded or Failed"
Jun  9 10:48:34.226: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Pending", Reason="", readiness=false. Elapsed: 6.247359ms
Jun  9 10:48:36.236: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 2.016024625s
Jun  9 10:48:38.233: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 4.012550162s
Jun  9 10:48:40.234: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 6.013521529s
Jun  9 10:48:42.235: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 8.014412534s
Jun  9 10:48:44.233: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 10.013000792s
Jun  9 10:48:46.234: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 12.014160675s
Jun  9 10:48:48.234: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 14.013651981s
Jun  9 10:48:50.234: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 16.013341302s
Jun  9 10:48:52.244: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 18.023436566s
Jun  9 10:48:54.235: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 20.014384941s
Jun  9 10:48:56.235: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=false. Elapsed: 22.014442586s
Jun  9 10:48:58.236: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015399985s
STEP: Saw pod success 06/09/23 10:48:58.236
Jun  9 10:48:58.236: INFO: Pod "pod-subpath-test-configmap-nhph" satisfied condition "Succeeded or Failed"
Jun  9 10:48:58.245: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-subpath-test-configmap-nhph container test-container-subpath-configmap-nhph: <nil>
STEP: delete the pod 06/09/23 10:48:58.26
Jun  9 10:48:58.281: INFO: Waiting for pod pod-subpath-test-configmap-nhph to disappear
Jun  9 10:48:58.286: INFO: Pod pod-subpath-test-configmap-nhph no longer exists
STEP: Deleting pod pod-subpath-test-configmap-nhph 06/09/23 10:48:58.286
Jun  9 10:48:58.286: INFO: Deleting pod "pod-subpath-test-configmap-nhph" in namespace "subpath-9066"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun  9 10:48:58.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9066" for this suite. 06/09/23 10:48:58.301
------------------------------
• [SLOW TEST] [24.167 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:48:34.144
    Jun  9 10:48:34.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename subpath 06/09/23 10:48:34.145
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:34.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:34.174
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/09/23 10:48:34.18
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-nhph 06/09/23 10:48:34.195
    STEP: Creating a pod to test atomic-volume-subpath 06/09/23 10:48:34.195
    Jun  9 10:48:34.220: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nhph" in namespace "subpath-9066" to be "Succeeded or Failed"
    Jun  9 10:48:34.226: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Pending", Reason="", readiness=false. Elapsed: 6.247359ms
    Jun  9 10:48:36.236: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 2.016024625s
    Jun  9 10:48:38.233: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 4.012550162s
    Jun  9 10:48:40.234: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 6.013521529s
    Jun  9 10:48:42.235: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 8.014412534s
    Jun  9 10:48:44.233: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 10.013000792s
    Jun  9 10:48:46.234: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 12.014160675s
    Jun  9 10:48:48.234: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 14.013651981s
    Jun  9 10:48:50.234: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 16.013341302s
    Jun  9 10:48:52.244: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 18.023436566s
    Jun  9 10:48:54.235: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=true. Elapsed: 20.014384941s
    Jun  9 10:48:56.235: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Running", Reason="", readiness=false. Elapsed: 22.014442586s
    Jun  9 10:48:58.236: INFO: Pod "pod-subpath-test-configmap-nhph": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015399985s
    STEP: Saw pod success 06/09/23 10:48:58.236
    Jun  9 10:48:58.236: INFO: Pod "pod-subpath-test-configmap-nhph" satisfied condition "Succeeded or Failed"
    Jun  9 10:48:58.245: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-subpath-test-configmap-nhph container test-container-subpath-configmap-nhph: <nil>
    STEP: delete the pod 06/09/23 10:48:58.26
    Jun  9 10:48:58.281: INFO: Waiting for pod pod-subpath-test-configmap-nhph to disappear
    Jun  9 10:48:58.286: INFO: Pod pod-subpath-test-configmap-nhph no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-nhph 06/09/23 10:48:58.286
    Jun  9 10:48:58.286: INFO: Deleting pod "pod-subpath-test-configmap-nhph" in namespace "subpath-9066"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:48:58.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9066" for this suite. 06/09/23 10:48:58.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:48:58.312
Jun  9 10:48:58.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename gc 06/09/23 10:48:58.314
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:58.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:58.342
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 06/09/23 10:48:58.354
STEP: delete the rc 06/09/23 10:49:03.607
STEP: wait for the rc to be deleted 06/09/23 10:49:03.673
Jun  9 10:49:04.876: INFO: 80 pods remaining
Jun  9 10:49:04.876: INFO: 80 pods has nil DeletionTimestamp
Jun  9 10:49:04.876: INFO: 
Jun  9 10:49:05.699: INFO: 71 pods remaining
Jun  9 10:49:05.699: INFO: 70 pods has nil DeletionTimestamp
Jun  9 10:49:05.699: INFO: 
Jun  9 10:49:06.926: INFO: 59 pods remaining
Jun  9 10:49:06.926: INFO: 59 pods has nil DeletionTimestamp
Jun  9 10:49:06.926: INFO: 
Jun  9 10:49:08.138: INFO: 40 pods remaining
Jun  9 10:49:08.138: INFO: 40 pods has nil DeletionTimestamp
Jun  9 10:49:08.138: INFO: 
Jun  9 10:49:08.967: INFO: 25 pods remaining
Jun  9 10:49:08.967: INFO: 24 pods has nil DeletionTimestamp
Jun  9 10:49:08.967: INFO: 
Jun  9 10:49:09.975: INFO: 19 pods remaining
Jun  9 10:49:09.975: INFO: 19 pods has nil DeletionTimestamp
Jun  9 10:49:09.975: INFO: 
STEP: Gathering metrics 06/09/23 10:49:10.851
Jun  9 10:49:10.962: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" in namespace "kube-system" to be "running and ready"
Jun  9 10:49:10.984: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh": Phase="Running", Reason="", readiness=true. Elapsed: 21.946648ms
Jun  9 10:49:10.985: INFO: The phase of Pod kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh is Running (Ready = true)
Jun  9 10:49:10.985: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" satisfied condition "running and ready"
Jun  9 10:49:11.149: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun  9 10:49:11.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8264" for this suite. 06/09/23 10:49:11.388
------------------------------
• [SLOW TEST] [13.118 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:48:58.312
    Jun  9 10:48:58.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename gc 06/09/23 10:48:58.314
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:48:58.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:48:58.342
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 06/09/23 10:48:58.354
    STEP: delete the rc 06/09/23 10:49:03.607
    STEP: wait for the rc to be deleted 06/09/23 10:49:03.673
    Jun  9 10:49:04.876: INFO: 80 pods remaining
    Jun  9 10:49:04.876: INFO: 80 pods has nil DeletionTimestamp
    Jun  9 10:49:04.876: INFO: 
    Jun  9 10:49:05.699: INFO: 71 pods remaining
    Jun  9 10:49:05.699: INFO: 70 pods has nil DeletionTimestamp
    Jun  9 10:49:05.699: INFO: 
    Jun  9 10:49:06.926: INFO: 59 pods remaining
    Jun  9 10:49:06.926: INFO: 59 pods has nil DeletionTimestamp
    Jun  9 10:49:06.926: INFO: 
    Jun  9 10:49:08.138: INFO: 40 pods remaining
    Jun  9 10:49:08.138: INFO: 40 pods has nil DeletionTimestamp
    Jun  9 10:49:08.138: INFO: 
    Jun  9 10:49:08.967: INFO: 25 pods remaining
    Jun  9 10:49:08.967: INFO: 24 pods has nil DeletionTimestamp
    Jun  9 10:49:08.967: INFO: 
    Jun  9 10:49:09.975: INFO: 19 pods remaining
    Jun  9 10:49:09.975: INFO: 19 pods has nil DeletionTimestamp
    Jun  9 10:49:09.975: INFO: 
    STEP: Gathering metrics 06/09/23 10:49:10.851
    Jun  9 10:49:10.962: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" in namespace "kube-system" to be "running and ready"
    Jun  9 10:49:10.984: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh": Phase="Running", Reason="", readiness=true. Elapsed: 21.946648ms
    Jun  9 10:49:10.985: INFO: The phase of Pod kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh is Running (Ready = true)
    Jun  9 10:49:10.985: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" satisfied condition "running and ready"
    Jun  9 10:49:11.149: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:49:11.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8264" for this suite. 06/09/23 10:49:11.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:49:11.546
Jun  9 10:49:11.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename watch 06/09/23 10:49:11.548
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:11.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:11.611
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 06/09/23 10:49:11.616
STEP: starting a background goroutine to produce watch events 06/09/23 10:49:11.625
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 06/09/23 10:49:11.625
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun  9 10:49:15.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6139" for this suite. 06/09/23 10:49:15.772
------------------------------
• [4.237 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:49:11.546
    Jun  9 10:49:11.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename watch 06/09/23 10:49:11.548
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:11.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:11.611
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 06/09/23 10:49:11.616
    STEP: starting a background goroutine to produce watch events 06/09/23 10:49:11.625
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 06/09/23 10:49:11.625
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:49:15.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6139" for this suite. 06/09/23 10:49:15.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:49:15.784
Jun  9 10:49:15.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pod-network-test 06/09/23 10:49:15.785
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:15.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:15.82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-4826 06/09/23 10:49:15.828
STEP: creating a selector 06/09/23 10:49:15.828
STEP: Creating the service pods in kubernetes 06/09/23 10:49:15.828
Jun  9 10:49:15.828: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  9 10:49:15.890: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4826" to be "running and ready"
Jun  9 10:49:15.912: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.106027ms
Jun  9 10:49:15.912: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:49:18.148: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.258220587s
Jun  9 10:49:18.148: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:49:19.919: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029152266s
Jun  9 10:49:19.919: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:49:21.920: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.029929882s
Jun  9 10:49:21.920: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:49:23.921: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.030791811s
Jun  9 10:49:23.921: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:49:25.919: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.028781166s
Jun  9 10:49:25.919: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:49:27.919: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.028499586s
Jun  9 10:49:27.919: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:49:29.920: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.029892713s
Jun  9 10:49:29.920: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:49:31.927: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.037244049s
Jun  9 10:49:31.927: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:49:33.922: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.032196728s
Jun  9 10:49:33.922: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:49:35.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.043021335s
Jun  9 10:49:35.936: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:49:37.921: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.031055829s
Jun  9 10:49:37.921: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun  9 10:49:37.921: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun  9 10:49:37.927: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4826" to be "running and ready"
Jun  9 10:49:37.937: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 9.866407ms
Jun  9 10:49:37.937: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun  9 10:49:37.937: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun  9 10:49:37.943: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4826" to be "running and ready"
Jun  9 10:49:37.948: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.191662ms
Jun  9 10:49:37.948: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun  9 10:49:37.948: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/09/23 10:49:37.954
Jun  9 10:49:37.965: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4826" to be "running"
Jun  9 10:49:37.975: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.710191ms
Jun  9 10:49:39.988: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.022657572s
Jun  9 10:49:39.988: INFO: Pod "test-container-pod" satisfied condition "running"
Jun  9 10:49:39.993: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun  9 10:49:39.993: INFO: Breadth first check of 172.26.90.14 on host 10.255.64.104...
Jun  9 10:49:39.999: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.27.53.73:9080/dial?request=hostname&protocol=udp&host=172.26.90.14&port=8081&tries=1'] Namespace:pod-network-test-4826 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:49:39.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:49:40.000: INFO: ExecWithOptions: Clientset creation
Jun  9 10:49:40.000: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4826/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.27.53.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.26.90.14%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  9 10:49:40.150: INFO: Waiting for responses: map[]
Jun  9 10:49:40.150: INFO: reached 172.26.90.14 after 0/1 tries
Jun  9 10:49:40.150: INFO: Breadth first check of 172.30.17.136 on host 10.255.64.102...
Jun  9 10:49:40.160: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.27.53.73:9080/dial?request=hostname&protocol=udp&host=172.30.17.136&port=8081&tries=1'] Namespace:pod-network-test-4826 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:49:40.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:49:40.161: INFO: ExecWithOptions: Clientset creation
Jun  9 10:49:40.161: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4826/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.27.53.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.17.136%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  9 10:49:40.281: INFO: Waiting for responses: map[]
Jun  9 10:49:40.281: INFO: reached 172.30.17.136 after 0/1 tries
Jun  9 10:49:40.281: INFO: Breadth first check of 172.27.53.74 on host 10.255.64.103...
Jun  9 10:49:40.287: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.27.53.73:9080/dial?request=hostname&protocol=udp&host=172.27.53.74&port=8081&tries=1'] Namespace:pod-network-test-4826 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:49:40.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:49:40.289: INFO: ExecWithOptions: Clientset creation
Jun  9 10:49:40.289: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4826/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.27.53.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.27.53.74%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun  9 10:49:40.417: INFO: Waiting for responses: map[]
Jun  9 10:49:40.417: INFO: reached 172.27.53.74 after 0/1 tries
Jun  9 10:49:40.417: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jun  9 10:49:40.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4826" for this suite. 06/09/23 10:49:40.426
------------------------------
• [SLOW TEST] [24.656 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:49:15.784
    Jun  9 10:49:15.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pod-network-test 06/09/23 10:49:15.785
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:15.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:15.82
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-4826 06/09/23 10:49:15.828
    STEP: creating a selector 06/09/23 10:49:15.828
    STEP: Creating the service pods in kubernetes 06/09/23 10:49:15.828
    Jun  9 10:49:15.828: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun  9 10:49:15.890: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4826" to be "running and ready"
    Jun  9 10:49:15.912: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.106027ms
    Jun  9 10:49:15.912: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:49:18.148: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.258220587s
    Jun  9 10:49:18.148: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:49:19.919: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029152266s
    Jun  9 10:49:19.919: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:49:21.920: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.029929882s
    Jun  9 10:49:21.920: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:49:23.921: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.030791811s
    Jun  9 10:49:23.921: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:49:25.919: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.028781166s
    Jun  9 10:49:25.919: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:49:27.919: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.028499586s
    Jun  9 10:49:27.919: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:49:29.920: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.029892713s
    Jun  9 10:49:29.920: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:49:31.927: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.037244049s
    Jun  9 10:49:31.927: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:49:33.922: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.032196728s
    Jun  9 10:49:33.922: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:49:35.933: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.043021335s
    Jun  9 10:49:35.936: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:49:37.921: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.031055829s
    Jun  9 10:49:37.921: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun  9 10:49:37.921: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun  9 10:49:37.927: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4826" to be "running and ready"
    Jun  9 10:49:37.937: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 9.866407ms
    Jun  9 10:49:37.937: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun  9 10:49:37.937: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun  9 10:49:37.943: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4826" to be "running and ready"
    Jun  9 10:49:37.948: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.191662ms
    Jun  9 10:49:37.948: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun  9 10:49:37.948: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/09/23 10:49:37.954
    Jun  9 10:49:37.965: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4826" to be "running"
    Jun  9 10:49:37.975: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.710191ms
    Jun  9 10:49:39.988: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.022657572s
    Jun  9 10:49:39.988: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun  9 10:49:39.993: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun  9 10:49:39.993: INFO: Breadth first check of 172.26.90.14 on host 10.255.64.104...
    Jun  9 10:49:39.999: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.27.53.73:9080/dial?request=hostname&protocol=udp&host=172.26.90.14&port=8081&tries=1'] Namespace:pod-network-test-4826 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:49:39.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:49:40.000: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:49:40.000: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4826/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.27.53.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.26.90.14%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  9 10:49:40.150: INFO: Waiting for responses: map[]
    Jun  9 10:49:40.150: INFO: reached 172.26.90.14 after 0/1 tries
    Jun  9 10:49:40.150: INFO: Breadth first check of 172.30.17.136 on host 10.255.64.102...
    Jun  9 10:49:40.160: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.27.53.73:9080/dial?request=hostname&protocol=udp&host=172.30.17.136&port=8081&tries=1'] Namespace:pod-network-test-4826 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:49:40.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:49:40.161: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:49:40.161: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4826/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.27.53.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.17.136%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  9 10:49:40.281: INFO: Waiting for responses: map[]
    Jun  9 10:49:40.281: INFO: reached 172.30.17.136 after 0/1 tries
    Jun  9 10:49:40.281: INFO: Breadth first check of 172.27.53.74 on host 10.255.64.103...
    Jun  9 10:49:40.287: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.27.53.73:9080/dial?request=hostname&protocol=udp&host=172.27.53.74&port=8081&tries=1'] Namespace:pod-network-test-4826 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:49:40.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:49:40.289: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:49:40.289: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4826/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.27.53.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.27.53.74%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun  9 10:49:40.417: INFO: Waiting for responses: map[]
    Jun  9 10:49:40.417: INFO: reached 172.27.53.74 after 0/1 tries
    Jun  9 10:49:40.417: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:49:40.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4826" for this suite. 06/09/23 10:49:40.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:49:40.442
Jun  9 10:49:40.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 06/09/23 10:49:40.446
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:40.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:40.481
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 06/09/23 10:49:40.49
STEP: Creating hostNetwork=false pod 06/09/23 10:49:40.49
Jun  9 10:49:40.509: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-582" to be "running and ready"
Jun  9 10:49:40.515: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.194234ms
Jun  9 10:49:40.515: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:49:42.530: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020935561s
Jun  9 10:49:42.530: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:49:44.524: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.015313707s
Jun  9 10:49:44.524: INFO: The phase of Pod test-pod is Running (Ready = true)
Jun  9 10:49:44.524: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 06/09/23 10:49:44.534
Jun  9 10:49:44.545: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-582" to be "running and ready"
Jun  9 10:49:44.565: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.740481ms
Jun  9 10:49:44.565: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:49:46.581: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.035530639s
Jun  9 10:49:46.581: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jun  9 10:49:46.581: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 06/09/23 10:49:46.587
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 06/09/23 10:49:46.587
Jun  9 10:49:46.587: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:49:46.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:49:46.594: INFO: ExecWithOptions: Clientset creation
Jun  9 10:49:46.594: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun  9 10:49:46.788: INFO: Exec stderr: ""
Jun  9 10:49:46.788: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:49:46.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:49:46.789: INFO: ExecWithOptions: Clientset creation
Jun  9 10:49:46.789: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun  9 10:49:46.908: INFO: Exec stderr: ""
Jun  9 10:49:46.908: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:49:46.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:49:46.909: INFO: ExecWithOptions: Clientset creation
Jun  9 10:49:46.909: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun  9 10:49:46.999: INFO: Exec stderr: ""
Jun  9 10:49:46.999: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:49:46.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:49:47.000: INFO: ExecWithOptions: Clientset creation
Jun  9 10:49:47.000: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun  9 10:49:47.080: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 06/09/23 10:49:47.08
Jun  9 10:49:47.080: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:49:47.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:49:47.080: INFO: ExecWithOptions: Clientset creation
Jun  9 10:49:47.081: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jun  9 10:49:47.161: INFO: Exec stderr: ""
Jun  9 10:49:47.161: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:49:47.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:49:47.162: INFO: ExecWithOptions: Clientset creation
Jun  9 10:49:47.162: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jun  9 10:49:47.244: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 06/09/23 10:49:47.244
Jun  9 10:49:47.244: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:49:47.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:49:47.245: INFO: ExecWithOptions: Clientset creation
Jun  9 10:49:47.246: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun  9 10:49:47.345: INFO: Exec stderr: ""
Jun  9 10:49:47.345: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:49:47.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:49:47.346: INFO: ExecWithOptions: Clientset creation
Jun  9 10:49:47.346: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun  9 10:49:47.430: INFO: Exec stderr: ""
Jun  9 10:49:47.430: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:49:47.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:49:47.431: INFO: ExecWithOptions: Clientset creation
Jun  9 10:49:47.431: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun  9 10:49:47.509: INFO: Exec stderr: ""
Jun  9 10:49:47.509: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:49:47.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:49:47.509: INFO: ExecWithOptions: Clientset creation
Jun  9 10:49:47.510: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun  9 10:49:47.583: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jun  9 10:49:47.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-582" for this suite. 06/09/23 10:49:47.592
------------------------------
• [SLOW TEST] [7.222 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:49:40.442
    Jun  9 10:49:40.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 06/09/23 10:49:40.446
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:40.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:40.481
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 06/09/23 10:49:40.49
    STEP: Creating hostNetwork=false pod 06/09/23 10:49:40.49
    Jun  9 10:49:40.509: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-582" to be "running and ready"
    Jun  9 10:49:40.515: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.194234ms
    Jun  9 10:49:40.515: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:49:42.530: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020935561s
    Jun  9 10:49:42.530: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:49:44.524: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.015313707s
    Jun  9 10:49:44.524: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jun  9 10:49:44.524: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 06/09/23 10:49:44.534
    Jun  9 10:49:44.545: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-582" to be "running and ready"
    Jun  9 10:49:44.565: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.740481ms
    Jun  9 10:49:44.565: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:49:46.581: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.035530639s
    Jun  9 10:49:46.581: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jun  9 10:49:46.581: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 06/09/23 10:49:46.587
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 06/09/23 10:49:46.587
    Jun  9 10:49:46.587: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:49:46.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:49:46.594: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:49:46.594: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun  9 10:49:46.788: INFO: Exec stderr: ""
    Jun  9 10:49:46.788: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:49:46.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:49:46.789: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:49:46.789: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun  9 10:49:46.908: INFO: Exec stderr: ""
    Jun  9 10:49:46.908: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:49:46.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:49:46.909: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:49:46.909: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun  9 10:49:46.999: INFO: Exec stderr: ""
    Jun  9 10:49:46.999: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:49:46.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:49:47.000: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:49:47.000: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun  9 10:49:47.080: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 06/09/23 10:49:47.08
    Jun  9 10:49:47.080: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:49:47.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:49:47.080: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:49:47.081: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jun  9 10:49:47.161: INFO: Exec stderr: ""
    Jun  9 10:49:47.161: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:49:47.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:49:47.162: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:49:47.162: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jun  9 10:49:47.244: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 06/09/23 10:49:47.244
    Jun  9 10:49:47.244: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:49:47.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:49:47.245: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:49:47.246: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun  9 10:49:47.345: INFO: Exec stderr: ""
    Jun  9 10:49:47.345: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:49:47.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:49:47.346: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:49:47.346: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun  9 10:49:47.430: INFO: Exec stderr: ""
    Jun  9 10:49:47.430: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:49:47.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:49:47.431: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:49:47.431: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun  9 10:49:47.509: INFO: Exec stderr: ""
    Jun  9 10:49:47.509: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-582 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:49:47.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:49:47.509: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:49:47.510: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-582/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun  9 10:49:47.583: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:49:47.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-582" for this suite. 06/09/23 10:49:47.592
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:49:47.664
Jun  9 10:49:47.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename custom-resource-definition 06/09/23 10:49:47.667
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:47.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:47.72
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 06/09/23 10:49:47.724
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 06/09/23 10:49:47.727
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 06/09/23 10:49:47.727
STEP: fetching the /apis/apiextensions.k8s.io discovery document 06/09/23 10:49:47.727
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 06/09/23 10:49:47.73
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 06/09/23 10:49:47.73
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 06/09/23 10:49:47.731
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:49:47.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2676" for this suite. 06/09/23 10:49:47.74
------------------------------
• [0.087 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:49:47.664
    Jun  9 10:49:47.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename custom-resource-definition 06/09/23 10:49:47.667
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:47.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:47.72
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 06/09/23 10:49:47.724
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 06/09/23 10:49:47.727
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 06/09/23 10:49:47.727
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 06/09/23 10:49:47.727
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 06/09/23 10:49:47.73
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 06/09/23 10:49:47.73
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 06/09/23 10:49:47.731
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:49:47.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2676" for this suite. 06/09/23 10:49:47.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:49:47.752
Jun  9 10:49:47.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename secrets 06/09/23 10:49:47.753
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:47.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:47.78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-775e7548-da99-4e02-8c38-ff69a93f46d4 06/09/23 10:49:47.794
STEP: Creating a pod to test consume secrets 06/09/23 10:49:47.804
Jun  9 10:49:47.819: INFO: Waiting up to 5m0s for pod "pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842" in namespace "secrets-9873" to be "Succeeded or Failed"
Jun  9 10:49:47.829: INFO: Pod "pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842": Phase="Pending", Reason="", readiness=false. Elapsed: 10.228873ms
Jun  9 10:49:49.837: INFO: Pod "pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017818968s
Jun  9 10:49:51.836: INFO: Pod "pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016971344s
STEP: Saw pod success 06/09/23 10:49:51.836
Jun  9 10:49:51.836: INFO: Pod "pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842" satisfied condition "Succeeded or Failed"
Jun  9 10:49:51.843: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842 container secret-volume-test: <nil>
STEP: delete the pod 06/09/23 10:49:51.867
Jun  9 10:49:51.887: INFO: Waiting for pod pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842 to disappear
Jun  9 10:49:51.893: INFO: Pod pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  9 10:49:51.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9873" for this suite. 06/09/23 10:49:51.902
------------------------------
• [4.172 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:49:47.752
    Jun  9 10:49:47.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename secrets 06/09/23 10:49:47.753
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:47.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:47.78
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-775e7548-da99-4e02-8c38-ff69a93f46d4 06/09/23 10:49:47.794
    STEP: Creating a pod to test consume secrets 06/09/23 10:49:47.804
    Jun  9 10:49:47.819: INFO: Waiting up to 5m0s for pod "pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842" in namespace "secrets-9873" to be "Succeeded or Failed"
    Jun  9 10:49:47.829: INFO: Pod "pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842": Phase="Pending", Reason="", readiness=false. Elapsed: 10.228873ms
    Jun  9 10:49:49.837: INFO: Pod "pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017818968s
    Jun  9 10:49:51.836: INFO: Pod "pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016971344s
    STEP: Saw pod success 06/09/23 10:49:51.836
    Jun  9 10:49:51.836: INFO: Pod "pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842" satisfied condition "Succeeded or Failed"
    Jun  9 10:49:51.843: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842 container secret-volume-test: <nil>
    STEP: delete the pod 06/09/23 10:49:51.867
    Jun  9 10:49:51.887: INFO: Waiting for pod pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842 to disappear
    Jun  9 10:49:51.893: INFO: Pod pod-secrets-0f9bd52a-87b1-4267-a932-685483af9842 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:49:51.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9873" for this suite. 06/09/23 10:49:51.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:49:51.925
Jun  9 10:49:51.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 10:49:51.926
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:51.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:51.969
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 06/09/23 10:49:51.974
Jun  9 10:49:51.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-7574 create -f -'
Jun  9 10:49:53.525: INFO: stderr: ""
Jun  9 10:49:53.525: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 06/09/23 10:49:53.525
Jun  9 10:49:53.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-7574 diff -f -'
Jun  9 10:49:54.867: INFO: rc: 1
Jun  9 10:49:54.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-7574 delete -f -'
Jun  9 10:49:54.956: INFO: stderr: ""
Jun  9 10:49:54.956: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 10:49:54.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7574" for this suite. 06/09/23 10:49:54.965
------------------------------
• [3.053 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:49:51.925
    Jun  9 10:49:51.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 10:49:51.926
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:51.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:51.969
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 06/09/23 10:49:51.974
    Jun  9 10:49:51.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-7574 create -f -'
    Jun  9 10:49:53.525: INFO: stderr: ""
    Jun  9 10:49:53.525: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 06/09/23 10:49:53.525
    Jun  9 10:49:53.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-7574 diff -f -'
    Jun  9 10:49:54.867: INFO: rc: 1
    Jun  9 10:49:54.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-7574 delete -f -'
    Jun  9 10:49:54.956: INFO: stderr: ""
    Jun  9 10:49:54.956: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:49:54.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7574" for this suite. 06/09/23 10:49:54.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:49:54.98
Jun  9 10:49:54.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename endpointslice 06/09/23 10:49:54.982
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:55.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:55.007
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 06/09/23 10:49:55.012
STEP: getting /apis/discovery.k8s.io 06/09/23 10:49:55.016
STEP: getting /apis/discovery.k8s.iov1 06/09/23 10:49:55.018
STEP: creating 06/09/23 10:49:55.02
STEP: getting 06/09/23 10:49:55.047
STEP: listing 06/09/23 10:49:55.053
STEP: watching 06/09/23 10:49:55.059
Jun  9 10:49:55.059: INFO: starting watch
STEP: cluster-wide listing 06/09/23 10:49:55.061
STEP: cluster-wide watching 06/09/23 10:49:55.067
Jun  9 10:49:55.067: INFO: starting watch
STEP: patching 06/09/23 10:49:55.069
STEP: updating 06/09/23 10:49:55.079
Jun  9 10:49:55.093: INFO: waiting for watch events with expected annotations
Jun  9 10:49:55.093: INFO: saw patched and updated annotations
STEP: deleting 06/09/23 10:49:55.093
STEP: deleting a collection 06/09/23 10:49:55.113
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jun  9 10:49:55.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4429" for this suite. 06/09/23 10:49:55.153
------------------------------
• [0.193 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:49:54.98
    Jun  9 10:49:54.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename endpointslice 06/09/23 10:49:54.982
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:55.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:55.007
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 06/09/23 10:49:55.012
    STEP: getting /apis/discovery.k8s.io 06/09/23 10:49:55.016
    STEP: getting /apis/discovery.k8s.iov1 06/09/23 10:49:55.018
    STEP: creating 06/09/23 10:49:55.02
    STEP: getting 06/09/23 10:49:55.047
    STEP: listing 06/09/23 10:49:55.053
    STEP: watching 06/09/23 10:49:55.059
    Jun  9 10:49:55.059: INFO: starting watch
    STEP: cluster-wide listing 06/09/23 10:49:55.061
    STEP: cluster-wide watching 06/09/23 10:49:55.067
    Jun  9 10:49:55.067: INFO: starting watch
    STEP: patching 06/09/23 10:49:55.069
    STEP: updating 06/09/23 10:49:55.079
    Jun  9 10:49:55.093: INFO: waiting for watch events with expected annotations
    Jun  9 10:49:55.093: INFO: saw patched and updated annotations
    STEP: deleting 06/09/23 10:49:55.093
    STEP: deleting a collection 06/09/23 10:49:55.113
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:49:55.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4429" for this suite. 06/09/23 10:49:55.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:49:55.176
Jun  9 10:49:55.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-probe 06/09/23 10:49:55.177
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:55.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:55.214
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 in namespace container-probe-3933 06/09/23 10:49:55.221
Jun  9 10:49:55.238: INFO: Waiting up to 5m0s for pod "liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9" in namespace "container-probe-3933" to be "not pending"
Jun  9 10:49:55.249: INFO: Pod "liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.357241ms
Jun  9 10:49:57.258: INFO: Pod "liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.019853837s
Jun  9 10:49:57.258: INFO: Pod "liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9" satisfied condition "not pending"
Jun  9 10:49:57.258: INFO: Started pod liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 in namespace container-probe-3933
STEP: checking the pod's current state and verifying that restartCount is present 06/09/23 10:49:57.258
Jun  9 10:49:57.265: INFO: Initial restart count of pod liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 is 0
Jun  9 10:50:17.392: INFO: Restart count of pod container-probe-3933/liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 is now 1 (20.12623261s elapsed)
Jun  9 10:50:37.592: INFO: Restart count of pod container-probe-3933/liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 is now 2 (40.326961737s elapsed)
Jun  9 10:50:57.777: INFO: Restart count of pod container-probe-3933/liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 is now 3 (1m0.511601351s elapsed)
Jun  9 10:51:15.905: INFO: Restart count of pod container-probe-3933/liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 is now 4 (1m18.63922519s elapsed)
Jun  9 10:52:20.487: INFO: Restart count of pod container-probe-3933/liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 is now 5 (2m23.222006344s elapsed)
STEP: deleting the pod 06/09/23 10:52:20.487
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  9 10:52:20.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3933" for this suite. 06/09/23 10:52:20.527
------------------------------
• [SLOW TEST] [145.376 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:49:55.176
    Jun  9 10:49:55.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-probe 06/09/23 10:49:55.177
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:49:55.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:49:55.214
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 in namespace container-probe-3933 06/09/23 10:49:55.221
    Jun  9 10:49:55.238: INFO: Waiting up to 5m0s for pod "liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9" in namespace "container-probe-3933" to be "not pending"
    Jun  9 10:49:55.249: INFO: Pod "liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.357241ms
    Jun  9 10:49:57.258: INFO: Pod "liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.019853837s
    Jun  9 10:49:57.258: INFO: Pod "liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9" satisfied condition "not pending"
    Jun  9 10:49:57.258: INFO: Started pod liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 in namespace container-probe-3933
    STEP: checking the pod's current state and verifying that restartCount is present 06/09/23 10:49:57.258
    Jun  9 10:49:57.265: INFO: Initial restart count of pod liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 is 0
    Jun  9 10:50:17.392: INFO: Restart count of pod container-probe-3933/liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 is now 1 (20.12623261s elapsed)
    Jun  9 10:50:37.592: INFO: Restart count of pod container-probe-3933/liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 is now 2 (40.326961737s elapsed)
    Jun  9 10:50:57.777: INFO: Restart count of pod container-probe-3933/liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 is now 3 (1m0.511601351s elapsed)
    Jun  9 10:51:15.905: INFO: Restart count of pod container-probe-3933/liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 is now 4 (1m18.63922519s elapsed)
    Jun  9 10:52:20.487: INFO: Restart count of pod container-probe-3933/liveness-e7b3f0c1-19fe-4f89-aa1c-b85c1aee77a9 is now 5 (2m23.222006344s elapsed)
    STEP: deleting the pod 06/09/23 10:52:20.487
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:52:20.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3933" for this suite. 06/09/23 10:52:20.527
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:52:20.553
Jun  9 10:52:20.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pod-network-test 06/09/23 10:52:20.555
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:20.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:20.596
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-192 06/09/23 10:52:20.603
STEP: creating a selector 06/09/23 10:52:20.606
STEP: Creating the service pods in kubernetes 06/09/23 10:52:20.607
Jun  9 10:52:20.607: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  9 10:52:20.699: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-192" to be "running and ready"
Jun  9 10:52:20.723: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.225821ms
Jun  9 10:52:20.723: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:52:22.734: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.035447178s
Jun  9 10:52:22.734: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:52:24.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.03327859s
Jun  9 10:52:24.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:52:26.731: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.032229393s
Jun  9 10:52:26.731: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:52:28.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.033454871s
Jun  9 10:52:28.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:52:30.731: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.032070707s
Jun  9 10:52:30.731: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 10:52:32.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.032730325s
Jun  9 10:52:32.732: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun  9 10:52:32.732: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun  9 10:52:32.737: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-192" to be "running and ready"
Jun  9 10:52:32.743: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.858824ms
Jun  9 10:52:32.743: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun  9 10:52:32.743: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun  9 10:52:32.751: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-192" to be "running and ready"
Jun  9 10:52:32.757: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.991261ms
Jun  9 10:52:32.757: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun  9 10:52:32.757: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/09/23 10:52:32.763
Jun  9 10:52:32.781: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-192" to be "running"
Jun  9 10:52:32.789: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.919116ms
Jun  9 10:52:34.797: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016158859s
Jun  9 10:52:34.797: INFO: Pod "test-container-pod" satisfied condition "running"
Jun  9 10:52:34.805: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-192" to be "running"
Jun  9 10:52:34.812: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.294909ms
Jun  9 10:52:34.812: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jun  9 10:52:34.818: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun  9 10:52:34.818: INFO: Going to poll 172.26.90.32 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun  9 10:52:34.825: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.26.90.32 8081 | grep -v '^\s*$'] Namespace:pod-network-test-192 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:52:34.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:52:34.826: INFO: ExecWithOptions: Clientset creation
Jun  9 10:52:34.826: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-192/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.26.90.32+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  9 10:52:35.958: INFO: Found all 1 expected endpoints: [netserver-0]
Jun  9 10:52:35.958: INFO: Going to poll 172.30.17.165 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun  9 10:52:35.970: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.17.165 8081 | grep -v '^\s*$'] Namespace:pod-network-test-192 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:52:35.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:52:35.971: INFO: ExecWithOptions: Clientset creation
Jun  9 10:52:35.971: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-192/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.17.165+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  9 10:52:37.075: INFO: Found all 1 expected endpoints: [netserver-1]
Jun  9 10:52:37.075: INFO: Going to poll 172.27.53.77 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun  9 10:52:37.083: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.27.53.77 8081 | grep -v '^\s*$'] Namespace:pod-network-test-192 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 10:52:37.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:52:37.083: INFO: ExecWithOptions: Clientset creation
Jun  9 10:52:37.084: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-192/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.27.53.77+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  9 10:52:38.177: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jun  9 10:52:38.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-192" for this suite. 06/09/23 10:52:38.187
------------------------------
• [SLOW TEST] [17.644 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:52:20.553
    Jun  9 10:52:20.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pod-network-test 06/09/23 10:52:20.555
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:20.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:20.596
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-192 06/09/23 10:52:20.603
    STEP: creating a selector 06/09/23 10:52:20.606
    STEP: Creating the service pods in kubernetes 06/09/23 10:52:20.607
    Jun  9 10:52:20.607: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun  9 10:52:20.699: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-192" to be "running and ready"
    Jun  9 10:52:20.723: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.225821ms
    Jun  9 10:52:20.723: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:52:22.734: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.035447178s
    Jun  9 10:52:22.734: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:52:24.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.03327859s
    Jun  9 10:52:24.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:52:26.731: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.032229393s
    Jun  9 10:52:26.731: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:52:28.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.033454871s
    Jun  9 10:52:28.732: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:52:30.731: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.032070707s
    Jun  9 10:52:30.731: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 10:52:32.732: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.032730325s
    Jun  9 10:52:32.732: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun  9 10:52:32.732: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun  9 10:52:32.737: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-192" to be "running and ready"
    Jun  9 10:52:32.743: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.858824ms
    Jun  9 10:52:32.743: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun  9 10:52:32.743: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun  9 10:52:32.751: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-192" to be "running and ready"
    Jun  9 10:52:32.757: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.991261ms
    Jun  9 10:52:32.757: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun  9 10:52:32.757: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/09/23 10:52:32.763
    Jun  9 10:52:32.781: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-192" to be "running"
    Jun  9 10:52:32.789: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.919116ms
    Jun  9 10:52:34.797: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016158859s
    Jun  9 10:52:34.797: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun  9 10:52:34.805: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-192" to be "running"
    Jun  9 10:52:34.812: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.294909ms
    Jun  9 10:52:34.812: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jun  9 10:52:34.818: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun  9 10:52:34.818: INFO: Going to poll 172.26.90.32 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun  9 10:52:34.825: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.26.90.32 8081 | grep -v '^\s*$'] Namespace:pod-network-test-192 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:52:34.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:52:34.826: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:52:34.826: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-192/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.26.90.32+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  9 10:52:35.958: INFO: Found all 1 expected endpoints: [netserver-0]
    Jun  9 10:52:35.958: INFO: Going to poll 172.30.17.165 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun  9 10:52:35.970: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.17.165 8081 | grep -v '^\s*$'] Namespace:pod-network-test-192 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:52:35.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:52:35.971: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:52:35.971: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-192/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.17.165+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  9 10:52:37.075: INFO: Found all 1 expected endpoints: [netserver-1]
    Jun  9 10:52:37.075: INFO: Going to poll 172.27.53.77 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun  9 10:52:37.083: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.27.53.77 8081 | grep -v '^\s*$'] Namespace:pod-network-test-192 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 10:52:37.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:52:37.083: INFO: ExecWithOptions: Clientset creation
    Jun  9 10:52:37.084: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-192/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.27.53.77+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  9 10:52:38.177: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:52:38.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-192" for this suite. 06/09/23 10:52:38.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:52:38.198
Jun  9 10:52:38.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename runtimeclass 06/09/23 10:52:38.199
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:38.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:38.229
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jun  9 10:52:38.252: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6680 to be scheduled
Jun  9 10:52:38.257: INFO: 1 pods are not scheduled: [runtimeclass-6680/test-runtimeclass-runtimeclass-6680-preconfigured-handler-s7pjb(0b941362-1309-4473-8564-5f79953e1b01)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun  9 10:52:40.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6680" for this suite. 06/09/23 10:52:40.281
------------------------------
• [2.094 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:52:38.198
    Jun  9 10:52:38.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename runtimeclass 06/09/23 10:52:38.199
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:38.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:38.229
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jun  9 10:52:38.252: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6680 to be scheduled
    Jun  9 10:52:38.257: INFO: 1 pods are not scheduled: [runtimeclass-6680/test-runtimeclass-runtimeclass-6680-preconfigured-handler-s7pjb(0b941362-1309-4473-8564-5f79953e1b01)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:52:40.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6680" for this suite. 06/09/23 10:52:40.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:52:40.296
Jun  9 10:52:40.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 10:52:40.297
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:40.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:40.325
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-4a4db666-71a5-4703-a111-8c2bbe65d7bf 06/09/23 10:52:40.331
STEP: Creating a pod to test consume configMaps 06/09/23 10:52:40.34
Jun  9 10:52:40.355: INFO: Waiting up to 5m0s for pod "pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726" in namespace "configmap-4214" to be "Succeeded or Failed"
Jun  9 10:52:40.375: INFO: Pod "pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726": Phase="Pending", Reason="", readiness=false. Elapsed: 19.734643ms
Jun  9 10:52:42.382: INFO: Pod "pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026939366s
Jun  9 10:52:44.384: INFO: Pod "pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028873486s
STEP: Saw pod success 06/09/23 10:52:44.384
Jun  9 10:52:44.385: INFO: Pod "pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726" satisfied condition "Succeeded or Failed"
Jun  9 10:52:44.390: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726 container agnhost-container: <nil>
STEP: delete the pod 06/09/23 10:52:44.412
Jun  9 10:52:44.430: INFO: Waiting for pod pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726 to disappear
Jun  9 10:52:44.437: INFO: Pod pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 10:52:44.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4214" for this suite. 06/09/23 10:52:44.444
------------------------------
• [4.161 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:52:40.296
    Jun  9 10:52:40.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 10:52:40.297
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:40.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:40.325
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-4a4db666-71a5-4703-a111-8c2bbe65d7bf 06/09/23 10:52:40.331
    STEP: Creating a pod to test consume configMaps 06/09/23 10:52:40.34
    Jun  9 10:52:40.355: INFO: Waiting up to 5m0s for pod "pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726" in namespace "configmap-4214" to be "Succeeded or Failed"
    Jun  9 10:52:40.375: INFO: Pod "pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726": Phase="Pending", Reason="", readiness=false. Elapsed: 19.734643ms
    Jun  9 10:52:42.382: INFO: Pod "pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026939366s
    Jun  9 10:52:44.384: INFO: Pod "pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028873486s
    STEP: Saw pod success 06/09/23 10:52:44.384
    Jun  9 10:52:44.385: INFO: Pod "pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726" satisfied condition "Succeeded or Failed"
    Jun  9 10:52:44.390: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726 container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 10:52:44.412
    Jun  9 10:52:44.430: INFO: Waiting for pod pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726 to disappear
    Jun  9 10:52:44.437: INFO: Pod pod-configmaps-baf38edf-2b15-4c79-ba5d-3fe3dc623726 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:52:44.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4214" for this suite. 06/09/23 10:52:44.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:52:44.46
Jun  9 10:52:44.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename svcaccounts 06/09/23 10:52:44.461
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:44.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:44.487
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-hf2rk"  06/09/23 10:52:44.491
Jun  9 10:52:44.499: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-hf2rk"  06/09/23 10:52:44.499
Jun  9 10:52:44.511: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun  9 10:52:44.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9665" for this suite. 06/09/23 10:52:44.518
------------------------------
• [0.072 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:52:44.46
    Jun  9 10:52:44.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename svcaccounts 06/09/23 10:52:44.461
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:44.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:44.487
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-hf2rk"  06/09/23 10:52:44.491
    Jun  9 10:52:44.499: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-hf2rk"  06/09/23 10:52:44.499
    Jun  9 10:52:44.511: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:52:44.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9665" for this suite. 06/09/23 10:52:44.518
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:52:44.533
Jun  9 10:52:44.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename crd-webhook 06/09/23 10:52:44.534
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:44.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:44.563
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 06/09/23 10:52:44.567
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/09/23 10:52:45.168
STEP: Deploying the custom resource conversion webhook pod 06/09/23 10:52:45.186
STEP: Wait for the deployment to be ready 06/09/23 10:52:45.21
Jun  9 10:52:45.220: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 10:52:47.263
STEP: Verifying the service has paired with the endpoint 06/09/23 10:52:47.337
Jun  9 10:52:48.339: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jun  9 10:52:48.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Creating a v1 custom resource 06/09/23 10:52:50.97
STEP: v2 custom resource should be converted 06/09/23 10:52:50.98
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:52:51.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-7346" for this suite. 06/09/23 10:52:51.725
------------------------------
• [SLOW TEST] [7.209 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:52:44.533
    Jun  9 10:52:44.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename crd-webhook 06/09/23 10:52:44.534
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:44.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:44.563
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 06/09/23 10:52:44.567
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/09/23 10:52:45.168
    STEP: Deploying the custom resource conversion webhook pod 06/09/23 10:52:45.186
    STEP: Wait for the deployment to be ready 06/09/23 10:52:45.21
    Jun  9 10:52:45.220: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 10:52:47.263
    STEP: Verifying the service has paired with the endpoint 06/09/23 10:52:47.337
    Jun  9 10:52:48.339: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jun  9 10:52:48.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Creating a v1 custom resource 06/09/23 10:52:50.97
    STEP: v2 custom resource should be converted 06/09/23 10:52:50.98
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:52:51.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-7346" for this suite. 06/09/23 10:52:51.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:52:51.744
Jun  9 10:52:51.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename secrets 06/09/23 10:52:51.745
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:51.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:51.78
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-65ddfc65-317f-4d85-b361-6669b89c4546 06/09/23 10:52:51.786
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  9 10:52:51.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9127" for this suite. 06/09/23 10:52:51.8
------------------------------
• [0.070 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:52:51.744
    Jun  9 10:52:51.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename secrets 06/09/23 10:52:51.745
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:51.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:51.78
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-65ddfc65-317f-4d85-b361-6669b89c4546 06/09/23 10:52:51.786
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:52:51.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9127" for this suite. 06/09/23 10:52:51.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:52:51.816
Jun  9 10:52:51.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename ingress 06/09/23 10:52:51.817
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:51.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:51.848
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 06/09/23 10:52:51.854
STEP: getting /apis/networking.k8s.io 06/09/23 10:52:51.858
STEP: getting /apis/networking.k8s.iov1 06/09/23 10:52:51.86
STEP: creating 06/09/23 10:52:51.862
STEP: getting 06/09/23 10:52:51.89
STEP: listing 06/09/23 10:52:51.897
STEP: watching 06/09/23 10:52:51.904
Jun  9 10:52:51.904: INFO: starting watch
STEP: cluster-wide listing 06/09/23 10:52:51.912
STEP: cluster-wide watching 06/09/23 10:52:51.92
Jun  9 10:52:51.920: INFO: starting watch
STEP: patching 06/09/23 10:52:51.922
STEP: updating 06/09/23 10:52:51.933
Jun  9 10:52:51.952: INFO: waiting for watch events with expected annotations
Jun  9 10:52:51.952: INFO: saw patched and updated annotations
STEP: patching /status 06/09/23 10:52:51.952
STEP: updating /status 06/09/23 10:52:51.963
STEP: get /status 06/09/23 10:52:51.977
STEP: deleting 06/09/23 10:52:51.983
STEP: deleting a collection 06/09/23 10:52:52.005
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jun  9 10:52:52.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-3530" for this suite. 06/09/23 10:52:52.046
------------------------------
• [0.241 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:52:51.816
    Jun  9 10:52:51.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename ingress 06/09/23 10:52:51.817
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:51.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:51.848
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 06/09/23 10:52:51.854
    STEP: getting /apis/networking.k8s.io 06/09/23 10:52:51.858
    STEP: getting /apis/networking.k8s.iov1 06/09/23 10:52:51.86
    STEP: creating 06/09/23 10:52:51.862
    STEP: getting 06/09/23 10:52:51.89
    STEP: listing 06/09/23 10:52:51.897
    STEP: watching 06/09/23 10:52:51.904
    Jun  9 10:52:51.904: INFO: starting watch
    STEP: cluster-wide listing 06/09/23 10:52:51.912
    STEP: cluster-wide watching 06/09/23 10:52:51.92
    Jun  9 10:52:51.920: INFO: starting watch
    STEP: patching 06/09/23 10:52:51.922
    STEP: updating 06/09/23 10:52:51.933
    Jun  9 10:52:51.952: INFO: waiting for watch events with expected annotations
    Jun  9 10:52:51.952: INFO: saw patched and updated annotations
    STEP: patching /status 06/09/23 10:52:51.952
    STEP: updating /status 06/09/23 10:52:51.963
    STEP: get /status 06/09/23 10:52:51.977
    STEP: deleting 06/09/23 10:52:51.983
    STEP: deleting a collection 06/09/23 10:52:52.005
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:52:52.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-3530" for this suite. 06/09/23 10:52:52.046
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:52:52.058
Jun  9 10:52:52.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 10:52:52.06
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:52.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:52.086
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 10:52:52.112
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:52:52.653
STEP: Deploying the webhook pod 06/09/23 10:52:52.663
STEP: Wait for the deployment to be ready 06/09/23 10:52:52.686
Jun  9 10:52:52.706: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 10:52:54.725
STEP: Verifying the service has paired with the endpoint 06/09/23 10:52:54.753
Jun  9 10:52:55.753: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 06/09/23 10:52:55.771
STEP: Creating a custom resource definition that should be denied by the webhook 06/09/23 10:52:55.794
Jun  9 10:52:55.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:52:55.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2091" for this suite. 06/09/23 10:52:55.946
STEP: Destroying namespace "webhook-2091-markers" for this suite. 06/09/23 10:52:55.96
------------------------------
• [3.916 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:52:52.058
    Jun  9 10:52:52.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 10:52:52.06
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:52.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:52.086
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 10:52:52.112
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:52:52.653
    STEP: Deploying the webhook pod 06/09/23 10:52:52.663
    STEP: Wait for the deployment to be ready 06/09/23 10:52:52.686
    Jun  9 10:52:52.706: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 10:52:54.725
    STEP: Verifying the service has paired with the endpoint 06/09/23 10:52:54.753
    Jun  9 10:52:55.753: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 06/09/23 10:52:55.771
    STEP: Creating a custom resource definition that should be denied by the webhook 06/09/23 10:52:55.794
    Jun  9 10:52:55.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:52:55.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2091" for this suite. 06/09/23 10:52:55.946
    STEP: Destroying namespace "webhook-2091-markers" for this suite. 06/09/23 10:52:55.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:52:55.974
Jun  9 10:52:55.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 10:52:55.975
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:56.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:56.012
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 06/09/23 10:52:56.017
Jun  9 10:52:56.042: INFO: Waiting up to 5m0s for pod "downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5" in namespace "downward-api-6942" to be "Succeeded or Failed"
Jun  9 10:52:56.056: INFO: Pod "downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.499903ms
Jun  9 10:52:58.063: INFO: Pod "downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020882714s
Jun  9 10:53:00.074: INFO: Pod "downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031715797s
STEP: Saw pod success 06/09/23 10:53:00.074
Jun  9 10:53:00.074: INFO: Pod "downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5" satisfied condition "Succeeded or Failed"
Jun  9 10:53:00.081: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5 container dapi-container: <nil>
STEP: delete the pod 06/09/23 10:53:00.116
Jun  9 10:53:00.140: INFO: Waiting for pod downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5 to disappear
Jun  9 10:53:00.147: INFO: Pod downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun  9 10:53:00.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6942" for this suite. 06/09/23 10:53:00.157
------------------------------
• [4.196 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:52:55.974
    Jun  9 10:52:55.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 10:52:55.975
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:52:56.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:52:56.012
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 06/09/23 10:52:56.017
    Jun  9 10:52:56.042: INFO: Waiting up to 5m0s for pod "downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5" in namespace "downward-api-6942" to be "Succeeded or Failed"
    Jun  9 10:52:56.056: INFO: Pod "downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.499903ms
    Jun  9 10:52:58.063: INFO: Pod "downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020882714s
    Jun  9 10:53:00.074: INFO: Pod "downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031715797s
    STEP: Saw pod success 06/09/23 10:53:00.074
    Jun  9 10:53:00.074: INFO: Pod "downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5" satisfied condition "Succeeded or Failed"
    Jun  9 10:53:00.081: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5 container dapi-container: <nil>
    STEP: delete the pod 06/09/23 10:53:00.116
    Jun  9 10:53:00.140: INFO: Waiting for pod downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5 to disappear
    Jun  9 10:53:00.147: INFO: Pod downward-api-285d7d6a-acf2-491c-abfa-3b638c65a6c5 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:53:00.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6942" for this suite. 06/09/23 10:53:00.157
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:53:00.171
Jun  9 10:53:00.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 10:53:00.172
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:00.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:00.213
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6323 06/09/23 10:53:00.218
STEP: changing the ExternalName service to type=NodePort 06/09/23 10:53:00.225
STEP: creating replication controller externalname-service in namespace services-6323 06/09/23 10:53:00.281
I0609 10:53:00.292336      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6323, replica count: 2
I0609 10:53:03.343263      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 10:53:03.343: INFO: Creating new exec pod
Jun  9 10:53:03.358: INFO: Waiting up to 5m0s for pod "execpod2wkq2" in namespace "services-6323" to be "running"
Jun  9 10:53:03.364: INFO: Pod "execpod2wkq2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.64117ms
Jun  9 10:53:05.373: INFO: Pod "execpod2wkq2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014749196s
Jun  9 10:53:05.373: INFO: Pod "execpod2wkq2" satisfied condition "running"
Jun  9 10:53:06.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6323 exec execpod2wkq2 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jun  9 10:53:06.589: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  9 10:53:06.589: INFO: stdout: ""
Jun  9 10:53:06.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6323 exec execpod2wkq2 -- /bin/sh -x -c nc -v -z -w 2 10.96.12.234 80'
Jun  9 10:53:06.762: INFO: stderr: "+ nc -v -z -w 2 10.96.12.234 80\nConnection to 10.96.12.234 80 port [tcp/http] succeeded!\n"
Jun  9 10:53:06.762: INFO: stdout: ""
Jun  9 10:53:06.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6323 exec execpod2wkq2 -- /bin/sh -x -c nc -v -z -w 2 10.255.64.103 30177'
Jun  9 10:53:06.923: INFO: stderr: "+ nc -v -z -w 2 10.255.64.103 30177\nConnection to 10.255.64.103 30177 port [tcp/*] succeeded!\n"
Jun  9 10:53:06.923: INFO: stdout: ""
Jun  9 10:53:06.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6323 exec execpod2wkq2 -- /bin/sh -x -c nc -v -z -w 2 10.255.64.104 30177'
Jun  9 10:53:07.101: INFO: stderr: "+ nc -v -z -w 2 10.255.64.104 30177\nConnection to 10.255.64.104 30177 port [tcp/*] succeeded!\n"
Jun  9 10:53:07.101: INFO: stdout: ""
Jun  9 10:53:07.101: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 10:53:07.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6323" for this suite. 06/09/23 10:53:07.161
------------------------------
• [SLOW TEST] [7.000 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:53:00.171
    Jun  9 10:53:00.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 10:53:00.172
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:00.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:00.213
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6323 06/09/23 10:53:00.218
    STEP: changing the ExternalName service to type=NodePort 06/09/23 10:53:00.225
    STEP: creating replication controller externalname-service in namespace services-6323 06/09/23 10:53:00.281
    I0609 10:53:00.292336      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6323, replica count: 2
    I0609 10:53:03.343263      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  9 10:53:03.343: INFO: Creating new exec pod
    Jun  9 10:53:03.358: INFO: Waiting up to 5m0s for pod "execpod2wkq2" in namespace "services-6323" to be "running"
    Jun  9 10:53:03.364: INFO: Pod "execpod2wkq2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.64117ms
    Jun  9 10:53:05.373: INFO: Pod "execpod2wkq2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014749196s
    Jun  9 10:53:05.373: INFO: Pod "execpod2wkq2" satisfied condition "running"
    Jun  9 10:53:06.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6323 exec execpod2wkq2 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jun  9 10:53:06.589: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun  9 10:53:06.589: INFO: stdout: ""
    Jun  9 10:53:06.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6323 exec execpod2wkq2 -- /bin/sh -x -c nc -v -z -w 2 10.96.12.234 80'
    Jun  9 10:53:06.762: INFO: stderr: "+ nc -v -z -w 2 10.96.12.234 80\nConnection to 10.96.12.234 80 port [tcp/http] succeeded!\n"
    Jun  9 10:53:06.762: INFO: stdout: ""
    Jun  9 10:53:06.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6323 exec execpod2wkq2 -- /bin/sh -x -c nc -v -z -w 2 10.255.64.103 30177'
    Jun  9 10:53:06.923: INFO: stderr: "+ nc -v -z -w 2 10.255.64.103 30177\nConnection to 10.255.64.103 30177 port [tcp/*] succeeded!\n"
    Jun  9 10:53:06.923: INFO: stdout: ""
    Jun  9 10:53:06.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6323 exec execpod2wkq2 -- /bin/sh -x -c nc -v -z -w 2 10.255.64.104 30177'
    Jun  9 10:53:07.101: INFO: stderr: "+ nc -v -z -w 2 10.255.64.104 30177\nConnection to 10.255.64.104 30177 port [tcp/*] succeeded!\n"
    Jun  9 10:53:07.101: INFO: stdout: ""
    Jun  9 10:53:07.101: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:53:07.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6323" for this suite. 06/09/23 10:53:07.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:53:07.172
Jun  9 10:53:07.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename daemonsets 06/09/23 10:53:07.174
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:07.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:07.211
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Jun  9 10:53:07.268: INFO: Create a RollingUpdate DaemonSet
Jun  9 10:53:07.277: INFO: Check that daemon pods launch on every node of the cluster
Jun  9 10:53:07.287: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:07.287: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:07.287: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:07.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:53:07.292: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:53:08.312: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:08.313: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:08.313: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:08.335: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:53:08.335: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 10:53:09.302: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:09.302: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:09.302: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:09.309: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  9 10:53:09.309: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jun  9 10:53:09.309: INFO: Update the DaemonSet to trigger a rollout
Jun  9 10:53:09.327: INFO: Updating DaemonSet daemon-set
Jun  9 10:53:12.363: INFO: Roll back the DaemonSet before rollout is complete
Jun  9 10:53:12.396: INFO: Updating DaemonSet daemon-set
Jun  9 10:53:12.396: INFO: Make sure DaemonSet rollback is complete
Jun  9 10:53:12.431: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:12.431: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:12.431: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:13.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:13.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:13.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:14.453: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:14.453: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:14.453: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:15.447: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:15.447: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:15.447: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:16.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:16.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:16.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:17.438: INFO: Pod daemon-set-xhhvq is not available
Jun  9 10:53:17.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:17.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 10:53:17.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/09/23 10:53:17.457
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3021, will wait for the garbage collector to delete the pods 06/09/23 10:53:17.457
Jun  9 10:53:17.523: INFO: Deleting DaemonSet.extensions daemon-set took: 10.772393ms
Jun  9 10:53:17.624: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.351284ms
Jun  9 10:53:19.232: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 10:53:19.232: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  9 10:53:19.240: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"89173"},"items":null}

Jun  9 10:53:19.246: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"89173"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:53:19.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3021" for this suite. 06/09/23 10:53:19.284
------------------------------
• [SLOW TEST] [12.124 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:53:07.172
    Jun  9 10:53:07.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename daemonsets 06/09/23 10:53:07.174
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:07.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:07.211
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Jun  9 10:53:07.268: INFO: Create a RollingUpdate DaemonSet
    Jun  9 10:53:07.277: INFO: Check that daemon pods launch on every node of the cluster
    Jun  9 10:53:07.287: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:07.287: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:07.287: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:07.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:53:07.292: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:53:08.312: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:08.313: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:08.313: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:08.335: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:53:08.335: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 10:53:09.302: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:09.302: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:09.302: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:09.309: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun  9 10:53:09.309: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Jun  9 10:53:09.309: INFO: Update the DaemonSet to trigger a rollout
    Jun  9 10:53:09.327: INFO: Updating DaemonSet daemon-set
    Jun  9 10:53:12.363: INFO: Roll back the DaemonSet before rollout is complete
    Jun  9 10:53:12.396: INFO: Updating DaemonSet daemon-set
    Jun  9 10:53:12.396: INFO: Make sure DaemonSet rollback is complete
    Jun  9 10:53:12.431: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:12.431: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:12.431: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:13.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:13.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:13.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:14.453: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:14.453: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:14.453: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:15.447: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:15.447: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:15.447: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:16.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:16.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:16.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:17.438: INFO: Pod daemon-set-xhhvq is not available
    Jun  9 10:53:17.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:17.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 10:53:17.445: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/09/23 10:53:17.457
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3021, will wait for the garbage collector to delete the pods 06/09/23 10:53:17.457
    Jun  9 10:53:17.523: INFO: Deleting DaemonSet.extensions daemon-set took: 10.772393ms
    Jun  9 10:53:17.624: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.351284ms
    Jun  9 10:53:19.232: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 10:53:19.232: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun  9 10:53:19.240: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"89173"},"items":null}

    Jun  9 10:53:19.246: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"89173"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:53:19.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3021" for this suite. 06/09/23 10:53:19.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:53:19.303
Jun  9 10:53:19.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename statefulset 06/09/23 10:53:19.304
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:19.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:19.334
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6177 06/09/23 10:53:19.34
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 06/09/23 10:53:19.349
STEP: Creating pod with conflicting port in namespace statefulset-6177 06/09/23 10:53:19.36
STEP: Waiting until pod test-pod will start running in namespace statefulset-6177 06/09/23 10:53:19.372
Jun  9 10:53:19.373: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6177" to be "running"
Jun  9 10:53:19.380: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.035593ms
Jun  9 10:53:21.388: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015914656s
Jun  9 10:53:21.388: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-6177 06/09/23 10:53:21.389
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6177 06/09/23 10:53:21.399
Jun  9 10:53:21.421: INFO: Observed stateful pod in namespace: statefulset-6177, name: ss-0, uid: f1c0d6dd-6d5e-4594-b1ce-09312ec43672, status phase: Pending. Waiting for statefulset controller to delete.
Jun  9 10:53:21.436: INFO: Observed stateful pod in namespace: statefulset-6177, name: ss-0, uid: f1c0d6dd-6d5e-4594-b1ce-09312ec43672, status phase: Failed. Waiting for statefulset controller to delete.
Jun  9 10:53:21.451: INFO: Observed stateful pod in namespace: statefulset-6177, name: ss-0, uid: f1c0d6dd-6d5e-4594-b1ce-09312ec43672, status phase: Failed. Waiting for statefulset controller to delete.
Jun  9 10:53:21.455: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6177
STEP: Removing pod with conflicting port in namespace statefulset-6177 06/09/23 10:53:21.455
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6177 and will be in running state 06/09/23 10:53:21.484
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  9 10:53:23.499: INFO: Deleting all statefulset in ns statefulset-6177
Jun  9 10:53:23.505: INFO: Scaling statefulset ss to 0
Jun  9 10:53:33.535: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 10:53:33.546: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  9 10:53:33.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6177" for this suite. 06/09/23 10:53:33.581
------------------------------
• [SLOW TEST] [14.292 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:53:19.303
    Jun  9 10:53:19.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename statefulset 06/09/23 10:53:19.304
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:19.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:19.334
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6177 06/09/23 10:53:19.34
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 06/09/23 10:53:19.349
    STEP: Creating pod with conflicting port in namespace statefulset-6177 06/09/23 10:53:19.36
    STEP: Waiting until pod test-pod will start running in namespace statefulset-6177 06/09/23 10:53:19.372
    Jun  9 10:53:19.373: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6177" to be "running"
    Jun  9 10:53:19.380: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.035593ms
    Jun  9 10:53:21.388: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015914656s
    Jun  9 10:53:21.388: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-6177 06/09/23 10:53:21.389
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6177 06/09/23 10:53:21.399
    Jun  9 10:53:21.421: INFO: Observed stateful pod in namespace: statefulset-6177, name: ss-0, uid: f1c0d6dd-6d5e-4594-b1ce-09312ec43672, status phase: Pending. Waiting for statefulset controller to delete.
    Jun  9 10:53:21.436: INFO: Observed stateful pod in namespace: statefulset-6177, name: ss-0, uid: f1c0d6dd-6d5e-4594-b1ce-09312ec43672, status phase: Failed. Waiting for statefulset controller to delete.
    Jun  9 10:53:21.451: INFO: Observed stateful pod in namespace: statefulset-6177, name: ss-0, uid: f1c0d6dd-6d5e-4594-b1ce-09312ec43672, status phase: Failed. Waiting for statefulset controller to delete.
    Jun  9 10:53:21.455: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6177
    STEP: Removing pod with conflicting port in namespace statefulset-6177 06/09/23 10:53:21.455
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6177 and will be in running state 06/09/23 10:53:21.484
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  9 10:53:23.499: INFO: Deleting all statefulset in ns statefulset-6177
    Jun  9 10:53:23.505: INFO: Scaling statefulset ss to 0
    Jun  9 10:53:33.535: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  9 10:53:33.546: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:53:33.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6177" for this suite. 06/09/23 10:53:33.581
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:53:33.596
Jun  9 10:53:33.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:53:33.597
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:33.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:33.641
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-8e34f086-2967-4a40-9c6f-5fee2ab302c0 06/09/23 10:53:33.647
STEP: Creating secret with name secret-projected-all-test-volume-87d31bbe-692f-405b-a717-4230414a20f2 06/09/23 10:53:33.656
STEP: Creating a pod to test Check all projections for projected volume plugin 06/09/23 10:53:33.67
Jun  9 10:53:33.692: INFO: Waiting up to 5m0s for pod "projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd" in namespace "projected-500" to be "Succeeded or Failed"
Jun  9 10:53:33.700: INFO: Pod "projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.803191ms
Jun  9 10:53:35.708: INFO: Pod "projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015775617s
Jun  9 10:53:37.707: INFO: Pod "projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014951952s
STEP: Saw pod success 06/09/23 10:53:37.707
Jun  9 10:53:37.707: INFO: Pod "projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd" satisfied condition "Succeeded or Failed"
Jun  9 10:53:37.714: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd container projected-all-volume-test: <nil>
STEP: delete the pod 06/09/23 10:53:37.73
Jun  9 10:53:37.747: INFO: Waiting for pod projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd to disappear
Jun  9 10:53:37.753: INFO: Pod projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jun  9 10:53:37.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-500" for this suite. 06/09/23 10:53:37.763
------------------------------
• [4.180 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:53:33.596
    Jun  9 10:53:33.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:53:33.597
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:33.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:33.641
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-8e34f086-2967-4a40-9c6f-5fee2ab302c0 06/09/23 10:53:33.647
    STEP: Creating secret with name secret-projected-all-test-volume-87d31bbe-692f-405b-a717-4230414a20f2 06/09/23 10:53:33.656
    STEP: Creating a pod to test Check all projections for projected volume plugin 06/09/23 10:53:33.67
    Jun  9 10:53:33.692: INFO: Waiting up to 5m0s for pod "projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd" in namespace "projected-500" to be "Succeeded or Failed"
    Jun  9 10:53:33.700: INFO: Pod "projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.803191ms
    Jun  9 10:53:35.708: INFO: Pod "projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015775617s
    Jun  9 10:53:37.707: INFO: Pod "projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014951952s
    STEP: Saw pod success 06/09/23 10:53:37.707
    Jun  9 10:53:37.707: INFO: Pod "projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd" satisfied condition "Succeeded or Failed"
    Jun  9 10:53:37.714: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd container projected-all-volume-test: <nil>
    STEP: delete the pod 06/09/23 10:53:37.73
    Jun  9 10:53:37.747: INFO: Waiting for pod projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd to disappear
    Jun  9 10:53:37.753: INFO: Pod projected-volume-8d21735c-7ddb-4349-b796-9952334b37dd no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:53:37.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-500" for this suite. 06/09/23 10:53:37.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:53:37.777
Jun  9 10:53:37.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 10:53:37.779
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:37.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:37.807
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 06/09/23 10:53:37.813
Jun  9 10:53:37.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 create -f -'
Jun  9 10:53:39.193: INFO: stderr: ""
Jun  9 10:53:39.193: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/09/23 10:53:39.193
Jun  9 10:53:39.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  9 10:53:39.284: INFO: stderr: ""
Jun  9 10:53:39.284: INFO: stdout: "update-demo-nautilus-ddj55 update-demo-nautilus-kv25m "
Jun  9 10:53:39.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods update-demo-nautilus-ddj55 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  9 10:53:39.395: INFO: stderr: ""
Jun  9 10:53:39.396: INFO: stdout: ""
Jun  9 10:53:39.396: INFO: update-demo-nautilus-ddj55 is created but not running
Jun  9 10:53:44.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  9 10:53:44.479: INFO: stderr: ""
Jun  9 10:53:44.479: INFO: stdout: "update-demo-nautilus-ddj55 update-demo-nautilus-kv25m "
Jun  9 10:53:44.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods update-demo-nautilus-ddj55 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  9 10:53:44.552: INFO: stderr: ""
Jun  9 10:53:44.553: INFO: stdout: "true"
Jun  9 10:53:44.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods update-demo-nautilus-ddj55 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  9 10:53:44.638: INFO: stderr: ""
Jun  9 10:53:44.638: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  9 10:53:44.638: INFO: validating pod update-demo-nautilus-ddj55
Jun  9 10:53:44.646: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 10:53:44.646: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 10:53:44.646: INFO: update-demo-nautilus-ddj55 is verified up and running
Jun  9 10:53:44.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods update-demo-nautilus-kv25m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  9 10:53:44.731: INFO: stderr: ""
Jun  9 10:53:44.731: INFO: stdout: "true"
Jun  9 10:53:44.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods update-demo-nautilus-kv25m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  9 10:53:44.809: INFO: stderr: ""
Jun  9 10:53:44.809: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun  9 10:53:44.809: INFO: validating pod update-demo-nautilus-kv25m
Jun  9 10:53:44.817: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  9 10:53:44.818: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  9 10:53:44.818: INFO: update-demo-nautilus-kv25m is verified up and running
STEP: using delete to clean up resources 06/09/23 10:53:44.818
Jun  9 10:53:44.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 delete --grace-period=0 --force -f -'
Jun  9 10:53:44.904: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 10:53:44.905: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun  9 10:53:44.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get rc,svc -l name=update-demo --no-headers'
Jun  9 10:53:45.026: INFO: stderr: "No resources found in kubectl-1627 namespace.\n"
Jun  9 10:53:45.026: INFO: stdout: ""
Jun  9 10:53:45.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  9 10:53:45.145: INFO: stderr: ""
Jun  9 10:53:45.145: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 10:53:45.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1627" for this suite. 06/09/23 10:53:45.158
------------------------------
• [SLOW TEST] [7.394 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:53:37.777
    Jun  9 10:53:37.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 10:53:37.779
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:37.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:37.807
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 06/09/23 10:53:37.813
    Jun  9 10:53:37.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 create -f -'
    Jun  9 10:53:39.193: INFO: stderr: ""
    Jun  9 10:53:39.193: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/09/23 10:53:39.193
    Jun  9 10:53:39.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  9 10:53:39.284: INFO: stderr: ""
    Jun  9 10:53:39.284: INFO: stdout: "update-demo-nautilus-ddj55 update-demo-nautilus-kv25m "
    Jun  9 10:53:39.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods update-demo-nautilus-ddj55 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  9 10:53:39.395: INFO: stderr: ""
    Jun  9 10:53:39.396: INFO: stdout: ""
    Jun  9 10:53:39.396: INFO: update-demo-nautilus-ddj55 is created but not running
    Jun  9 10:53:44.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun  9 10:53:44.479: INFO: stderr: ""
    Jun  9 10:53:44.479: INFO: stdout: "update-demo-nautilus-ddj55 update-demo-nautilus-kv25m "
    Jun  9 10:53:44.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods update-demo-nautilus-ddj55 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  9 10:53:44.552: INFO: stderr: ""
    Jun  9 10:53:44.553: INFO: stdout: "true"
    Jun  9 10:53:44.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods update-demo-nautilus-ddj55 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  9 10:53:44.638: INFO: stderr: ""
    Jun  9 10:53:44.638: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  9 10:53:44.638: INFO: validating pod update-demo-nautilus-ddj55
    Jun  9 10:53:44.646: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  9 10:53:44.646: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  9 10:53:44.646: INFO: update-demo-nautilus-ddj55 is verified up and running
    Jun  9 10:53:44.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods update-demo-nautilus-kv25m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun  9 10:53:44.731: INFO: stderr: ""
    Jun  9 10:53:44.731: INFO: stdout: "true"
    Jun  9 10:53:44.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods update-demo-nautilus-kv25m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun  9 10:53:44.809: INFO: stderr: ""
    Jun  9 10:53:44.809: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun  9 10:53:44.809: INFO: validating pod update-demo-nautilus-kv25m
    Jun  9 10:53:44.817: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun  9 10:53:44.818: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun  9 10:53:44.818: INFO: update-demo-nautilus-kv25m is verified up and running
    STEP: using delete to clean up resources 06/09/23 10:53:44.818
    Jun  9 10:53:44.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 delete --grace-period=0 --force -f -'
    Jun  9 10:53:44.904: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  9 10:53:44.905: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jun  9 10:53:44.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get rc,svc -l name=update-demo --no-headers'
    Jun  9 10:53:45.026: INFO: stderr: "No resources found in kubectl-1627 namespace.\n"
    Jun  9 10:53:45.026: INFO: stdout: ""
    Jun  9 10:53:45.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1627 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun  9 10:53:45.145: INFO: stderr: ""
    Jun  9 10:53:45.145: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:53:45.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1627" for this suite. 06/09/23 10:53:45.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:53:45.172
Jun  9 10:53:45.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:53:45.173
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:45.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:45.213
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 06/09/23 10:53:45.228
Jun  9 10:53:45.252: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f" in namespace "projected-5348" to be "Succeeded or Failed"
Jun  9 10:53:45.262: INFO: Pod "downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.049029ms
Jun  9 10:53:47.270: INFO: Pod "downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01725343s
Jun  9 10:53:49.281: INFO: Pod "downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028616225s
STEP: Saw pod success 06/09/23 10:53:49.281
Jun  9 10:53:49.281: INFO: Pod "downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f" satisfied condition "Succeeded or Failed"
Jun  9 10:53:49.289: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f container client-container: <nil>
STEP: delete the pod 06/09/23 10:53:49.303
Jun  9 10:53:49.321: INFO: Waiting for pod downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f to disappear
Jun  9 10:53:49.327: INFO: Pod downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  9 10:53:49.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5348" for this suite. 06/09/23 10:53:49.339
------------------------------
• [4.185 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:53:45.172
    Jun  9 10:53:45.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:53:45.173
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:45.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:45.213
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 06/09/23 10:53:45.228
    Jun  9 10:53:45.252: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f" in namespace "projected-5348" to be "Succeeded or Failed"
    Jun  9 10:53:45.262: INFO: Pod "downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.049029ms
    Jun  9 10:53:47.270: INFO: Pod "downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01725343s
    Jun  9 10:53:49.281: INFO: Pod "downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028616225s
    STEP: Saw pod success 06/09/23 10:53:49.281
    Jun  9 10:53:49.281: INFO: Pod "downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f" satisfied condition "Succeeded or Failed"
    Jun  9 10:53:49.289: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f container client-container: <nil>
    STEP: delete the pod 06/09/23 10:53:49.303
    Jun  9 10:53:49.321: INFO: Waiting for pod downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f to disappear
    Jun  9 10:53:49.327: INFO: Pod downwardapi-volume-e0e01e51-2e9d-4658-aea8-a9881a93034f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:53:49.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5348" for this suite. 06/09/23 10:53:49.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:53:49.359
Jun  9 10:53:49.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename job 06/09/23 10:53:49.36
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:49.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:49.396
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 06/09/23 10:53:49.401
STEP: Ensuring active pods == parallelism 06/09/23 10:53:49.414
STEP: Orphaning one of the Job's Pods 06/09/23 10:53:53.423
Jun  9 10:53:53.947: INFO: Successfully updated pod "adopt-release-lwqbt"
STEP: Checking that the Job readopts the Pod 06/09/23 10:53:53.947
Jun  9 10:53:53.948: INFO: Waiting up to 15m0s for pod "adopt-release-lwqbt" in namespace "job-570" to be "adopted"
Jun  9 10:53:53.959: INFO: Pod "adopt-release-lwqbt": Phase="Running", Reason="", readiness=true. Elapsed: 11.000933ms
Jun  9 10:53:55.966: INFO: Pod "adopt-release-lwqbt": Phase="Running", Reason="", readiness=true. Elapsed: 2.018277065s
Jun  9 10:53:55.966: INFO: Pod "adopt-release-lwqbt" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 06/09/23 10:53:55.966
Jun  9 10:53:56.485: INFO: Successfully updated pod "adopt-release-lwqbt"
STEP: Checking that the Job releases the Pod 06/09/23 10:53:56.485
Jun  9 10:53:56.485: INFO: Waiting up to 15m0s for pod "adopt-release-lwqbt" in namespace "job-570" to be "released"
Jun  9 10:53:56.491: INFO: Pod "adopt-release-lwqbt": Phase="Running", Reason="", readiness=true. Elapsed: 5.585888ms
Jun  9 10:53:58.499: INFO: Pod "adopt-release-lwqbt": Phase="Running", Reason="", readiness=true. Elapsed: 2.013764466s
Jun  9 10:53:58.499: INFO: Pod "adopt-release-lwqbt" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun  9 10:53:58.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-570" for this suite. 06/09/23 10:53:58.508
------------------------------
• [SLOW TEST] [9.160 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:53:49.359
    Jun  9 10:53:49.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename job 06/09/23 10:53:49.36
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:49.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:49.396
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 06/09/23 10:53:49.401
    STEP: Ensuring active pods == parallelism 06/09/23 10:53:49.414
    STEP: Orphaning one of the Job's Pods 06/09/23 10:53:53.423
    Jun  9 10:53:53.947: INFO: Successfully updated pod "adopt-release-lwqbt"
    STEP: Checking that the Job readopts the Pod 06/09/23 10:53:53.947
    Jun  9 10:53:53.948: INFO: Waiting up to 15m0s for pod "adopt-release-lwqbt" in namespace "job-570" to be "adopted"
    Jun  9 10:53:53.959: INFO: Pod "adopt-release-lwqbt": Phase="Running", Reason="", readiness=true. Elapsed: 11.000933ms
    Jun  9 10:53:55.966: INFO: Pod "adopt-release-lwqbt": Phase="Running", Reason="", readiness=true. Elapsed: 2.018277065s
    Jun  9 10:53:55.966: INFO: Pod "adopt-release-lwqbt" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 06/09/23 10:53:55.966
    Jun  9 10:53:56.485: INFO: Successfully updated pod "adopt-release-lwqbt"
    STEP: Checking that the Job releases the Pod 06/09/23 10:53:56.485
    Jun  9 10:53:56.485: INFO: Waiting up to 15m0s for pod "adopt-release-lwqbt" in namespace "job-570" to be "released"
    Jun  9 10:53:56.491: INFO: Pod "adopt-release-lwqbt": Phase="Running", Reason="", readiness=true. Elapsed: 5.585888ms
    Jun  9 10:53:58.499: INFO: Pod "adopt-release-lwqbt": Phase="Running", Reason="", readiness=true. Elapsed: 2.013764466s
    Jun  9 10:53:58.499: INFO: Pod "adopt-release-lwqbt" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:53:58.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-570" for this suite. 06/09/23 10:53:58.508
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:53:58.519
Jun  9 10:53:58.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-runtime 06/09/23 10:53:58.521
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:58.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:58.556
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 06/09/23 10:53:58.561
STEP: wait for the container to reach Succeeded 06/09/23 10:53:58.574
STEP: get the container status 06/09/23 10:54:02.608
STEP: the container should be terminated 06/09/23 10:54:02.615
STEP: the termination message should be set 06/09/23 10:54:02.615
Jun  9 10:54:02.615: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 06/09/23 10:54:02.616
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun  9 10:54:02.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7616" for this suite. 06/09/23 10:54:02.651
------------------------------
• [4.151 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:53:58.519
    Jun  9 10:53:58.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-runtime 06/09/23 10:53:58.521
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:53:58.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:53:58.556
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 06/09/23 10:53:58.561
    STEP: wait for the container to reach Succeeded 06/09/23 10:53:58.574
    STEP: get the container status 06/09/23 10:54:02.608
    STEP: the container should be terminated 06/09/23 10:54:02.615
    STEP: the termination message should be set 06/09/23 10:54:02.615
    Jun  9 10:54:02.615: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 06/09/23 10:54:02.616
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:54:02.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7616" for this suite. 06/09/23 10:54:02.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:54:02.672
Jun  9 10:54:02.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pods 06/09/23 10:54:02.674
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:54:02.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:54:02.725
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 06/09/23 10:54:02.73
Jun  9 10:54:02.747: INFO: created test-pod-1
Jun  9 10:54:02.760: INFO: created test-pod-2
Jun  9 10:54:02.773: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 06/09/23 10:54:02.773
Jun  9 10:54:02.773: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2451' to be running and ready
Jun  9 10:54:02.796: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun  9 10:54:02.796: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun  9 10:54:02.796: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun  9 10:54:02.796: INFO: 0 / 3 pods in namespace 'pods-2451' are running and ready (0 seconds elapsed)
Jun  9 10:54:02.796: INFO: expected 0 pod replicas in namespace 'pods-2451', 0 are Running and Ready.
Jun  9 10:54:02.796: INFO: POD         NODE                                PHASE    GRACE  CONDITIONS
Jun  9 10:54:02.796: INFO: test-pod-1  sks-test-v1-26.4-workergroup-q5bjm  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC  }]
Jun  9 10:54:02.796: INFO: test-pod-2  sks-test-v1-26.4-workergroup-4hkw9  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC  }]
Jun  9 10:54:02.796: INFO: test-pod-3  sks-test-v1-26.4-workergroup-qdprq  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC  }]
Jun  9 10:54:02.796: INFO: 
Jun  9 10:54:04.813: INFO: 3 / 3 pods in namespace 'pods-2451' are running and ready (2 seconds elapsed)
Jun  9 10:54:04.813: INFO: expected 0 pod replicas in namespace 'pods-2451', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 06/09/23 10:54:04.848
Jun  9 10:54:04.853: INFO: Pod quantity 3 is different from expected quantity 0
Jun  9 10:54:05.861: INFO: Pod quantity 3 is different from expected quantity 0
Jun  9 10:54:06.860: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  9 10:54:07.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2451" for this suite. 06/09/23 10:54:07.87
------------------------------
• [SLOW TEST] [5.212 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:54:02.672
    Jun  9 10:54:02.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pods 06/09/23 10:54:02.674
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:54:02.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:54:02.725
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 06/09/23 10:54:02.73
    Jun  9 10:54:02.747: INFO: created test-pod-1
    Jun  9 10:54:02.760: INFO: created test-pod-2
    Jun  9 10:54:02.773: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 06/09/23 10:54:02.773
    Jun  9 10:54:02.773: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2451' to be running and ready
    Jun  9 10:54:02.796: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun  9 10:54:02.796: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun  9 10:54:02.796: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun  9 10:54:02.796: INFO: 0 / 3 pods in namespace 'pods-2451' are running and ready (0 seconds elapsed)
    Jun  9 10:54:02.796: INFO: expected 0 pod replicas in namespace 'pods-2451', 0 are Running and Ready.
    Jun  9 10:54:02.796: INFO: POD         NODE                                PHASE    GRACE  CONDITIONS
    Jun  9 10:54:02.796: INFO: test-pod-1  sks-test-v1-26.4-workergroup-q5bjm  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC  }]
    Jun  9 10:54:02.796: INFO: test-pod-2  sks-test-v1-26.4-workergroup-4hkw9  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC  }]
    Jun  9 10:54:02.796: INFO: test-pod-3  sks-test-v1-26.4-workergroup-qdprq  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-09 10:54:02 +0000 UTC  }]
    Jun  9 10:54:02.796: INFO: 
    Jun  9 10:54:04.813: INFO: 3 / 3 pods in namespace 'pods-2451' are running and ready (2 seconds elapsed)
    Jun  9 10:54:04.813: INFO: expected 0 pod replicas in namespace 'pods-2451', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 06/09/23 10:54:04.848
    Jun  9 10:54:04.853: INFO: Pod quantity 3 is different from expected quantity 0
    Jun  9 10:54:05.861: INFO: Pod quantity 3 is different from expected quantity 0
    Jun  9 10:54:06.860: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:54:07.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2451" for this suite. 06/09/23 10:54:07.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:54:07.885
Jun  9 10:54:07.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:54:07.887
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:54:07.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:54:07.918
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-c2c1ff1b-403b-49ec-b92b-dd100059803a 06/09/23 10:54:07.939
STEP: Creating configMap with name cm-test-opt-upd-fc48bbe5-a6ed-40d1-ba9b-283a62ba8cc7 06/09/23 10:54:07.951
STEP: Creating the pod 06/09/23 10:54:07.96
Jun  9 10:54:07.986: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-efcec2cb-9f36-4bda-aa8b-d421037fa7b6" in namespace "projected-3688" to be "running and ready"
Jun  9 10:54:07.995: INFO: Pod "pod-projected-configmaps-efcec2cb-9f36-4bda-aa8b-d421037fa7b6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.969234ms
Jun  9 10:54:07.995: INFO: The phase of Pod pod-projected-configmaps-efcec2cb-9f36-4bda-aa8b-d421037fa7b6 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:54:10.001: INFO: Pod "pod-projected-configmaps-efcec2cb-9f36-4bda-aa8b-d421037fa7b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.01575146s
Jun  9 10:54:10.001: INFO: The phase of Pod pod-projected-configmaps-efcec2cb-9f36-4bda-aa8b-d421037fa7b6 is Running (Ready = true)
Jun  9 10:54:10.001: INFO: Pod "pod-projected-configmaps-efcec2cb-9f36-4bda-aa8b-d421037fa7b6" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-c2c1ff1b-403b-49ec-b92b-dd100059803a 06/09/23 10:54:10.038
STEP: Updating configmap cm-test-opt-upd-fc48bbe5-a6ed-40d1-ba9b-283a62ba8cc7 06/09/23 10:54:10.047
STEP: Creating configMap with name cm-test-opt-create-8a400c66-ce06-42cb-b350-d7f016015108 06/09/23 10:54:10.056
STEP: waiting to observe update in volume 06/09/23 10:54:10.064
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  9 10:54:12.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3688" for this suite. 06/09/23 10:54:12.122
------------------------------
• [4.248 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:54:07.885
    Jun  9 10:54:07.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:54:07.887
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:54:07.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:54:07.918
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-c2c1ff1b-403b-49ec-b92b-dd100059803a 06/09/23 10:54:07.939
    STEP: Creating configMap with name cm-test-opt-upd-fc48bbe5-a6ed-40d1-ba9b-283a62ba8cc7 06/09/23 10:54:07.951
    STEP: Creating the pod 06/09/23 10:54:07.96
    Jun  9 10:54:07.986: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-efcec2cb-9f36-4bda-aa8b-d421037fa7b6" in namespace "projected-3688" to be "running and ready"
    Jun  9 10:54:07.995: INFO: Pod "pod-projected-configmaps-efcec2cb-9f36-4bda-aa8b-d421037fa7b6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.969234ms
    Jun  9 10:54:07.995: INFO: The phase of Pod pod-projected-configmaps-efcec2cb-9f36-4bda-aa8b-d421037fa7b6 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:54:10.001: INFO: Pod "pod-projected-configmaps-efcec2cb-9f36-4bda-aa8b-d421037fa7b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.01575146s
    Jun  9 10:54:10.001: INFO: The phase of Pod pod-projected-configmaps-efcec2cb-9f36-4bda-aa8b-d421037fa7b6 is Running (Ready = true)
    Jun  9 10:54:10.001: INFO: Pod "pod-projected-configmaps-efcec2cb-9f36-4bda-aa8b-d421037fa7b6" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-c2c1ff1b-403b-49ec-b92b-dd100059803a 06/09/23 10:54:10.038
    STEP: Updating configmap cm-test-opt-upd-fc48bbe5-a6ed-40d1-ba9b-283a62ba8cc7 06/09/23 10:54:10.047
    STEP: Creating configMap with name cm-test-opt-create-8a400c66-ce06-42cb-b350-d7f016015108 06/09/23 10:54:10.056
    STEP: waiting to observe update in volume 06/09/23 10:54:10.064
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:54:12.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3688" for this suite. 06/09/23 10:54:12.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:54:12.134
Jun  9 10:54:12.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 10:54:12.136
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:54:12.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:54:12.162
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 10:54:12.196
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:54:12.883
STEP: Deploying the webhook pod 06/09/23 10:54:12.898
STEP: Wait for the deployment to be ready 06/09/23 10:54:12.916
Jun  9 10:54:12.927: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 10:54:14.945
STEP: Verifying the service has paired with the endpoint 06/09/23 10:54:14.973
Jun  9 10:54:15.974: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 06/09/23 10:54:15.982
STEP: Registering slow webhook via the AdmissionRegistration API 06/09/23 10:54:15.982
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 06/09/23 10:54:16.022
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 06/09/23 10:54:17.055
STEP: Registering slow webhook via the AdmissionRegistration API 06/09/23 10:54:17.055
STEP: Having no error when timeout is longer than webhook latency 06/09/23 10:54:18.142
STEP: Registering slow webhook via the AdmissionRegistration API 06/09/23 10:54:18.142
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 06/09/23 10:54:23.194
STEP: Registering slow webhook via the AdmissionRegistration API 06/09/23 10:54:23.195
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:54:28.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6302" for this suite. 06/09/23 10:54:28.317
STEP: Destroying namespace "webhook-6302-markers" for this suite. 06/09/23 10:54:28.331
------------------------------
• [SLOW TEST] [16.215 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:54:12.134
    Jun  9 10:54:12.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 10:54:12.136
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:54:12.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:54:12.162
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 10:54:12.196
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:54:12.883
    STEP: Deploying the webhook pod 06/09/23 10:54:12.898
    STEP: Wait for the deployment to be ready 06/09/23 10:54:12.916
    Jun  9 10:54:12.927: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 10:54:14.945
    STEP: Verifying the service has paired with the endpoint 06/09/23 10:54:14.973
    Jun  9 10:54:15.974: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 06/09/23 10:54:15.982
    STEP: Registering slow webhook via the AdmissionRegistration API 06/09/23 10:54:15.982
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 06/09/23 10:54:16.022
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 06/09/23 10:54:17.055
    STEP: Registering slow webhook via the AdmissionRegistration API 06/09/23 10:54:17.055
    STEP: Having no error when timeout is longer than webhook latency 06/09/23 10:54:18.142
    STEP: Registering slow webhook via the AdmissionRegistration API 06/09/23 10:54:18.142
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 06/09/23 10:54:23.194
    STEP: Registering slow webhook via the AdmissionRegistration API 06/09/23 10:54:23.195
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:54:28.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6302" for this suite. 06/09/23 10:54:28.317
    STEP: Destroying namespace "webhook-6302-markers" for this suite. 06/09/23 10:54:28.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:54:28.35
Jun  9 10:54:28.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:54:28.351
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:54:28.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:54:28.395
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-a27881b9-f4f5-4a4e-8b96-55851e6aa10f 06/09/23 10:54:28.403
STEP: Creating a pod to test consume secrets 06/09/23 10:54:28.414
Jun  9 10:54:28.431: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6" in namespace "projected-4430" to be "Succeeded or Failed"
Jun  9 10:54:28.441: INFO: Pod "pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.095741ms
Jun  9 10:54:30.469: INFO: Pod "pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038160846s
Jun  9 10:54:32.460: INFO: Pod "pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029538513s
STEP: Saw pod success 06/09/23 10:54:32.46
Jun  9 10:54:32.461: INFO: Pod "pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6" satisfied condition "Succeeded or Failed"
Jun  9 10:54:32.467: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6 container secret-volume-test: <nil>
STEP: delete the pod 06/09/23 10:54:32.485
Jun  9 10:54:32.505: INFO: Waiting for pod pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6 to disappear
Jun  9 10:54:32.511: INFO: Pod pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun  9 10:54:32.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4430" for this suite. 06/09/23 10:54:32.521
------------------------------
• [4.185 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:54:28.35
    Jun  9 10:54:28.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:54:28.351
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:54:28.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:54:28.395
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-a27881b9-f4f5-4a4e-8b96-55851e6aa10f 06/09/23 10:54:28.403
    STEP: Creating a pod to test consume secrets 06/09/23 10:54:28.414
    Jun  9 10:54:28.431: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6" in namespace "projected-4430" to be "Succeeded or Failed"
    Jun  9 10:54:28.441: INFO: Pod "pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.095741ms
    Jun  9 10:54:30.469: INFO: Pod "pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038160846s
    Jun  9 10:54:32.460: INFO: Pod "pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029538513s
    STEP: Saw pod success 06/09/23 10:54:32.46
    Jun  9 10:54:32.461: INFO: Pod "pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6" satisfied condition "Succeeded or Failed"
    Jun  9 10:54:32.467: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6 container secret-volume-test: <nil>
    STEP: delete the pod 06/09/23 10:54:32.485
    Jun  9 10:54:32.505: INFO: Waiting for pod pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6 to disappear
    Jun  9 10:54:32.511: INFO: Pod pod-projected-secrets-0cfd1334-e213-4e64-ac45-de916eb279f6 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:54:32.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4430" for this suite. 06/09/23 10:54:32.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:54:32.536
Jun  9 10:54:32.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename init-container 06/09/23 10:54:32.538
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:54:32.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:54:32.572
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 06/09/23 10:54:32.579
Jun  9 10:54:32.580: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:54:38.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7941" for this suite. 06/09/23 10:54:38.538
------------------------------
• [SLOW TEST] [6.013 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:54:32.536
    Jun  9 10:54:32.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename init-container 06/09/23 10:54:32.538
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:54:32.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:54:32.572
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 06/09/23 10:54:32.579
    Jun  9 10:54:32.580: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:54:38.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7941" for this suite. 06/09/23 10:54:38.538
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:54:38.55
Jun  9 10:54:38.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename runtimeclass 06/09/23 10:54:38.552
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:54:38.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:54:38.583
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jun  9 10:54:38.612: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4343 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun  9 10:54:38.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4343" for this suite. 06/09/23 10:54:38.64
------------------------------
• [0.102 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:54:38.55
    Jun  9 10:54:38.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename runtimeclass 06/09/23 10:54:38.552
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:54:38.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:54:38.583
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jun  9 10:54:38.612: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4343 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:54:38.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4343" for this suite. 06/09/23 10:54:38.64
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:54:38.653
Jun  9 10:54:38.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename job 06/09/23 10:54:38.654
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:54:38.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:54:38.682
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 06/09/23 10:54:38.688
STEP: Ensuring active pods == parallelism 06/09/23 10:54:38.699
STEP: delete a job 06/09/23 10:54:40.706
STEP: deleting Job.batch foo in namespace job-1899, will wait for the garbage collector to delete the pods 06/09/23 10:54:40.706
Jun  9 10:54:40.775: INFO: Deleting Job.batch foo took: 10.100055ms
Jun  9 10:54:40.876: INFO: Terminating Job.batch foo pods took: 100.911126ms
STEP: Ensuring job was deleted 06/09/23 10:55:13.577
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun  9 10:55:13.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1899" for this suite. 06/09/23 10:55:13.594
------------------------------
• [SLOW TEST] [34.954 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:54:38.653
    Jun  9 10:54:38.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename job 06/09/23 10:54:38.654
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:54:38.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:54:38.682
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 06/09/23 10:54:38.688
    STEP: Ensuring active pods == parallelism 06/09/23 10:54:38.699
    STEP: delete a job 06/09/23 10:54:40.706
    STEP: deleting Job.batch foo in namespace job-1899, will wait for the garbage collector to delete the pods 06/09/23 10:54:40.706
    Jun  9 10:54:40.775: INFO: Deleting Job.batch foo took: 10.100055ms
    Jun  9 10:54:40.876: INFO: Terminating Job.batch foo pods took: 100.911126ms
    STEP: Ensuring job was deleted 06/09/23 10:55:13.577
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:55:13.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1899" for this suite. 06/09/23 10:55:13.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:55:13.608
Jun  9 10:55:13.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 10:55:13.61
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:13.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:13.648
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 10:55:13.687
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:55:14.278
STEP: Deploying the webhook pod 06/09/23 10:55:14.29
STEP: Wait for the deployment to be ready 06/09/23 10:55:14.314
Jun  9 10:55:14.333: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 10:55:16.355
STEP: Verifying the service has paired with the endpoint 06/09/23 10:55:16.388
Jun  9 10:55:17.388: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jun  9 10:55:17.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5773-crds.webhook.example.com via the AdmissionRegistration API 06/09/23 10:55:17.911
STEP: Creating a custom resource that should be mutated by the webhook 06/09/23 10:55:17.964
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:55:20.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7883" for this suite. 06/09/23 10:55:20.696
STEP: Destroying namespace "webhook-7883-markers" for this suite. 06/09/23 10:55:20.729
------------------------------
• [SLOW TEST] [7.131 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:55:13.608
    Jun  9 10:55:13.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 10:55:13.61
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:13.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:13.648
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 10:55:13.687
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 10:55:14.278
    STEP: Deploying the webhook pod 06/09/23 10:55:14.29
    STEP: Wait for the deployment to be ready 06/09/23 10:55:14.314
    Jun  9 10:55:14.333: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 10:55:16.355
    STEP: Verifying the service has paired with the endpoint 06/09/23 10:55:16.388
    Jun  9 10:55:17.388: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jun  9 10:55:17.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5773-crds.webhook.example.com via the AdmissionRegistration API 06/09/23 10:55:17.911
    STEP: Creating a custom resource that should be mutated by the webhook 06/09/23 10:55:17.964
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:55:20.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7883" for this suite. 06/09/23 10:55:20.696
    STEP: Destroying namespace "webhook-7883-markers" for this suite. 06/09/23 10:55:20.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:55:20.74
Jun  9 10:55:20.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename runtimeclass 06/09/23 10:55:20.741
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:20.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:20.799
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 06/09/23 10:55:20.804
STEP: getting /apis/node.k8s.io 06/09/23 10:55:20.808
STEP: getting /apis/node.k8s.io/v1 06/09/23 10:55:20.81
STEP: creating 06/09/23 10:55:20.812
STEP: watching 06/09/23 10:55:20.857
Jun  9 10:55:20.857: INFO: starting watch
STEP: getting 06/09/23 10:55:20.877
STEP: listing 06/09/23 10:55:20.883
STEP: patching 06/09/23 10:55:20.889
STEP: updating 06/09/23 10:55:20.901
Jun  9 10:55:20.927: INFO: waiting for watch events with expected annotations
STEP: deleting 06/09/23 10:55:20.927
STEP: deleting a collection 06/09/23 10:55:20.952
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun  9 10:55:20.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4605" for this suite. 06/09/23 10:55:20.998
------------------------------
• [0.273 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:55:20.74
    Jun  9 10:55:20.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename runtimeclass 06/09/23 10:55:20.741
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:20.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:20.799
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 06/09/23 10:55:20.804
    STEP: getting /apis/node.k8s.io 06/09/23 10:55:20.808
    STEP: getting /apis/node.k8s.io/v1 06/09/23 10:55:20.81
    STEP: creating 06/09/23 10:55:20.812
    STEP: watching 06/09/23 10:55:20.857
    Jun  9 10:55:20.857: INFO: starting watch
    STEP: getting 06/09/23 10:55:20.877
    STEP: listing 06/09/23 10:55:20.883
    STEP: patching 06/09/23 10:55:20.889
    STEP: updating 06/09/23 10:55:20.901
    Jun  9 10:55:20.927: INFO: waiting for watch events with expected annotations
    STEP: deleting 06/09/23 10:55:20.927
    STEP: deleting a collection 06/09/23 10:55:20.952
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:55:20.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4605" for this suite. 06/09/23 10:55:20.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:55:21.014
Jun  9 10:55:21.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 10:55:21.017
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:21.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:21.045
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-9309d20a-3932-4078-b583-0949c41f256a 06/09/23 10:55:21.049
STEP: Creating a pod to test consume configMaps 06/09/23 10:55:21.066
Jun  9 10:55:21.081: INFO: Waiting up to 5m0s for pod "pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3" in namespace "configmap-3618" to be "Succeeded or Failed"
Jun  9 10:55:21.103: INFO: Pod "pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3": Phase="Pending", Reason="", readiness=false. Elapsed: 21.836108ms
Jun  9 10:55:23.109: INFO: Pod "pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027745837s
Jun  9 10:55:25.109: INFO: Pod "pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027837622s
STEP: Saw pod success 06/09/23 10:55:25.109
Jun  9 10:55:25.109: INFO: Pod "pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3" satisfied condition "Succeeded or Failed"
Jun  9 10:55:25.115: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3 container agnhost-container: <nil>
STEP: delete the pod 06/09/23 10:55:25.138
Jun  9 10:55:25.156: INFO: Waiting for pod pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3 to disappear
Jun  9 10:55:25.163: INFO: Pod pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 10:55:25.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3618" for this suite. 06/09/23 10:55:25.172
------------------------------
• [4.168 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:55:21.014
    Jun  9 10:55:21.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 10:55:21.017
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:21.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:21.045
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-9309d20a-3932-4078-b583-0949c41f256a 06/09/23 10:55:21.049
    STEP: Creating a pod to test consume configMaps 06/09/23 10:55:21.066
    Jun  9 10:55:21.081: INFO: Waiting up to 5m0s for pod "pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3" in namespace "configmap-3618" to be "Succeeded or Failed"
    Jun  9 10:55:21.103: INFO: Pod "pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3": Phase="Pending", Reason="", readiness=false. Elapsed: 21.836108ms
    Jun  9 10:55:23.109: INFO: Pod "pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027745837s
    Jun  9 10:55:25.109: INFO: Pod "pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027837622s
    STEP: Saw pod success 06/09/23 10:55:25.109
    Jun  9 10:55:25.109: INFO: Pod "pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3" satisfied condition "Succeeded or Failed"
    Jun  9 10:55:25.115: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3 container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 10:55:25.138
    Jun  9 10:55:25.156: INFO: Waiting for pod pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3 to disappear
    Jun  9 10:55:25.163: INFO: Pod pod-configmaps-ad7461bd-c36b-4b69-a4b9-43d7dc21cfb3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:55:25.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3618" for this suite. 06/09/23 10:55:25.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:55:25.183
Jun  9 10:55:25.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename watch 06/09/23 10:55:25.185
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:25.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:25.214
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 06/09/23 10:55:25.219
STEP: creating a watch on configmaps with label B 06/09/23 10:55:25.221
STEP: creating a watch on configmaps with label A or B 06/09/23 10:55:25.224
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 06/09/23 10:55:25.226
Jun  9 10:55:25.239: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90588 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  9 10:55:25.240: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90588 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 06/09/23 10:55:25.24
Jun  9 10:55:25.254: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90589 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  9 10:55:25.254: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90589 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 06/09/23 10:55:25.254
Jun  9 10:55:25.268: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90590 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  9 10:55:25.268: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90590 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 06/09/23 10:55:25.268
Jun  9 10:55:25.284: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90591 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  9 10:55:25.284: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90591 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 06/09/23 10:55:25.284
Jun  9 10:55:25.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7063  5821e5ff-c368-490b-bd7e-87f08bc11dab 90592 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  9 10:55:25.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7063  5821e5ff-c368-490b-bd7e-87f08bc11dab 90592 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 06/09/23 10:55:35.297
Jun  9 10:55:35.328: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7063  5821e5ff-c368-490b-bd7e-87f08bc11dab 90664 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  9 10:55:35.328: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7063  5821e5ff-c368-490b-bd7e-87f08bc11dab 90664 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun  9 10:55:45.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7063" for this suite. 06/09/23 10:55:45.347
------------------------------
• [SLOW TEST] [20.175 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:55:25.183
    Jun  9 10:55:25.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename watch 06/09/23 10:55:25.185
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:25.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:25.214
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 06/09/23 10:55:25.219
    STEP: creating a watch on configmaps with label B 06/09/23 10:55:25.221
    STEP: creating a watch on configmaps with label A or B 06/09/23 10:55:25.224
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 06/09/23 10:55:25.226
    Jun  9 10:55:25.239: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90588 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  9 10:55:25.240: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90588 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 06/09/23 10:55:25.24
    Jun  9 10:55:25.254: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90589 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  9 10:55:25.254: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90589 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 06/09/23 10:55:25.254
    Jun  9 10:55:25.268: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90590 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  9 10:55:25.268: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90590 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 06/09/23 10:55:25.268
    Jun  9 10:55:25.284: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90591 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  9 10:55:25.284: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7063  805ffb21-7a43-48b4-b43d-45415ce48aeb 90591 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 06/09/23 10:55:25.284
    Jun  9 10:55:25.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7063  5821e5ff-c368-490b-bd7e-87f08bc11dab 90592 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  9 10:55:25.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7063  5821e5ff-c368-490b-bd7e-87f08bc11dab 90592 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 06/09/23 10:55:35.297
    Jun  9 10:55:35.328: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7063  5821e5ff-c368-490b-bd7e-87f08bc11dab 90664 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  9 10:55:35.328: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7063  5821e5ff-c368-490b-bd7e-87f08bc11dab 90664 0 2023-06-09 10:55:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-09 10:55:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:55:45.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7063" for this suite. 06/09/23 10:55:45.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:55:45.362
Jun  9 10:55:45.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 10:55:45.364
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:45.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:45.39
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 06/09/23 10:55:45.395
Jun  9 10:55:45.411: INFO: Waiting up to 5m0s for pod "downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec" in namespace "downward-api-3341" to be "Succeeded or Failed"
Jun  9 10:55:45.418: INFO: Pod "downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.873018ms
Jun  9 10:55:47.425: INFO: Pod "downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014068948s
Jun  9 10:55:49.426: INFO: Pod "downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014643018s
STEP: Saw pod success 06/09/23 10:55:49.426
Jun  9 10:55:49.426: INFO: Pod "downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec" satisfied condition "Succeeded or Failed"
Jun  9 10:55:49.432: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec container dapi-container: <nil>
STEP: delete the pod 06/09/23 10:55:49.442
Jun  9 10:55:49.459: INFO: Waiting for pod downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec to disappear
Jun  9 10:55:49.465: INFO: Pod downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun  9 10:55:49.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3341" for this suite. 06/09/23 10:55:49.472
------------------------------
• [4.121 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:55:45.362
    Jun  9 10:55:45.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 10:55:45.364
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:45.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:45.39
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 06/09/23 10:55:45.395
    Jun  9 10:55:45.411: INFO: Waiting up to 5m0s for pod "downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec" in namespace "downward-api-3341" to be "Succeeded or Failed"
    Jun  9 10:55:45.418: INFO: Pod "downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.873018ms
    Jun  9 10:55:47.425: INFO: Pod "downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014068948s
    Jun  9 10:55:49.426: INFO: Pod "downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014643018s
    STEP: Saw pod success 06/09/23 10:55:49.426
    Jun  9 10:55:49.426: INFO: Pod "downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec" satisfied condition "Succeeded or Failed"
    Jun  9 10:55:49.432: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec container dapi-container: <nil>
    STEP: delete the pod 06/09/23 10:55:49.442
    Jun  9 10:55:49.459: INFO: Waiting for pod downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec to disappear
    Jun  9 10:55:49.465: INFO: Pod downward-api-0475541f-ca18-4de9-92a2-a8d4cc8e9cec no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:55:49.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3341" for this suite. 06/09/23 10:55:49.472
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:55:49.483
Jun  9 10:55:49.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 10:55:49.484
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:49.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:49.511
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 06/09/23 10:55:49.515
Jun  9 10:55:49.527: INFO: Waiting up to 5m0s for pod "pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b" in namespace "emptydir-8762" to be "Succeeded or Failed"
Jun  9 10:55:49.534: INFO: Pod "pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.546701ms
Jun  9 10:55:51.541: INFO: Pod "pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013557588s
Jun  9 10:55:53.542: INFO: Pod "pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014794812s
STEP: Saw pod success 06/09/23 10:55:53.542
Jun  9 10:55:53.542: INFO: Pod "pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b" satisfied condition "Succeeded or Failed"
Jun  9 10:55:53.549: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b container test-container: <nil>
STEP: delete the pod 06/09/23 10:55:53.558
Jun  9 10:55:53.571: INFO: Waiting for pod pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b to disappear
Jun  9 10:55:53.576: INFO: Pod pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 10:55:53.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8762" for this suite. 06/09/23 10:55:53.583
------------------------------
• [4.111 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:55:49.483
    Jun  9 10:55:49.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 10:55:49.484
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:49.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:49.511
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 06/09/23 10:55:49.515
    Jun  9 10:55:49.527: INFO: Waiting up to 5m0s for pod "pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b" in namespace "emptydir-8762" to be "Succeeded or Failed"
    Jun  9 10:55:49.534: INFO: Pod "pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.546701ms
    Jun  9 10:55:51.541: INFO: Pod "pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013557588s
    Jun  9 10:55:53.542: INFO: Pod "pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014794812s
    STEP: Saw pod success 06/09/23 10:55:53.542
    Jun  9 10:55:53.542: INFO: Pod "pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b" satisfied condition "Succeeded or Failed"
    Jun  9 10:55:53.549: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b container test-container: <nil>
    STEP: delete the pod 06/09/23 10:55:53.558
    Jun  9 10:55:53.571: INFO: Waiting for pod pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b to disappear
    Jun  9 10:55:53.576: INFO: Pod pod-f5dd930c-9a6b-48c4-92d9-f8182712f56b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:55:53.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8762" for this suite. 06/09/23 10:55:53.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:55:53.595
Jun  9 10:55:53.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename var-expansion 06/09/23 10:55:53.596
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:53.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:53.622
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 06/09/23 10:55:53.627
Jun  9 10:55:53.643: INFO: Waiting up to 5m0s for pod "var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba" in namespace "var-expansion-4879" to be "Succeeded or Failed"
Jun  9 10:55:53.654: INFO: Pod "var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba": Phase="Pending", Reason="", readiness=false. Elapsed: 10.426825ms
Jun  9 10:55:55.664: INFO: Pod "var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020948042s
Jun  9 10:55:57.661: INFO: Pod "var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018107739s
STEP: Saw pod success 06/09/23 10:55:57.662
Jun  9 10:55:57.662: INFO: Pod "var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba" satisfied condition "Succeeded or Failed"
Jun  9 10:55:57.668: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba container dapi-container: <nil>
STEP: delete the pod 06/09/23 10:55:57.68
Jun  9 10:55:57.699: INFO: Waiting for pod var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba to disappear
Jun  9 10:55:57.712: INFO: Pod var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  9 10:55:57.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4879" for this suite. 06/09/23 10:55:57.721
------------------------------
• [4.138 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:55:53.595
    Jun  9 10:55:53.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename var-expansion 06/09/23 10:55:53.596
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:53.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:53.622
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 06/09/23 10:55:53.627
    Jun  9 10:55:53.643: INFO: Waiting up to 5m0s for pod "var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba" in namespace "var-expansion-4879" to be "Succeeded or Failed"
    Jun  9 10:55:53.654: INFO: Pod "var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba": Phase="Pending", Reason="", readiness=false. Elapsed: 10.426825ms
    Jun  9 10:55:55.664: INFO: Pod "var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020948042s
    Jun  9 10:55:57.661: INFO: Pod "var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018107739s
    STEP: Saw pod success 06/09/23 10:55:57.662
    Jun  9 10:55:57.662: INFO: Pod "var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba" satisfied condition "Succeeded or Failed"
    Jun  9 10:55:57.668: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba container dapi-container: <nil>
    STEP: delete the pod 06/09/23 10:55:57.68
    Jun  9 10:55:57.699: INFO: Waiting for pod var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba to disappear
    Jun  9 10:55:57.712: INFO: Pod var-expansion-ca5305e8-4139-4567-8f86-ddb0665136ba no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:55:57.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4879" for this suite. 06/09/23 10:55:57.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:55:57.734
Jun  9 10:55:57.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 10:55:57.735
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:57.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:57.789
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 06/09/23 10:55:57.795
Jun  9 10:55:57.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 10:56:00.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:56:09.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3289" for this suite. 06/09/23 10:56:09.687
------------------------------
• [SLOW TEST] [11.961 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:55:57.734
    Jun  9 10:55:57.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 10:55:57.735
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:55:57.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:55:57.789
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 06/09/23 10:55:57.795
    Jun  9 10:55:57.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 10:56:00.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:56:09.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3289" for this suite. 06/09/23 10:56:09.687
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:56:09.696
Jun  9 10:56:09.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename resourcequota 06/09/23 10:56:09.697
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:56:09.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:56:09.72
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 06/09/23 10:56:09.723
STEP: Ensuring ResourceQuota status is calculated 06/09/23 10:56:09.73
STEP: Creating a ResourceQuota with not best effort scope 06/09/23 10:56:11.736
STEP: Ensuring ResourceQuota status is calculated 06/09/23 10:56:11.744
STEP: Creating a best-effort pod 06/09/23 10:56:13.75
STEP: Ensuring resource quota with best effort scope captures the pod usage 06/09/23 10:56:13.769
STEP: Ensuring resource quota with not best effort ignored the pod usage 06/09/23 10:56:15.777
STEP: Deleting the pod 06/09/23 10:56:17.784
STEP: Ensuring resource quota status released the pod usage 06/09/23 10:56:17.832
STEP: Creating a not best-effort pod 06/09/23 10:56:19.84
STEP: Ensuring resource quota with not best effort scope captures the pod usage 06/09/23 10:56:19.854
STEP: Ensuring resource quota with best effort scope ignored the pod usage 06/09/23 10:56:21.861
STEP: Deleting the pod 06/09/23 10:56:23.879
STEP: Ensuring resource quota status released the pod usage 06/09/23 10:56:23.908
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  9 10:56:25.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4059" for this suite. 06/09/23 10:56:25.924
------------------------------
• [SLOW TEST] [16.242 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:56:09.696
    Jun  9 10:56:09.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename resourcequota 06/09/23 10:56:09.697
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:56:09.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:56:09.72
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 06/09/23 10:56:09.723
    STEP: Ensuring ResourceQuota status is calculated 06/09/23 10:56:09.73
    STEP: Creating a ResourceQuota with not best effort scope 06/09/23 10:56:11.736
    STEP: Ensuring ResourceQuota status is calculated 06/09/23 10:56:11.744
    STEP: Creating a best-effort pod 06/09/23 10:56:13.75
    STEP: Ensuring resource quota with best effort scope captures the pod usage 06/09/23 10:56:13.769
    STEP: Ensuring resource quota with not best effort ignored the pod usage 06/09/23 10:56:15.777
    STEP: Deleting the pod 06/09/23 10:56:17.784
    STEP: Ensuring resource quota status released the pod usage 06/09/23 10:56:17.832
    STEP: Creating a not best-effort pod 06/09/23 10:56:19.84
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 06/09/23 10:56:19.854
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 06/09/23 10:56:21.861
    STEP: Deleting the pod 06/09/23 10:56:23.879
    STEP: Ensuring resource quota status released the pod usage 06/09/23 10:56:23.908
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:56:25.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4059" for this suite. 06/09/23 10:56:25.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:56:25.939
Jun  9 10:56:25.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 10:56:25.941
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:56:25.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:56:25.967
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 06/09/23 10:56:25.975
Jun  9 10:56:25.996: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789" in namespace "downward-api-8003" to be "Succeeded or Failed"
Jun  9 10:56:26.002: INFO: Pod "downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789": Phase="Pending", Reason="", readiness=false. Elapsed: 5.757107ms
Jun  9 10:56:28.009: INFO: Pod "downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012764743s
Jun  9 10:56:30.008: INFO: Pod "downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012560325s
STEP: Saw pod success 06/09/23 10:56:30.008
Jun  9 10:56:30.008: INFO: Pod "downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789" satisfied condition "Succeeded or Failed"
Jun  9 10:56:30.013: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789 container client-container: <nil>
STEP: delete the pod 06/09/23 10:56:30.032
Jun  9 10:56:30.048: INFO: Waiting for pod downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789 to disappear
Jun  9 10:56:30.053: INFO: Pod downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  9 10:56:30.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8003" for this suite. 06/09/23 10:56:30.061
------------------------------
• [4.133 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:56:25.939
    Jun  9 10:56:25.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 10:56:25.941
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:56:25.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:56:25.967
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 06/09/23 10:56:25.975
    Jun  9 10:56:25.996: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789" in namespace "downward-api-8003" to be "Succeeded or Failed"
    Jun  9 10:56:26.002: INFO: Pod "downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789": Phase="Pending", Reason="", readiness=false. Elapsed: 5.757107ms
    Jun  9 10:56:28.009: INFO: Pod "downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012764743s
    Jun  9 10:56:30.008: INFO: Pod "downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012560325s
    STEP: Saw pod success 06/09/23 10:56:30.008
    Jun  9 10:56:30.008: INFO: Pod "downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789" satisfied condition "Succeeded or Failed"
    Jun  9 10:56:30.013: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789 container client-container: <nil>
    STEP: delete the pod 06/09/23 10:56:30.032
    Jun  9 10:56:30.048: INFO: Waiting for pod downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789 to disappear
    Jun  9 10:56:30.053: INFO: Pod downwardapi-volume-b28e381a-948c-4267-9a6a-b0d831546789 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:56:30.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8003" for this suite. 06/09/23 10:56:30.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:56:30.072
Jun  9 10:56:30.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:56:30.073
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:56:30.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:56:30.104
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 06/09/23 10:56:30.111
Jun  9 10:56:30.128: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13" in namespace "projected-1796" to be "Succeeded or Failed"
Jun  9 10:56:30.136: INFO: Pod "downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13": Phase="Pending", Reason="", readiness=false. Elapsed: 7.999019ms
Jun  9 10:56:32.143: INFO: Pod "downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014929979s
Jun  9 10:56:34.143: INFO: Pod "downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014936044s
STEP: Saw pod success 06/09/23 10:56:34.143
Jun  9 10:56:34.143: INFO: Pod "downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13" satisfied condition "Succeeded or Failed"
Jun  9 10:56:34.150: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13 container client-container: <nil>
STEP: delete the pod 06/09/23 10:56:34.158
Jun  9 10:56:34.180: INFO: Waiting for pod downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13 to disappear
Jun  9 10:56:34.186: INFO: Pod downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  9 10:56:34.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1796" for this suite. 06/09/23 10:56:34.196
------------------------------
• [4.135 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:56:30.072
    Jun  9 10:56:30.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:56:30.073
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:56:30.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:56:30.104
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 06/09/23 10:56:30.111
    Jun  9 10:56:30.128: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13" in namespace "projected-1796" to be "Succeeded or Failed"
    Jun  9 10:56:30.136: INFO: Pod "downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13": Phase="Pending", Reason="", readiness=false. Elapsed: 7.999019ms
    Jun  9 10:56:32.143: INFO: Pod "downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014929979s
    Jun  9 10:56:34.143: INFO: Pod "downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014936044s
    STEP: Saw pod success 06/09/23 10:56:34.143
    Jun  9 10:56:34.143: INFO: Pod "downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13" satisfied condition "Succeeded or Failed"
    Jun  9 10:56:34.150: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13 container client-container: <nil>
    STEP: delete the pod 06/09/23 10:56:34.158
    Jun  9 10:56:34.180: INFO: Waiting for pod downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13 to disappear
    Jun  9 10:56:34.186: INFO: Pod downwardapi-volume-3d3edf75-6459-4376-8e3b-22aae4a0cb13 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:56:34.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1796" for this suite. 06/09/23 10:56:34.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:56:34.208
Jun  9 10:56:34.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename var-expansion 06/09/23 10:56:34.21
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:56:34.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:56:34.234
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Jun  9 10:56:34.297: INFO: Waiting up to 2m0s for pod "var-expansion-270c2d2e-1893-4378-ac97-2b723a9e3eb2" in namespace "var-expansion-443" to be "container 0 failed with reason CreateContainerConfigError"
Jun  9 10:56:34.303: INFO: Pod "var-expansion-270c2d2e-1893-4378-ac97-2b723a9e3eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.158916ms
Jun  9 10:56:36.313: INFO: Pod "var-expansion-270c2d2e-1893-4378-ac97-2b723a9e3eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016447624s
Jun  9 10:56:36.313: INFO: Pod "var-expansion-270c2d2e-1893-4378-ac97-2b723a9e3eb2" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jun  9 10:56:36.313: INFO: Deleting pod "var-expansion-270c2d2e-1893-4378-ac97-2b723a9e3eb2" in namespace "var-expansion-443"
Jun  9 10:56:36.328: INFO: Wait up to 5m0s for pod "var-expansion-270c2d2e-1893-4378-ac97-2b723a9e3eb2" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  9 10:56:40.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-443" for this suite. 06/09/23 10:56:40.351
------------------------------
• [SLOW TEST] [6.153 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:56:34.208
    Jun  9 10:56:34.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename var-expansion 06/09/23 10:56:34.21
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:56:34.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:56:34.234
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Jun  9 10:56:34.297: INFO: Waiting up to 2m0s for pod "var-expansion-270c2d2e-1893-4378-ac97-2b723a9e3eb2" in namespace "var-expansion-443" to be "container 0 failed with reason CreateContainerConfigError"
    Jun  9 10:56:34.303: INFO: Pod "var-expansion-270c2d2e-1893-4378-ac97-2b723a9e3eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.158916ms
    Jun  9 10:56:36.313: INFO: Pod "var-expansion-270c2d2e-1893-4378-ac97-2b723a9e3eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016447624s
    Jun  9 10:56:36.313: INFO: Pod "var-expansion-270c2d2e-1893-4378-ac97-2b723a9e3eb2" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jun  9 10:56:36.313: INFO: Deleting pod "var-expansion-270c2d2e-1893-4378-ac97-2b723a9e3eb2" in namespace "var-expansion-443"
    Jun  9 10:56:36.328: INFO: Wait up to 5m0s for pod "var-expansion-270c2d2e-1893-4378-ac97-2b723a9e3eb2" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:56:40.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-443" for this suite. 06/09/23 10:56:40.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:56:40.364
Jun  9 10:56:40.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:56:40.365
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:56:40.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:56:40.386
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-07dfc713-a79b-4237-aa70-309d336fffbd 06/09/23 10:56:40.391
STEP: Creating a pod to test consume secrets 06/09/23 10:56:40.4
Jun  9 10:56:40.412: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd" in namespace "projected-119" to be "Succeeded or Failed"
Jun  9 10:56:40.418: INFO: Pod "pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.301374ms
Jun  9 10:56:42.428: INFO: Pod "pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015620164s
Jun  9 10:56:44.438: INFO: Pod "pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025940454s
STEP: Saw pod success 06/09/23 10:56:44.438
Jun  9 10:56:44.438: INFO: Pod "pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd" satisfied condition "Succeeded or Failed"
Jun  9 10:56:44.454: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd container projected-secret-volume-test: <nil>
STEP: delete the pod 06/09/23 10:56:44.471
Jun  9 10:56:44.488: INFO: Waiting for pod pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd to disappear
Jun  9 10:56:44.493: INFO: Pod pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun  9 10:56:44.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-119" for this suite. 06/09/23 10:56:44.503
------------------------------
• [4.151 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:56:40.364
    Jun  9 10:56:40.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:56:40.365
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:56:40.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:56:40.386
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-07dfc713-a79b-4237-aa70-309d336fffbd 06/09/23 10:56:40.391
    STEP: Creating a pod to test consume secrets 06/09/23 10:56:40.4
    Jun  9 10:56:40.412: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd" in namespace "projected-119" to be "Succeeded or Failed"
    Jun  9 10:56:40.418: INFO: Pod "pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.301374ms
    Jun  9 10:56:42.428: INFO: Pod "pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015620164s
    Jun  9 10:56:44.438: INFO: Pod "pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025940454s
    STEP: Saw pod success 06/09/23 10:56:44.438
    Jun  9 10:56:44.438: INFO: Pod "pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd" satisfied condition "Succeeded or Failed"
    Jun  9 10:56:44.454: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/09/23 10:56:44.471
    Jun  9 10:56:44.488: INFO: Waiting for pod pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd to disappear
    Jun  9 10:56:44.493: INFO: Pod pod-projected-secrets-64406f3d-8680-4703-a9d6-2d59e57b01fd no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:56:44.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-119" for this suite. 06/09/23 10:56:44.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:56:44.516
Jun  9 10:56:44.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename resourcequota 06/09/23 10:56:44.518
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:56:44.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:56:44.546
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 06/09/23 10:56:44.553
STEP: Creating a ResourceQuota 06/09/23 10:56:49.561
STEP: Ensuring resource quota status is calculated 06/09/23 10:56:49.574
STEP: Creating a Service 06/09/23 10:56:51.581
STEP: Creating a NodePort Service 06/09/23 10:56:51.612
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 06/09/23 10:56:51.65
STEP: Ensuring resource quota status captures service creation 06/09/23 10:56:51.705
STEP: Deleting Services 06/09/23 10:56:53.715
STEP: Ensuring resource quota status released usage 06/09/23 10:56:53.791
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  9 10:56:55.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4382" for this suite. 06/09/23 10:56:55.842
------------------------------
• [SLOW TEST] [11.358 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:56:44.516
    Jun  9 10:56:44.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename resourcequota 06/09/23 10:56:44.518
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:56:44.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:56:44.546
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 06/09/23 10:56:44.553
    STEP: Creating a ResourceQuota 06/09/23 10:56:49.561
    STEP: Ensuring resource quota status is calculated 06/09/23 10:56:49.574
    STEP: Creating a Service 06/09/23 10:56:51.581
    STEP: Creating a NodePort Service 06/09/23 10:56:51.612
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 06/09/23 10:56:51.65
    STEP: Ensuring resource quota status captures service creation 06/09/23 10:56:51.705
    STEP: Deleting Services 06/09/23 10:56:53.715
    STEP: Ensuring resource quota status released usage 06/09/23 10:56:53.791
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:56:55.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4382" for this suite. 06/09/23 10:56:55.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:56:55.876
Jun  9 10:56:55.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename sched-preemption 06/09/23 10:56:55.878
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:56:55.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:56:55.954
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jun  9 10:56:56.040: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  9 10:57:56.166: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 06/09/23 10:57:56.173
Jun  9 10:57:56.210: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun  9 10:57:56.223: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun  9 10:57:56.257: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun  9 10:57:56.271: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun  9 10:57:56.298: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun  9 10:57:56.321: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 06/09/23 10:57:56.321
Jun  9 10:57:56.322: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2062" to be "running"
Jun  9 10:57:56.335: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.918409ms
Jun  9 10:57:58.341: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.019495039s
Jun  9 10:57:58.341: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jun  9 10:57:58.341: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2062" to be "running"
Jun  9 10:57:58.347: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.600342ms
Jun  9 10:57:58.347: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jun  9 10:57:58.347: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2062" to be "running"
Jun  9 10:57:58.353: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.727079ms
Jun  9 10:57:58.353: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jun  9 10:57:58.353: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2062" to be "running"
Jun  9 10:57:58.358: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.958625ms
Jun  9 10:57:58.358: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jun  9 10:57:58.358: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2062" to be "running"
Jun  9 10:57:58.362: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.405615ms
Jun  9 10:57:58.362: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jun  9 10:57:58.362: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2062" to be "running"
Jun  9 10:57:58.367: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.137568ms
Jun  9 10:57:58.367: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 06/09/23 10:57:58.367
Jun  9 10:57:58.381: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jun  9 10:57:58.388: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.056265ms
Jun  9 10:58:00.396: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014727897s
Jun  9 10:58:02.396: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014686667s
Jun  9 10:58:04.399: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.017529295s
Jun  9 10:58:04.399: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 10:58:04.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-2062" for this suite. 06/09/23 10:58:04.632
------------------------------
• [SLOW TEST] [68.788 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:56:55.876
    Jun  9 10:56:55.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename sched-preemption 06/09/23 10:56:55.878
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:56:55.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:56:55.954
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jun  9 10:56:56.040: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun  9 10:57:56.166: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 06/09/23 10:57:56.173
    Jun  9 10:57:56.210: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jun  9 10:57:56.223: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jun  9 10:57:56.257: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jun  9 10:57:56.271: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jun  9 10:57:56.298: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jun  9 10:57:56.321: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 06/09/23 10:57:56.321
    Jun  9 10:57:56.322: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2062" to be "running"
    Jun  9 10:57:56.335: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.918409ms
    Jun  9 10:57:58.341: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.019495039s
    Jun  9 10:57:58.341: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jun  9 10:57:58.341: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2062" to be "running"
    Jun  9 10:57:58.347: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.600342ms
    Jun  9 10:57:58.347: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun  9 10:57:58.347: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2062" to be "running"
    Jun  9 10:57:58.353: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.727079ms
    Jun  9 10:57:58.353: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun  9 10:57:58.353: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2062" to be "running"
    Jun  9 10:57:58.358: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.958625ms
    Jun  9 10:57:58.358: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun  9 10:57:58.358: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2062" to be "running"
    Jun  9 10:57:58.362: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.405615ms
    Jun  9 10:57:58.362: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun  9 10:57:58.362: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2062" to be "running"
    Jun  9 10:57:58.367: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.137568ms
    Jun  9 10:57:58.367: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 06/09/23 10:57:58.367
    Jun  9 10:57:58.381: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jun  9 10:57:58.388: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.056265ms
    Jun  9 10:58:00.396: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014727897s
    Jun  9 10:58:02.396: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014686667s
    Jun  9 10:58:04.399: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.017529295s
    Jun  9 10:58:04.399: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:58:04.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-2062" for this suite. 06/09/23 10:58:04.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:58:04.666
Jun  9 10:58:04.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename svcaccounts 06/09/23 10:58:04.668
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:04.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:04.704
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 06/09/23 10:58:04.711
STEP: watching for the ServiceAccount to be added 06/09/23 10:58:04.728
STEP: patching the ServiceAccount 06/09/23 10:58:04.733
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 06/09/23 10:58:04.742
STEP: deleting the ServiceAccount 06/09/23 10:58:04.751
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun  9 10:58:04.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7341" for this suite. 06/09/23 10:58:04.817
------------------------------
• [0.170 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:58:04.666
    Jun  9 10:58:04.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename svcaccounts 06/09/23 10:58:04.668
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:04.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:04.704
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 06/09/23 10:58:04.711
    STEP: watching for the ServiceAccount to be added 06/09/23 10:58:04.728
    STEP: patching the ServiceAccount 06/09/23 10:58:04.733
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 06/09/23 10:58:04.742
    STEP: deleting the ServiceAccount 06/09/23 10:58:04.751
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:58:04.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7341" for this suite. 06/09/23 10:58:04.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:58:04.837
Jun  9 10:58:04.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubelet-test 06/09/23 10:58:04.839
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:04.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:04.866
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jun  9 10:58:04.890: INFO: Waiting up to 5m0s for pod "busybox-scheduling-cb9633cd-a534-4f32-8491-24a81d41b17c" in namespace "kubelet-test-9422" to be "running and ready"
Jun  9 10:58:04.899: INFO: Pod "busybox-scheduling-cb9633cd-a534-4f32-8491-24a81d41b17c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.783973ms
Jun  9 10:58:04.899: INFO: The phase of Pod busybox-scheduling-cb9633cd-a534-4f32-8491-24a81d41b17c is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:58:06.911: INFO: Pod "busybox-scheduling-cb9633cd-a534-4f32-8491-24a81d41b17c": Phase="Running", Reason="", readiness=true. Elapsed: 2.021137683s
Jun  9 10:58:06.911: INFO: The phase of Pod busybox-scheduling-cb9633cd-a534-4f32-8491-24a81d41b17c is Running (Ready = true)
Jun  9 10:58:06.911: INFO: Pod "busybox-scheduling-cb9633cd-a534-4f32-8491-24a81d41b17c" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun  9 10:58:06.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9422" for this suite. 06/09/23 10:58:06.95
------------------------------
• [2.123 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:58:04.837
    Jun  9 10:58:04.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubelet-test 06/09/23 10:58:04.839
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:04.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:04.866
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jun  9 10:58:04.890: INFO: Waiting up to 5m0s for pod "busybox-scheduling-cb9633cd-a534-4f32-8491-24a81d41b17c" in namespace "kubelet-test-9422" to be "running and ready"
    Jun  9 10:58:04.899: INFO: Pod "busybox-scheduling-cb9633cd-a534-4f32-8491-24a81d41b17c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.783973ms
    Jun  9 10:58:04.899: INFO: The phase of Pod busybox-scheduling-cb9633cd-a534-4f32-8491-24a81d41b17c is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:58:06.911: INFO: Pod "busybox-scheduling-cb9633cd-a534-4f32-8491-24a81d41b17c": Phase="Running", Reason="", readiness=true. Elapsed: 2.021137683s
    Jun  9 10:58:06.911: INFO: The phase of Pod busybox-scheduling-cb9633cd-a534-4f32-8491-24a81d41b17c is Running (Ready = true)
    Jun  9 10:58:06.911: INFO: Pod "busybox-scheduling-cb9633cd-a534-4f32-8491-24a81d41b17c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:58:06.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9422" for this suite. 06/09/23 10:58:06.95
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:58:06.961
Jun  9 10:58:06.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename dns 06/09/23 10:58:06.964
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:07.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:07.012
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 06/09/23 10:58:07.018
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 127.97.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.97.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.97.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.97.127_tcp@PTR;sleep 1; done
 06/09/23 10:58:07.047
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 127.97.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.97.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.97.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.97.127_tcp@PTR;sleep 1; done
 06/09/23 10:58:07.047
STEP: creating a pod to probe DNS 06/09/23 10:58:07.047
STEP: submitting the pod to kubernetes 06/09/23 10:58:07.047
Jun  9 10:58:07.063: INFO: Waiting up to 15m0s for pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3" in namespace "dns-6240" to be "running"
Jun  9 10:58:07.069: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.111653ms
Jun  9 10:58:09.077: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014750865s
Jun  9 10:58:11.074: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011303666s
Jun  9 10:58:13.077: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014311318s
Jun  9 10:58:15.082: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018978488s
Jun  9 10:58:17.087: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024278739s
Jun  9 10:58:19.090: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02790327s
Jun  9 10:58:21.076: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013293436s
Jun  9 10:58:23.091: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.028229965s
Jun  9 10:58:25.077: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.014187931s
Jun  9 10:58:27.080: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.01789562s
Jun  9 10:58:29.083: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.020720677s
Jun  9 10:58:31.083: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.020889777s
Jun  9 10:58:33.079: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Running", Reason="", readiness=true. Elapsed: 26.016580731s
Jun  9 10:58:33.079: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3" satisfied condition "running"
STEP: retrieving the pod 06/09/23 10:58:33.079
STEP: looking for the results for each expected name from probers 06/09/23 10:58:33.088
Jun  9 10:58:33.114: INFO: Unable to read wheezy_udp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:33.132: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:33.150: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:33.165: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:33.248: INFO: Unable to read jessie_udp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:33.257: INFO: Unable to read jessie_tcp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:33.277: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:33.301: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:33.352: INFO: Lookups using dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3 failed for: [wheezy_udp@dns-test-service.dns-6240.svc.cluster.local wheezy_tcp@dns-test-service.dns-6240.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local jessie_udp@dns-test-service.dns-6240.svc.cluster.local jessie_tcp@dns-test-service.dns-6240.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local]

Jun  9 10:58:38.361: INFO: Unable to read wheezy_udp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:38.368: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:38.376: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:38.384: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:38.421: INFO: Unable to read jessie_udp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:38.429: INFO: Unable to read jessie_tcp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:38.434: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:38.441: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
Jun  9 10:58:38.470: INFO: Lookups using dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3 failed for: [wheezy_udp@dns-test-service.dns-6240.svc.cluster.local wheezy_tcp@dns-test-service.dns-6240.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local jessie_udp@dns-test-service.dns-6240.svc.cluster.local jessie_tcp@dns-test-service.dns-6240.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local]

Jun  9 10:58:43.451: INFO: DNS probes using dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3 succeeded

STEP: deleting the pod 06/09/23 10:58:43.451
STEP: deleting the test service 06/09/23 10:58:43.478
STEP: deleting the test headless service 06/09/23 10:58:43.52
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  9 10:58:43.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6240" for this suite. 06/09/23 10:58:43.553
------------------------------
• [SLOW TEST] [36.608 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:58:06.961
    Jun  9 10:58:06.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename dns 06/09/23 10:58:06.964
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:07.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:07.012
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 06/09/23 10:58:07.018
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 127.97.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.97.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.97.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.97.127_tcp@PTR;sleep 1; done
     06/09/23 10:58:07.047
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 127.97.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.97.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.97.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.97.127_tcp@PTR;sleep 1; done
     06/09/23 10:58:07.047
    STEP: creating a pod to probe DNS 06/09/23 10:58:07.047
    STEP: submitting the pod to kubernetes 06/09/23 10:58:07.047
    Jun  9 10:58:07.063: INFO: Waiting up to 15m0s for pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3" in namespace "dns-6240" to be "running"
    Jun  9 10:58:07.069: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.111653ms
    Jun  9 10:58:09.077: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014750865s
    Jun  9 10:58:11.074: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011303666s
    Jun  9 10:58:13.077: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014311318s
    Jun  9 10:58:15.082: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018978488s
    Jun  9 10:58:17.087: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024278739s
    Jun  9 10:58:19.090: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02790327s
    Jun  9 10:58:21.076: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013293436s
    Jun  9 10:58:23.091: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.028229965s
    Jun  9 10:58:25.077: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.014187931s
    Jun  9 10:58:27.080: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.01789562s
    Jun  9 10:58:29.083: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.020720677s
    Jun  9 10:58:31.083: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.020889777s
    Jun  9 10:58:33.079: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3": Phase="Running", Reason="", readiness=true. Elapsed: 26.016580731s
    Jun  9 10:58:33.079: INFO: Pod "dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3" satisfied condition "running"
    STEP: retrieving the pod 06/09/23 10:58:33.079
    STEP: looking for the results for each expected name from probers 06/09/23 10:58:33.088
    Jun  9 10:58:33.114: INFO: Unable to read wheezy_udp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:33.132: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:33.150: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:33.165: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:33.248: INFO: Unable to read jessie_udp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:33.257: INFO: Unable to read jessie_tcp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:33.277: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:33.301: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:33.352: INFO: Lookups using dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3 failed for: [wheezy_udp@dns-test-service.dns-6240.svc.cluster.local wheezy_tcp@dns-test-service.dns-6240.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local jessie_udp@dns-test-service.dns-6240.svc.cluster.local jessie_tcp@dns-test-service.dns-6240.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local]

    Jun  9 10:58:38.361: INFO: Unable to read wheezy_udp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:38.368: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:38.376: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:38.384: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:38.421: INFO: Unable to read jessie_udp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:38.429: INFO: Unable to read jessie_tcp@dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:38.434: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:38.441: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local from pod dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3: the server could not find the requested resource (get pods dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3)
    Jun  9 10:58:38.470: INFO: Lookups using dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3 failed for: [wheezy_udp@dns-test-service.dns-6240.svc.cluster.local wheezy_tcp@dns-test-service.dns-6240.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local jessie_udp@dns-test-service.dns-6240.svc.cluster.local jessie_tcp@dns-test-service.dns-6240.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6240.svc.cluster.local]

    Jun  9 10:58:43.451: INFO: DNS probes using dns-6240/dns-test-f2867754-2697-4ed7-aef1-e96152fb0be3 succeeded

    STEP: deleting the pod 06/09/23 10:58:43.451
    STEP: deleting the test service 06/09/23 10:58:43.478
    STEP: deleting the test headless service 06/09/23 10:58:43.52
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:58:43.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6240" for this suite. 06/09/23 10:58:43.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:58:43.645
Jun  9 10:58:43.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename replicaset 06/09/23 10:58:43.647
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:43.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:43.745
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 06/09/23 10:58:43.755
STEP: Verify that the required pods have come up 06/09/23 10:58:43.765
Jun  9 10:58:43.773: INFO: Pod name sample-pod: Found 0 pods out of 3
Jun  9 10:58:48.782: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 06/09/23 10:58:48.782
Jun  9 10:58:48.786: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 06/09/23 10:58:48.786
STEP: DeleteCollection of the ReplicaSets 06/09/23 10:58:48.793
STEP: After DeleteCollection verify that ReplicaSets have been deleted 06/09/23 10:58:48.803
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun  9 10:58:48.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2965" for this suite. 06/09/23 10:58:48.821
------------------------------
• [SLOW TEST] [5.185 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:58:43.645
    Jun  9 10:58:43.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename replicaset 06/09/23 10:58:43.647
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:43.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:43.745
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 06/09/23 10:58:43.755
    STEP: Verify that the required pods have come up 06/09/23 10:58:43.765
    Jun  9 10:58:43.773: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jun  9 10:58:48.782: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 06/09/23 10:58:48.782
    Jun  9 10:58:48.786: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 06/09/23 10:58:48.786
    STEP: DeleteCollection of the ReplicaSets 06/09/23 10:58:48.793
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 06/09/23 10:58:48.803
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:58:48.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2965" for this suite. 06/09/23 10:58:48.821
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:58:48.831
Jun  9 10:58:48.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename secrets 06/09/23 10:58:48.832
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:48.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:48.859
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-f0c81bda-1a5c-4ed5-a35a-419b6551176c 06/09/23 10:58:48.869
STEP: Creating a pod to test consume secrets 06/09/23 10:58:48.879
Jun  9 10:58:48.889: INFO: Waiting up to 5m0s for pod "pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336" in namespace "secrets-4624" to be "Succeeded or Failed"
Jun  9 10:58:48.895: INFO: Pod "pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336": Phase="Pending", Reason="", readiness=false. Elapsed: 5.461471ms
Jun  9 10:58:50.900: INFO: Pod "pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010535474s
Jun  9 10:58:52.902: INFO: Pod "pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012253667s
STEP: Saw pod success 06/09/23 10:58:52.902
Jun  9 10:58:52.902: INFO: Pod "pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336" satisfied condition "Succeeded or Failed"
Jun  9 10:58:52.907: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336 container secret-volume-test: <nil>
STEP: delete the pod 06/09/23 10:58:52.926
Jun  9 10:58:52.940: INFO: Waiting for pod pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336 to disappear
Jun  9 10:58:52.945: INFO: Pod pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  9 10:58:52.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4624" for this suite. 06/09/23 10:58:52.954
------------------------------
• [4.133 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:58:48.831
    Jun  9 10:58:48.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename secrets 06/09/23 10:58:48.832
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:48.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:48.859
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-f0c81bda-1a5c-4ed5-a35a-419b6551176c 06/09/23 10:58:48.869
    STEP: Creating a pod to test consume secrets 06/09/23 10:58:48.879
    Jun  9 10:58:48.889: INFO: Waiting up to 5m0s for pod "pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336" in namespace "secrets-4624" to be "Succeeded or Failed"
    Jun  9 10:58:48.895: INFO: Pod "pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336": Phase="Pending", Reason="", readiness=false. Elapsed: 5.461471ms
    Jun  9 10:58:50.900: INFO: Pod "pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010535474s
    Jun  9 10:58:52.902: INFO: Pod "pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012253667s
    STEP: Saw pod success 06/09/23 10:58:52.902
    Jun  9 10:58:52.902: INFO: Pod "pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336" satisfied condition "Succeeded or Failed"
    Jun  9 10:58:52.907: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336 container secret-volume-test: <nil>
    STEP: delete the pod 06/09/23 10:58:52.926
    Jun  9 10:58:52.940: INFO: Waiting for pod pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336 to disappear
    Jun  9 10:58:52.945: INFO: Pod pod-secrets-8c7a458c-57f3-4d27-9c5b-0e45d239b336 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:58:52.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4624" for this suite. 06/09/23 10:58:52.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:58:52.967
Jun  9 10:58:52.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename secrets 06/09/23 10:58:52.969
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:52.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:52.994
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-392bc9e3-6b3c-421c-b648-1ad1efaf1467 06/09/23 10:58:53.001
STEP: Creating a pod to test consume secrets 06/09/23 10:58:53.009
Jun  9 10:58:53.021: INFO: Waiting up to 5m0s for pod "pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142" in namespace "secrets-778" to be "Succeeded or Failed"
Jun  9 10:58:53.029: INFO: Pod "pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142": Phase="Pending", Reason="", readiness=false. Elapsed: 7.241002ms
Jun  9 10:58:55.036: INFO: Pod "pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014659725s
Jun  9 10:58:57.035: INFO: Pod "pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013613124s
STEP: Saw pod success 06/09/23 10:58:57.035
Jun  9 10:58:57.035: INFO: Pod "pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142" satisfied condition "Succeeded or Failed"
Jun  9 10:58:57.041: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142 container secret-volume-test: <nil>
STEP: delete the pod 06/09/23 10:58:57.049
Jun  9 10:58:57.066: INFO: Waiting for pod pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142 to disappear
Jun  9 10:58:57.071: INFO: Pod pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  9 10:58:57.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-778" for this suite. 06/09/23 10:58:57.08
------------------------------
• [4.124 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:58:52.967
    Jun  9 10:58:52.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename secrets 06/09/23 10:58:52.969
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:52.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:52.994
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-392bc9e3-6b3c-421c-b648-1ad1efaf1467 06/09/23 10:58:53.001
    STEP: Creating a pod to test consume secrets 06/09/23 10:58:53.009
    Jun  9 10:58:53.021: INFO: Waiting up to 5m0s for pod "pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142" in namespace "secrets-778" to be "Succeeded or Failed"
    Jun  9 10:58:53.029: INFO: Pod "pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142": Phase="Pending", Reason="", readiness=false. Elapsed: 7.241002ms
    Jun  9 10:58:55.036: INFO: Pod "pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014659725s
    Jun  9 10:58:57.035: INFO: Pod "pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013613124s
    STEP: Saw pod success 06/09/23 10:58:57.035
    Jun  9 10:58:57.035: INFO: Pod "pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142" satisfied condition "Succeeded or Failed"
    Jun  9 10:58:57.041: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142 container secret-volume-test: <nil>
    STEP: delete the pod 06/09/23 10:58:57.049
    Jun  9 10:58:57.066: INFO: Waiting for pod pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142 to disappear
    Jun  9 10:58:57.071: INFO: Pod pod-secrets-e8b406d6-17ad-4ebc-ac92-20a9eca2e142 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:58:57.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-778" for this suite. 06/09/23 10:58:57.08
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:58:57.092
Jun  9 10:58:57.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename events 06/09/23 10:58:57.093
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:57.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:57.118
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 06/09/23 10:58:57.124
STEP: listing all events in all namespaces 06/09/23 10:58:57.135
STEP: patching the test event 06/09/23 10:58:57.144
STEP: fetching the test event 06/09/23 10:58:57.156
STEP: updating the test event 06/09/23 10:58:57.164
STEP: getting the test event 06/09/23 10:58:57.182
STEP: deleting the test event 06/09/23 10:58:57.189
STEP: listing all events in all namespaces 06/09/23 10:58:57.202
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jun  9 10:58:57.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7864" for this suite. 06/09/23 10:58:57.221
------------------------------
• [0.141 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:58:57.092
    Jun  9 10:58:57.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename events 06/09/23 10:58:57.093
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:57.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:57.118
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 06/09/23 10:58:57.124
    STEP: listing all events in all namespaces 06/09/23 10:58:57.135
    STEP: patching the test event 06/09/23 10:58:57.144
    STEP: fetching the test event 06/09/23 10:58:57.156
    STEP: updating the test event 06/09/23 10:58:57.164
    STEP: getting the test event 06/09/23 10:58:57.182
    STEP: deleting the test event 06/09/23 10:58:57.189
    STEP: listing all events in all namespaces 06/09/23 10:58:57.202
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:58:57.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7864" for this suite. 06/09/23 10:58:57.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:58:57.236
Jun  9 10:58:57.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pods 06/09/23 10:58:57.237
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:57.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:57.262
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jun  9 10:58:57.279: INFO: Waiting up to 5m0s for pod "server-envvars-20e865c6-1b0e-4aa0-8b29-d7a5cbb93621" in namespace "pods-1318" to be "running and ready"
Jun  9 10:58:57.285: INFO: Pod "server-envvars-20e865c6-1b0e-4aa0-8b29-d7a5cbb93621": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052114ms
Jun  9 10:58:57.285: INFO: The phase of Pod server-envvars-20e865c6-1b0e-4aa0-8b29-d7a5cbb93621 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:58:59.293: INFO: Pod "server-envvars-20e865c6-1b0e-4aa0-8b29-d7a5cbb93621": Phase="Running", Reason="", readiness=true. Elapsed: 2.013917543s
Jun  9 10:58:59.293: INFO: The phase of Pod server-envvars-20e865c6-1b0e-4aa0-8b29-d7a5cbb93621 is Running (Ready = true)
Jun  9 10:58:59.293: INFO: Pod "server-envvars-20e865c6-1b0e-4aa0-8b29-d7a5cbb93621" satisfied condition "running and ready"
Jun  9 10:58:59.338: INFO: Waiting up to 5m0s for pod "client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23" in namespace "pods-1318" to be "Succeeded or Failed"
Jun  9 10:58:59.344: INFO: Pod "client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23": Phase="Pending", Reason="", readiness=false. Elapsed: 5.741626ms
Jun  9 10:59:01.361: INFO: Pod "client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023020355s
Jun  9 10:59:03.351: INFO: Pod "client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012284741s
STEP: Saw pod success 06/09/23 10:59:03.351
Jun  9 10:59:03.351: INFO: Pod "client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23" satisfied condition "Succeeded or Failed"
Jun  9 10:59:03.358: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23 container env3cont: <nil>
STEP: delete the pod 06/09/23 10:59:03.369
Jun  9 10:59:03.386: INFO: Waiting for pod client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23 to disappear
Jun  9 10:59:03.392: INFO: Pod client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  9 10:59:03.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1318" for this suite. 06/09/23 10:59:03.402
------------------------------
• [SLOW TEST] [6.177 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:58:57.236
    Jun  9 10:58:57.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pods 06/09/23 10:58:57.237
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:58:57.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:58:57.262
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jun  9 10:58:57.279: INFO: Waiting up to 5m0s for pod "server-envvars-20e865c6-1b0e-4aa0-8b29-d7a5cbb93621" in namespace "pods-1318" to be "running and ready"
    Jun  9 10:58:57.285: INFO: Pod "server-envvars-20e865c6-1b0e-4aa0-8b29-d7a5cbb93621": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052114ms
    Jun  9 10:58:57.285: INFO: The phase of Pod server-envvars-20e865c6-1b0e-4aa0-8b29-d7a5cbb93621 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:58:59.293: INFO: Pod "server-envvars-20e865c6-1b0e-4aa0-8b29-d7a5cbb93621": Phase="Running", Reason="", readiness=true. Elapsed: 2.013917543s
    Jun  9 10:58:59.293: INFO: The phase of Pod server-envvars-20e865c6-1b0e-4aa0-8b29-d7a5cbb93621 is Running (Ready = true)
    Jun  9 10:58:59.293: INFO: Pod "server-envvars-20e865c6-1b0e-4aa0-8b29-d7a5cbb93621" satisfied condition "running and ready"
    Jun  9 10:58:59.338: INFO: Waiting up to 5m0s for pod "client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23" in namespace "pods-1318" to be "Succeeded or Failed"
    Jun  9 10:58:59.344: INFO: Pod "client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23": Phase="Pending", Reason="", readiness=false. Elapsed: 5.741626ms
    Jun  9 10:59:01.361: INFO: Pod "client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023020355s
    Jun  9 10:59:03.351: INFO: Pod "client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012284741s
    STEP: Saw pod success 06/09/23 10:59:03.351
    Jun  9 10:59:03.351: INFO: Pod "client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23" satisfied condition "Succeeded or Failed"
    Jun  9 10:59:03.358: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23 container env3cont: <nil>
    STEP: delete the pod 06/09/23 10:59:03.369
    Jun  9 10:59:03.386: INFO: Waiting for pod client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23 to disappear
    Jun  9 10:59:03.392: INFO: Pod client-envvars-f8fa60a9-91ee-4607-8d12-7023e6147a23 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  9 10:59:03.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1318" for this suite. 06/09/23 10:59:03.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 10:59:03.415
Jun  9 10:59:03.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 10:59:03.417
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:59:03.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:59:03.442
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-ce6da12e-1378-48ff-b643-88d68a7a29a8 06/09/23 10:59:03.457
STEP: Creating secret with name s-test-opt-upd-280f3fe0-3da1-4342-a34a-657720a6fe9b 06/09/23 10:59:03.466
STEP: Creating the pod 06/09/23 10:59:03.474
Jun  9 10:59:03.498: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5" in namespace "projected-2541" to be "running and ready"
Jun  9 10:59:03.510: INFO: Pod "pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.044537ms
Jun  9 10:59:03.510: INFO: The phase of Pod pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:59:05.515: INFO: Pod "pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017592441s
Jun  9 10:59:05.515: INFO: The phase of Pod pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 10:59:07.516: INFO: Pod "pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5": Phase="Running", Reason="", readiness=true. Elapsed: 4.018301373s
Jun  9 10:59:07.516: INFO: The phase of Pod pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5 is Running (Ready = true)
Jun  9 10:59:07.516: INFO: Pod "pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-ce6da12e-1378-48ff-b643-88d68a7a29a8 06/09/23 10:59:07.55
STEP: Updating secret s-test-opt-upd-280f3fe0-3da1-4342-a34a-657720a6fe9b 06/09/23 10:59:07.559
STEP: Creating secret with name s-test-opt-create-f3ea0e9d-4107-4559-aeb7-975a9cf53f7e 06/09/23 10:59:07.567
STEP: waiting to observe update in volume 06/09/23 10:59:07.576
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun  9 11:00:38.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2541" for this suite. 06/09/23 11:00:38.534
------------------------------
• [SLOW TEST] [95.129 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 10:59:03.415
    Jun  9 10:59:03.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 10:59:03.417
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 10:59:03.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 10:59:03.442
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-ce6da12e-1378-48ff-b643-88d68a7a29a8 06/09/23 10:59:03.457
    STEP: Creating secret with name s-test-opt-upd-280f3fe0-3da1-4342-a34a-657720a6fe9b 06/09/23 10:59:03.466
    STEP: Creating the pod 06/09/23 10:59:03.474
    Jun  9 10:59:03.498: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5" in namespace "projected-2541" to be "running and ready"
    Jun  9 10:59:03.510: INFO: Pod "pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.044537ms
    Jun  9 10:59:03.510: INFO: The phase of Pod pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:59:05.515: INFO: Pod "pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017592441s
    Jun  9 10:59:05.515: INFO: The phase of Pod pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 10:59:07.516: INFO: Pod "pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5": Phase="Running", Reason="", readiness=true. Elapsed: 4.018301373s
    Jun  9 10:59:07.516: INFO: The phase of Pod pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5 is Running (Ready = true)
    Jun  9 10:59:07.516: INFO: Pod "pod-projected-secrets-232f6c12-1402-42dc-b650-ab3507f616a5" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-ce6da12e-1378-48ff-b643-88d68a7a29a8 06/09/23 10:59:07.55
    STEP: Updating secret s-test-opt-upd-280f3fe0-3da1-4342-a34a-657720a6fe9b 06/09/23 10:59:07.559
    STEP: Creating secret with name s-test-opt-create-f3ea0e9d-4107-4559-aeb7-975a9cf53f7e 06/09/23 10:59:07.567
    STEP: waiting to observe update in volume 06/09/23 10:59:07.576
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:00:38.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2541" for this suite. 06/09/23 11:00:38.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:00:38.546
Jun  9 11:00:38.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 11:00:38.548
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:00:38.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:00:38.595
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 06/09/23 11:00:38.602
Jun  9 11:00:38.603: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun  9 11:00:38.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 create -f -'
Jun  9 11:00:39.751: INFO: stderr: ""
Jun  9 11:00:39.751: INFO: stdout: "service/agnhost-replica created\n"
Jun  9 11:00:39.751: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun  9 11:00:39.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 create -f -'
Jun  9 11:00:40.160: INFO: stderr: ""
Jun  9 11:00:40.160: INFO: stdout: "service/agnhost-primary created\n"
Jun  9 11:00:40.160: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun  9 11:00:40.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 create -f -'
Jun  9 11:00:41.062: INFO: stderr: ""
Jun  9 11:00:41.062: INFO: stdout: "service/frontend created\n"
Jun  9 11:00:41.062: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun  9 11:00:41.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 create -f -'
Jun  9 11:00:41.383: INFO: stderr: ""
Jun  9 11:00:41.383: INFO: stdout: "deployment.apps/frontend created\n"
Jun  9 11:00:41.383: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun  9 11:00:41.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 create -f -'
Jun  9 11:00:41.686: INFO: stderr: ""
Jun  9 11:00:41.686: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun  9 11:00:41.687: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun  9 11:00:41.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 create -f -'
Jun  9 11:00:42.122: INFO: stderr: ""
Jun  9 11:00:42.122: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 06/09/23 11:00:42.122
Jun  9 11:00:42.122: INFO: Waiting for all frontend pods to be Running.
Jun  9 11:00:47.176: INFO: Waiting for frontend to serve content.
Jun  9 11:00:47.198: INFO: Trying to add a new entry to the guestbook.
Jun  9 11:00:47.216: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 06/09/23 11:00:47.234
Jun  9 11:00:47.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 delete --grace-period=0 --force -f -'
Jun  9 11:00:47.401: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 11:00:47.401: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 06/09/23 11:00:47.401
Jun  9 11:00:47.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 delete --grace-period=0 --force -f -'
Jun  9 11:00:47.543: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 11:00:47.543: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 06/09/23 11:00:47.543
Jun  9 11:00:47.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 delete --grace-period=0 --force -f -'
Jun  9 11:00:47.708: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 11:00:47.708: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 06/09/23 11:00:47.709
Jun  9 11:00:47.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 delete --grace-period=0 --force -f -'
Jun  9 11:00:47.803: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 11:00:47.803: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 06/09/23 11:00:47.803
Jun  9 11:00:47.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 delete --grace-period=0 --force -f -'
Jun  9 11:00:47.929: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 11:00:47.929: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 06/09/23 11:00:47.929
Jun  9 11:00:47.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 delete --grace-period=0 --force -f -'
Jun  9 11:00:48.158: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 11:00:48.158: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 11:00:48.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1662" for this suite. 06/09/23 11:00:48.174
------------------------------
• [SLOW TEST] [9.645 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:00:38.546
    Jun  9 11:00:38.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 11:00:38.548
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:00:38.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:00:38.595
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 06/09/23 11:00:38.602
    Jun  9 11:00:38.603: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jun  9 11:00:38.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 create -f -'
    Jun  9 11:00:39.751: INFO: stderr: ""
    Jun  9 11:00:39.751: INFO: stdout: "service/agnhost-replica created\n"
    Jun  9 11:00:39.751: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jun  9 11:00:39.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 create -f -'
    Jun  9 11:00:40.160: INFO: stderr: ""
    Jun  9 11:00:40.160: INFO: stdout: "service/agnhost-primary created\n"
    Jun  9 11:00:40.160: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jun  9 11:00:40.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 create -f -'
    Jun  9 11:00:41.062: INFO: stderr: ""
    Jun  9 11:00:41.062: INFO: stdout: "service/frontend created\n"
    Jun  9 11:00:41.062: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jun  9 11:00:41.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 create -f -'
    Jun  9 11:00:41.383: INFO: stderr: ""
    Jun  9 11:00:41.383: INFO: stdout: "deployment.apps/frontend created\n"
    Jun  9 11:00:41.383: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jun  9 11:00:41.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 create -f -'
    Jun  9 11:00:41.686: INFO: stderr: ""
    Jun  9 11:00:41.686: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jun  9 11:00:41.687: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jun  9 11:00:41.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 create -f -'
    Jun  9 11:00:42.122: INFO: stderr: ""
    Jun  9 11:00:42.122: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 06/09/23 11:00:42.122
    Jun  9 11:00:42.122: INFO: Waiting for all frontend pods to be Running.
    Jun  9 11:00:47.176: INFO: Waiting for frontend to serve content.
    Jun  9 11:00:47.198: INFO: Trying to add a new entry to the guestbook.
    Jun  9 11:00:47.216: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 06/09/23 11:00:47.234
    Jun  9 11:00:47.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 delete --grace-period=0 --force -f -'
    Jun  9 11:00:47.401: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  9 11:00:47.401: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 06/09/23 11:00:47.401
    Jun  9 11:00:47.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 delete --grace-period=0 --force -f -'
    Jun  9 11:00:47.543: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  9 11:00:47.543: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 06/09/23 11:00:47.543
    Jun  9 11:00:47.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 delete --grace-period=0 --force -f -'
    Jun  9 11:00:47.708: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  9 11:00:47.708: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 06/09/23 11:00:47.709
    Jun  9 11:00:47.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 delete --grace-period=0 --force -f -'
    Jun  9 11:00:47.803: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  9 11:00:47.803: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 06/09/23 11:00:47.803
    Jun  9 11:00:47.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 delete --grace-period=0 --force -f -'
    Jun  9 11:00:47.929: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  9 11:00:47.929: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 06/09/23 11:00:47.929
    Jun  9 11:00:47.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1662 delete --grace-period=0 --force -f -'
    Jun  9 11:00:48.158: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  9 11:00:48.158: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:00:48.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1662" for this suite. 06/09/23 11:00:48.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:00:48.193
Jun  9 11:00:48.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 11:00:48.194
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:00:48.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:00:48.233
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 11:00:48.292
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:00:48.621
STEP: Deploying the webhook pod 06/09/23 11:00:48.634
STEP: Wait for the deployment to be ready 06/09/23 11:00:48.653
Jun  9 11:00:48.689: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 11:00:50.707
STEP: Verifying the service has paired with the endpoint 06/09/23 11:00:50.728
Jun  9 11:00:51.728: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 06/09/23 11:00:51.733
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 06/09/23 11:00:51.735
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 06/09/23 11:00:51.735
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 06/09/23 11:00:51.735
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 06/09/23 11:00:51.737
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 06/09/23 11:00:51.737
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 06/09/23 11:00:51.739
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:00:51.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6424" for this suite. 06/09/23 11:00:51.804
STEP: Destroying namespace "webhook-6424-markers" for this suite. 06/09/23 11:00:51.815
------------------------------
• [3.630 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:00:48.193
    Jun  9 11:00:48.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 11:00:48.194
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:00:48.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:00:48.233
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 11:00:48.292
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:00:48.621
    STEP: Deploying the webhook pod 06/09/23 11:00:48.634
    STEP: Wait for the deployment to be ready 06/09/23 11:00:48.653
    Jun  9 11:00:48.689: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 11:00:50.707
    STEP: Verifying the service has paired with the endpoint 06/09/23 11:00:50.728
    Jun  9 11:00:51.728: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 06/09/23 11:00:51.733
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 06/09/23 11:00:51.735
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 06/09/23 11:00:51.735
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 06/09/23 11:00:51.735
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 06/09/23 11:00:51.737
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 06/09/23 11:00:51.737
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 06/09/23 11:00:51.739
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:00:51.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6424" for this suite. 06/09/23 11:00:51.804
    STEP: Destroying namespace "webhook-6424-markers" for this suite. 06/09/23 11:00:51.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:00:51.823
Jun  9 11:00:51.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename csistoragecapacity 06/09/23 11:00:51.824
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:00:51.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:00:51.847
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 06/09/23 11:00:51.851
STEP: getting /apis/storage.k8s.io 06/09/23 11:00:51.855
STEP: getting /apis/storage.k8s.io/v1 06/09/23 11:00:51.856
STEP: creating 06/09/23 11:00:51.858
STEP: watching 06/09/23 11:00:51.878
Jun  9 11:00:51.879: INFO: starting watch
STEP: getting 06/09/23 11:00:51.891
STEP: listing in namespace 06/09/23 11:00:51.895
STEP: listing across namespaces 06/09/23 11:00:51.901
STEP: patching 06/09/23 11:00:51.906
STEP: updating 06/09/23 11:00:51.912
Jun  9 11:00:51.920: INFO: waiting for watch events with expected annotations in namespace
Jun  9 11:00:51.920: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 06/09/23 11:00:51.92
STEP: deleting a collection 06/09/23 11:00:51.938
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jun  9 11:00:51.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-1006" for this suite. 06/09/23 11:00:51.971
------------------------------
• [0.157 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:00:51.823
    Jun  9 11:00:51.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename csistoragecapacity 06/09/23 11:00:51.824
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:00:51.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:00:51.847
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 06/09/23 11:00:51.851
    STEP: getting /apis/storage.k8s.io 06/09/23 11:00:51.855
    STEP: getting /apis/storage.k8s.io/v1 06/09/23 11:00:51.856
    STEP: creating 06/09/23 11:00:51.858
    STEP: watching 06/09/23 11:00:51.878
    Jun  9 11:00:51.879: INFO: starting watch
    STEP: getting 06/09/23 11:00:51.891
    STEP: listing in namespace 06/09/23 11:00:51.895
    STEP: listing across namespaces 06/09/23 11:00:51.901
    STEP: patching 06/09/23 11:00:51.906
    STEP: updating 06/09/23 11:00:51.912
    Jun  9 11:00:51.920: INFO: waiting for watch events with expected annotations in namespace
    Jun  9 11:00:51.920: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 06/09/23 11:00:51.92
    STEP: deleting a collection 06/09/23 11:00:51.938
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:00:51.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-1006" for this suite. 06/09/23 11:00:51.971
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:00:51.98
Jun  9 11:00:51.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 11:00:51.982
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:00:52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:00:52.005
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 06/09/23 11:00:52.01
Jun  9 11:00:52.022: INFO: Waiting up to 5m0s for pod "downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd" in namespace "downward-api-4748" to be "Succeeded or Failed"
Jun  9 11:00:52.027: INFO: Pod "downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.601513ms
Jun  9 11:00:54.032: INFO: Pod "downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010108255s
Jun  9 11:00:56.032: INFO: Pod "downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010057787s
STEP: Saw pod success 06/09/23 11:00:56.032
Jun  9 11:00:56.033: INFO: Pod "downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd" satisfied condition "Succeeded or Failed"
Jun  9 11:00:56.038: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd container dapi-container: <nil>
STEP: delete the pod 06/09/23 11:00:56.058
Jun  9 11:00:56.073: INFO: Waiting for pod downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd to disappear
Jun  9 11:00:56.079: INFO: Pod downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun  9 11:00:56.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4748" for this suite. 06/09/23 11:00:56.086
------------------------------
• [4.116 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:00:51.98
    Jun  9 11:00:51.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 11:00:51.982
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:00:52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:00:52.005
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 06/09/23 11:00:52.01
    Jun  9 11:00:52.022: INFO: Waiting up to 5m0s for pod "downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd" in namespace "downward-api-4748" to be "Succeeded or Failed"
    Jun  9 11:00:52.027: INFO: Pod "downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.601513ms
    Jun  9 11:00:54.032: INFO: Pod "downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010108255s
    Jun  9 11:00:56.032: INFO: Pod "downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010057787s
    STEP: Saw pod success 06/09/23 11:00:56.032
    Jun  9 11:00:56.033: INFO: Pod "downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd" satisfied condition "Succeeded or Failed"
    Jun  9 11:00:56.038: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd container dapi-container: <nil>
    STEP: delete the pod 06/09/23 11:00:56.058
    Jun  9 11:00:56.073: INFO: Waiting for pod downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd to disappear
    Jun  9 11:00:56.079: INFO: Pod downward-api-328074d7-fceb-4a38-a55e-e2ad87650bbd no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:00:56.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4748" for this suite. 06/09/23 11:00:56.086
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:00:56.097
Jun  9 11:00:56.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 11:00:56.099
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:00:56.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:00:56.121
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 06/09/23 11:00:56.128
Jun  9 11:00:56.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-5301 api-versions'
Jun  9 11:00:56.247: INFO: stderr: ""
Jun  9 11:00:56.247: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndata.packaging.carvel.dev/v1alpha1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ninternal.packaging.carvel.dev/v1alpha1\nkappctrl.k14s.io/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperator.tigera.io/v1\npackaging.carvel.dev/v1alpha1\npolicy/v1\nprojectcalico.org/v3\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 11:00:56.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5301" for this suite. 06/09/23 11:00:56.255
------------------------------
• [0.167 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:00:56.097
    Jun  9 11:00:56.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 11:00:56.099
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:00:56.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:00:56.121
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 06/09/23 11:00:56.128
    Jun  9 11:00:56.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-5301 api-versions'
    Jun  9 11:00:56.247: INFO: stderr: ""
    Jun  9 11:00:56.247: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndata.packaging.carvel.dev/v1alpha1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ninternal.packaging.carvel.dev/v1alpha1\nkappctrl.k14s.io/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperator.tigera.io/v1\npackaging.carvel.dev/v1alpha1\npolicy/v1\nprojectcalico.org/v3\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:00:56.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5301" for this suite. 06/09/23 11:00:56.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:00:56.266
Jun  9 11:00:56.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-runtime 06/09/23 11:00:56.269
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:00:56.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:00:56.296
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 06/09/23 11:00:56.301
STEP: wait for the container to reach Failed 06/09/23 11:00:56.32
STEP: get the container status 06/09/23 11:01:00.369
STEP: the container should be terminated 06/09/23 11:01:00.375
STEP: the termination message should be set 06/09/23 11:01:00.375
Jun  9 11:01:00.375: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 06/09/23 11:01:00.375
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun  9 11:01:00.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6994" for this suite. 06/09/23 11:01:00.428
------------------------------
• [4.215 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:00:56.266
    Jun  9 11:00:56.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-runtime 06/09/23 11:00:56.269
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:00:56.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:00:56.296
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 06/09/23 11:00:56.301
    STEP: wait for the container to reach Failed 06/09/23 11:00:56.32
    STEP: get the container status 06/09/23 11:01:00.369
    STEP: the container should be terminated 06/09/23 11:01:00.375
    STEP: the termination message should be set 06/09/23 11:01:00.375
    Jun  9 11:01:00.375: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 06/09/23 11:01:00.375
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:01:00.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6994" for this suite. 06/09/23 11:01:00.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:01:00.482
Jun  9 11:01:00.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename namespaces 06/09/23 11:01:00.484
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:01:00.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:01:00.535
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 06/09/23 11:01:00.542
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:01:00.558
STEP: Creating a service in the namespace 06/09/23 11:01:00.562
STEP: Deleting the namespace 06/09/23 11:01:00.614
STEP: Waiting for the namespace to be removed. 06/09/23 11:01:00.629
STEP: Recreating the namespace 06/09/23 11:01:06.638
STEP: Verifying there is no service in the namespace 06/09/23 11:01:06.654
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:01:06.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1384" for this suite. 06/09/23 11:01:06.668
STEP: Destroying namespace "nsdeletetest-6479" for this suite. 06/09/23 11:01:06.676
Jun  9 11:01:06.681: INFO: Namespace nsdeletetest-6479 was already deleted
STEP: Destroying namespace "nsdeletetest-5920" for this suite. 06/09/23 11:01:06.681
------------------------------
• [SLOW TEST] [6.207 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:01:00.482
    Jun  9 11:01:00.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename namespaces 06/09/23 11:01:00.484
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:01:00.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:01:00.535
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 06/09/23 11:01:00.542
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:01:00.558
    STEP: Creating a service in the namespace 06/09/23 11:01:00.562
    STEP: Deleting the namespace 06/09/23 11:01:00.614
    STEP: Waiting for the namespace to be removed. 06/09/23 11:01:00.629
    STEP: Recreating the namespace 06/09/23 11:01:06.638
    STEP: Verifying there is no service in the namespace 06/09/23 11:01:06.654
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:01:06.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1384" for this suite. 06/09/23 11:01:06.668
    STEP: Destroying namespace "nsdeletetest-6479" for this suite. 06/09/23 11:01:06.676
    Jun  9 11:01:06.681: INFO: Namespace nsdeletetest-6479 was already deleted
    STEP: Destroying namespace "nsdeletetest-5920" for this suite. 06/09/23 11:01:06.681
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:01:06.689
Jun  9 11:01:06.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 11:01:06.692
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:01:06.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:01:06.715
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jun  9 11:01:06.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/09/23 11:01:09.933
Jun  9 11:01:09.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-2220 --namespace=crd-publish-openapi-2220 create -f -'
Jun  9 11:01:10.883: INFO: stderr: ""
Jun  9 11:01:10.883: INFO: stdout: "e2e-test-crd-publish-openapi-1226-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun  9 11:01:10.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-2220 --namespace=crd-publish-openapi-2220 delete e2e-test-crd-publish-openapi-1226-crds test-cr'
Jun  9 11:01:11.031: INFO: stderr: ""
Jun  9 11:01:11.031: INFO: stdout: "e2e-test-crd-publish-openapi-1226-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun  9 11:01:11.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-2220 --namespace=crd-publish-openapi-2220 apply -f -'
Jun  9 11:01:11.372: INFO: stderr: ""
Jun  9 11:01:11.372: INFO: stdout: "e2e-test-crd-publish-openapi-1226-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun  9 11:01:11.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-2220 --namespace=crd-publish-openapi-2220 delete e2e-test-crd-publish-openapi-1226-crds test-cr'
Jun  9 11:01:11.519: INFO: stderr: ""
Jun  9 11:01:11.519: INFO: stdout: "e2e-test-crd-publish-openapi-1226-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 06/09/23 11:01:11.519
Jun  9 11:01:11.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-2220 explain e2e-test-crd-publish-openapi-1226-crds'
Jun  9 11:01:11.860: INFO: stderr: ""
Jun  9 11:01:11.860: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1226-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:01:14.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2220" for this suite. 06/09/23 11:01:14.535
------------------------------
• [SLOW TEST] [7.855 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:01:06.689
    Jun  9 11:01:06.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 11:01:06.692
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:01:06.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:01:06.715
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jun  9 11:01:06.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/09/23 11:01:09.933
    Jun  9 11:01:09.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-2220 --namespace=crd-publish-openapi-2220 create -f -'
    Jun  9 11:01:10.883: INFO: stderr: ""
    Jun  9 11:01:10.883: INFO: stdout: "e2e-test-crd-publish-openapi-1226-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jun  9 11:01:10.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-2220 --namespace=crd-publish-openapi-2220 delete e2e-test-crd-publish-openapi-1226-crds test-cr'
    Jun  9 11:01:11.031: INFO: stderr: ""
    Jun  9 11:01:11.031: INFO: stdout: "e2e-test-crd-publish-openapi-1226-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jun  9 11:01:11.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-2220 --namespace=crd-publish-openapi-2220 apply -f -'
    Jun  9 11:01:11.372: INFO: stderr: ""
    Jun  9 11:01:11.372: INFO: stdout: "e2e-test-crd-publish-openapi-1226-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jun  9 11:01:11.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-2220 --namespace=crd-publish-openapi-2220 delete e2e-test-crd-publish-openapi-1226-crds test-cr'
    Jun  9 11:01:11.519: INFO: stderr: ""
    Jun  9 11:01:11.519: INFO: stdout: "e2e-test-crd-publish-openapi-1226-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 06/09/23 11:01:11.519
    Jun  9 11:01:11.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-2220 explain e2e-test-crd-publish-openapi-1226-crds'
    Jun  9 11:01:11.860: INFO: stderr: ""
    Jun  9 11:01:11.860: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1226-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:01:14.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2220" for this suite. 06/09/23 11:01:14.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:01:14.546
Jun  9 11:01:14.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename statefulset 06/09/23 11:01:14.548
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:01:14.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:01:14.574
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4178 06/09/23 11:01:14.58
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 06/09/23 11:01:14.592
Jun  9 11:01:14.613: INFO: Found 0 stateful pods, waiting for 3
Jun  9 11:01:24.640: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 11:01:24.640: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 11:01:24.640: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 11:01:24.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-4178 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 11:01:24.936: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 11:01:24.936: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 11:01:24.936: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 06/09/23 11:01:35.023
Jun  9 11:01:35.068: INFO: Updating stateful set ss2
STEP: Creating a new revision 06/09/23 11:01:35.068
STEP: Updating Pods in reverse ordinal order 06/09/23 11:01:45.116
Jun  9 11:01:45.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-4178 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 11:01:45.327: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  9 11:01:45.327: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 11:01:45.327: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 06/09/23 11:01:55.37
Jun  9 11:01:55.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-4178 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 11:01:55.585: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 11:01:55.585: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 11:01:55.585: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 11:02:06.451: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 06/09/23 11:02:06.463
Jun  9 11:02:06.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-4178 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 11:02:06.765: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  9 11:02:06.765: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 11:02:06.765: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 11:02:16.805: INFO: Waiting for StatefulSet statefulset-4178/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  9 11:02:26.819: INFO: Deleting all statefulset in ns statefulset-4178
Jun  9 11:02:26.824: INFO: Scaling statefulset ss2 to 0
Jun  9 11:02:36.974: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 11:02:36.981: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  9 11:02:37.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4178" for this suite. 06/09/23 11:02:37.02
------------------------------
• [SLOW TEST] [82.492 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:01:14.546
    Jun  9 11:01:14.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename statefulset 06/09/23 11:01:14.548
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:01:14.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:01:14.574
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4178 06/09/23 11:01:14.58
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 06/09/23 11:01:14.592
    Jun  9 11:01:14.613: INFO: Found 0 stateful pods, waiting for 3
    Jun  9 11:01:24.640: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun  9 11:01:24.640: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun  9 11:01:24.640: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jun  9 11:01:24.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-4178 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  9 11:01:24.936: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  9 11:01:24.936: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  9 11:01:24.936: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 06/09/23 11:01:35.023
    Jun  9 11:01:35.068: INFO: Updating stateful set ss2
    STEP: Creating a new revision 06/09/23 11:01:35.068
    STEP: Updating Pods in reverse ordinal order 06/09/23 11:01:45.116
    Jun  9 11:01:45.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-4178 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  9 11:01:45.327: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun  9 11:01:45.327: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  9 11:01:45.327: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 06/09/23 11:01:55.37
    Jun  9 11:01:55.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-4178 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  9 11:01:55.585: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  9 11:01:55.585: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  9 11:01:55.585: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  9 11:02:06.451: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 06/09/23 11:02:06.463
    Jun  9 11:02:06.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-4178 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  9 11:02:06.765: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun  9 11:02:06.765: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  9 11:02:06.765: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  9 11:02:16.805: INFO: Waiting for StatefulSet statefulset-4178/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  9 11:02:26.819: INFO: Deleting all statefulset in ns statefulset-4178
    Jun  9 11:02:26.824: INFO: Scaling statefulset ss2 to 0
    Jun  9 11:02:36.974: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  9 11:02:36.981: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:02:37.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4178" for this suite. 06/09/23 11:02:37.02
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:02:37.039
Jun  9 11:02:37.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename secrets 06/09/23 11:02:37.041
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:02:37.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:02:37.078
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-36b61325-bc8c-40d1-a815-4d3291074173 06/09/23 11:02:37.094
STEP: Creating secret with name s-test-opt-upd-e76d6819-e525-46d2-b4db-3ea2a564671e 06/09/23 11:02:37.104
STEP: Creating the pod 06/09/23 11:02:37.111
Jun  9 11:02:37.127: INFO: Waiting up to 5m0s for pod "pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2" in namespace "secrets-2516" to be "running and ready"
Jun  9 11:02:37.135: INFO: Pod "pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.108599ms
Jun  9 11:02:37.135: INFO: The phase of Pod pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:02:39.142: INFO: Pod "pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014953649s
Jun  9 11:02:39.142: INFO: The phase of Pod pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:02:41.143: INFO: Pod "pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2": Phase="Running", Reason="", readiness=true. Elapsed: 4.016200505s
Jun  9 11:02:41.143: INFO: The phase of Pod pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2 is Running (Ready = true)
Jun  9 11:02:41.143: INFO: Pod "pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-36b61325-bc8c-40d1-a815-4d3291074173 06/09/23 11:02:41.192
STEP: Updating secret s-test-opt-upd-e76d6819-e525-46d2-b4db-3ea2a564671e 06/09/23 11:02:41.206
STEP: Creating secret with name s-test-opt-create-3201570a-3d9a-44f5-8c4c-c103c2234cd6 06/09/23 11:02:41.216
STEP: waiting to observe update in volume 06/09/23 11:02:41.224
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  9 11:03:53.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2516" for this suite. 06/09/23 11:03:53.871
------------------------------
• [SLOW TEST] [76.867 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:02:37.039
    Jun  9 11:02:37.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename secrets 06/09/23 11:02:37.041
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:02:37.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:02:37.078
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-36b61325-bc8c-40d1-a815-4d3291074173 06/09/23 11:02:37.094
    STEP: Creating secret with name s-test-opt-upd-e76d6819-e525-46d2-b4db-3ea2a564671e 06/09/23 11:02:37.104
    STEP: Creating the pod 06/09/23 11:02:37.111
    Jun  9 11:02:37.127: INFO: Waiting up to 5m0s for pod "pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2" in namespace "secrets-2516" to be "running and ready"
    Jun  9 11:02:37.135: INFO: Pod "pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.108599ms
    Jun  9 11:02:37.135: INFO: The phase of Pod pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:02:39.142: INFO: Pod "pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014953649s
    Jun  9 11:02:39.142: INFO: The phase of Pod pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:02:41.143: INFO: Pod "pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2": Phase="Running", Reason="", readiness=true. Elapsed: 4.016200505s
    Jun  9 11:02:41.143: INFO: The phase of Pod pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2 is Running (Ready = true)
    Jun  9 11:02:41.143: INFO: Pod "pod-secrets-d01ebdd5-994b-47cd-b793-b0ef0e13c2a2" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-36b61325-bc8c-40d1-a815-4d3291074173 06/09/23 11:02:41.192
    STEP: Updating secret s-test-opt-upd-e76d6819-e525-46d2-b4db-3ea2a564671e 06/09/23 11:02:41.206
    STEP: Creating secret with name s-test-opt-create-3201570a-3d9a-44f5-8c4c-c103c2234cd6 06/09/23 11:02:41.216
    STEP: waiting to observe update in volume 06/09/23 11:02:41.224
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:03:53.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2516" for this suite. 06/09/23 11:03:53.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:03:53.908
Jun  9 11:03:53.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename dns 06/09/23 11:03:53.91
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:03:53.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:03:53.975
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 06/09/23 11:03:53.981
Jun  9 11:03:54.010: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3832  3e8491a8-11be-43c3-a807-27f6bb485c0c 94578 0 2023-06-09 11:03:53 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-06-09 11:03:53 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f55s9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f55s9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:03:54.011: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3832" to be "running and ready"
Jun  9 11:03:54.036: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 25.681049ms
Jun  9 11:03:54.036: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:03:56.049: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038483215s
Jun  9 11:03:56.049: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:03:58.043: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.032725765s
Jun  9 11:03:58.043: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jun  9 11:03:58.043: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 06/09/23 11:03:58.043
Jun  9 11:03:58.044: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3832 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 11:03:58.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 11:03:58.044: INFO: ExecWithOptions: Clientset creation
Jun  9 11:03:58.045: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3832/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 06/09/23 11:03:58.168
Jun  9 11:03:58.169: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3832 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 11:03:58.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 11:03:58.170: INFO: ExecWithOptions: Clientset creation
Jun  9 11:03:58.170: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3832/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  9 11:03:58.268: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  9 11:03:58.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3832" for this suite. 06/09/23 11:03:58.296
------------------------------
• [4.399 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:03:53.908
    Jun  9 11:03:53.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename dns 06/09/23 11:03:53.91
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:03:53.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:03:53.975
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 06/09/23 11:03:53.981
    Jun  9 11:03:54.010: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3832  3e8491a8-11be-43c3-a807-27f6bb485c0c 94578 0 2023-06-09 11:03:53 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-06-09 11:03:53 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f55s9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f55s9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:03:54.011: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3832" to be "running and ready"
    Jun  9 11:03:54.036: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 25.681049ms
    Jun  9 11:03:54.036: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:03:56.049: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038483215s
    Jun  9 11:03:56.049: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:03:58.043: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.032725765s
    Jun  9 11:03:58.043: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jun  9 11:03:58.043: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 06/09/23 11:03:58.043
    Jun  9 11:03:58.044: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3832 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 11:03:58.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 11:03:58.044: INFO: ExecWithOptions: Clientset creation
    Jun  9 11:03:58.045: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3832/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 06/09/23 11:03:58.168
    Jun  9 11:03:58.169: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3832 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 11:03:58.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 11:03:58.170: INFO: ExecWithOptions: Clientset creation
    Jun  9 11:03:58.170: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3832/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  9 11:03:58.268: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:03:58.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3832" for this suite. 06/09/23 11:03:58.296
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:03:58.307
Jun  9 11:03:58.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 11:03:58.308
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:03:58.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:03:58.34
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/09/23 11:03:58.346
Jun  9 11:03:58.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-6029 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun  9 11:03:58.451: INFO: stderr: ""
Jun  9 11:03:58.451: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 06/09/23 11:03:58.451
Jun  9 11:03:58.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-6029 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Jun  9 11:03:59.893: INFO: stderr: ""
Jun  9 11:03:59.894: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/09/23 11:03:59.894
Jun  9 11:03:59.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-6029 delete pods e2e-test-httpd-pod'
Jun  9 11:04:02.271: INFO: stderr: ""
Jun  9 11:04:02.271: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 11:04:02.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6029" for this suite. 06/09/23 11:04:02.28
------------------------------
• [3.998 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:03:58.307
    Jun  9 11:03:58.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 11:03:58.308
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:03:58.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:03:58.34
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/09/23 11:03:58.346
    Jun  9 11:03:58.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-6029 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jun  9 11:03:58.451: INFO: stderr: ""
    Jun  9 11:03:58.451: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 06/09/23 11:03:58.451
    Jun  9 11:03:58.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-6029 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Jun  9 11:03:59.893: INFO: stderr: ""
    Jun  9 11:03:59.894: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/09/23 11:03:59.894
    Jun  9 11:03:59.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-6029 delete pods e2e-test-httpd-pod'
    Jun  9 11:04:02.271: INFO: stderr: ""
    Jun  9 11:04:02.271: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:04:02.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6029" for this suite. 06/09/23 11:04:02.28
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:04:02.306
Jun  9 11:04:02.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 11:04:02.307
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:04:02.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:04:02.682
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 11:04:02.717
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:04:03.32
STEP: Deploying the webhook pod 06/09/23 11:04:03.334
STEP: Wait for the deployment to be ready 06/09/23 11:04:03.352
Jun  9 11:04:03.362: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 11:04:05.385
STEP: Verifying the service has paired with the endpoint 06/09/23 11:04:05.486
Jun  9 11:04:06.487: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jun  9 11:04:06.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1070-crds.webhook.example.com via the AdmissionRegistration API 06/09/23 11:04:07.031
STEP: Creating a custom resource that should be mutated by the webhook 06/09/23 11:04:07.053
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:04:09.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6902" for this suite. 06/09/23 11:04:09.727
STEP: Destroying namespace "webhook-6902-markers" for this suite. 06/09/23 11:04:09.746
------------------------------
• [SLOW TEST] [7.454 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:04:02.306
    Jun  9 11:04:02.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 11:04:02.307
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:04:02.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:04:02.682
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 11:04:02.717
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:04:03.32
    STEP: Deploying the webhook pod 06/09/23 11:04:03.334
    STEP: Wait for the deployment to be ready 06/09/23 11:04:03.352
    Jun  9 11:04:03.362: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 11:04:05.385
    STEP: Verifying the service has paired with the endpoint 06/09/23 11:04:05.486
    Jun  9 11:04:06.487: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jun  9 11:04:06.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1070-crds.webhook.example.com via the AdmissionRegistration API 06/09/23 11:04:07.031
    STEP: Creating a custom resource that should be mutated by the webhook 06/09/23 11:04:07.053
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:04:09.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6902" for this suite. 06/09/23 11:04:09.727
    STEP: Destroying namespace "webhook-6902-markers" for this suite. 06/09/23 11:04:09.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:04:09.761
Jun  9 11:04:09.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename deployment 06/09/23 11:04:09.763
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:04:09.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:04:09.796
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jun  9 11:04:09.804: INFO: Creating simple deployment test-new-deployment
Jun  9 11:04:09.833: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 06/09/23 11:04:11.86
STEP: updating a scale subresource 06/09/23 11:04:11.867
STEP: verifying the deployment Spec.Replicas was modified 06/09/23 11:04:11.878
STEP: Patch a scale subresource 06/09/23 11:04:11.887
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  9 11:04:11.936: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-8299  34ed920b-e50d-45ec-9a06-72f6e1930dc1 94840 3 2023-06-09 11:04:09 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-09 11:04:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000fd3c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-06-09 11:04:11 +0000 UTC,LastTransitionTime:2023-06-09 11:04:09 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-09 11:04:11 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  9 11:04:11.946: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-8299  25724e97-a0ac-4bb2-b924-930ff45933f2 94843 3 2023-06-09 11:04:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 34ed920b-e50d-45ec-9a06-72f6e1930dc1 0xc00070f6e7 0xc00070f6e8}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ed920b-e50d-45ec-9a06-72f6e1930dc1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00070f778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  9 11:04:11.958: INFO: Pod "test-new-deployment-7f5969cbc7-dd87l" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-dd87l test-new-deployment-7f5969cbc7- deployment-8299  8788410a-c66b-48b2-aec2-5a51be7003f5 94844 0 2023-06-09 11:04:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 25724e97-a0ac-4bb2-b924-930ff45933f2 0xc0015fe987 0xc0015fe988}] [] [{kube-controller-manager Update v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25724e97-a0ac-4bb2-b924-930ff45933f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5cvp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5cvp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-09 11:04:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:04:11.959: INFO: Pod "test-new-deployment-7f5969cbc7-nrwwq" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-nrwwq test-new-deployment-7f5969cbc7- deployment-8299  7d5ea416-40a7-40f3-a401-ebc78696607f 94850 0 2023-06-09 11:04:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 25724e97-a0ac-4bb2-b924-930ff45933f2 0xc0015ff2c7 0xc0015ff2c8}] [] [{kube-controller-manager Update v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25724e97-a0ac-4bb2-b924-930ff45933f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkgjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkgjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:04:11.959: INFO: Pod "test-new-deployment-7f5969cbc7-rmtbd" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-rmtbd test-new-deployment-7f5969cbc7- deployment-8299  ba2fb712-f93a-4848-8953-1d505ae50a1b 94849 0 2023-06-09 11:04:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 25724e97-a0ac-4bb2-b924-930ff45933f2 0xc0015ff7a7 0xc0015ff7a8}] [] [{kube-controller-manager Update v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25724e97-a0ac-4bb2-b924-930ff45933f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nfbnn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nfbnn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:04:11.959: INFO: Pod "test-new-deployment-7f5969cbc7-rwfrr" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-rwfrr test-new-deployment-7f5969cbc7- deployment-8299  8ec5ed41-368e-47f7-a689-be41cf48b1bc 94829 0 2023-06-09 11:04:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e94ee30c79869587da4a1fb649ae2102996d7af2fe8d38c589bafa1679dd88b0 cni.projectcalico.org/podIP:172.27.53.94/32 cni.projectcalico.org/podIPs:172.27.53.94/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 25724e97-a0ac-4bb2-b924-930ff45933f2 0xc0015ffd60 0xc0015ffd61}] [] [{kube-controller-manager Update v1 2023-06-09 11:04:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25724e97-a0ac-4bb2-b924-930ff45933f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:04:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.27.53.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q67rs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q67rs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.27.53.94,StartTime:2023-06-09 11:04:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:04:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://308e628c3cbf005fc88634affa22f6b8e2f32e0a231974212621f60f8ed7972b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.27.53.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  9 11:04:11.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8299" for this suite. 06/09/23 11:04:11.968
------------------------------
• [2.227 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:04:09.761
    Jun  9 11:04:09.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename deployment 06/09/23 11:04:09.763
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:04:09.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:04:09.796
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jun  9 11:04:09.804: INFO: Creating simple deployment test-new-deployment
    Jun  9 11:04:09.833: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 06/09/23 11:04:11.86
    STEP: updating a scale subresource 06/09/23 11:04:11.867
    STEP: verifying the deployment Spec.Replicas was modified 06/09/23 11:04:11.878
    STEP: Patch a scale subresource 06/09/23 11:04:11.887
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  9 11:04:11.936: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-8299  34ed920b-e50d-45ec-9a06-72f6e1930dc1 94840 3 2023-06-09 11:04:09 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-09 11:04:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000fd3c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-06-09 11:04:11 +0000 UTC,LastTransitionTime:2023-06-09 11:04:09 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-09 11:04:11 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun  9 11:04:11.946: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-8299  25724e97-a0ac-4bb2-b924-930ff45933f2 94843 3 2023-06-09 11:04:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 34ed920b-e50d-45ec-9a06-72f6e1930dc1 0xc00070f6e7 0xc00070f6e8}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ed920b-e50d-45ec-9a06-72f6e1930dc1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00070f778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun  9 11:04:11.958: INFO: Pod "test-new-deployment-7f5969cbc7-dd87l" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-dd87l test-new-deployment-7f5969cbc7- deployment-8299  8788410a-c66b-48b2-aec2-5a51be7003f5 94844 0 2023-06-09 11:04:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 25724e97-a0ac-4bb2-b924-930ff45933f2 0xc0015fe987 0xc0015fe988}] [] [{kube-controller-manager Update v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25724e97-a0ac-4bb2-b924-930ff45933f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5cvp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5cvp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-09 11:04:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:04:11.959: INFO: Pod "test-new-deployment-7f5969cbc7-nrwwq" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-nrwwq test-new-deployment-7f5969cbc7- deployment-8299  7d5ea416-40a7-40f3-a401-ebc78696607f 94850 0 2023-06-09 11:04:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 25724e97-a0ac-4bb2-b924-930ff45933f2 0xc0015ff2c7 0xc0015ff2c8}] [] [{kube-controller-manager Update v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25724e97-a0ac-4bb2-b924-930ff45933f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkgjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkgjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:04:11.959: INFO: Pod "test-new-deployment-7f5969cbc7-rmtbd" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-rmtbd test-new-deployment-7f5969cbc7- deployment-8299  ba2fb712-f93a-4848-8953-1d505ae50a1b 94849 0 2023-06-09 11:04:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 25724e97-a0ac-4bb2-b924-930ff45933f2 0xc0015ff7a7 0xc0015ff7a8}] [] [{kube-controller-manager Update v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25724e97-a0ac-4bb2-b924-930ff45933f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nfbnn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nfbnn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:04:11.959: INFO: Pod "test-new-deployment-7f5969cbc7-rwfrr" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-rwfrr test-new-deployment-7f5969cbc7- deployment-8299  8ec5ed41-368e-47f7-a689-be41cf48b1bc 94829 0 2023-06-09 11:04:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e94ee30c79869587da4a1fb649ae2102996d7af2fe8d38c589bafa1679dd88b0 cni.projectcalico.org/podIP:172.27.53.94/32 cni.projectcalico.org/podIPs:172.27.53.94/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 25724e97-a0ac-4bb2-b924-930ff45933f2 0xc0015ffd60 0xc0015ffd61}] [] [{kube-controller-manager Update v1 2023-06-09 11:04:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25724e97-a0ac-4bb2-b924-930ff45933f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:04:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:04:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.27.53.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q67rs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q67rs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:04:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.27.53.94,StartTime:2023-06-09 11:04:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:04:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://308e628c3cbf005fc88634affa22f6b8e2f32e0a231974212621f60f8ed7972b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.27.53.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:04:11.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8299" for this suite. 06/09/23 11:04:11.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:04:11.989
Jun  9 11:04:11.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 11:04:11.99
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:04:12.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:04:12.023
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-1382/configmap-test-af027432-0d95-4503-9f35-17ec57c01fd7 06/09/23 11:04:12.029
STEP: Creating a pod to test consume configMaps 06/09/23 11:04:12.043
Jun  9 11:04:12.059: INFO: Waiting up to 5m0s for pod "pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d" in namespace "configmap-1382" to be "Succeeded or Failed"
Jun  9 11:04:12.070: INFO: Pod "pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.100867ms
Jun  9 11:04:14.076: INFO: Pod "pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016865669s
Jun  9 11:04:16.076: INFO: Pod "pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016998569s
STEP: Saw pod success 06/09/23 11:04:16.076
Jun  9 11:04:16.077: INFO: Pod "pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d" satisfied condition "Succeeded or Failed"
Jun  9 11:04:16.083: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d container env-test: <nil>
STEP: delete the pod 06/09/23 11:04:16.104
Jun  9 11:04:16.131: INFO: Waiting for pod pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d to disappear
Jun  9 11:04:16.140: INFO: Pod pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 11:04:16.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1382" for this suite. 06/09/23 11:04:16.149
------------------------------
• [4.180 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:04:11.989
    Jun  9 11:04:11.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 11:04:11.99
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:04:12.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:04:12.023
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-1382/configmap-test-af027432-0d95-4503-9f35-17ec57c01fd7 06/09/23 11:04:12.029
    STEP: Creating a pod to test consume configMaps 06/09/23 11:04:12.043
    Jun  9 11:04:12.059: INFO: Waiting up to 5m0s for pod "pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d" in namespace "configmap-1382" to be "Succeeded or Failed"
    Jun  9 11:04:12.070: INFO: Pod "pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.100867ms
    Jun  9 11:04:14.076: INFO: Pod "pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016865669s
    Jun  9 11:04:16.076: INFO: Pod "pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016998569s
    STEP: Saw pod success 06/09/23 11:04:16.076
    Jun  9 11:04:16.077: INFO: Pod "pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d" satisfied condition "Succeeded or Failed"
    Jun  9 11:04:16.083: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d container env-test: <nil>
    STEP: delete the pod 06/09/23 11:04:16.104
    Jun  9 11:04:16.131: INFO: Waiting for pod pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d to disappear
    Jun  9 11:04:16.140: INFO: Pod pod-configmaps-3e4fe20d-a8bf-43e5-8173-ad366b224e0d no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:04:16.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1382" for this suite. 06/09/23 11:04:16.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:04:16.169
Jun  9 11:04:16.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-probe 06/09/23 11:04:16.171
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:04:16.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:04:16.212
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-ddee036d-0be4-4020-b5b5-529864a29d60 in namespace container-probe-4322 06/09/23 11:04:16.218
Jun  9 11:04:16.233: INFO: Waiting up to 5m0s for pod "liveness-ddee036d-0be4-4020-b5b5-529864a29d60" in namespace "container-probe-4322" to be "not pending"
Jun  9 11:04:16.248: INFO: Pod "liveness-ddee036d-0be4-4020-b5b5-529864a29d60": Phase="Pending", Reason="", readiness=false. Elapsed: 15.273525ms
Jun  9 11:04:18.256: INFO: Pod "liveness-ddee036d-0be4-4020-b5b5-529864a29d60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022426088s
Jun  9 11:04:20.256: INFO: Pod "liveness-ddee036d-0be4-4020-b5b5-529864a29d60": Phase="Running", Reason="", readiness=true. Elapsed: 4.022723053s
Jun  9 11:04:20.256: INFO: Pod "liveness-ddee036d-0be4-4020-b5b5-529864a29d60" satisfied condition "not pending"
Jun  9 11:04:20.256: INFO: Started pod liveness-ddee036d-0be4-4020-b5b5-529864a29d60 in namespace container-probe-4322
STEP: checking the pod's current state and verifying that restartCount is present 06/09/23 11:04:20.256
Jun  9 11:04:20.263: INFO: Initial restart count of pod liveness-ddee036d-0be4-4020-b5b5-529864a29d60 is 0
STEP: deleting the pod 06/09/23 11:08:21.341
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  9 11:08:21.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4322" for this suite. 06/09/23 11:08:21.529
------------------------------
• [SLOW TEST] [245.372 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:04:16.169
    Jun  9 11:04:16.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-probe 06/09/23 11:04:16.171
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:04:16.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:04:16.212
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-ddee036d-0be4-4020-b5b5-529864a29d60 in namespace container-probe-4322 06/09/23 11:04:16.218
    Jun  9 11:04:16.233: INFO: Waiting up to 5m0s for pod "liveness-ddee036d-0be4-4020-b5b5-529864a29d60" in namespace "container-probe-4322" to be "not pending"
    Jun  9 11:04:16.248: INFO: Pod "liveness-ddee036d-0be4-4020-b5b5-529864a29d60": Phase="Pending", Reason="", readiness=false. Elapsed: 15.273525ms
    Jun  9 11:04:18.256: INFO: Pod "liveness-ddee036d-0be4-4020-b5b5-529864a29d60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022426088s
    Jun  9 11:04:20.256: INFO: Pod "liveness-ddee036d-0be4-4020-b5b5-529864a29d60": Phase="Running", Reason="", readiness=true. Elapsed: 4.022723053s
    Jun  9 11:04:20.256: INFO: Pod "liveness-ddee036d-0be4-4020-b5b5-529864a29d60" satisfied condition "not pending"
    Jun  9 11:04:20.256: INFO: Started pod liveness-ddee036d-0be4-4020-b5b5-529864a29d60 in namespace container-probe-4322
    STEP: checking the pod's current state and verifying that restartCount is present 06/09/23 11:04:20.256
    Jun  9 11:04:20.263: INFO: Initial restart count of pod liveness-ddee036d-0be4-4020-b5b5-529864a29d60 is 0
    STEP: deleting the pod 06/09/23 11:08:21.341
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:08:21.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4322" for this suite. 06/09/23 11:08:21.529
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:08:21.542
Jun  9 11:08:21.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-probe 06/09/23 11:08:21.543
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:08:21.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:08:21.62
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364 in namespace container-probe-8240 06/09/23 11:08:21.626
Jun  9 11:08:21.640: INFO: Waiting up to 5m0s for pod "busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364" in namespace "container-probe-8240" to be "not pending"
Jun  9 11:08:21.649: INFO: Pod "busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364": Phase="Pending", Reason="", readiness=false. Elapsed: 8.484447ms
Jun  9 11:08:23.655: INFO: Pod "busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364": Phase="Running", Reason="", readiness=true. Elapsed: 2.014832045s
Jun  9 11:08:23.655: INFO: Pod "busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364" satisfied condition "not pending"
Jun  9 11:08:23.655: INFO: Started pod busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364 in namespace container-probe-8240
STEP: checking the pod's current state and verifying that restartCount is present 06/09/23 11:08:23.655
Jun  9 11:08:23.661: INFO: Initial restart count of pod busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364 is 0
Jun  9 11:09:13.934: INFO: Restart count of pod container-probe-8240/busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364 is now 1 (50.272622506s elapsed)
STEP: deleting the pod 06/09/23 11:09:13.934
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  9 11:09:13.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8240" for this suite. 06/09/23 11:09:13.963
------------------------------
• [SLOW TEST] [52.431 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:08:21.542
    Jun  9 11:08:21.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-probe 06/09/23 11:08:21.543
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:08:21.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:08:21.62
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364 in namespace container-probe-8240 06/09/23 11:08:21.626
    Jun  9 11:08:21.640: INFO: Waiting up to 5m0s for pod "busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364" in namespace "container-probe-8240" to be "not pending"
    Jun  9 11:08:21.649: INFO: Pod "busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364": Phase="Pending", Reason="", readiness=false. Elapsed: 8.484447ms
    Jun  9 11:08:23.655: INFO: Pod "busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364": Phase="Running", Reason="", readiness=true. Elapsed: 2.014832045s
    Jun  9 11:08:23.655: INFO: Pod "busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364" satisfied condition "not pending"
    Jun  9 11:08:23.655: INFO: Started pod busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364 in namespace container-probe-8240
    STEP: checking the pod's current state and verifying that restartCount is present 06/09/23 11:08:23.655
    Jun  9 11:08:23.661: INFO: Initial restart count of pod busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364 is 0
    Jun  9 11:09:13.934: INFO: Restart count of pod container-probe-8240/busybox-185c5e8b-8450-4fd9-befc-66b36c3f4364 is now 1 (50.272622506s elapsed)
    STEP: deleting the pod 06/09/23 11:09:13.934
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:09:13.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8240" for this suite. 06/09/23 11:09:13.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:09:13.973
Jun  9 11:09:13.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename security-context-test 06/09/23 11:09:13.975
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:09:13.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:09:13.999
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Jun  9 11:09:14.018: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-4b7c53a9-d4f9-49c9-a442-e41da6ae9422" in namespace "security-context-test-1865" to be "Succeeded or Failed"
Jun  9 11:09:14.025: INFO: Pod "alpine-nnp-false-4b7c53a9-d4f9-49c9-a442-e41da6ae9422": Phase="Pending", Reason="", readiness=false. Elapsed: 6.216265ms
Jun  9 11:09:16.032: INFO: Pod "alpine-nnp-false-4b7c53a9-d4f9-49c9-a442-e41da6ae9422": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01392078s
Jun  9 11:09:18.034: INFO: Pod "alpine-nnp-false-4b7c53a9-d4f9-49c9-a442-e41da6ae9422": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015209983s
Jun  9 11:09:20.071: INFO: Pod "alpine-nnp-false-4b7c53a9-d4f9-49c9-a442-e41da6ae9422": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052799963s
Jun  9 11:09:20.071: INFO: Pod "alpine-nnp-false-4b7c53a9-d4f9-49c9-a442-e41da6ae9422" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun  9 11:09:20.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1865" for this suite. 06/09/23 11:09:20.099
------------------------------
• [SLOW TEST] [6.135 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:09:13.973
    Jun  9 11:09:13.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename security-context-test 06/09/23 11:09:13.975
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:09:13.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:09:13.999
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Jun  9 11:09:14.018: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-4b7c53a9-d4f9-49c9-a442-e41da6ae9422" in namespace "security-context-test-1865" to be "Succeeded or Failed"
    Jun  9 11:09:14.025: INFO: Pod "alpine-nnp-false-4b7c53a9-d4f9-49c9-a442-e41da6ae9422": Phase="Pending", Reason="", readiness=false. Elapsed: 6.216265ms
    Jun  9 11:09:16.032: INFO: Pod "alpine-nnp-false-4b7c53a9-d4f9-49c9-a442-e41da6ae9422": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01392078s
    Jun  9 11:09:18.034: INFO: Pod "alpine-nnp-false-4b7c53a9-d4f9-49c9-a442-e41da6ae9422": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015209983s
    Jun  9 11:09:20.071: INFO: Pod "alpine-nnp-false-4b7c53a9-d4f9-49c9-a442-e41da6ae9422": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052799963s
    Jun  9 11:09:20.071: INFO: Pod "alpine-nnp-false-4b7c53a9-d4f9-49c9-a442-e41da6ae9422" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:09:20.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1865" for this suite. 06/09/23 11:09:20.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:09:20.109
Jun  9 11:09:20.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 11:09:20.111
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:09:20.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:09:20.14
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6447 06/09/23 11:09:20.144
STEP: changing the ExternalName service to type=ClusterIP 06/09/23 11:09:20.152
STEP: creating replication controller externalname-service in namespace services-6447 06/09/23 11:09:20.19
I0609 11:09:20.198949      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6447, replica count: 2
I0609 11:09:23.250020      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 11:09:23.250: INFO: Creating new exec pod
Jun  9 11:09:23.277: INFO: Waiting up to 5m0s for pod "execpod2n7cd" in namespace "services-6447" to be "running"
Jun  9 11:09:23.284: INFO: Pod "execpod2n7cd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.622384ms
Jun  9 11:09:25.291: INFO: Pod "execpod2n7cd": Phase="Running", Reason="", readiness=true. Elapsed: 2.013414542s
Jun  9 11:09:25.291: INFO: Pod "execpod2n7cd" satisfied condition "running"
Jun  9 11:09:26.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6447 exec execpod2n7cd -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jun  9 11:09:26.506: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  9 11:09:26.506: INFO: stdout: ""
Jun  9 11:09:26.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6447 exec execpod2n7cd -- /bin/sh -x -c nc -v -z -w 2 10.98.48.33 80'
Jun  9 11:09:26.669: INFO: stderr: "+ nc -v -z -w 2 10.98.48.33 80\nConnection to 10.98.48.33 80 port [tcp/http] succeeded!\n"
Jun  9 11:09:26.669: INFO: stdout: ""
Jun  9 11:09:26.669: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 11:09:26.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6447" for this suite. 06/09/23 11:09:26.787
------------------------------
• [SLOW TEST] [6.691 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:09:20.109
    Jun  9 11:09:20.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 11:09:20.111
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:09:20.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:09:20.14
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6447 06/09/23 11:09:20.144
    STEP: changing the ExternalName service to type=ClusterIP 06/09/23 11:09:20.152
    STEP: creating replication controller externalname-service in namespace services-6447 06/09/23 11:09:20.19
    I0609 11:09:20.198949      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6447, replica count: 2
    I0609 11:09:23.250020      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  9 11:09:23.250: INFO: Creating new exec pod
    Jun  9 11:09:23.277: INFO: Waiting up to 5m0s for pod "execpod2n7cd" in namespace "services-6447" to be "running"
    Jun  9 11:09:23.284: INFO: Pod "execpod2n7cd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.622384ms
    Jun  9 11:09:25.291: INFO: Pod "execpod2n7cd": Phase="Running", Reason="", readiness=true. Elapsed: 2.013414542s
    Jun  9 11:09:25.291: INFO: Pod "execpod2n7cd" satisfied condition "running"
    Jun  9 11:09:26.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6447 exec execpod2n7cd -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jun  9 11:09:26.506: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun  9 11:09:26.506: INFO: stdout: ""
    Jun  9 11:09:26.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-6447 exec execpod2n7cd -- /bin/sh -x -c nc -v -z -w 2 10.98.48.33 80'
    Jun  9 11:09:26.669: INFO: stderr: "+ nc -v -z -w 2 10.98.48.33 80\nConnection to 10.98.48.33 80 port [tcp/http] succeeded!\n"
    Jun  9 11:09:26.669: INFO: stdout: ""
    Jun  9 11:09:26.669: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:09:26.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6447" for this suite. 06/09/23 11:09:26.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:09:26.801
Jun  9 11:09:26.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 11:09:26.802
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:09:26.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:09:26.831
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 06/09/23 11:09:26.837
Jun  9 11:09:26.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855" in namespace "projected-3611" to be "Succeeded or Failed"
Jun  9 11:09:26.864: INFO: Pod "downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855": Phase="Pending", Reason="", readiness=false. Elapsed: 9.567314ms
Jun  9 11:09:28.871: INFO: Pod "downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01643743s
Jun  9 11:09:30.884: INFO: Pod "downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0294738s
STEP: Saw pod success 06/09/23 11:09:30.884
Jun  9 11:09:30.884: INFO: Pod "downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855" satisfied condition "Succeeded or Failed"
Jun  9 11:09:30.889: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855 container client-container: <nil>
STEP: delete the pod 06/09/23 11:09:30.917
Jun  9 11:09:30.933: INFO: Waiting for pod downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855 to disappear
Jun  9 11:09:30.939: INFO: Pod downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  9 11:09:30.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3611" for this suite. 06/09/23 11:09:30.946
------------------------------
• [4.171 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:09:26.801
    Jun  9 11:09:26.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 11:09:26.802
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:09:26.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:09:26.831
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 06/09/23 11:09:26.837
    Jun  9 11:09:26.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855" in namespace "projected-3611" to be "Succeeded or Failed"
    Jun  9 11:09:26.864: INFO: Pod "downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855": Phase="Pending", Reason="", readiness=false. Elapsed: 9.567314ms
    Jun  9 11:09:28.871: INFO: Pod "downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01643743s
    Jun  9 11:09:30.884: INFO: Pod "downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0294738s
    STEP: Saw pod success 06/09/23 11:09:30.884
    Jun  9 11:09:30.884: INFO: Pod "downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855" satisfied condition "Succeeded or Failed"
    Jun  9 11:09:30.889: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855 container client-container: <nil>
    STEP: delete the pod 06/09/23 11:09:30.917
    Jun  9 11:09:30.933: INFO: Waiting for pod downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855 to disappear
    Jun  9 11:09:30.939: INFO: Pod downwardapi-volume-dbfcf91b-48b1-4307-86b6-931a7e275855 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:09:30.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3611" for this suite. 06/09/23 11:09:30.946
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:09:30.973
Jun  9 11:09:30.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 11:09:30.974
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:09:31.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:09:31.01
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-63f2ed22-b4ec-4930-9193-a1cb1eef7efa 06/09/23 11:09:31.014
STEP: Creating a pod to test consume configMaps 06/09/23 11:09:31.06
Jun  9 11:09:31.104: INFO: Waiting up to 5m0s for pod "pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98" in namespace "configmap-230" to be "Succeeded or Failed"
Jun  9 11:09:31.111: INFO: Pod "pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98": Phase="Pending", Reason="", readiness=false. Elapsed: 7.235121ms
Jun  9 11:09:33.118: INFO: Pod "pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98": Phase="Running", Reason="", readiness=true. Elapsed: 2.014018685s
Jun  9 11:09:35.118: INFO: Pod "pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98": Phase="Running", Reason="", readiness=false. Elapsed: 4.014207158s
Jun  9 11:09:37.120: INFO: Pod "pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015955955s
STEP: Saw pod success 06/09/23 11:09:37.12
Jun  9 11:09:37.120: INFO: Pod "pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98" satisfied condition "Succeeded or Failed"
Jun  9 11:09:37.127: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98 container agnhost-container: <nil>
STEP: delete the pod 06/09/23 11:09:37.149
Jun  9 11:09:37.167: INFO: Waiting for pod pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98 to disappear
Jun  9 11:09:37.173: INFO: Pod pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 11:09:37.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-230" for this suite. 06/09/23 11:09:37.18
------------------------------
• [SLOW TEST] [6.218 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:09:30.973
    Jun  9 11:09:30.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 11:09:30.974
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:09:31.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:09:31.01
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-63f2ed22-b4ec-4930-9193-a1cb1eef7efa 06/09/23 11:09:31.014
    STEP: Creating a pod to test consume configMaps 06/09/23 11:09:31.06
    Jun  9 11:09:31.104: INFO: Waiting up to 5m0s for pod "pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98" in namespace "configmap-230" to be "Succeeded or Failed"
    Jun  9 11:09:31.111: INFO: Pod "pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98": Phase="Pending", Reason="", readiness=false. Elapsed: 7.235121ms
    Jun  9 11:09:33.118: INFO: Pod "pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98": Phase="Running", Reason="", readiness=true. Elapsed: 2.014018685s
    Jun  9 11:09:35.118: INFO: Pod "pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98": Phase="Running", Reason="", readiness=false. Elapsed: 4.014207158s
    Jun  9 11:09:37.120: INFO: Pod "pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015955955s
    STEP: Saw pod success 06/09/23 11:09:37.12
    Jun  9 11:09:37.120: INFO: Pod "pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98" satisfied condition "Succeeded or Failed"
    Jun  9 11:09:37.127: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98 container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 11:09:37.149
    Jun  9 11:09:37.167: INFO: Waiting for pod pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98 to disappear
    Jun  9 11:09:37.173: INFO: Pod pod-configmaps-6baee5c2-95fd-4e1d-bd21-debc270fad98 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:09:37.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-230" for this suite. 06/09/23 11:09:37.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:09:37.199
Jun  9 11:09:37.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename aggregator 06/09/23 11:09:37.201
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:09:37.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:09:37.228
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jun  9 11:09:37.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 06/09/23 11:09:37.234
Jun  9 11:09:37.644: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jun  9 11:09:39.729: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:09:41.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:09:43.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:09:45.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:09:47.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:09:49.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:09:51.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:09:53.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:09:55.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:09:57.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:09:59.738: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:01.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:03.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:05.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:07.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:09.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:11.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:13.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:15.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:17.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:19.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:21.742: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:23.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:25.738: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:27.740: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:29.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:31.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:33.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:35.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:37.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:39.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:41.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:43.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:45.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:47.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:49.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:10:51.888: INFO: Waited 126.25586ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 06/09/23 11:10:51.969
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 06/09/23 11:10:51.975
STEP: List APIServices 06/09/23 11:10:51.986
Jun  9 11:10:51.994: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jun  9 11:10:52.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-1811" for this suite. 06/09/23 11:10:52.49
------------------------------
• [SLOW TEST] [75.343 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:09:37.199
    Jun  9 11:09:37.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename aggregator 06/09/23 11:09:37.201
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:09:37.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:09:37.228
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jun  9 11:09:37.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 06/09/23 11:09:37.234
    Jun  9 11:09:37.644: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Jun  9 11:09:39.729: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:09:41.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:09:43.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:09:45.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:09:47.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:09:49.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:09:51.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:09:53.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:09:55.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:09:57.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:09:59.738: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:01.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:03.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:05.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:07.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:09.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:11.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:13.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:15.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:17.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:19.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:21.742: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:23.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:25.738: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:27.740: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:29.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:31.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:33.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:35.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:37.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:39.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:41.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:43.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:45.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:47.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:49.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 9, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:10:51.888: INFO: Waited 126.25586ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 06/09/23 11:10:51.969
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 06/09/23 11:10:51.975
    STEP: List APIServices 06/09/23 11:10:51.986
    Jun  9 11:10:51.994: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:10:52.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-1811" for this suite. 06/09/23 11:10:52.49
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:10:52.544
Jun  9 11:10:52.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-probe 06/09/23 11:10:52.546
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:10:52.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:10:52.587
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-ab8ee4d6-d700-4fd1-a5cf-53fcb9d5d3e8 in namespace container-probe-6822 06/09/23 11:10:52.594
Jun  9 11:10:52.634: INFO: Waiting up to 5m0s for pod "test-webserver-ab8ee4d6-d700-4fd1-a5cf-53fcb9d5d3e8" in namespace "container-probe-6822" to be "not pending"
Jun  9 11:10:52.641: INFO: Pod "test-webserver-ab8ee4d6-d700-4fd1-a5cf-53fcb9d5d3e8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.209689ms
Jun  9 11:10:54.649: INFO: Pod "test-webserver-ab8ee4d6-d700-4fd1-a5cf-53fcb9d5d3e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.014485318s
Jun  9 11:10:54.649: INFO: Pod "test-webserver-ab8ee4d6-d700-4fd1-a5cf-53fcb9d5d3e8" satisfied condition "not pending"
Jun  9 11:10:54.649: INFO: Started pod test-webserver-ab8ee4d6-d700-4fd1-a5cf-53fcb9d5d3e8 in namespace container-probe-6822
STEP: checking the pod's current state and verifying that restartCount is present 06/09/23 11:10:54.649
Jun  9 11:10:54.654: INFO: Initial restart count of pod test-webserver-ab8ee4d6-d700-4fd1-a5cf-53fcb9d5d3e8 is 0
STEP: deleting the pod 06/09/23 11:14:56.157
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  9 11:14:56.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6822" for this suite. 06/09/23 11:14:56.194
------------------------------
• [SLOW TEST] [243.668 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:10:52.544
    Jun  9 11:10:52.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-probe 06/09/23 11:10:52.546
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:10:52.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:10:52.587
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-ab8ee4d6-d700-4fd1-a5cf-53fcb9d5d3e8 in namespace container-probe-6822 06/09/23 11:10:52.594
    Jun  9 11:10:52.634: INFO: Waiting up to 5m0s for pod "test-webserver-ab8ee4d6-d700-4fd1-a5cf-53fcb9d5d3e8" in namespace "container-probe-6822" to be "not pending"
    Jun  9 11:10:52.641: INFO: Pod "test-webserver-ab8ee4d6-d700-4fd1-a5cf-53fcb9d5d3e8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.209689ms
    Jun  9 11:10:54.649: INFO: Pod "test-webserver-ab8ee4d6-d700-4fd1-a5cf-53fcb9d5d3e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.014485318s
    Jun  9 11:10:54.649: INFO: Pod "test-webserver-ab8ee4d6-d700-4fd1-a5cf-53fcb9d5d3e8" satisfied condition "not pending"
    Jun  9 11:10:54.649: INFO: Started pod test-webserver-ab8ee4d6-d700-4fd1-a5cf-53fcb9d5d3e8 in namespace container-probe-6822
    STEP: checking the pod's current state and verifying that restartCount is present 06/09/23 11:10:54.649
    Jun  9 11:10:54.654: INFO: Initial restart count of pod test-webserver-ab8ee4d6-d700-4fd1-a5cf-53fcb9d5d3e8 is 0
    STEP: deleting the pod 06/09/23 11:14:56.157
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:14:56.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6822" for this suite. 06/09/23 11:14:56.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:14:56.213
Jun  9 11:14:56.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 11:14:56.215
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:14:56.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:14:56.277
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-9f029538-fdf2-46a4-b44f-20496f213398 06/09/23 11:14:56.283
STEP: Creating a pod to test consume secrets 06/09/23 11:14:56.293
Jun  9 11:14:56.335: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70" in namespace "projected-3734" to be "Succeeded or Failed"
Jun  9 11:14:56.344: INFO: Pod "pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70": Phase="Pending", Reason="", readiness=false. Elapsed: 9.316026ms
Jun  9 11:14:58.353: INFO: Pod "pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017905234s
Jun  9 11:15:00.351: INFO: Pod "pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015621893s
STEP: Saw pod success 06/09/23 11:15:00.351
Jun  9 11:15:00.351: INFO: Pod "pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70" satisfied condition "Succeeded or Failed"
Jun  9 11:15:00.357: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/09/23 11:15:00.385
Jun  9 11:15:00.409: INFO: Waiting for pod pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70 to disappear
Jun  9 11:15:00.416: INFO: Pod pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun  9 11:15:00.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3734" for this suite. 06/09/23 11:15:00.424
------------------------------
• [4.228 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:14:56.213
    Jun  9 11:14:56.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 11:14:56.215
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:14:56.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:14:56.277
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-9f029538-fdf2-46a4-b44f-20496f213398 06/09/23 11:14:56.283
    STEP: Creating a pod to test consume secrets 06/09/23 11:14:56.293
    Jun  9 11:14:56.335: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70" in namespace "projected-3734" to be "Succeeded or Failed"
    Jun  9 11:14:56.344: INFO: Pod "pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70": Phase="Pending", Reason="", readiness=false. Elapsed: 9.316026ms
    Jun  9 11:14:58.353: INFO: Pod "pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017905234s
    Jun  9 11:15:00.351: INFO: Pod "pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015621893s
    STEP: Saw pod success 06/09/23 11:15:00.351
    Jun  9 11:15:00.351: INFO: Pod "pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70" satisfied condition "Succeeded or Failed"
    Jun  9 11:15:00.357: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/09/23 11:15:00.385
    Jun  9 11:15:00.409: INFO: Waiting for pod pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70 to disappear
    Jun  9 11:15:00.416: INFO: Pod pod-projected-secrets-44d1987c-9e9d-47af-9f37-7be6ff614a70 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:15:00.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3734" for this suite. 06/09/23 11:15:00.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:15:00.443
Jun  9 11:15:00.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename lease-test 06/09/23 11:15:00.444
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:15:00.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:15:00.495
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jun  9 11:15:00.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-1581" for this suite. 06/09/23 11:15:00.616
------------------------------
• [0.187 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:15:00.443
    Jun  9 11:15:00.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename lease-test 06/09/23 11:15:00.444
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:15:00.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:15:00.495
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:15:00.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-1581" for this suite. 06/09/23 11:15:00.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:15:00.631
Jun  9 11:15:00.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename var-expansion 06/09/23 11:15:00.632
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:15:00.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:15:00.676
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 06/09/23 11:15:00.679
STEP: waiting for pod running 06/09/23 11:15:00.698
Jun  9 11:15:00.698: INFO: Waiting up to 2m0s for pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b" in namespace "var-expansion-2339" to be "running"
Jun  9 11:15:00.704: INFO: Pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.784407ms
Jun  9 11:15:02.712: INFO: Pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b": Phase="Running", Reason="", readiness=true. Elapsed: 2.014258236s
Jun  9 11:15:02.712: INFO: Pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b" satisfied condition "running"
STEP: creating a file in subpath 06/09/23 11:15:02.712
Jun  9 11:15:02.720: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2339 PodName:var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 11:15:02.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 11:15:02.721: INFO: ExecWithOptions: Clientset creation
Jun  9 11:15:02.721: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2339/pods/var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 06/09/23 11:15:02.828
Jun  9 11:15:02.837: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2339 PodName:var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 11:15:02.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 11:15:02.837: INFO: ExecWithOptions: Clientset creation
Jun  9 11:15:02.837: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2339/pods/var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 06/09/23 11:15:02.923
Jun  9 11:15:03.444: INFO: Successfully updated pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b"
STEP: waiting for annotated pod running 06/09/23 11:15:03.444
Jun  9 11:15:03.444: INFO: Waiting up to 2m0s for pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b" in namespace "var-expansion-2339" to be "running"
Jun  9 11:15:03.450: INFO: Pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b": Phase="Running", Reason="", readiness=true. Elapsed: 6.014674ms
Jun  9 11:15:03.450: INFO: Pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b" satisfied condition "running"
STEP: deleting the pod gracefully 06/09/23 11:15:03.45
Jun  9 11:15:03.450: INFO: Deleting pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b" in namespace "var-expansion-2339"
Jun  9 11:15:03.464: INFO: Wait up to 5m0s for pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  9 11:15:37.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2339" for this suite. 06/09/23 11:15:37.485
------------------------------
• [SLOW TEST] [36.927 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:15:00.631
    Jun  9 11:15:00.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename var-expansion 06/09/23 11:15:00.632
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:15:00.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:15:00.676
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 06/09/23 11:15:00.679
    STEP: waiting for pod running 06/09/23 11:15:00.698
    Jun  9 11:15:00.698: INFO: Waiting up to 2m0s for pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b" in namespace "var-expansion-2339" to be "running"
    Jun  9 11:15:00.704: INFO: Pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.784407ms
    Jun  9 11:15:02.712: INFO: Pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b": Phase="Running", Reason="", readiness=true. Elapsed: 2.014258236s
    Jun  9 11:15:02.712: INFO: Pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b" satisfied condition "running"
    STEP: creating a file in subpath 06/09/23 11:15:02.712
    Jun  9 11:15:02.720: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2339 PodName:var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 11:15:02.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 11:15:02.721: INFO: ExecWithOptions: Clientset creation
    Jun  9 11:15:02.721: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2339/pods/var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 06/09/23 11:15:02.828
    Jun  9 11:15:02.837: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2339 PodName:var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 11:15:02.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 11:15:02.837: INFO: ExecWithOptions: Clientset creation
    Jun  9 11:15:02.837: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2339/pods/var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 06/09/23 11:15:02.923
    Jun  9 11:15:03.444: INFO: Successfully updated pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b"
    STEP: waiting for annotated pod running 06/09/23 11:15:03.444
    Jun  9 11:15:03.444: INFO: Waiting up to 2m0s for pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b" in namespace "var-expansion-2339" to be "running"
    Jun  9 11:15:03.450: INFO: Pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b": Phase="Running", Reason="", readiness=true. Elapsed: 6.014674ms
    Jun  9 11:15:03.450: INFO: Pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b" satisfied condition "running"
    STEP: deleting the pod gracefully 06/09/23 11:15:03.45
    Jun  9 11:15:03.450: INFO: Deleting pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b" in namespace "var-expansion-2339"
    Jun  9 11:15:03.464: INFO: Wait up to 5m0s for pod "var-expansion-08d706bb-2b0e-4bff-8274-11ec81b8926b" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:15:37.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2339" for this suite. 06/09/23 11:15:37.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:15:37.559
Jun  9 11:15:37.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pods 06/09/23 11:15:37.561
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:15:37.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:15:37.64
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jun  9 11:15:37.644: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: creating the pod 06/09/23 11:15:37.645
STEP: submitting the pod to kubernetes 06/09/23 11:15:37.645
Jun  9 11:15:37.706: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-8ae44dbb-1548-4817-872b-d54b28688afb" in namespace "pods-885" to be "running and ready"
Jun  9 11:15:37.712: INFO: Pod "pod-exec-websocket-8ae44dbb-1548-4817-872b-d54b28688afb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.39717ms
Jun  9 11:15:37.712: INFO: The phase of Pod pod-exec-websocket-8ae44dbb-1548-4817-872b-d54b28688afb is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:15:39.720: INFO: Pod "pod-exec-websocket-8ae44dbb-1548-4817-872b-d54b28688afb": Phase="Running", Reason="", readiness=true. Elapsed: 2.013971855s
Jun  9 11:15:39.720: INFO: The phase of Pod pod-exec-websocket-8ae44dbb-1548-4817-872b-d54b28688afb is Running (Ready = true)
Jun  9 11:15:39.720: INFO: Pod "pod-exec-websocket-8ae44dbb-1548-4817-872b-d54b28688afb" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  9 11:15:39.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-885" for this suite. 06/09/23 11:15:39.837
------------------------------
• [2.310 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:15:37.559
    Jun  9 11:15:37.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pods 06/09/23 11:15:37.561
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:15:37.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:15:37.64
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jun  9 11:15:37.644: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: creating the pod 06/09/23 11:15:37.645
    STEP: submitting the pod to kubernetes 06/09/23 11:15:37.645
    Jun  9 11:15:37.706: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-8ae44dbb-1548-4817-872b-d54b28688afb" in namespace "pods-885" to be "running and ready"
    Jun  9 11:15:37.712: INFO: Pod "pod-exec-websocket-8ae44dbb-1548-4817-872b-d54b28688afb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.39717ms
    Jun  9 11:15:37.712: INFO: The phase of Pod pod-exec-websocket-8ae44dbb-1548-4817-872b-d54b28688afb is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:15:39.720: INFO: Pod "pod-exec-websocket-8ae44dbb-1548-4817-872b-d54b28688afb": Phase="Running", Reason="", readiness=true. Elapsed: 2.013971855s
    Jun  9 11:15:39.720: INFO: The phase of Pod pod-exec-websocket-8ae44dbb-1548-4817-872b-d54b28688afb is Running (Ready = true)
    Jun  9 11:15:39.720: INFO: Pod "pod-exec-websocket-8ae44dbb-1548-4817-872b-d54b28688afb" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:15:39.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-885" for this suite. 06/09/23 11:15:39.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:15:39.871
Jun  9 11:15:39.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename var-expansion 06/09/23 11:15:39.872
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:15:39.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:15:39.913
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 06/09/23 11:15:39.917
Jun  9 11:15:39.931: INFO: Waiting up to 5m0s for pod "var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54" in namespace "var-expansion-8039" to be "Succeeded or Failed"
Jun  9 11:15:39.939: INFO: Pod "var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54": Phase="Pending", Reason="", readiness=false. Elapsed: 7.59588ms
Jun  9 11:15:41.945: INFO: Pod "var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013911377s
Jun  9 11:15:43.945: INFO: Pod "var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013848312s
STEP: Saw pod success 06/09/23 11:15:43.945
Jun  9 11:15:43.945: INFO: Pod "var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54" satisfied condition "Succeeded or Failed"
Jun  9 11:15:43.952: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54 container dapi-container: <nil>
STEP: delete the pod 06/09/23 11:15:43.974
Jun  9 11:15:43.991: INFO: Waiting for pod var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54 to disappear
Jun  9 11:15:43.996: INFO: Pod var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  9 11:15:43.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8039" for this suite. 06/09/23 11:15:44.004
------------------------------
• [4.180 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:15:39.871
    Jun  9 11:15:39.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename var-expansion 06/09/23 11:15:39.872
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:15:39.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:15:39.913
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 06/09/23 11:15:39.917
    Jun  9 11:15:39.931: INFO: Waiting up to 5m0s for pod "var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54" in namespace "var-expansion-8039" to be "Succeeded or Failed"
    Jun  9 11:15:39.939: INFO: Pod "var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54": Phase="Pending", Reason="", readiness=false. Elapsed: 7.59588ms
    Jun  9 11:15:41.945: INFO: Pod "var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013911377s
    Jun  9 11:15:43.945: INFO: Pod "var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013848312s
    STEP: Saw pod success 06/09/23 11:15:43.945
    Jun  9 11:15:43.945: INFO: Pod "var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54" satisfied condition "Succeeded or Failed"
    Jun  9 11:15:43.952: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54 container dapi-container: <nil>
    STEP: delete the pod 06/09/23 11:15:43.974
    Jun  9 11:15:43.991: INFO: Waiting for pod var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54 to disappear
    Jun  9 11:15:43.996: INFO: Pod var-expansion-2c67ce01-5d75-4da7-8f1b-d23854ce9d54 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:15:43.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8039" for this suite. 06/09/23 11:15:44.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:15:44.052
Jun  9 11:15:44.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 11:15:44.053
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:15:44.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:15:44.103
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 06/09/23 11:15:44.11
Jun  9 11:15:44.122: INFO: Waiting up to 5m0s for pod "labelsupdatec67ea750-dae6-4402-93aa-b4db0a7d59f6" in namespace "downward-api-1702" to be "running and ready"
Jun  9 11:15:44.133: INFO: Pod "labelsupdatec67ea750-dae6-4402-93aa-b4db0a7d59f6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.805844ms
Jun  9 11:15:44.133: INFO: The phase of Pod labelsupdatec67ea750-dae6-4402-93aa-b4db0a7d59f6 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:15:46.146: INFO: Pod "labelsupdatec67ea750-dae6-4402-93aa-b4db0a7d59f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.023203271s
Jun  9 11:15:46.146: INFO: The phase of Pod labelsupdatec67ea750-dae6-4402-93aa-b4db0a7d59f6 is Running (Ready = true)
Jun  9 11:15:46.146: INFO: Pod "labelsupdatec67ea750-dae6-4402-93aa-b4db0a7d59f6" satisfied condition "running and ready"
Jun  9 11:15:46.741: INFO: Successfully updated pod "labelsupdatec67ea750-dae6-4402-93aa-b4db0a7d59f6"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  9 11:15:50.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1702" for this suite. 06/09/23 11:15:50.841
------------------------------
• [SLOW TEST] [6.827 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:15:44.052
    Jun  9 11:15:44.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 11:15:44.053
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:15:44.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:15:44.103
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 06/09/23 11:15:44.11
    Jun  9 11:15:44.122: INFO: Waiting up to 5m0s for pod "labelsupdatec67ea750-dae6-4402-93aa-b4db0a7d59f6" in namespace "downward-api-1702" to be "running and ready"
    Jun  9 11:15:44.133: INFO: Pod "labelsupdatec67ea750-dae6-4402-93aa-b4db0a7d59f6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.805844ms
    Jun  9 11:15:44.133: INFO: The phase of Pod labelsupdatec67ea750-dae6-4402-93aa-b4db0a7d59f6 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:15:46.146: INFO: Pod "labelsupdatec67ea750-dae6-4402-93aa-b4db0a7d59f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.023203271s
    Jun  9 11:15:46.146: INFO: The phase of Pod labelsupdatec67ea750-dae6-4402-93aa-b4db0a7d59f6 is Running (Ready = true)
    Jun  9 11:15:46.146: INFO: Pod "labelsupdatec67ea750-dae6-4402-93aa-b4db0a7d59f6" satisfied condition "running and ready"
    Jun  9 11:15:46.741: INFO: Successfully updated pod "labelsupdatec67ea750-dae6-4402-93aa-b4db0a7d59f6"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:15:50.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1702" for this suite. 06/09/23 11:15:50.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:15:50.879
Jun  9 11:15:50.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 11:15:50.88
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:15:50.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:15:50.962
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 06/09/23 11:15:50.966
Jun  9 11:15:51.002: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394" in namespace "downward-api-833" to be "Succeeded or Failed"
Jun  9 11:15:51.114: INFO: Pod "downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394": Phase="Pending", Reason="", readiness=false. Elapsed: 111.799325ms
Jun  9 11:15:53.179: INFO: Pod "downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.17693479s
Jun  9 11:15:55.123: INFO: Pod "downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.121053114s
STEP: Saw pod success 06/09/23 11:15:55.123
Jun  9 11:15:55.124: INFO: Pod "downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394" satisfied condition "Succeeded or Failed"
Jun  9 11:15:55.135: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394 container client-container: <nil>
STEP: delete the pod 06/09/23 11:15:55.151
Jun  9 11:15:55.180: INFO: Waiting for pod downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394 to disappear
Jun  9 11:15:55.191: INFO: Pod downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  9 11:15:55.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-833" for this suite. 06/09/23 11:15:55.203
------------------------------
• [4.336 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:15:50.879
    Jun  9 11:15:50.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 11:15:50.88
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:15:50.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:15:50.962
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 06/09/23 11:15:50.966
    Jun  9 11:15:51.002: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394" in namespace "downward-api-833" to be "Succeeded or Failed"
    Jun  9 11:15:51.114: INFO: Pod "downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394": Phase="Pending", Reason="", readiness=false. Elapsed: 111.799325ms
    Jun  9 11:15:53.179: INFO: Pod "downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.17693479s
    Jun  9 11:15:55.123: INFO: Pod "downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.121053114s
    STEP: Saw pod success 06/09/23 11:15:55.123
    Jun  9 11:15:55.124: INFO: Pod "downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394" satisfied condition "Succeeded or Failed"
    Jun  9 11:15:55.135: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394 container client-container: <nil>
    STEP: delete the pod 06/09/23 11:15:55.151
    Jun  9 11:15:55.180: INFO: Waiting for pod downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394 to disappear
    Jun  9 11:15:55.191: INFO: Pod downwardapi-volume-f9d19b27-8189-4555-93f3-84aad37f7394 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:15:55.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-833" for this suite. 06/09/23 11:15:55.203
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:15:55.216
Jun  9 11:15:55.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename statefulset 06/09/23 11:15:55.217
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:15:55.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:15:55.249
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7262 06/09/23 11:15:55.253
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 06/09/23 11:15:55.265
STEP: Creating stateful set ss in namespace statefulset-7262 06/09/23 11:15:55.274
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7262 06/09/23 11:15:55.286
Jun  9 11:15:55.293: INFO: Found 0 stateful pods, waiting for 1
Jun  9 11:16:05.300: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 06/09/23 11:16:05.3
Jun  9 11:16:05.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 11:16:05.528: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 11:16:05.528: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 11:16:05.528: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 11:16:05.536: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun  9 11:16:15.550: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 11:16:15.550: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 11:16:15.578: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999632s
Jun  9 11:16:16.590: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992780263s
Jun  9 11:16:17.603: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.981191779s
Jun  9 11:16:18.609: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.968332755s
Jun  9 11:16:19.616: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.962767588s
Jun  9 11:16:20.623: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.955625556s
Jun  9 11:16:21.635: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.947557696s
Jun  9 11:16:22.641: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.936764248s
Jun  9 11:16:23.648: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.930316358s
Jun  9 11:16:24.657: INFO: Verifying statefulset ss doesn't scale past 1 for another 923.774418ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7262 06/09/23 11:16:25.657
Jun  9 11:16:25.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 11:16:25.847: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  9 11:16:25.847: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 11:16:25.847: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 11:16:25.852: INFO: Found 1 stateful pods, waiting for 3
Jun  9 11:16:35.860: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 11:16:35.860: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 11:16:35.860: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 06/09/23 11:16:35.86
STEP: Scale down will halt with unhealthy stateful pod 06/09/23 11:16:35.86
Jun  9 11:16:35.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 11:16:36.058: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 11:16:36.058: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 11:16:36.058: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 11:16:36.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 11:16:36.255: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 11:16:36.255: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 11:16:36.255: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 11:16:36.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  9 11:16:36.448: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  9 11:16:36.448: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  9 11:16:36.448: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  9 11:16:36.448: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 11:16:36.455: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun  9 11:16:46.474: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 11:16:46.474: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 11:16:46.474: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun  9 11:16:46.501: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999564s
Jun  9 11:16:47.519: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990448829s
Jun  9 11:16:48.527: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972319868s
Jun  9 11:16:49.533: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.964921786s
Jun  9 11:16:50.540: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.958761723s
Jun  9 11:16:51.554: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.951831811s
Jun  9 11:16:52.562: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.937124286s
Jun  9 11:16:53.571: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.928838544s
Jun  9 11:16:54.578: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.920480513s
Jun  9 11:16:55.583: INFO: Verifying statefulset ss doesn't scale past 3 for another 913.921053ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7262 06/09/23 11:16:56.585
Jun  9 11:16:56.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 11:16:56.793: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  9 11:16:56.793: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 11:16:56.793: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 11:16:56.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 11:16:56.988: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  9 11:16:56.988: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 11:16:56.988: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 11:16:56.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  9 11:16:57.184: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  9 11:16:57.184: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  9 11:16:57.184: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  9 11:16:57.184: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 06/09/23 11:17:07.21
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  9 11:17:07.211: INFO: Deleting all statefulset in ns statefulset-7262
Jun  9 11:17:07.217: INFO: Scaling statefulset ss to 0
Jun  9 11:17:07.238: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 11:17:07.246: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  9 11:17:07.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7262" for this suite. 06/09/23 11:17:07.281
------------------------------
• [SLOW TEST] [72.082 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:15:55.216
    Jun  9 11:15:55.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename statefulset 06/09/23 11:15:55.217
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:15:55.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:15:55.249
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7262 06/09/23 11:15:55.253
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 06/09/23 11:15:55.265
    STEP: Creating stateful set ss in namespace statefulset-7262 06/09/23 11:15:55.274
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7262 06/09/23 11:15:55.286
    Jun  9 11:15:55.293: INFO: Found 0 stateful pods, waiting for 1
    Jun  9 11:16:05.300: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 06/09/23 11:16:05.3
    Jun  9 11:16:05.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  9 11:16:05.528: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  9 11:16:05.528: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  9 11:16:05.528: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  9 11:16:05.536: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jun  9 11:16:15.550: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun  9 11:16:15.550: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  9 11:16:15.578: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999632s
    Jun  9 11:16:16.590: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992780263s
    Jun  9 11:16:17.603: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.981191779s
    Jun  9 11:16:18.609: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.968332755s
    Jun  9 11:16:19.616: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.962767588s
    Jun  9 11:16:20.623: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.955625556s
    Jun  9 11:16:21.635: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.947557696s
    Jun  9 11:16:22.641: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.936764248s
    Jun  9 11:16:23.648: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.930316358s
    Jun  9 11:16:24.657: INFO: Verifying statefulset ss doesn't scale past 1 for another 923.774418ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7262 06/09/23 11:16:25.657
    Jun  9 11:16:25.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  9 11:16:25.847: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun  9 11:16:25.847: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  9 11:16:25.847: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  9 11:16:25.852: INFO: Found 1 stateful pods, waiting for 3
    Jun  9 11:16:35.860: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun  9 11:16:35.860: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun  9 11:16:35.860: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 06/09/23 11:16:35.86
    STEP: Scale down will halt with unhealthy stateful pod 06/09/23 11:16:35.86
    Jun  9 11:16:35.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  9 11:16:36.058: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  9 11:16:36.058: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  9 11:16:36.058: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  9 11:16:36.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  9 11:16:36.255: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  9 11:16:36.255: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  9 11:16:36.255: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  9 11:16:36.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun  9 11:16:36.448: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun  9 11:16:36.448: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun  9 11:16:36.448: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun  9 11:16:36.448: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  9 11:16:36.455: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jun  9 11:16:46.474: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun  9 11:16:46.474: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jun  9 11:16:46.474: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jun  9 11:16:46.501: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999564s
    Jun  9 11:16:47.519: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990448829s
    Jun  9 11:16:48.527: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972319868s
    Jun  9 11:16:49.533: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.964921786s
    Jun  9 11:16:50.540: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.958761723s
    Jun  9 11:16:51.554: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.951831811s
    Jun  9 11:16:52.562: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.937124286s
    Jun  9 11:16:53.571: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.928838544s
    Jun  9 11:16:54.578: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.920480513s
    Jun  9 11:16:55.583: INFO: Verifying statefulset ss doesn't scale past 3 for another 913.921053ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7262 06/09/23 11:16:56.585
    Jun  9 11:16:56.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  9 11:16:56.793: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun  9 11:16:56.793: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  9 11:16:56.793: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  9 11:16:56.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  9 11:16:56.988: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun  9 11:16:56.988: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  9 11:16:56.988: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  9 11:16:56.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=statefulset-7262 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun  9 11:16:57.184: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun  9 11:16:57.184: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun  9 11:16:57.184: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun  9 11:16:57.184: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 06/09/23 11:17:07.21
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  9 11:17:07.211: INFO: Deleting all statefulset in ns statefulset-7262
    Jun  9 11:17:07.217: INFO: Scaling statefulset ss to 0
    Jun  9 11:17:07.238: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  9 11:17:07.246: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:17:07.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7262" for this suite. 06/09/23 11:17:07.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:17:07.299
Jun  9 11:17:07.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 11:17:07.301
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:07.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:07.332
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-83484b6e-3fe2-4840-bcce-3d0e18bbb1af 06/09/23 11:17:07.337
STEP: Creating a pod to test consume configMaps 06/09/23 11:17:07.345
Jun  9 11:17:07.362: INFO: Waiting up to 5m0s for pod "pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5" in namespace "configmap-2822" to be "Succeeded or Failed"
Jun  9 11:17:07.369: INFO: Pod "pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.939866ms
Jun  9 11:17:09.376: INFO: Pod "pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013490003s
Jun  9 11:17:11.377: INFO: Pod "pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014759081s
STEP: Saw pod success 06/09/23 11:17:11.377
Jun  9 11:17:11.377: INFO: Pod "pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5" satisfied condition "Succeeded or Failed"
Jun  9 11:17:11.384: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5 container agnhost-container: <nil>
STEP: delete the pod 06/09/23 11:17:11.394
Jun  9 11:17:11.412: INFO: Waiting for pod pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5 to disappear
Jun  9 11:17:11.417: INFO: Pod pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 11:17:11.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2822" for this suite. 06/09/23 11:17:11.425
------------------------------
• [4.138 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:17:07.299
    Jun  9 11:17:07.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 11:17:07.301
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:07.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:07.332
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-83484b6e-3fe2-4840-bcce-3d0e18bbb1af 06/09/23 11:17:07.337
    STEP: Creating a pod to test consume configMaps 06/09/23 11:17:07.345
    Jun  9 11:17:07.362: INFO: Waiting up to 5m0s for pod "pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5" in namespace "configmap-2822" to be "Succeeded or Failed"
    Jun  9 11:17:07.369: INFO: Pod "pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.939866ms
    Jun  9 11:17:09.376: INFO: Pod "pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013490003s
    Jun  9 11:17:11.377: INFO: Pod "pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014759081s
    STEP: Saw pod success 06/09/23 11:17:11.377
    Jun  9 11:17:11.377: INFO: Pod "pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5" satisfied condition "Succeeded or Failed"
    Jun  9 11:17:11.384: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5 container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 11:17:11.394
    Jun  9 11:17:11.412: INFO: Waiting for pod pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5 to disappear
    Jun  9 11:17:11.417: INFO: Pod pod-configmaps-896da9c3-d7ea-4277-b335-ad40ea1d47a5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:17:11.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2822" for this suite. 06/09/23 11:17:11.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:17:11.438
Jun  9 11:17:11.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 11:17:11.44
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:11.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:11.464
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 06/09/23 11:17:11.47
Jun  9 11:17:11.484: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725" in namespace "projected-9383" to be "Succeeded or Failed"
Jun  9 11:17:11.519: INFO: Pod "downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725": Phase="Pending", Reason="", readiness=false. Elapsed: 34.220755ms
Jun  9 11:17:13.527: INFO: Pod "downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043072712s
Jun  9 11:17:15.527: INFO: Pod "downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042977439s
STEP: Saw pod success 06/09/23 11:17:15.527
Jun  9 11:17:15.528: INFO: Pod "downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725" satisfied condition "Succeeded or Failed"
Jun  9 11:17:15.534: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725 container client-container: <nil>
STEP: delete the pod 06/09/23 11:17:15.543
Jun  9 11:17:15.564: INFO: Waiting for pod downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725 to disappear
Jun  9 11:17:15.571: INFO: Pod downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  9 11:17:15.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9383" for this suite. 06/09/23 11:17:15.579
------------------------------
• [4.152 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:17:11.438
    Jun  9 11:17:11.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 11:17:11.44
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:11.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:11.464
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 06/09/23 11:17:11.47
    Jun  9 11:17:11.484: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725" in namespace "projected-9383" to be "Succeeded or Failed"
    Jun  9 11:17:11.519: INFO: Pod "downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725": Phase="Pending", Reason="", readiness=false. Elapsed: 34.220755ms
    Jun  9 11:17:13.527: INFO: Pod "downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043072712s
    Jun  9 11:17:15.527: INFO: Pod "downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042977439s
    STEP: Saw pod success 06/09/23 11:17:15.527
    Jun  9 11:17:15.528: INFO: Pod "downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725" satisfied condition "Succeeded or Failed"
    Jun  9 11:17:15.534: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725 container client-container: <nil>
    STEP: delete the pod 06/09/23 11:17:15.543
    Jun  9 11:17:15.564: INFO: Waiting for pod downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725 to disappear
    Jun  9 11:17:15.571: INFO: Pod downwardapi-volume-02e50982-7890-4d51-a759-4be883ad7725 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:17:15.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9383" for this suite. 06/09/23 11:17:15.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:17:15.592
Jun  9 11:17:15.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename replication-controller 06/09/23 11:17:15.593
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:15.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:15.629
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-wg55d" 06/09/23 11:17:15.635
Jun  9 11:17:15.646: INFO: Get Replication Controller "e2e-rc-wg55d" to confirm replicas
Jun  9 11:17:16.655: INFO: Get Replication Controller "e2e-rc-wg55d" to confirm replicas
Jun  9 11:17:16.663: INFO: Found 1 replicas for "e2e-rc-wg55d" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-wg55d" 06/09/23 11:17:16.663
STEP: Updating a scale subresource 06/09/23 11:17:16.668
STEP: Verifying replicas where modified for replication controller "e2e-rc-wg55d" 06/09/23 11:17:16.694
Jun  9 11:17:16.695: INFO: Get Replication Controller "e2e-rc-wg55d" to confirm replicas
Jun  9 11:17:17.701: INFO: Get Replication Controller "e2e-rc-wg55d" to confirm replicas
Jun  9 11:17:17.710: INFO: Found 2 replicas for "e2e-rc-wg55d" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun  9 11:17:17.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-287" for this suite. 06/09/23 11:17:17.718
------------------------------
• [2.139 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:17:15.592
    Jun  9 11:17:15.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename replication-controller 06/09/23 11:17:15.593
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:15.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:15.629
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-wg55d" 06/09/23 11:17:15.635
    Jun  9 11:17:15.646: INFO: Get Replication Controller "e2e-rc-wg55d" to confirm replicas
    Jun  9 11:17:16.655: INFO: Get Replication Controller "e2e-rc-wg55d" to confirm replicas
    Jun  9 11:17:16.663: INFO: Found 1 replicas for "e2e-rc-wg55d" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-wg55d" 06/09/23 11:17:16.663
    STEP: Updating a scale subresource 06/09/23 11:17:16.668
    STEP: Verifying replicas where modified for replication controller "e2e-rc-wg55d" 06/09/23 11:17:16.694
    Jun  9 11:17:16.695: INFO: Get Replication Controller "e2e-rc-wg55d" to confirm replicas
    Jun  9 11:17:17.701: INFO: Get Replication Controller "e2e-rc-wg55d" to confirm replicas
    Jun  9 11:17:17.710: INFO: Found 2 replicas for "e2e-rc-wg55d" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:17:17.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-287" for this suite. 06/09/23 11:17:17.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:17:17.731
Jun  9 11:17:17.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 11:17:17.733
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:17.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:17.773
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 06/09/23 11:17:17.777
Jun  9 11:17:17.791: INFO: Waiting up to 5m0s for pod "labelsupdatec9f937f5-b771-4ff6-a806-328fce1968bc" in namespace "projected-6966" to be "running and ready"
Jun  9 11:17:17.803: INFO: Pod "labelsupdatec9f937f5-b771-4ff6-a806-328fce1968bc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.25124ms
Jun  9 11:17:17.804: INFO: The phase of Pod labelsupdatec9f937f5-b771-4ff6-a806-328fce1968bc is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:17:19.924: INFO: Pod "labelsupdatec9f937f5-b771-4ff6-a806-328fce1968bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.133057785s
Jun  9 11:17:19.924: INFO: The phase of Pod labelsupdatec9f937f5-b771-4ff6-a806-328fce1968bc is Running (Ready = true)
Jun  9 11:17:19.924: INFO: Pod "labelsupdatec9f937f5-b771-4ff6-a806-328fce1968bc" satisfied condition "running and ready"
Jun  9 11:17:20.520: INFO: Successfully updated pod "labelsupdatec9f937f5-b771-4ff6-a806-328fce1968bc"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  9 11:17:22.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6966" for this suite. 06/09/23 11:17:22.553
------------------------------
• [4.838 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:17:17.731
    Jun  9 11:17:17.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 11:17:17.733
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:17.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:17.773
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 06/09/23 11:17:17.777
    Jun  9 11:17:17.791: INFO: Waiting up to 5m0s for pod "labelsupdatec9f937f5-b771-4ff6-a806-328fce1968bc" in namespace "projected-6966" to be "running and ready"
    Jun  9 11:17:17.803: INFO: Pod "labelsupdatec9f937f5-b771-4ff6-a806-328fce1968bc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.25124ms
    Jun  9 11:17:17.804: INFO: The phase of Pod labelsupdatec9f937f5-b771-4ff6-a806-328fce1968bc is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:17:19.924: INFO: Pod "labelsupdatec9f937f5-b771-4ff6-a806-328fce1968bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.133057785s
    Jun  9 11:17:19.924: INFO: The phase of Pod labelsupdatec9f937f5-b771-4ff6-a806-328fce1968bc is Running (Ready = true)
    Jun  9 11:17:19.924: INFO: Pod "labelsupdatec9f937f5-b771-4ff6-a806-328fce1968bc" satisfied condition "running and ready"
    Jun  9 11:17:20.520: INFO: Successfully updated pod "labelsupdatec9f937f5-b771-4ff6-a806-328fce1968bc"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:17:22.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6966" for this suite. 06/09/23 11:17:22.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:17:22.572
Jun  9 11:17:22.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 11:17:22.573
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:22.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:22.613
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-2620 06/09/23 11:17:22.618
STEP: creating service affinity-nodeport in namespace services-2620 06/09/23 11:17:22.618
STEP: creating replication controller affinity-nodeport in namespace services-2620 06/09/23 11:17:22.677
I0609 11:17:22.689756      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2620, replica count: 3
I0609 11:17:25.740849      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 11:17:25.761: INFO: Creating new exec pod
Jun  9 11:17:25.774: INFO: Waiting up to 5m0s for pod "execpod-affinityscdhd" in namespace "services-2620" to be "running"
Jun  9 11:17:25.781: INFO: Pod "execpod-affinityscdhd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.617539ms
Jun  9 11:17:27.787: INFO: Pod "execpod-affinityscdhd": Phase="Running", Reason="", readiness=true. Elapsed: 2.013075121s
Jun  9 11:17:27.787: INFO: Pod "execpod-affinityscdhd" satisfied condition "running"
Jun  9 11:17:28.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-2620 exec execpod-affinityscdhd -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jun  9 11:17:29.120: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun  9 11:17:29.121: INFO: stdout: ""
Jun  9 11:17:29.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-2620 exec execpod-affinityscdhd -- /bin/sh -x -c nc -v -z -w 2 10.103.83.6 80'
Jun  9 11:17:29.321: INFO: stderr: "+ nc -v -z -w 2 10.103.83.6 80\nConnection to 10.103.83.6 80 port [tcp/http] succeeded!\n"
Jun  9 11:17:29.321: INFO: stdout: ""
Jun  9 11:17:29.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-2620 exec execpod-affinityscdhd -- /bin/sh -x -c nc -v -z -w 2 10.255.64.102 31007'
Jun  9 11:17:29.501: INFO: stderr: "+ nc -v -z -w 2 10.255.64.102 31007\nConnection to 10.255.64.102 31007 port [tcp/*] succeeded!\n"
Jun  9 11:17:29.501: INFO: stdout: ""
Jun  9 11:17:29.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-2620 exec execpod-affinityscdhd -- /bin/sh -x -c nc -v -z -w 2 10.255.64.104 31007'
Jun  9 11:17:29.680: INFO: stderr: "+ nc -v -z -w 2 10.255.64.104 31007\nConnection to 10.255.64.104 31007 port [tcp/*] succeeded!\n"
Jun  9 11:17:29.680: INFO: stdout: ""
Jun  9 11:17:29.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-2620 exec execpod-affinityscdhd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.64.104:31007/ ; done'
Jun  9 11:17:29.945: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n"
Jun  9 11:17:29.945: INFO: stdout: "\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p"
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
Jun  9 11:17:29.945: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2620, will wait for the garbage collector to delete the pods 06/09/23 11:17:29.967
Jun  9 11:17:30.035: INFO: Deleting ReplicationController affinity-nodeport took: 11.311223ms
Jun  9 11:17:30.136: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.956237ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 11:17:32.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2620" for this suite. 06/09/23 11:17:32.014
------------------------------
• [SLOW TEST] [9.458 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:17:22.572
    Jun  9 11:17:22.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 11:17:22.573
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:22.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:22.613
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-2620 06/09/23 11:17:22.618
    STEP: creating service affinity-nodeport in namespace services-2620 06/09/23 11:17:22.618
    STEP: creating replication controller affinity-nodeport in namespace services-2620 06/09/23 11:17:22.677
    I0609 11:17:22.689756      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2620, replica count: 3
    I0609 11:17:25.740849      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  9 11:17:25.761: INFO: Creating new exec pod
    Jun  9 11:17:25.774: INFO: Waiting up to 5m0s for pod "execpod-affinityscdhd" in namespace "services-2620" to be "running"
    Jun  9 11:17:25.781: INFO: Pod "execpod-affinityscdhd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.617539ms
    Jun  9 11:17:27.787: INFO: Pod "execpod-affinityscdhd": Phase="Running", Reason="", readiness=true. Elapsed: 2.013075121s
    Jun  9 11:17:27.787: INFO: Pod "execpod-affinityscdhd" satisfied condition "running"
    Jun  9 11:17:28.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-2620 exec execpod-affinityscdhd -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jun  9 11:17:29.120: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jun  9 11:17:29.121: INFO: stdout: ""
    Jun  9 11:17:29.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-2620 exec execpod-affinityscdhd -- /bin/sh -x -c nc -v -z -w 2 10.103.83.6 80'
    Jun  9 11:17:29.321: INFO: stderr: "+ nc -v -z -w 2 10.103.83.6 80\nConnection to 10.103.83.6 80 port [tcp/http] succeeded!\n"
    Jun  9 11:17:29.321: INFO: stdout: ""
    Jun  9 11:17:29.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-2620 exec execpod-affinityscdhd -- /bin/sh -x -c nc -v -z -w 2 10.255.64.102 31007'
    Jun  9 11:17:29.501: INFO: stderr: "+ nc -v -z -w 2 10.255.64.102 31007\nConnection to 10.255.64.102 31007 port [tcp/*] succeeded!\n"
    Jun  9 11:17:29.501: INFO: stdout: ""
    Jun  9 11:17:29.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-2620 exec execpod-affinityscdhd -- /bin/sh -x -c nc -v -z -w 2 10.255.64.104 31007'
    Jun  9 11:17:29.680: INFO: stderr: "+ nc -v -z -w 2 10.255.64.104 31007\nConnection to 10.255.64.104 31007 port [tcp/*] succeeded!\n"
    Jun  9 11:17:29.680: INFO: stdout: ""
    Jun  9 11:17:29.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-2620 exec execpod-affinityscdhd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.64.104:31007/ ; done'
    Jun  9 11:17:29.945: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:31007/\n"
    Jun  9 11:17:29.945: INFO: stdout: "\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p\naffinity-nodeport-gqb8p"
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Received response from host: affinity-nodeport-gqb8p
    Jun  9 11:17:29.945: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-2620, will wait for the garbage collector to delete the pods 06/09/23 11:17:29.967
    Jun  9 11:17:30.035: INFO: Deleting ReplicationController affinity-nodeport took: 11.311223ms
    Jun  9 11:17:30.136: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.956237ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:17:32.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2620" for this suite. 06/09/23 11:17:32.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:17:32.031
Jun  9 11:17:32.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 11:17:32.034
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:32.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:32.062
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jun  9 11:17:32.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1566 create -f -'
Jun  9 11:17:33.704: INFO: stderr: ""
Jun  9 11:17:33.704: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun  9 11:17:33.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1566 create -f -'
Jun  9 11:17:35.163: INFO: stderr: ""
Jun  9 11:17:35.163: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/09/23 11:17:35.163
Jun  9 11:17:36.170: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 11:17:36.171: INFO: Found 1 / 1
Jun  9 11:17:36.171: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun  9 11:17:36.178: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 11:17:36.178: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  9 11:17:36.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1566 describe pod agnhost-primary-p8dwz'
Jun  9 11:17:36.285: INFO: stderr: ""
Jun  9 11:17:36.285: INFO: stdout: "Name:             agnhost-primary-p8dwz\nNamespace:        kubectl-1566\nPriority:         0\nService Account:  default\nNode:             sks-test-v1-26.4-workergroup-qdprq/10.255.64.103\nStart Time:       Fri, 09 Jun 2023 11:17:33 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: fe82dbbc50df10287538f5898c0358720a3f6317e841824efb6432212b6e83d5\n                  cni.projectcalico.org/podIP: 172.27.53.73/32\n                  cni.projectcalico.org/podIPs: 172.27.53.73/32\nStatus:           Running\nIP:               172.27.53.73\nIPs:\n  IP:           172.27.53.73\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://76e43a75035b7b8c14d21e9a0aecf610997bbacdc7fdee623f93ca2ff4a8f403\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 09 Jun 2023 11:17:34 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5bl76 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5bl76:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-1566/agnhost-primary-p8dwz to sks-test-v1-26.4-workergroup-qdprq\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jun  9 11:17:36.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1566 describe rc agnhost-primary'
Jun  9 11:17:36.421: INFO: stderr: ""
Jun  9 11:17:36.421: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1566\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-p8dwz\n"
Jun  9 11:17:36.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1566 describe service agnhost-primary'
Jun  9 11:17:36.531: INFO: stderr: ""
Jun  9 11:17:36.531: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1566\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.100.51.85\nIPs:               10.100.51.85\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.27.53.73:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun  9 11:17:36.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1566 describe node sks-test-v1-26.4-controlplane-hvp4c'
Jun  9 11:17:36.679: INFO: stderr: ""
Jun  9 11:17:36.679: INFO: stdout: "Name:               sks-test-v1-26.4-controlplane-hvp4c\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    cape.infrastructure.cluster.x-k8s.io/host-server-id=clb291e3200ff0958593x2v64\n                    cape.infrastructure.cluster.x-k8s.io/host-server-name=node20-217\n                    cape.infrastructure.cluster.x-k8s.io/node-group=controlplane\n                    cape.infrastructure.cluster.x-k8s.io/tower-vm-id=clio5o62frsez0858ns5ktxpb\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=sks-test-v1-26.4-controlplane-hvp4c\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        cluster.x-k8s.io/cluster-name: sks-test-v1-26.4\n                    cluster.x-k8s.io/cluster-namespace: default\n                    cluster.x-k8s.io/labels-from-machine: \n                    cluster.x-k8s.io/machine: sks-test-v1-26.4-controlplane-cv26m\n                    cluster.x-k8s.io/owner-kind: KubeadmControlPlane\n                    cluster.x-k8s.io/owner-name: sks-test-v1-26.4-controlplane\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"com.smartx.elf-csi-driver\":\"sks-test-v1-26.4-controlplane-hvp4c\",\"csi.tigera.io\":\"sks-test-v1-26.4-controlplane-hvp4c\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.255.64.105/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.28.127.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 09 Jun 2023 05:59:15 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  sks-test-v1-26.4-controlplane-hvp4c\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 09 Jun 2023 11:17:33 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 09 Jun 2023 06:03:19 +0000   Fri, 09 Jun 2023 06:03:19 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 09 Jun 2023 11:12:31 +0000   Fri, 09 Jun 2023 05:59:15 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 09 Jun 2023 11:12:31 +0000   Fri, 09 Jun 2023 05:59:15 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 09 Jun 2023 11:12:31 +0000   Fri, 09 Jun 2023 05:59:15 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 09 Jun 2023 11:12:31 +0000   Fri, 09 Jun 2023 06:03:22 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.255.64.105\n  Hostname:    sks-test-v1-26.4-controlplane-hvp4c\nCapacity:\n  cpu:                8\n  ephemeral-storage:  64198016Ki\n  hugepages-2Mi:      0\n  memory:             7888372Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  59164891448\n  hugepages-2Mi:      0\n  memory:             7785972Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 bc54609f616f4f1aa91395a0ccf064ae\n  System UUID:                bc54609f-616f-4f1a-a913-95a0ccf064ae\n  Boot ID:                    fb24e7a2-3468-4394-8f8b-6b9fa975bfac\n  Kernel Version:             4.18.0-372.9.1.el8.x86_64\n  OS Image:                   Rocky Linux 8.6 (Green Obsidian)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.4\n  Kubelet Version:            v1.26.4\n  Kube-Proxy Version:         v1.26.4\nPodCIDR:                      172.16.0.0/24\nPodCIDRs:                     172.16.0.0/24\nProviderID:                   elf://cda59d49-cc83-455b-913e-d074248b34b7\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                           ------------  ----------  ---------------  -------------  ---\n  calico-apiserver            calico-apiserver-595ff5cd8-wmq6t                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h12m\n  calico-system               calico-node-8bwr6                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h14m\n  calico-system               csi-node-driver-8rt84                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h14m\n  kapp-controller             kapp-controller-5d848dc768-nttdm                               240m (3%)     0 (0%)      200Mi (2%)       0 (0%)         5h17m\n  kube-system                 etcd-sks-test-v1-26.4-controlplane-hvp4c                       100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         5h18m\n  kube-system                 kube-apiserver-sks-test-v1-26.4-controlplane-hvp4c             250m (3%)     0 (0%)      0 (0%)           0 (0%)         5h18m\n  kube-system                 kube-controller-manager-sks-test-v1-26.4-controlplane-hvp4c    200m (2%)     0 (0%)      0 (0%)           0 (0%)         5h18m\n  kube-system                 kube-proxy-f6xhm                                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h18m\n  kube-system                 kube-scheduler-sks-test-v1-26.4-controlplane-hvp4c             100m (1%)     0 (0%)      0 (0%)           0 (0%)         5h18m\n  kube-system                 kube-vip-sks-test-v1-26.4-controlplane-hvp4c                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h18m\n  sks-system                  smtx-elf-csi-driver-controller-plugin-7bb788b57-w5kwc          510m (6%)     600m (7%)   450Mi (5%)       1050Mi (13%)   5h13m\n  sks-system                  smtx-elf-csi-driver-node-plugin-n8k5v                          210m (2%)     300m (3%)   200Mi (2%)       550Mi (7%)     5h13m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-9nfgp        0 (0%)        0 (0%)      0 (0%)           0 (0%)         59m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1610m (20%)  900m (11%)\n  memory             950Mi (12%)  1600Mi (21%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
Jun  9 11:17:36.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1566 describe namespace kubectl-1566'
Jun  9 11:17:36.797: INFO: stderr: ""
Jun  9 11:17:36.797: INFO: stdout: "Name:         kubectl-1566\nLabels:       e2e-framework=kubectl\n              e2e-run=1e3b8b3b-6336-49ab-9439-1cfe1adcddf6\n              kubernetes.io/metadata.name=kubectl-1566\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 11:17:36.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1566" for this suite. 06/09/23 11:17:36.807
------------------------------
• [4.787 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:17:32.031
    Jun  9 11:17:32.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 11:17:32.034
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:32.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:32.062
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jun  9 11:17:32.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1566 create -f -'
    Jun  9 11:17:33.704: INFO: stderr: ""
    Jun  9 11:17:33.704: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jun  9 11:17:33.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1566 create -f -'
    Jun  9 11:17:35.163: INFO: stderr: ""
    Jun  9 11:17:35.163: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/09/23 11:17:35.163
    Jun  9 11:17:36.170: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  9 11:17:36.171: INFO: Found 1 / 1
    Jun  9 11:17:36.171: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jun  9 11:17:36.178: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  9 11:17:36.178: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun  9 11:17:36.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1566 describe pod agnhost-primary-p8dwz'
    Jun  9 11:17:36.285: INFO: stderr: ""
    Jun  9 11:17:36.285: INFO: stdout: "Name:             agnhost-primary-p8dwz\nNamespace:        kubectl-1566\nPriority:         0\nService Account:  default\nNode:             sks-test-v1-26.4-workergroup-qdprq/10.255.64.103\nStart Time:       Fri, 09 Jun 2023 11:17:33 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: fe82dbbc50df10287538f5898c0358720a3f6317e841824efb6432212b6e83d5\n                  cni.projectcalico.org/podIP: 172.27.53.73/32\n                  cni.projectcalico.org/podIPs: 172.27.53.73/32\nStatus:           Running\nIP:               172.27.53.73\nIPs:\n  IP:           172.27.53.73\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://76e43a75035b7b8c14d21e9a0aecf610997bbacdc7fdee623f93ca2ff4a8f403\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 09 Jun 2023 11:17:34 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5bl76 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5bl76:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-1566/agnhost-primary-p8dwz to sks-test-v1-26.4-workergroup-qdprq\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Jun  9 11:17:36.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1566 describe rc agnhost-primary'
    Jun  9 11:17:36.421: INFO: stderr: ""
    Jun  9 11:17:36.421: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1566\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-p8dwz\n"
    Jun  9 11:17:36.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1566 describe service agnhost-primary'
    Jun  9 11:17:36.531: INFO: stderr: ""
    Jun  9 11:17:36.531: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1566\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.100.51.85\nIPs:               10.100.51.85\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.27.53.73:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jun  9 11:17:36.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1566 describe node sks-test-v1-26.4-controlplane-hvp4c'
    Jun  9 11:17:36.679: INFO: stderr: ""
    Jun  9 11:17:36.679: INFO: stdout: "Name:               sks-test-v1-26.4-controlplane-hvp4c\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    cape.infrastructure.cluster.x-k8s.io/host-server-id=clb291e3200ff0958593x2v64\n                    cape.infrastructure.cluster.x-k8s.io/host-server-name=node20-217\n                    cape.infrastructure.cluster.x-k8s.io/node-group=controlplane\n                    cape.infrastructure.cluster.x-k8s.io/tower-vm-id=clio5o62frsez0858ns5ktxpb\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=sks-test-v1-26.4-controlplane-hvp4c\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        cluster.x-k8s.io/cluster-name: sks-test-v1-26.4\n                    cluster.x-k8s.io/cluster-namespace: default\n                    cluster.x-k8s.io/labels-from-machine: \n                    cluster.x-k8s.io/machine: sks-test-v1-26.4-controlplane-cv26m\n                    cluster.x-k8s.io/owner-kind: KubeadmControlPlane\n                    cluster.x-k8s.io/owner-name: sks-test-v1-26.4-controlplane\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"com.smartx.elf-csi-driver\":\"sks-test-v1-26.4-controlplane-hvp4c\",\"csi.tigera.io\":\"sks-test-v1-26.4-controlplane-hvp4c\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.255.64.105/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.28.127.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 09 Jun 2023 05:59:15 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  sks-test-v1-26.4-controlplane-hvp4c\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 09 Jun 2023 11:17:33 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 09 Jun 2023 06:03:19 +0000   Fri, 09 Jun 2023 06:03:19 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 09 Jun 2023 11:12:31 +0000   Fri, 09 Jun 2023 05:59:15 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 09 Jun 2023 11:12:31 +0000   Fri, 09 Jun 2023 05:59:15 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 09 Jun 2023 11:12:31 +0000   Fri, 09 Jun 2023 05:59:15 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 09 Jun 2023 11:12:31 +0000   Fri, 09 Jun 2023 06:03:22 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.255.64.105\n  Hostname:    sks-test-v1-26.4-controlplane-hvp4c\nCapacity:\n  cpu:                8\n  ephemeral-storage:  64198016Ki\n  hugepages-2Mi:      0\n  memory:             7888372Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  59164891448\n  hugepages-2Mi:      0\n  memory:             7785972Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 bc54609f616f4f1aa91395a0ccf064ae\n  System UUID:                bc54609f-616f-4f1a-a913-95a0ccf064ae\n  Boot ID:                    fb24e7a2-3468-4394-8f8b-6b9fa975bfac\n  Kernel Version:             4.18.0-372.9.1.el8.x86_64\n  OS Image:                   Rocky Linux 8.6 (Green Obsidian)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.4\n  Kubelet Version:            v1.26.4\n  Kube-Proxy Version:         v1.26.4\nPodCIDR:                      172.16.0.0/24\nPodCIDRs:                     172.16.0.0/24\nProviderID:                   elf://cda59d49-cc83-455b-913e-d074248b34b7\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                           ------------  ----------  ---------------  -------------  ---\n  calico-apiserver            calico-apiserver-595ff5cd8-wmq6t                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h12m\n  calico-system               calico-node-8bwr6                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h14m\n  calico-system               csi-node-driver-8rt84                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h14m\n  kapp-controller             kapp-controller-5d848dc768-nttdm                               240m (3%)     0 (0%)      200Mi (2%)       0 (0%)         5h17m\n  kube-system                 etcd-sks-test-v1-26.4-controlplane-hvp4c                       100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         5h18m\n  kube-system                 kube-apiserver-sks-test-v1-26.4-controlplane-hvp4c             250m (3%)     0 (0%)      0 (0%)           0 (0%)         5h18m\n  kube-system                 kube-controller-manager-sks-test-v1-26.4-controlplane-hvp4c    200m (2%)     0 (0%)      0 (0%)           0 (0%)         5h18m\n  kube-system                 kube-proxy-f6xhm                                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h18m\n  kube-system                 kube-scheduler-sks-test-v1-26.4-controlplane-hvp4c             100m (1%)     0 (0%)      0 (0%)           0 (0%)         5h18m\n  kube-system                 kube-vip-sks-test-v1-26.4-controlplane-hvp4c                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h18m\n  sks-system                  smtx-elf-csi-driver-controller-plugin-7bb788b57-w5kwc          510m (6%)     600m (7%)   450Mi (5%)       1050Mi (13%)   5h13m\n  sks-system                  smtx-elf-csi-driver-node-plugin-n8k5v                          210m (2%)     300m (3%)   200Mi (2%)       550Mi (7%)     5h13m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-9nfgp        0 (0%)        0 (0%)      0 (0%)           0 (0%)         59m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1610m (20%)  900m (11%)\n  memory             950Mi (12%)  1600Mi (21%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
    Jun  9 11:17:36.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1566 describe namespace kubectl-1566'
    Jun  9 11:17:36.797: INFO: stderr: ""
    Jun  9 11:17:36.797: INFO: stdout: "Name:         kubectl-1566\nLabels:       e2e-framework=kubectl\n              e2e-run=1e3b8b3b-6336-49ab-9439-1cfe1adcddf6\n              kubernetes.io/metadata.name=kubectl-1566\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:17:36.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1566" for this suite. 06/09/23 11:17:36.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:17:36.818
Jun  9 11:17:36.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pods 06/09/23 11:17:36.819
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:36.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:36.853
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 06/09/23 11:17:36.86
Jun  9 11:17:36.877: INFO: Waiting up to 5m0s for pod "pod-hh9cz" in namespace "pods-518" to be "running"
Jun  9 11:17:36.887: INFO: Pod "pod-hh9cz": Phase="Pending", Reason="", readiness=false. Elapsed: 10.41411ms
Jun  9 11:17:38.927: INFO: Pod "pod-hh9cz": Phase="Running", Reason="", readiness=true. Elapsed: 2.050015831s
Jun  9 11:17:38.927: INFO: Pod "pod-hh9cz" satisfied condition "running"
STEP: patching /status 06/09/23 11:17:38.927
Jun  9 11:17:38.946: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  9 11:17:38.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-518" for this suite. 06/09/23 11:17:38.956
------------------------------
• [2.170 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:17:36.818
    Jun  9 11:17:36.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pods 06/09/23 11:17:36.819
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:36.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:36.853
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 06/09/23 11:17:36.86
    Jun  9 11:17:36.877: INFO: Waiting up to 5m0s for pod "pod-hh9cz" in namespace "pods-518" to be "running"
    Jun  9 11:17:36.887: INFO: Pod "pod-hh9cz": Phase="Pending", Reason="", readiness=false. Elapsed: 10.41411ms
    Jun  9 11:17:38.927: INFO: Pod "pod-hh9cz": Phase="Running", Reason="", readiness=true. Elapsed: 2.050015831s
    Jun  9 11:17:38.927: INFO: Pod "pod-hh9cz" satisfied condition "running"
    STEP: patching /status 06/09/23 11:17:38.927
    Jun  9 11:17:38.946: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:17:38.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-518" for this suite. 06/09/23 11:17:38.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:17:38.988
Jun  9 11:17:38.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename replication-controller 06/09/23 11:17:38.99
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:39.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:39.087
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 06/09/23 11:17:39.096
STEP: waiting for RC to be added 06/09/23 11:17:39.214
STEP: waiting for available Replicas 06/09/23 11:17:39.215
STEP: patching ReplicationController 06/09/23 11:17:40.539
STEP: waiting for RC to be modified 06/09/23 11:17:40.56
STEP: patching ReplicationController status 06/09/23 11:17:40.56
STEP: waiting for RC to be modified 06/09/23 11:17:40.577
STEP: waiting for available Replicas 06/09/23 11:17:40.577
STEP: fetching ReplicationController status 06/09/23 11:17:40.584
STEP: patching ReplicationController scale 06/09/23 11:17:40.595
STEP: waiting for RC to be modified 06/09/23 11:17:40.609
STEP: waiting for ReplicationController's scale to be the max amount 06/09/23 11:17:40.609
STEP: fetching ReplicationController; ensuring that it's patched 06/09/23 11:17:41.619
STEP: updating ReplicationController status 06/09/23 11:17:41.628
STEP: waiting for RC to be modified 06/09/23 11:17:41.64
STEP: listing all ReplicationControllers 06/09/23 11:17:41.641
STEP: checking that ReplicationController has expected values 06/09/23 11:17:41.65
STEP: deleting ReplicationControllers by collection 06/09/23 11:17:41.65
STEP: waiting for ReplicationController to have a DELETED watchEvent 06/09/23 11:17:41.671
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun  9 11:17:41.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4556" for this suite. 06/09/23 11:17:41.733
------------------------------
• [2.757 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:17:38.988
    Jun  9 11:17:38.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename replication-controller 06/09/23 11:17:38.99
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:39.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:39.087
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 06/09/23 11:17:39.096
    STEP: waiting for RC to be added 06/09/23 11:17:39.214
    STEP: waiting for available Replicas 06/09/23 11:17:39.215
    STEP: patching ReplicationController 06/09/23 11:17:40.539
    STEP: waiting for RC to be modified 06/09/23 11:17:40.56
    STEP: patching ReplicationController status 06/09/23 11:17:40.56
    STEP: waiting for RC to be modified 06/09/23 11:17:40.577
    STEP: waiting for available Replicas 06/09/23 11:17:40.577
    STEP: fetching ReplicationController status 06/09/23 11:17:40.584
    STEP: patching ReplicationController scale 06/09/23 11:17:40.595
    STEP: waiting for RC to be modified 06/09/23 11:17:40.609
    STEP: waiting for ReplicationController's scale to be the max amount 06/09/23 11:17:40.609
    STEP: fetching ReplicationController; ensuring that it's patched 06/09/23 11:17:41.619
    STEP: updating ReplicationController status 06/09/23 11:17:41.628
    STEP: waiting for RC to be modified 06/09/23 11:17:41.64
    STEP: listing all ReplicationControllers 06/09/23 11:17:41.641
    STEP: checking that ReplicationController has expected values 06/09/23 11:17:41.65
    STEP: deleting ReplicationControllers by collection 06/09/23 11:17:41.65
    STEP: waiting for ReplicationController to have a DELETED watchEvent 06/09/23 11:17:41.671
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:17:41.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4556" for this suite. 06/09/23 11:17:41.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:17:41.747
Jun  9 11:17:41.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename job 06/09/23 11:17:41.748
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:41.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:41.779
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 06/09/23 11:17:41.787
STEP: Ensure pods equal to parallelism count is attached to the job 06/09/23 11:17:41.801
STEP: patching /status 06/09/23 11:17:45.807
STEP: updating /status 06/09/23 11:17:45.818
STEP: get /status 06/09/23 11:17:45.832
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun  9 11:17:45.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7463" for this suite. 06/09/23 11:17:45.848
------------------------------
• [4.114 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:17:41.747
    Jun  9 11:17:41.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename job 06/09/23 11:17:41.748
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:41.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:41.779
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 06/09/23 11:17:41.787
    STEP: Ensure pods equal to parallelism count is attached to the job 06/09/23 11:17:41.801
    STEP: patching /status 06/09/23 11:17:45.807
    STEP: updating /status 06/09/23 11:17:45.818
    STEP: get /status 06/09/23 11:17:45.832
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:17:45.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7463" for this suite. 06/09/23 11:17:45.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:17:45.861
Jun  9 11:17:45.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-probe 06/09/23 11:17:45.864
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:45.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:45.893
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-c22f8387-3756-441d-9d1c-67a328f101e0 in namespace container-probe-1113 06/09/23 11:17:45.899
Jun  9 11:17:45.917: INFO: Waiting up to 5m0s for pod "busybox-c22f8387-3756-441d-9d1c-67a328f101e0" in namespace "container-probe-1113" to be "not pending"
Jun  9 11:17:45.935: INFO: Pod "busybox-c22f8387-3756-441d-9d1c-67a328f101e0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.667539ms
Jun  9 11:17:47.941: INFO: Pod "busybox-c22f8387-3756-441d-9d1c-67a328f101e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023745866s
Jun  9 11:17:49.960: INFO: Pod "busybox-c22f8387-3756-441d-9d1c-67a328f101e0": Phase="Running", Reason="", readiness=true. Elapsed: 4.042913651s
Jun  9 11:17:49.960: INFO: Pod "busybox-c22f8387-3756-441d-9d1c-67a328f101e0" satisfied condition "not pending"
Jun  9 11:17:49.960: INFO: Started pod busybox-c22f8387-3756-441d-9d1c-67a328f101e0 in namespace container-probe-1113
STEP: checking the pod's current state and verifying that restartCount is present 06/09/23 11:17:49.96
Jun  9 11:17:49.979: INFO: Initial restart count of pod busybox-c22f8387-3756-441d-9d1c-67a328f101e0 is 0
STEP: deleting the pod 06/09/23 11:21:51.973
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  9 11:21:52.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1113" for this suite. 06/09/23 11:21:52.167
------------------------------
• [SLOW TEST] [246.415 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:17:45.861
    Jun  9 11:17:45.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-probe 06/09/23 11:17:45.864
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:17:45.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:17:45.893
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-c22f8387-3756-441d-9d1c-67a328f101e0 in namespace container-probe-1113 06/09/23 11:17:45.899
    Jun  9 11:17:45.917: INFO: Waiting up to 5m0s for pod "busybox-c22f8387-3756-441d-9d1c-67a328f101e0" in namespace "container-probe-1113" to be "not pending"
    Jun  9 11:17:45.935: INFO: Pod "busybox-c22f8387-3756-441d-9d1c-67a328f101e0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.667539ms
    Jun  9 11:17:47.941: INFO: Pod "busybox-c22f8387-3756-441d-9d1c-67a328f101e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023745866s
    Jun  9 11:17:49.960: INFO: Pod "busybox-c22f8387-3756-441d-9d1c-67a328f101e0": Phase="Running", Reason="", readiness=true. Elapsed: 4.042913651s
    Jun  9 11:17:49.960: INFO: Pod "busybox-c22f8387-3756-441d-9d1c-67a328f101e0" satisfied condition "not pending"
    Jun  9 11:17:49.960: INFO: Started pod busybox-c22f8387-3756-441d-9d1c-67a328f101e0 in namespace container-probe-1113
    STEP: checking the pod's current state and verifying that restartCount is present 06/09/23 11:17:49.96
    Jun  9 11:17:49.979: INFO: Initial restart count of pod busybox-c22f8387-3756-441d-9d1c-67a328f101e0 is 0
    STEP: deleting the pod 06/09/23 11:21:51.973
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:21:52.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1113" for this suite. 06/09/23 11:21:52.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:21:52.278
Jun  9 11:21:52.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 11:21:52.28
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:21:52.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:21:52.423
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jun  9 11:21:52.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/09/23 11:21:55.289
Jun  9 11:21:55.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-8150 --namespace=crd-publish-openapi-8150 create -f -'
Jun  9 11:21:56.696: INFO: stderr: ""
Jun  9 11:21:56.696: INFO: stdout: "e2e-test-crd-publish-openapi-4982-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun  9 11:21:56.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-8150 --namespace=crd-publish-openapi-8150 delete e2e-test-crd-publish-openapi-4982-crds test-cr'
Jun  9 11:21:56.893: INFO: stderr: ""
Jun  9 11:21:56.893: INFO: stdout: "e2e-test-crd-publish-openapi-4982-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun  9 11:21:56.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-8150 --namespace=crd-publish-openapi-8150 apply -f -'
Jun  9 11:21:58.021: INFO: stderr: ""
Jun  9 11:21:58.021: INFO: stdout: "e2e-test-crd-publish-openapi-4982-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun  9 11:21:58.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-8150 --namespace=crd-publish-openapi-8150 delete e2e-test-crd-publish-openapi-4982-crds test-cr'
Jun  9 11:21:58.271: INFO: stderr: ""
Jun  9 11:21:58.271: INFO: stdout: "e2e-test-crd-publish-openapi-4982-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 06/09/23 11:21:58.271
Jun  9 11:21:58.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-8150 explain e2e-test-crd-publish-openapi-4982-crds'
Jun  9 11:21:58.714: INFO: stderr: ""
Jun  9 11:21:58.714: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4982-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:22:01.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8150" for this suite. 06/09/23 11:22:01.876
------------------------------
• [SLOW TEST] [9.674 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:21:52.278
    Jun  9 11:21:52.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 11:21:52.28
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:21:52.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:21:52.423
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jun  9 11:21:52.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/09/23 11:21:55.289
    Jun  9 11:21:55.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-8150 --namespace=crd-publish-openapi-8150 create -f -'
    Jun  9 11:21:56.696: INFO: stderr: ""
    Jun  9 11:21:56.696: INFO: stdout: "e2e-test-crd-publish-openapi-4982-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jun  9 11:21:56.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-8150 --namespace=crd-publish-openapi-8150 delete e2e-test-crd-publish-openapi-4982-crds test-cr'
    Jun  9 11:21:56.893: INFO: stderr: ""
    Jun  9 11:21:56.893: INFO: stdout: "e2e-test-crd-publish-openapi-4982-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jun  9 11:21:56.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-8150 --namespace=crd-publish-openapi-8150 apply -f -'
    Jun  9 11:21:58.021: INFO: stderr: ""
    Jun  9 11:21:58.021: INFO: stdout: "e2e-test-crd-publish-openapi-4982-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jun  9 11:21:58.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-8150 --namespace=crd-publish-openapi-8150 delete e2e-test-crd-publish-openapi-4982-crds test-cr'
    Jun  9 11:21:58.271: INFO: stderr: ""
    Jun  9 11:21:58.271: INFO: stdout: "e2e-test-crd-publish-openapi-4982-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 06/09/23 11:21:58.271
    Jun  9 11:21:58.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-8150 explain e2e-test-crd-publish-openapi-4982-crds'
    Jun  9 11:21:58.714: INFO: stderr: ""
    Jun  9 11:21:58.714: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4982-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:22:01.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8150" for this suite. 06/09/23 11:22:01.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:22:01.954
Jun  9 11:22:01.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename replicaset 06/09/23 11:22:01.956
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:22:01.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:22:01.988
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 06/09/23 11:22:02.003
STEP: Verify that the required pods have come up. 06/09/23 11:22:02.016
Jun  9 11:22:02.022: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  9 11:22:07.058: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/09/23 11:22:07.058
STEP: Getting /status 06/09/23 11:22:07.058
Jun  9 11:22:07.109: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 06/09/23 11:22:07.109
Jun  9 11:22:07.187: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 06/09/23 11:22:07.187
Jun  9 11:22:07.192: INFO: Observed &ReplicaSet event: ADDED
Jun  9 11:22:07.192: INFO: Observed &ReplicaSet event: MODIFIED
Jun  9 11:22:07.192: INFO: Observed &ReplicaSet event: MODIFIED
Jun  9 11:22:07.193: INFO: Observed &ReplicaSet event: MODIFIED
Jun  9 11:22:07.193: INFO: Found replicaset test-rs in namespace replicaset-2618 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  9 11:22:07.193: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 06/09/23 11:22:07.193
Jun  9 11:22:07.193: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun  9 11:22:07.242: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 06/09/23 11:22:07.242
Jun  9 11:22:07.246: INFO: Observed &ReplicaSet event: ADDED
Jun  9 11:22:07.246: INFO: Observed &ReplicaSet event: MODIFIED
Jun  9 11:22:07.246: INFO: Observed &ReplicaSet event: MODIFIED
Jun  9 11:22:07.246: INFO: Observed &ReplicaSet event: MODIFIED
Jun  9 11:22:07.246: INFO: Observed replicaset test-rs in namespace replicaset-2618 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  9 11:22:07.247: INFO: Observed &ReplicaSet event: MODIFIED
Jun  9 11:22:07.247: INFO: Found replicaset test-rs in namespace replicaset-2618 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jun  9 11:22:07.247: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun  9 11:22:07.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2618" for this suite. 06/09/23 11:22:07.285
------------------------------
• [SLOW TEST] [5.435 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:22:01.954
    Jun  9 11:22:01.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename replicaset 06/09/23 11:22:01.956
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:22:01.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:22:01.988
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 06/09/23 11:22:02.003
    STEP: Verify that the required pods have come up. 06/09/23 11:22:02.016
    Jun  9 11:22:02.022: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun  9 11:22:07.058: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/09/23 11:22:07.058
    STEP: Getting /status 06/09/23 11:22:07.058
    Jun  9 11:22:07.109: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 06/09/23 11:22:07.109
    Jun  9 11:22:07.187: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 06/09/23 11:22:07.187
    Jun  9 11:22:07.192: INFO: Observed &ReplicaSet event: ADDED
    Jun  9 11:22:07.192: INFO: Observed &ReplicaSet event: MODIFIED
    Jun  9 11:22:07.192: INFO: Observed &ReplicaSet event: MODIFIED
    Jun  9 11:22:07.193: INFO: Observed &ReplicaSet event: MODIFIED
    Jun  9 11:22:07.193: INFO: Found replicaset test-rs in namespace replicaset-2618 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun  9 11:22:07.193: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 06/09/23 11:22:07.193
    Jun  9 11:22:07.193: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun  9 11:22:07.242: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 06/09/23 11:22:07.242
    Jun  9 11:22:07.246: INFO: Observed &ReplicaSet event: ADDED
    Jun  9 11:22:07.246: INFO: Observed &ReplicaSet event: MODIFIED
    Jun  9 11:22:07.246: INFO: Observed &ReplicaSet event: MODIFIED
    Jun  9 11:22:07.246: INFO: Observed &ReplicaSet event: MODIFIED
    Jun  9 11:22:07.246: INFO: Observed replicaset test-rs in namespace replicaset-2618 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun  9 11:22:07.247: INFO: Observed &ReplicaSet event: MODIFIED
    Jun  9 11:22:07.247: INFO: Found replicaset test-rs in namespace replicaset-2618 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jun  9 11:22:07.247: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:22:07.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2618" for this suite. 06/09/23 11:22:07.285
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:22:07.388
Jun  9 11:22:07.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 11:22:07.39
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:22:07.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:22:07.523
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 06/09/23 11:22:07.532
Jun  9 11:22:07.533: INFO: Creating e2e-svc-a-vz9wk
Jun  9 11:22:07.720: INFO: Creating e2e-svc-b-wjdgl
Jun  9 11:22:07.859: INFO: Creating e2e-svc-c-xt89k
STEP: deleting service collection 06/09/23 11:22:07.982
Jun  9 11:22:08.170: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 11:22:08.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3243" for this suite. 06/09/23 11:22:08.179
------------------------------
• [0.806 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:22:07.388
    Jun  9 11:22:07.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 11:22:07.39
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:22:07.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:22:07.523
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 06/09/23 11:22:07.532
    Jun  9 11:22:07.533: INFO: Creating e2e-svc-a-vz9wk
    Jun  9 11:22:07.720: INFO: Creating e2e-svc-b-wjdgl
    Jun  9 11:22:07.859: INFO: Creating e2e-svc-c-xt89k
    STEP: deleting service collection 06/09/23 11:22:07.982
    Jun  9 11:22:08.170: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:22:08.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3243" for this suite. 06/09/23 11:22:08.179
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:22:08.195
Jun  9 11:22:08.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 11:22:08.197
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:22:08.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:22:08.317
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 06/09/23 11:22:08.322
Jun  9 11:22:08.322: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1410 proxy --unix-socket=/tmp/kubectl-proxy-unix1385602983/test'
STEP: retrieving proxy /api/ output 06/09/23 11:22:08.38
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 11:22:08.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1410" for this suite. 06/09/23 11:22:08.392
------------------------------
• [0.208 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:22:08.195
    Jun  9 11:22:08.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 11:22:08.197
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:22:08.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:22:08.317
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 06/09/23 11:22:08.322
    Jun  9 11:22:08.322: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-1410 proxy --unix-socket=/tmp/kubectl-proxy-unix1385602983/test'
    STEP: retrieving proxy /api/ output 06/09/23 11:22:08.38
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:22:08.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1410" for this suite. 06/09/23 11:22:08.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:22:08.403
Jun  9 11:22:08.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename podtemplate 06/09/23 11:22:08.404
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:22:08.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:22:08.439
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jun  9 11:22:08.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8536" for this suite. 06/09/23 11:22:08.523
------------------------------
• [0.133 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:22:08.403
    Jun  9 11:22:08.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename podtemplate 06/09/23 11:22:08.404
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:22:08.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:22:08.439
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:22:08.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8536" for this suite. 06/09/23 11:22:08.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:22:08.537
Jun  9 11:22:08.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 11:22:08.538
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:22:08.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:22:08.578
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 06/09/23 11:22:08.584
Jun  9 11:22:08.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 06/09/23 11:22:18.566
Jun  9 11:22:18.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 11:22:21.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:22:32.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9522" for this suite. 06/09/23 11:22:32.385
------------------------------
• [SLOW TEST] [23.863 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:22:08.537
    Jun  9 11:22:08.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 11:22:08.538
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:22:08.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:22:08.578
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 06/09/23 11:22:08.584
    Jun  9 11:22:08.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 06/09/23 11:22:18.566
    Jun  9 11:22:18.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 11:22:21.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:22:32.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9522" for this suite. 06/09/23 11:22:32.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:22:32.401
Jun  9 11:22:32.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename daemonsets 06/09/23 11:22:32.403
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:22:32.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:22:32.444
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 06/09/23 11:22:32.493
STEP: Check that daemon pods launch on every node of the cluster. 06/09/23 11:22:32.56
Jun  9 11:22:32.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:22:32.567: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:22:32.567: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:22:32.571: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 11:22:32.571: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 11:22:33.583: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:22:33.583: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:22:33.583: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:22:33.595: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  9 11:22:33.595: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 11:22:34.579: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:22:34.579: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:22:34.579: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:22:34.584: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  9 11:22:34.584: INFO: Node sks-test-v1-26.4-workergroup-qdprq is running 0 daemon pod, expected 1
Jun  9 11:22:35.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:22:35.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:22:35.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:22:35.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  9 11:22:35.593: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 06/09/23 11:22:35.598
STEP: DeleteCollection of the DaemonSets 06/09/23 11:22:35.604
STEP: Verify that ReplicaSets have been deleted 06/09/23 11:22:35.618
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Jun  9 11:22:35.637: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"101165"},"items":null}

Jun  9 11:22:35.644: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"101165"},"items":[{"metadata":{"name":"daemon-set-d6q27","generateName":"daemon-set-","namespace":"daemonsets-3781","uid":"fb873185-b892-4db4-8edd-24a3a1201746","resourceVersion":"101158","creationTimestamp":"2023-06-09T11:22:32Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"9410e71023a9b1e3d73bae8d14e0996aca7e646ce5f62e89a18d8edfebb2439a","cni.projectcalico.org/podIP":"172.30.17.182/32","cni.projectcalico.org/podIPs":"172.30.17.182/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d4638a8a-ef4d-4ac3-9f38-c371fb8cb64a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4638a8a-ef4d-4ac3-9f38-c371fb8cb64a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.17.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-62ht6","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-62ht6","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"sks-test-v1-26.4-workergroup-q5bjm","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["sks-test-v1-26.4-workergroup-q5bjm"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:32Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:32Z"}],"hostIP":"10.255.64.102","podIP":"172.30.17.182","podIPs":[{"ip":"172.30.17.182"}],"startTime":"2023-06-09T11:22:32Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-09T11:22:33Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://52de306b81b75f77bc440479ae47d1db66f7bf2f42ef0f7a51bb4c373b2f7628","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ldrnz","generateName":"daemon-set-","namespace":"daemonsets-3781","uid":"e38f460e-d0b7-4d74-8f56-43a4cb97bf67","resourceVersion":"101156","creationTimestamp":"2023-06-09T11:22:32Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"f85e75a38ef6bf309b57c589a96462ebf835577e12ff6713081e3fad2e7fa0b9","cni.projectcalico.org/podIP":"172.26.90.43/32","cni.projectcalico.org/podIPs":"172.26.90.43/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d4638a8a-ef4d-4ac3-9f38-c371fb8cb64a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4638a8a-ef4d-4ac3-9f38-c371fb8cb64a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.90.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hbvhx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hbvhx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"sks-test-v1-26.4-workergroup-4hkw9","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["sks-test-v1-26.4-workergroup-4hkw9"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:32Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:32Z"}],"hostIP":"10.255.64.104","podIP":"172.26.90.43","podIPs":[{"ip":"172.26.90.43"}],"startTime":"2023-06-09T11:22:32Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-09T11:22:33Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://44a7e029d279a9426b5e0125575911faf90398c00cbc14786652bfc310509341","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lz655","generateName":"daemon-set-","namespace":"daemonsets-3781","uid":"1b52e543-2a92-4518-9957-cd2bf60ee405","resourceVersion":"101160","creationTimestamp":"2023-06-09T11:22:32Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6f8cfdd4337a3b512f2b423e9c88a69e4a765fb7273d74ec2664c9a070c00a10","cni.projectcalico.org/podIP":"172.27.53.76/32","cni.projectcalico.org/podIPs":"172.27.53.76/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d4638a8a-ef4d-4ac3-9f38-c371fb8cb64a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4638a8a-ef4d-4ac3-9f38-c371fb8cb64a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.27.53.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rflhk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rflhk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"sks-test-v1-26.4-workergroup-qdprq","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["sks-test-v1-26.4-workergroup-qdprq"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:32Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:32Z"}],"hostIP":"10.255.64.103","podIP":"172.27.53.76","podIPs":[{"ip":"172.27.53.76"}],"startTime":"2023-06-09T11:22:32Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-09T11:22:33Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://905156e33d262067fc34bffc7252daaefa5db91afc8da4707b1aee5f15b09321","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:22:35.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3781" for this suite. 06/09/23 11:22:35.691
------------------------------
• [3.303 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:22:32.401
    Jun  9 11:22:32.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename daemonsets 06/09/23 11:22:32.403
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:22:32.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:22:32.444
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 06/09/23 11:22:32.493
    STEP: Check that daemon pods launch on every node of the cluster. 06/09/23 11:22:32.56
    Jun  9 11:22:32.566: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:22:32.567: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:22:32.567: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:22:32.571: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 11:22:32.571: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 11:22:33.583: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:22:33.583: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:22:33.583: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:22:33.595: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun  9 11:22:33.595: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 11:22:34.579: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:22:34.579: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:22:34.579: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:22:34.584: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun  9 11:22:34.584: INFO: Node sks-test-v1-26.4-workergroup-qdprq is running 0 daemon pod, expected 1
    Jun  9 11:22:35.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:22:35.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:22:35.585: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:22:35.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun  9 11:22:35.593: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 06/09/23 11:22:35.598
    STEP: DeleteCollection of the DaemonSets 06/09/23 11:22:35.604
    STEP: Verify that ReplicaSets have been deleted 06/09/23 11:22:35.618
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Jun  9 11:22:35.637: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"101165"},"items":null}

    Jun  9 11:22:35.644: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"101165"},"items":[{"metadata":{"name":"daemon-set-d6q27","generateName":"daemon-set-","namespace":"daemonsets-3781","uid":"fb873185-b892-4db4-8edd-24a3a1201746","resourceVersion":"101158","creationTimestamp":"2023-06-09T11:22:32Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"9410e71023a9b1e3d73bae8d14e0996aca7e646ce5f62e89a18d8edfebb2439a","cni.projectcalico.org/podIP":"172.30.17.182/32","cni.projectcalico.org/podIPs":"172.30.17.182/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d4638a8a-ef4d-4ac3-9f38-c371fb8cb64a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4638a8a-ef4d-4ac3-9f38-c371fb8cb64a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.17.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-62ht6","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-62ht6","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"sks-test-v1-26.4-workergroup-q5bjm","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["sks-test-v1-26.4-workergroup-q5bjm"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:32Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:32Z"}],"hostIP":"10.255.64.102","podIP":"172.30.17.182","podIPs":[{"ip":"172.30.17.182"}],"startTime":"2023-06-09T11:22:32Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-09T11:22:33Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://52de306b81b75f77bc440479ae47d1db66f7bf2f42ef0f7a51bb4c373b2f7628","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ldrnz","generateName":"daemon-set-","namespace":"daemonsets-3781","uid":"e38f460e-d0b7-4d74-8f56-43a4cb97bf67","resourceVersion":"101156","creationTimestamp":"2023-06-09T11:22:32Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"f85e75a38ef6bf309b57c589a96462ebf835577e12ff6713081e3fad2e7fa0b9","cni.projectcalico.org/podIP":"172.26.90.43/32","cni.projectcalico.org/podIPs":"172.26.90.43/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d4638a8a-ef4d-4ac3-9f38-c371fb8cb64a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4638a8a-ef4d-4ac3-9f38-c371fb8cb64a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.90.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hbvhx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hbvhx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"sks-test-v1-26.4-workergroup-4hkw9","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["sks-test-v1-26.4-workergroup-4hkw9"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:32Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:32Z"}],"hostIP":"10.255.64.104","podIP":"172.26.90.43","podIPs":[{"ip":"172.26.90.43"}],"startTime":"2023-06-09T11:22:32Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-09T11:22:33Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://44a7e029d279a9426b5e0125575911faf90398c00cbc14786652bfc310509341","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lz655","generateName":"daemon-set-","namespace":"daemonsets-3781","uid":"1b52e543-2a92-4518-9957-cd2bf60ee405","resourceVersion":"101160","creationTimestamp":"2023-06-09T11:22:32Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6f8cfdd4337a3b512f2b423e9c88a69e4a765fb7273d74ec2664c9a070c00a10","cni.projectcalico.org/podIP":"172.27.53.76/32","cni.projectcalico.org/podIPs":"172.27.53.76/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d4638a8a-ef4d-4ac3-9f38-c371fb8cb64a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4638a8a-ef4d-4ac3-9f38-c371fb8cb64a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-09T11:22:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.27.53.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rflhk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rflhk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"sks-test-v1-26.4-workergroup-qdprq","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["sks-test-v1-26.4-workergroup-qdprq"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:32Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-09T11:22:32Z"}],"hostIP":"10.255.64.103","podIP":"172.27.53.76","podIPs":[{"ip":"172.27.53.76"}],"startTime":"2023-06-09T11:22:32Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-09T11:22:33Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://905156e33d262067fc34bffc7252daaefa5db91afc8da4707b1aee5f15b09321","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:22:35.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3781" for this suite. 06/09/23 11:22:35.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:22:35.705
Jun  9 11:22:35.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename dns 06/09/23 11:22:35.707
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:22:35.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:22:35.735
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 06/09/23 11:22:35.743
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9432.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local; sleep 1; done
 06/09/23 11:22:35.752
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9432.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local; sleep 1; done
 06/09/23 11:22:35.752
STEP: creating a pod to probe DNS 06/09/23 11:22:35.752
STEP: submitting the pod to kubernetes 06/09/23 11:22:35.753
Jun  9 11:22:35.769: INFO: Waiting up to 15m0s for pod "dns-test-86e1d228-b2c0-4e0e-8484-86670227c024" in namespace "dns-9432" to be "running"
Jun  9 11:22:35.778: INFO: Pod "dns-test-86e1d228-b2c0-4e0e-8484-86670227c024": Phase="Pending", Reason="", readiness=false. Elapsed: 9.272131ms
Jun  9 11:22:37.788: INFO: Pod "dns-test-86e1d228-b2c0-4e0e-8484-86670227c024": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018670033s
Jun  9 11:22:39.784: INFO: Pod "dns-test-86e1d228-b2c0-4e0e-8484-86670227c024": Phase="Running", Reason="", readiness=true. Elapsed: 4.01537054s
Jun  9 11:22:39.784: INFO: Pod "dns-test-86e1d228-b2c0-4e0e-8484-86670227c024" satisfied condition "running"
STEP: retrieving the pod 06/09/23 11:22:39.784
STEP: looking for the results for each expected name from probers 06/09/23 11:22:39.79
Jun  9 11:22:39.804: INFO: DNS probes using dns-test-86e1d228-b2c0-4e0e-8484-86670227c024 succeeded

STEP: deleting the pod 06/09/23 11:22:39.804
STEP: changing the externalName to bar.example.com 06/09/23 11:22:39.835
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9432.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local; sleep 1; done
 06/09/23 11:22:39.858
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9432.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local; sleep 1; done
 06/09/23 11:22:39.858
STEP: creating a second pod to probe DNS 06/09/23 11:22:39.858
STEP: submitting the pod to kubernetes 06/09/23 11:22:39.858
Jun  9 11:22:39.885: INFO: Waiting up to 15m0s for pod "dns-test-65edde23-7929-4bd4-83b3-ab6720a72471" in namespace "dns-9432" to be "running"
Jun  9 11:22:39.896: INFO: Pod "dns-test-65edde23-7929-4bd4-83b3-ab6720a72471": Phase="Pending", Reason="", readiness=false. Elapsed: 11.480986ms
Jun  9 11:22:41.903: INFO: Pod "dns-test-65edde23-7929-4bd4-83b3-ab6720a72471": Phase="Running", Reason="", readiness=true. Elapsed: 2.018819679s
Jun  9 11:22:41.903: INFO: Pod "dns-test-65edde23-7929-4bd4-83b3-ab6720a72471" satisfied condition "running"
STEP: retrieving the pod 06/09/23 11:22:41.904
STEP: looking for the results for each expected name from probers 06/09/23 11:22:41.909
Jun  9 11:22:41.921: INFO: File wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  9 11:22:41.927: INFO: File jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  9 11:22:41.927: INFO: Lookups using dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 failed for: [wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local]

Jun  9 11:22:46.936: INFO: File wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  9 11:22:46.943: INFO: File jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  9 11:22:46.943: INFO: Lookups using dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 failed for: [wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local]

Jun  9 11:22:51.949: INFO: File wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  9 11:22:51.959: INFO: File jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  9 11:22:51.959: INFO: Lookups using dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 failed for: [wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local]

Jun  9 11:22:56.938: INFO: File wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  9 11:22:56.943: INFO: File jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  9 11:22:56.944: INFO: Lookups using dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 failed for: [wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local]

Jun  9 11:23:01.934: INFO: File wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  9 11:23:01.940: INFO: File jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  9 11:23:01.940: INFO: Lookups using dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 failed for: [wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local]

Jun  9 11:23:06.937: INFO: File wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  9 11:23:06.960: INFO: File jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  9 11:23:06.960: INFO: Lookups using dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 failed for: [wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local]

Jun  9 11:23:11.959: INFO: DNS probes using dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 succeeded

STEP: deleting the pod 06/09/23 11:23:11.959
STEP: changing the service to type=ClusterIP 06/09/23 11:23:11.987
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9432.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local; sleep 1; done
 06/09/23 11:23:12.045
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9432.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local; sleep 1; done
 06/09/23 11:23:12.045
STEP: creating a third pod to probe DNS 06/09/23 11:23:12.045
STEP: submitting the pod to kubernetes 06/09/23 11:23:12.057
Jun  9 11:23:12.080: INFO: Waiting up to 15m0s for pod "dns-test-d52d1034-8f66-4c14-ad3f-c6e9461e38ce" in namespace "dns-9432" to be "running"
Jun  9 11:23:12.088: INFO: Pod "dns-test-d52d1034-8f66-4c14-ad3f-c6e9461e38ce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.934004ms
Jun  9 11:23:14.096: INFO: Pod "dns-test-d52d1034-8f66-4c14-ad3f-c6e9461e38ce": Phase="Running", Reason="", readiness=true. Elapsed: 2.016051499s
Jun  9 11:23:14.096: INFO: Pod "dns-test-d52d1034-8f66-4c14-ad3f-c6e9461e38ce" satisfied condition "running"
STEP: retrieving the pod 06/09/23 11:23:14.096
STEP: looking for the results for each expected name from probers 06/09/23 11:23:14.102
Jun  9 11:23:14.114: INFO: DNS probes using dns-test-d52d1034-8f66-4c14-ad3f-c6e9461e38ce succeeded

STEP: deleting the pod 06/09/23 11:23:14.114
STEP: deleting the test externalName service 06/09/23 11:23:14.142
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  9 11:23:14.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9432" for this suite. 06/09/23 11:23:14.192
------------------------------
• [SLOW TEST] [38.500 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:22:35.705
    Jun  9 11:22:35.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename dns 06/09/23 11:22:35.707
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:22:35.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:22:35.735
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 06/09/23 11:22:35.743
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9432.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local; sleep 1; done
     06/09/23 11:22:35.752
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9432.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local; sleep 1; done
     06/09/23 11:22:35.752
    STEP: creating a pod to probe DNS 06/09/23 11:22:35.752
    STEP: submitting the pod to kubernetes 06/09/23 11:22:35.753
    Jun  9 11:22:35.769: INFO: Waiting up to 15m0s for pod "dns-test-86e1d228-b2c0-4e0e-8484-86670227c024" in namespace "dns-9432" to be "running"
    Jun  9 11:22:35.778: INFO: Pod "dns-test-86e1d228-b2c0-4e0e-8484-86670227c024": Phase="Pending", Reason="", readiness=false. Elapsed: 9.272131ms
    Jun  9 11:22:37.788: INFO: Pod "dns-test-86e1d228-b2c0-4e0e-8484-86670227c024": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018670033s
    Jun  9 11:22:39.784: INFO: Pod "dns-test-86e1d228-b2c0-4e0e-8484-86670227c024": Phase="Running", Reason="", readiness=true. Elapsed: 4.01537054s
    Jun  9 11:22:39.784: INFO: Pod "dns-test-86e1d228-b2c0-4e0e-8484-86670227c024" satisfied condition "running"
    STEP: retrieving the pod 06/09/23 11:22:39.784
    STEP: looking for the results for each expected name from probers 06/09/23 11:22:39.79
    Jun  9 11:22:39.804: INFO: DNS probes using dns-test-86e1d228-b2c0-4e0e-8484-86670227c024 succeeded

    STEP: deleting the pod 06/09/23 11:22:39.804
    STEP: changing the externalName to bar.example.com 06/09/23 11:22:39.835
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9432.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local; sleep 1; done
     06/09/23 11:22:39.858
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9432.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local; sleep 1; done
     06/09/23 11:22:39.858
    STEP: creating a second pod to probe DNS 06/09/23 11:22:39.858
    STEP: submitting the pod to kubernetes 06/09/23 11:22:39.858
    Jun  9 11:22:39.885: INFO: Waiting up to 15m0s for pod "dns-test-65edde23-7929-4bd4-83b3-ab6720a72471" in namespace "dns-9432" to be "running"
    Jun  9 11:22:39.896: INFO: Pod "dns-test-65edde23-7929-4bd4-83b3-ab6720a72471": Phase="Pending", Reason="", readiness=false. Elapsed: 11.480986ms
    Jun  9 11:22:41.903: INFO: Pod "dns-test-65edde23-7929-4bd4-83b3-ab6720a72471": Phase="Running", Reason="", readiness=true. Elapsed: 2.018819679s
    Jun  9 11:22:41.903: INFO: Pod "dns-test-65edde23-7929-4bd4-83b3-ab6720a72471" satisfied condition "running"
    STEP: retrieving the pod 06/09/23 11:22:41.904
    STEP: looking for the results for each expected name from probers 06/09/23 11:22:41.909
    Jun  9 11:22:41.921: INFO: File wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  9 11:22:41.927: INFO: File jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  9 11:22:41.927: INFO: Lookups using dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 failed for: [wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local]

    Jun  9 11:22:46.936: INFO: File wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  9 11:22:46.943: INFO: File jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  9 11:22:46.943: INFO: Lookups using dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 failed for: [wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local]

    Jun  9 11:22:51.949: INFO: File wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  9 11:22:51.959: INFO: File jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  9 11:22:51.959: INFO: Lookups using dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 failed for: [wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local]

    Jun  9 11:22:56.938: INFO: File wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  9 11:22:56.943: INFO: File jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  9 11:22:56.944: INFO: Lookups using dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 failed for: [wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local]

    Jun  9 11:23:01.934: INFO: File wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  9 11:23:01.940: INFO: File jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  9 11:23:01.940: INFO: Lookups using dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 failed for: [wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local]

    Jun  9 11:23:06.937: INFO: File wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  9 11:23:06.960: INFO: File jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local from pod  dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun  9 11:23:06.960: INFO: Lookups using dns-9432/dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 failed for: [wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local]

    Jun  9 11:23:11.959: INFO: DNS probes using dns-test-65edde23-7929-4bd4-83b3-ab6720a72471 succeeded

    STEP: deleting the pod 06/09/23 11:23:11.959
    STEP: changing the service to type=ClusterIP 06/09/23 11:23:11.987
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9432.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9432.svc.cluster.local; sleep 1; done
     06/09/23 11:23:12.045
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9432.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9432.svc.cluster.local; sleep 1; done
     06/09/23 11:23:12.045
    STEP: creating a third pod to probe DNS 06/09/23 11:23:12.045
    STEP: submitting the pod to kubernetes 06/09/23 11:23:12.057
    Jun  9 11:23:12.080: INFO: Waiting up to 15m0s for pod "dns-test-d52d1034-8f66-4c14-ad3f-c6e9461e38ce" in namespace "dns-9432" to be "running"
    Jun  9 11:23:12.088: INFO: Pod "dns-test-d52d1034-8f66-4c14-ad3f-c6e9461e38ce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.934004ms
    Jun  9 11:23:14.096: INFO: Pod "dns-test-d52d1034-8f66-4c14-ad3f-c6e9461e38ce": Phase="Running", Reason="", readiness=true. Elapsed: 2.016051499s
    Jun  9 11:23:14.096: INFO: Pod "dns-test-d52d1034-8f66-4c14-ad3f-c6e9461e38ce" satisfied condition "running"
    STEP: retrieving the pod 06/09/23 11:23:14.096
    STEP: looking for the results for each expected name from probers 06/09/23 11:23:14.102
    Jun  9 11:23:14.114: INFO: DNS probes using dns-test-d52d1034-8f66-4c14-ad3f-c6e9461e38ce succeeded

    STEP: deleting the pod 06/09/23 11:23:14.114
    STEP: deleting the test externalName service 06/09/23 11:23:14.142
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:23:14.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9432" for this suite. 06/09/23 11:23:14.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:23:14.207
Jun  9 11:23:14.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename taint-multiple-pods 06/09/23 11:23:14.208
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:23:14.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:23:14.249
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jun  9 11:23:14.257: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  9 11:24:14.325: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jun  9 11:24:14.331: INFO: Starting informer...
STEP: Starting pods... 06/09/23 11:24:14.331
Jun  9 11:24:14.561: INFO: Pod1 is running on sks-test-v1-26.4-workergroup-qdprq. Tainting Node
Jun  9 11:24:14.799: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7315" to be "running"
Jun  9 11:24:14.813: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.740103ms
Jun  9 11:24:16.822: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.023362108s
Jun  9 11:24:16.822: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jun  9 11:24:16.822: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7315" to be "running"
Jun  9 11:24:16.832: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 9.940822ms
Jun  9 11:24:16.832: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jun  9 11:24:16.832: INFO: Pod2 is running on sks-test-v1-26.4-workergroup-qdprq. Tainting Node
STEP: Trying to apply a taint on the Node 06/09/23 11:24:16.832
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/09/23 11:24:16.852
STEP: Waiting for Pod1 and Pod2 to be deleted 06/09/23 11:24:16.857
Jun  9 11:24:23.286: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun  9 11:24:43.325: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/09/23 11:24:43.35
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:24:43.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-7315" for this suite. 06/09/23 11:24:43.365
------------------------------
• [SLOW TEST] [89.178 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:23:14.207
    Jun  9 11:23:14.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename taint-multiple-pods 06/09/23 11:23:14.208
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:23:14.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:23:14.249
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jun  9 11:23:14.257: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun  9 11:24:14.325: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jun  9 11:24:14.331: INFO: Starting informer...
    STEP: Starting pods... 06/09/23 11:24:14.331
    Jun  9 11:24:14.561: INFO: Pod1 is running on sks-test-v1-26.4-workergroup-qdprq. Tainting Node
    Jun  9 11:24:14.799: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7315" to be "running"
    Jun  9 11:24:14.813: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.740103ms
    Jun  9 11:24:16.822: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.023362108s
    Jun  9 11:24:16.822: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jun  9 11:24:16.822: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7315" to be "running"
    Jun  9 11:24:16.832: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 9.940822ms
    Jun  9 11:24:16.832: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jun  9 11:24:16.832: INFO: Pod2 is running on sks-test-v1-26.4-workergroup-qdprq. Tainting Node
    STEP: Trying to apply a taint on the Node 06/09/23 11:24:16.832
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/09/23 11:24:16.852
    STEP: Waiting for Pod1 and Pod2 to be deleted 06/09/23 11:24:16.857
    Jun  9 11:24:23.286: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jun  9 11:24:43.325: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/09/23 11:24:43.35
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:24:43.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-7315" for this suite. 06/09/23 11:24:43.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:24:43.392
Jun  9 11:24:43.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 11:24:43.394
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:24:43.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:24:43.443
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 06/09/23 11:24:43.452
Jun  9 11:24:43.467: INFO: Waiting up to 5m0s for pod "annotationupdatedf677b74-0a07-4a0f-b647-3e5c4ad5ed0d" in namespace "projected-47" to be "running and ready"
Jun  9 11:24:43.474: INFO: Pod "annotationupdatedf677b74-0a07-4a0f-b647-3e5c4ad5ed0d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.103899ms
Jun  9 11:24:43.474: INFO: The phase of Pod annotationupdatedf677b74-0a07-4a0f-b647-3e5c4ad5ed0d is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:24:45.489: INFO: Pod "annotationupdatedf677b74-0a07-4a0f-b647-3e5c4ad5ed0d": Phase="Running", Reason="", readiness=true. Elapsed: 2.022282555s
Jun  9 11:24:45.489: INFO: The phase of Pod annotationupdatedf677b74-0a07-4a0f-b647-3e5c4ad5ed0d is Running (Ready = true)
Jun  9 11:24:45.489: INFO: Pod "annotationupdatedf677b74-0a07-4a0f-b647-3e5c4ad5ed0d" satisfied condition "running and ready"
Jun  9 11:24:46.055: INFO: Successfully updated pod "annotationupdatedf677b74-0a07-4a0f-b647-3e5c4ad5ed0d"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  9 11:24:48.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-47" for this suite. 06/09/23 11:24:48.105
------------------------------
• [4.763 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:24:43.392
    Jun  9 11:24:43.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 11:24:43.394
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:24:43.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:24:43.443
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 06/09/23 11:24:43.452
    Jun  9 11:24:43.467: INFO: Waiting up to 5m0s for pod "annotationupdatedf677b74-0a07-4a0f-b647-3e5c4ad5ed0d" in namespace "projected-47" to be "running and ready"
    Jun  9 11:24:43.474: INFO: Pod "annotationupdatedf677b74-0a07-4a0f-b647-3e5c4ad5ed0d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.103899ms
    Jun  9 11:24:43.474: INFO: The phase of Pod annotationupdatedf677b74-0a07-4a0f-b647-3e5c4ad5ed0d is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:24:45.489: INFO: Pod "annotationupdatedf677b74-0a07-4a0f-b647-3e5c4ad5ed0d": Phase="Running", Reason="", readiness=true. Elapsed: 2.022282555s
    Jun  9 11:24:45.489: INFO: The phase of Pod annotationupdatedf677b74-0a07-4a0f-b647-3e5c4ad5ed0d is Running (Ready = true)
    Jun  9 11:24:45.489: INFO: Pod "annotationupdatedf677b74-0a07-4a0f-b647-3e5c4ad5ed0d" satisfied condition "running and ready"
    Jun  9 11:24:46.055: INFO: Successfully updated pod "annotationupdatedf677b74-0a07-4a0f-b647-3e5c4ad5ed0d"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:24:48.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-47" for this suite. 06/09/23 11:24:48.105
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:24:48.156
Jun  9 11:24:48.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 11:24:48.158
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:24:48.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:24:48.251
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-332dc4cf-a0d0-4f05-ae01-9338a954320e 06/09/23 11:24:48.262
STEP: Creating a pod to test consume configMaps 06/09/23 11:24:48.331
Jun  9 11:24:48.370: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d" in namespace "projected-3696" to be "Succeeded or Failed"
Jun  9 11:24:48.382: INFO: Pod "pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.811634ms
Jun  9 11:24:50.389: INFO: Pod "pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018658456s
Jun  9 11:24:52.389: INFO: Pod "pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01820426s
STEP: Saw pod success 06/09/23 11:24:52.389
Jun  9 11:24:52.389: INFO: Pod "pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d" satisfied condition "Succeeded or Failed"
Jun  9 11:24:52.394: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d container agnhost-container: <nil>
STEP: delete the pod 06/09/23 11:24:52.414
Jun  9 11:24:52.432: INFO: Waiting for pod pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d to disappear
Jun  9 11:24:52.438: INFO: Pod pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  9 11:24:52.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3696" for this suite. 06/09/23 11:24:52.446
------------------------------
• [4.299 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:24:48.156
    Jun  9 11:24:48.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 11:24:48.158
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:24:48.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:24:48.251
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-332dc4cf-a0d0-4f05-ae01-9338a954320e 06/09/23 11:24:48.262
    STEP: Creating a pod to test consume configMaps 06/09/23 11:24:48.331
    Jun  9 11:24:48.370: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d" in namespace "projected-3696" to be "Succeeded or Failed"
    Jun  9 11:24:48.382: INFO: Pod "pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.811634ms
    Jun  9 11:24:50.389: INFO: Pod "pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018658456s
    Jun  9 11:24:52.389: INFO: Pod "pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01820426s
    STEP: Saw pod success 06/09/23 11:24:52.389
    Jun  9 11:24:52.389: INFO: Pod "pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d" satisfied condition "Succeeded or Failed"
    Jun  9 11:24:52.394: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 11:24:52.414
    Jun  9 11:24:52.432: INFO: Waiting for pod pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d to disappear
    Jun  9 11:24:52.438: INFO: Pod pod-projected-configmaps-c4771a68-3019-4c2a-b85a-0ca8a6fa926d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:24:52.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3696" for this suite. 06/09/23 11:24:52.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:24:52.456
Jun  9 11:24:52.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename prestop 06/09/23 11:24:52.458
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:24:52.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:24:52.49
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-4987 06/09/23 11:24:52.495
STEP: Waiting for pods to come up. 06/09/23 11:24:52.507
Jun  9 11:24:52.508: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4987" to be "running"
Jun  9 11:24:52.514: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.941961ms
Jun  9 11:24:54.519: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.011728041s
Jun  9 11:24:54.519: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-4987 06/09/23 11:24:54.524
Jun  9 11:24:54.532: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4987" to be "running"
Jun  9 11:24:54.538: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.181572ms
Jun  9 11:24:56.546: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.013849372s
Jun  9 11:24:56.546: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 06/09/23 11:24:56.546
Jun  9 11:25:01.609: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 06/09/23 11:25:01.609
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jun  9 11:25:01.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-4987" for this suite. 06/09/23 11:25:01.635
------------------------------
• [SLOW TEST] [9.193 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:24:52.456
    Jun  9 11:24:52.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename prestop 06/09/23 11:24:52.458
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:24:52.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:24:52.49
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-4987 06/09/23 11:24:52.495
    STEP: Waiting for pods to come up. 06/09/23 11:24:52.507
    Jun  9 11:24:52.508: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-4987" to be "running"
    Jun  9 11:24:52.514: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.941961ms
    Jun  9 11:24:54.519: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.011728041s
    Jun  9 11:24:54.519: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-4987 06/09/23 11:24:54.524
    Jun  9 11:24:54.532: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-4987" to be "running"
    Jun  9 11:24:54.538: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.181572ms
    Jun  9 11:24:56.546: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.013849372s
    Jun  9 11:24:56.546: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 06/09/23 11:24:56.546
    Jun  9 11:25:01.609: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 06/09/23 11:25:01.609
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:25:01.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-4987" for this suite. 06/09/23 11:25:01.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:25:01.65
Jun  9 11:25:01.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename deployment 06/09/23 11:25:01.652
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:25:01.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:25:01.738
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jun  9 11:25:01.745: INFO: Creating deployment "test-recreate-deployment"
Jun  9 11:25:01.757: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun  9 11:25:01.822: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun  9 11:25:03.836: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun  9 11:25:03.843: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun  9 11:25:03.859: INFO: Updating deployment test-recreate-deployment
Jun  9 11:25:03.859: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  9 11:25:03.963: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6295  5b0e1668-03e6-47cb-8c27-62b2b0b65c5f 102216 2 2023-06-09 11:25:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f8c5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-09 11:25:03 +0000 UTC,LastTransitionTime:2023-06-09 11:25:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-06-09 11:25:03 +0000 UTC,LastTransitionTime:2023-06-09 11:25:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun  9 11:25:03.971: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-6295  957784d1-1951-411a-a8a3-c069b74f82d7 102215 1 2023-06-09 11:25:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5b0e1668-03e6-47cb-8c27-62b2b0b65c5f 0xc003f8cab0 0xc003f8cab1}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b0e1668-03e6-47cb-8c27-62b2b0b65c5f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f8cb48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  9 11:25:03.971: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun  9 11:25:03.972: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-6295  8bf352b2-b9c0-49dc-a033-e3882f5893ac 102205 2 2023-06-09 11:25:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5b0e1668-03e6-47cb-8c27-62b2b0b65c5f 0xc003f8c997 0xc003f8c998}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b0e1668-03e6-47cb-8c27-62b2b0b65c5f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f8ca48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  9 11:25:03.982: INFO: Pod "test-recreate-deployment-cff6dc657-z2bqd" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-z2bqd test-recreate-deployment-cff6dc657- deployment-6295  ce472584-cc64-4649-8a46-dbfa665b820e 102217 0 2023-06-09 11:25:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 957784d1-1951-411a-a8a3-c069b74f82d7 0xc003c9b420 0xc003c9b421}] [] [{kube-controller-manager Update v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"957784d1-1951-411a-a8a3-c069b74f82d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4dkb7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4dkb7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:25:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:25:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:25:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:25:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-09 11:25:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  9 11:25:03.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6295" for this suite. 06/09/23 11:25:03.992
------------------------------
• [2.352 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:25:01.65
    Jun  9 11:25:01.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename deployment 06/09/23 11:25:01.652
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:25:01.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:25:01.738
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jun  9 11:25:01.745: INFO: Creating deployment "test-recreate-deployment"
    Jun  9 11:25:01.757: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jun  9 11:25:01.822: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jun  9 11:25:03.836: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jun  9 11:25:03.843: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jun  9 11:25:03.859: INFO: Updating deployment test-recreate-deployment
    Jun  9 11:25:03.859: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  9 11:25:03.963: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-6295  5b0e1668-03e6-47cb-8c27-62b2b0b65c5f 102216 2 2023-06-09 11:25:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f8c5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-09 11:25:03 +0000 UTC,LastTransitionTime:2023-06-09 11:25:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-06-09 11:25:03 +0000 UTC,LastTransitionTime:2023-06-09 11:25:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jun  9 11:25:03.971: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-6295  957784d1-1951-411a-a8a3-c069b74f82d7 102215 1 2023-06-09 11:25:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5b0e1668-03e6-47cb-8c27-62b2b0b65c5f 0xc003f8cab0 0xc003f8cab1}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b0e1668-03e6-47cb-8c27-62b2b0b65c5f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f8cb48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun  9 11:25:03.971: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jun  9 11:25:03.972: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-6295  8bf352b2-b9c0-49dc-a033-e3882f5893ac 102205 2 2023-06-09 11:25:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5b0e1668-03e6-47cb-8c27-62b2b0b65c5f 0xc003f8c997 0xc003f8c998}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b0e1668-03e6-47cb-8c27-62b2b0b65c5f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f8ca48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun  9 11:25:03.982: INFO: Pod "test-recreate-deployment-cff6dc657-z2bqd" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-z2bqd test-recreate-deployment-cff6dc657- deployment-6295  ce472584-cc64-4649-8a46-dbfa665b820e 102217 0 2023-06-09 11:25:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 957784d1-1951-411a-a8a3-c069b74f82d7 0xc003c9b420 0xc003c9b421}] [] [{kube-controller-manager Update v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"957784d1-1951-411a-a8a3-c069b74f82d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:25:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4dkb7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4dkb7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:25:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:25:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:25:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:25:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-09 11:25:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:25:03.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6295" for this suite. 06/09/23 11:25:03.992
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:25:04.004
Jun  9 11:25:04.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 11:25:04.006
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:25:04.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:25:04.039
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 06/09/23 11:25:04.051
Jun  9 11:25:04.051: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-741 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 06/09/23 11:25:04.116
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 11:25:04.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-741" for this suite. 06/09/23 11:25:04.142
------------------------------
• [0.152 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:25:04.004
    Jun  9 11:25:04.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 11:25:04.006
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:25:04.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:25:04.039
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 06/09/23 11:25:04.051
    Jun  9 11:25:04.051: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-741 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 06/09/23 11:25:04.116
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:25:04.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-741" for this suite. 06/09/23 11:25:04.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:25:04.156
Jun  9 11:25:04.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename containers 06/09/23 11:25:04.158
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:25:04.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:25:04.189
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 06/09/23 11:25:04.195
Jun  9 11:25:04.218: INFO: Waiting up to 5m0s for pod "client-containers-81466e15-ff92-4401-9d04-2685362163b7" in namespace "containers-8290" to be "Succeeded or Failed"
Jun  9 11:25:04.242: INFO: Pod "client-containers-81466e15-ff92-4401-9d04-2685362163b7": Phase="Pending", Reason="", readiness=false. Elapsed: 24.556351ms
Jun  9 11:25:06.275: INFO: Pod "client-containers-81466e15-ff92-4401-9d04-2685362163b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057065115s
Jun  9 11:25:08.250: INFO: Pod "client-containers-81466e15-ff92-4401-9d04-2685362163b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032475099s
STEP: Saw pod success 06/09/23 11:25:08.25
Jun  9 11:25:08.251: INFO: Pod "client-containers-81466e15-ff92-4401-9d04-2685362163b7" satisfied condition "Succeeded or Failed"
Jun  9 11:25:08.256: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod client-containers-81466e15-ff92-4401-9d04-2685362163b7 container agnhost-container: <nil>
STEP: delete the pod 06/09/23 11:25:08.266
Jun  9 11:25:08.289: INFO: Waiting for pod client-containers-81466e15-ff92-4401-9d04-2685362163b7 to disappear
Jun  9 11:25:08.296: INFO: Pod client-containers-81466e15-ff92-4401-9d04-2685362163b7 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jun  9 11:25:08.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-8290" for this suite. 06/09/23 11:25:08.313
------------------------------
• [4.168 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:25:04.156
    Jun  9 11:25:04.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename containers 06/09/23 11:25:04.158
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:25:04.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:25:04.189
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 06/09/23 11:25:04.195
    Jun  9 11:25:04.218: INFO: Waiting up to 5m0s for pod "client-containers-81466e15-ff92-4401-9d04-2685362163b7" in namespace "containers-8290" to be "Succeeded or Failed"
    Jun  9 11:25:04.242: INFO: Pod "client-containers-81466e15-ff92-4401-9d04-2685362163b7": Phase="Pending", Reason="", readiness=false. Elapsed: 24.556351ms
    Jun  9 11:25:06.275: INFO: Pod "client-containers-81466e15-ff92-4401-9d04-2685362163b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057065115s
    Jun  9 11:25:08.250: INFO: Pod "client-containers-81466e15-ff92-4401-9d04-2685362163b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032475099s
    STEP: Saw pod success 06/09/23 11:25:08.25
    Jun  9 11:25:08.251: INFO: Pod "client-containers-81466e15-ff92-4401-9d04-2685362163b7" satisfied condition "Succeeded or Failed"
    Jun  9 11:25:08.256: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod client-containers-81466e15-ff92-4401-9d04-2685362163b7 container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 11:25:08.266
    Jun  9 11:25:08.289: INFO: Waiting for pod client-containers-81466e15-ff92-4401-9d04-2685362163b7 to disappear
    Jun  9 11:25:08.296: INFO: Pod client-containers-81466e15-ff92-4401-9d04-2685362163b7 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:25:08.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-8290" for this suite. 06/09/23 11:25:08.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:25:08.326
Jun  9 11:25:08.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-probe 06/09/23 11:25:08.327
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:25:08.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:25:08.373
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  9 11:26:08.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7733" for this suite. 06/09/23 11:26:08.413
------------------------------
• [SLOW TEST] [60.096 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:25:08.326
    Jun  9 11:25:08.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-probe 06/09/23 11:25:08.327
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:25:08.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:25:08.373
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:26:08.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7733" for this suite. 06/09/23 11:26:08.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:26:08.423
Jun  9 11:26:08.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 11:26:08.426
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:26:08.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:26:08.451
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 06/09/23 11:26:08.459
Jun  9 11:26:08.459: INFO: namespace kubectl-4263
Jun  9 11:26:08.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-4263 create -f -'
Jun  9 11:26:09.632: INFO: stderr: ""
Jun  9 11:26:09.632: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/09/23 11:26:09.632
Jun  9 11:26:10.639: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 11:26:10.639: INFO: Found 0 / 1
Jun  9 11:26:11.639: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 11:26:11.639: INFO: Found 1 / 1
Jun  9 11:26:11.639: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun  9 11:26:11.645: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 11:26:11.645: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  9 11:26:11.645: INFO: wait on agnhost-primary startup in kubectl-4263 
Jun  9 11:26:11.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-4263 logs agnhost-primary-jh8xk agnhost-primary'
Jun  9 11:26:11.771: INFO: stderr: ""
Jun  9 11:26:11.771: INFO: stdout: "Paused\n"
STEP: exposing RC 06/09/23 11:26:11.771
Jun  9 11:26:11.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-4263 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun  9 11:26:11.935: INFO: stderr: ""
Jun  9 11:26:11.935: INFO: stdout: "service/rm2 exposed\n"
Jun  9 11:26:11.940: INFO: Service rm2 in namespace kubectl-4263 found.
STEP: exposing service 06/09/23 11:26:14.005
Jun  9 11:26:14.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-4263 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun  9 11:26:14.194: INFO: stderr: ""
Jun  9 11:26:14.194: INFO: stdout: "service/rm3 exposed\n"
Jun  9 11:26:14.201: INFO: Service rm3 in namespace kubectl-4263 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 11:26:16.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4263" for this suite. 06/09/23 11:26:16.221
------------------------------
• [SLOW TEST] [7.840 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:26:08.423
    Jun  9 11:26:08.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 11:26:08.426
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:26:08.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:26:08.451
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 06/09/23 11:26:08.459
    Jun  9 11:26:08.459: INFO: namespace kubectl-4263
    Jun  9 11:26:08.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-4263 create -f -'
    Jun  9 11:26:09.632: INFO: stderr: ""
    Jun  9 11:26:09.632: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/09/23 11:26:09.632
    Jun  9 11:26:10.639: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  9 11:26:10.639: INFO: Found 0 / 1
    Jun  9 11:26:11.639: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  9 11:26:11.639: INFO: Found 1 / 1
    Jun  9 11:26:11.639: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jun  9 11:26:11.645: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  9 11:26:11.645: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun  9 11:26:11.645: INFO: wait on agnhost-primary startup in kubectl-4263 
    Jun  9 11:26:11.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-4263 logs agnhost-primary-jh8xk agnhost-primary'
    Jun  9 11:26:11.771: INFO: stderr: ""
    Jun  9 11:26:11.771: INFO: stdout: "Paused\n"
    STEP: exposing RC 06/09/23 11:26:11.771
    Jun  9 11:26:11.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-4263 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jun  9 11:26:11.935: INFO: stderr: ""
    Jun  9 11:26:11.935: INFO: stdout: "service/rm2 exposed\n"
    Jun  9 11:26:11.940: INFO: Service rm2 in namespace kubectl-4263 found.
    STEP: exposing service 06/09/23 11:26:14.005
    Jun  9 11:26:14.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-4263 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jun  9 11:26:14.194: INFO: stderr: ""
    Jun  9 11:26:14.194: INFO: stdout: "service/rm3 exposed\n"
    Jun  9 11:26:14.201: INFO: Service rm3 in namespace kubectl-4263 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:26:16.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4263" for this suite. 06/09/23 11:26:16.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:26:16.265
Jun  9 11:26:16.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename deployment 06/09/23 11:26:16.266
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:26:16.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:26:16.316
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jun  9 11:26:16.320: INFO: Creating deployment "webserver-deployment"
Jun  9 11:26:16.334: INFO: Waiting for observed generation 1
Jun  9 11:26:18.353: INFO: Waiting for all required pods to come up
Jun  9 11:26:18.360: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 06/09/23 11:26:18.36
Jun  9 11:26:18.360: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4xk9x" in namespace "deployment-6359" to be "running"
Jun  9 11:26:18.360: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6r922" in namespace "deployment-6359" to be "running"
Jun  9 11:26:18.360: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-7trkd" in namespace "deployment-6359" to be "running"
Jun  9 11:26:18.360: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-cbzdf" in namespace "deployment-6359" to be "running"
Jun  9 11:26:18.361: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jhzbl" in namespace "deployment-6359" to be "running"
Jun  9 11:26:18.361: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-stp76" in namespace "deployment-6359" to be "running"
Jun  9 11:26:18.361: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-ttp9m" in namespace "deployment-6359" to be "running"
Jun  9 11:26:18.366: INFO: Pod "webserver-deployment-7f5969cbc7-6r922": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061578ms
Jun  9 11:26:18.367: INFO: Pod "webserver-deployment-7f5969cbc7-7trkd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.094381ms
Jun  9 11:26:18.368: INFO: Pod "webserver-deployment-7f5969cbc7-stp76": Phase="Pending", Reason="", readiness=false. Elapsed: 6.9602ms
Jun  9 11:26:18.368: INFO: Pod "webserver-deployment-7f5969cbc7-ttp9m": Phase="Pending", Reason="", readiness=false. Elapsed: 7.008597ms
Jun  9 11:26:18.368: INFO: Pod "webserver-deployment-7f5969cbc7-jhzbl": Phase="Pending", Reason="", readiness=false. Elapsed: 7.278923ms
Jun  9 11:26:18.367: INFO: Pod "webserver-deployment-7f5969cbc7-cbzdf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.503618ms
Jun  9 11:26:18.369: INFO: Pod "webserver-deployment-7f5969cbc7-4xk9x": Phase="Pending", Reason="", readiness=false. Elapsed: 9.363028ms
Jun  9 11:26:20.380: INFO: Pod "webserver-deployment-7f5969cbc7-cbzdf": Phase="Running", Reason="", readiness=true. Elapsed: 2.019397673s
Jun  9 11:26:20.380: INFO: Pod "webserver-deployment-7f5969cbc7-cbzdf" satisfied condition "running"
Jun  9 11:26:20.381: INFO: Pod "webserver-deployment-7f5969cbc7-4xk9x": Phase="Running", Reason="", readiness=true. Elapsed: 2.020953571s
Jun  9 11:26:20.381: INFO: Pod "webserver-deployment-7f5969cbc7-4xk9x" satisfied condition "running"
Jun  9 11:26:20.381: INFO: Pod "webserver-deployment-7f5969cbc7-jhzbl": Phase="Running", Reason="", readiness=true. Elapsed: 2.020419748s
Jun  9 11:26:20.381: INFO: Pod "webserver-deployment-7f5969cbc7-jhzbl" satisfied condition "running"
Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-stp76": Phase="Running", Reason="", readiness=true. Elapsed: 2.021012268s
Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-stp76" satisfied condition "running"
Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-6r922": Phase="Running", Reason="", readiness=true. Elapsed: 2.021700191s
Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-6r922" satisfied condition "running"
Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-7trkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.021921198s
Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-7trkd" satisfied condition "running"
Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-ttp9m": Phase="Running", Reason="", readiness=true. Elapsed: 2.021725099s
Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-ttp9m" satisfied condition "running"
Jun  9 11:26:20.383: INFO: Waiting for deployment "webserver-deployment" to complete
Jun  9 11:26:20.406: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun  9 11:26:20.437: INFO: Updating deployment webserver-deployment
Jun  9 11:26:20.437: INFO: Waiting for observed generation 2
Jun  9 11:26:22.457: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun  9 11:26:22.469: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun  9 11:26:22.479: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun  9 11:26:22.514: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun  9 11:26:22.514: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun  9 11:26:22.522: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun  9 11:26:22.541: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun  9 11:26:22.541: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun  9 11:26:22.564: INFO: Updating deployment webserver-deployment
Jun  9 11:26:22.564: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun  9 11:26:22.575: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun  9 11:26:22.585: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  9 11:26:22.627: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6359  af5a43cd-9b78-4982-ba83-1b2eff8a10c4 102976 3 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e13e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-06-09 11:26:20 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-09 11:26:22 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun  9 11:26:22.653: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-6359  e2d1105f-758f-4773-881d-b4c4cc859589 102958 3 2023-06-09 11:26:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment af5a43cd-9b78-4982-ba83-1b2eff8a10c4 0xc0033673c7 0xc0033673c8}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af5a43cd-9b78-4982-ba83-1b2eff8a10c4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003367468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  9 11:26:22.653: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun  9 11:26:22.653: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-6359  e88f6fd8-c52c-445d-8620-22b874bda7fd 102955 3 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment af5a43cd-9b78-4982-ba83-1b2eff8a10c4 0xc0033672c7 0xc0033672c8}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af5a43cd-9b78-4982-ba83-1b2eff8a10c4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003367368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun  9 11:26:22.717: INFO: Pod "webserver-deployment-7f5969cbc7-2stft" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2stft webserver-deployment-7f5969cbc7- deployment-6359  539bb424-0fad-4f8d-9ce0-c900370f7aa5 102985 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc003367927 0xc003367928}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4g2zz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4g2zz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-09 11:26:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.718: INFO: Pod "webserver-deployment-7f5969cbc7-6r922" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6r922 webserver-deployment-7f5969cbc7- deployment-6359  a8d4a833-64dc-43be-81f7-496549992146 102808 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f2f617d4462ca22264ef18625855b42f3bb3294b299a9af95189edf898db43a5 cni.projectcalico.org/podIP:172.27.53.90/32 cni.projectcalico.org/podIPs:172.27.53.90/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc003367b27 0xc003367b28}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.27.53.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mgr25,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mgr25,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.27.53.90,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://587850ef382e8a9865b70f9a732d9092901714f1a47a2edca1ab6215fd88c252,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.27.53.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.718: INFO: Pod "webserver-deployment-7f5969cbc7-6vgz6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6vgz6 webserver-deployment-7f5969cbc7- deployment-6359  1ae76f76-cd16-408a-8c63-8906e84bdbe3 102990 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc003367f77 0xc003367f78}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fv7gr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fv7gr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.719: INFO: Pod "webserver-deployment-7f5969cbc7-86zrn" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-86zrn webserver-deployment-7f5969cbc7- deployment-6359  5c74c132-b4f7-4f9d-8d76-9f8317d51b0a 102991 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0009c8b20 0xc0009c8b21}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-svjdv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-svjdv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.719: INFO: Pod "webserver-deployment-7f5969cbc7-bff6x" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bff6x webserver-deployment-7f5969cbc7- deployment-6359  3a361a60-ddc7-4341-a11f-01c6255f68a4 102978 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0009c9390 0xc0009c9391}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nqqbm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nqqbm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.720: INFO: Pod "webserver-deployment-7f5969cbc7-cbzdf" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cbzdf webserver-deployment-7f5969cbc7- deployment-6359  13947265-6f4f-4bc7-abf4-2fbbba927dba 102821 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8b0fe5ca4f9bc718ed5571ce6a5a73848bc9f5ce7f4bc130fc8346a67689fd5c cni.projectcalico.org/podIP:172.26.90.56/32 cni.projectcalico.org/podIPs:172.26.90.56/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0009c9500 0xc0009c9501}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.90.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qfqfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qfqfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.26.90.56,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a9e943c4d4e1f72ac32468db74a7bdfda89a7cb6827f1fb7f0bdf61dbb0714d6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.90.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.720: INFO: Pod "webserver-deployment-7f5969cbc7-cgkvl" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cgkvl webserver-deployment-7f5969cbc7- deployment-6359  fd84428c-c81b-4715-b16e-120d5cd938bb 102966 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0009c9707 0xc0009c9708}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94g6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94g6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.720: INFO: Pod "webserver-deployment-7f5969cbc7-d9fpq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d9fpq webserver-deployment-7f5969cbc7- deployment-6359  4e65767f-c1da-476c-8b37-b2eb9484047b 102984 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0009c9860 0xc0009c9861}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hjz9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hjz9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.720: INFO: Pod "webserver-deployment-7f5969cbc7-fdcrj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fdcrj webserver-deployment-7f5969cbc7- deployment-6359  d8a5e704-0d26-4a96-80af-47cf0c7bb7bc 102986 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0009c99a0 0xc0009c99a1}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rx25z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rx25z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.720: INFO: Pod "webserver-deployment-7f5969cbc7-hgp9q" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hgp9q webserver-deployment-7f5969cbc7- deployment-6359  f13d6914-3deb-461b-891b-ee1ccee5e6a5 102980 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0058082c0 0xc0058082c1}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lnqvq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lnqvq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.720: INFO: Pod "webserver-deployment-7f5969cbc7-hzkcj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hzkcj webserver-deployment-7f5969cbc7- deployment-6359  13c15c61-113e-4687-94b4-61f6882cafd9 102992 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005808400 0xc005808401}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-prhkm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-prhkm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-09 11:26:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.720: INFO: Pod "webserver-deployment-7f5969cbc7-j88f9" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-j88f9 webserver-deployment-7f5969cbc7- deployment-6359  e3b30cdd-6888-4695-99e8-c7c31123d91b 102997 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0058085b7 0xc0058085b8}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnw96,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnw96,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.721: INFO: Pod "webserver-deployment-7f5969cbc7-jhzbl" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jhzbl webserver-deployment-7f5969cbc7- deployment-6359  98c72a48-f2ee-480e-ad7f-1d6ccd2f5d83 102828 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1cb3d2558c5ee05a92b9de967ab3801f1069e987ce94a7a411a5ed9d6461b4f4 cni.projectcalico.org/podIP:172.30.17.184/32 cni.projectcalico.org/podIPs:172.30.17.184/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005808730 0xc005808731}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.17.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zz6q9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zz6q9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.17.184,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c7e62bb37a83b4582218cfd0a6a9adceff120dac57bf540b7fffd791cd2fc51b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.17.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.721: INFO: Pod "webserver-deployment-7f5969cbc7-kxtj4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kxtj4 webserver-deployment-7f5969cbc7- deployment-6359  b691eb66-9eeb-42f9-9ece-5bf797ba2e2e 102995 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005808937 0xc005808938}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x9p6r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x9p6r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.721: INFO: Pod "webserver-deployment-7f5969cbc7-lf6g5" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lf6g5 webserver-deployment-7f5969cbc7- deployment-6359  e9034244-6d9a-4f3c-a20e-98e9695e07ca 102983 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005808a90 0xc005808a91}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jc4gr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jc4gr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.721: INFO: Pod "webserver-deployment-7f5969cbc7-nmqc4" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nmqc4 webserver-deployment-7f5969cbc7- deployment-6359  97729e95-dc69-46f1-81fd-efd462f454f2 102786 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8a2f43d10da55fe9c6c2175f4d6785decdee681de686359d5ab6fc2200fcbd12 cni.projectcalico.org/podIP:172.26.90.55/32 cni.projectcalico.org/podIPs:172.26.90.55/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005808bf0 0xc005808bf1}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.90.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hhp5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hhp5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.26.90.55,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://43bce70471d3197df7086f9a76bd9dbca1d93d72d9af630d16ead781ca0296f9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.90.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.722: INFO: Pod "webserver-deployment-7f5969cbc7-stp76" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-stp76 webserver-deployment-7f5969cbc7- deployment-6359  61052726-1712-4efc-a574-87de0bb6c62a 102840 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9af9bc73846105f72cf63804d6cb6242c9ac371e3ff3490a68afc03ca9dcfa50 cni.projectcalico.org/podIP:172.27.53.121/32 cni.projectcalico.org/podIPs:172.27.53.121/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005808e07 0xc005808e08}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.27.53.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4p4vj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4p4vj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.27.53.121,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2561eae39473ff09cd2b1426b0e8dd1f19dfd95f4b98ba5e8d6bc4455073fc96,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.27.53.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.722: INFO: Pod "webserver-deployment-7f5969cbc7-ttp9m" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ttp9m webserver-deployment-7f5969cbc7- deployment-6359  91a059b6-547e-43e3-b6a5-726d4fe95f7e 102826 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b33c56013d5fa2537aed9e20438ef2e763d0693930cd018971378744d16ad117 cni.projectcalico.org/podIP:172.30.17.185/32 cni.projectcalico.org/podIPs:172.30.17.185/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005809027 0xc005809028}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.17.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fh4k4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fh4k4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.17.185,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://13bf05e3f9f2bf48111c3129c94a7e089e14b78f191afff8cd3fb6b17ca9c66d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.17.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.722: INFO: Pod "webserver-deployment-7f5969cbc7-w2qkh" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-w2qkh webserver-deployment-7f5969cbc7- deployment-6359  ce76df2f-5332-4ffd-940d-1fab0f8ccd42 102784 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:fe8c70d7e51f42c90996c6efc5f451bea644f0eaa230d229f07eba68d22ee280 cni.projectcalico.org/podIP:172.26.90.58/32 cni.projectcalico.org/podIPs:172.26.90.58/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005809247 0xc005809248}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.90.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p89z7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p89z7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.26.90.58,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e236f0aa9b720b3846291bba9cd8957751e89a4793a9a2ffb003b9170f957778,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.90.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.722: INFO: Pod "webserver-deployment-7f5969cbc7-xshsh" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xshsh webserver-deployment-7f5969cbc7- deployment-6359  45794c0f-d035-4d1b-a225-99d6aa7fd275 102793 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a28481bdba923bd0260741f98edfedfb8e85567dd5cbedf0bea91aec5d44ee40 cni.projectcalico.org/podIP:172.30.17.183/32 cni.projectcalico.org/podIPs:172.30.17.183/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005809467 0xc005809468}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.17.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4swlq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4swlq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.17.183,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://28d81acb7fc6f138e3c69826f867cf2f228cc8a1196fba93cc1cbfa4fc9c9ba5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.17.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.725: INFO: Pod "webserver-deployment-d9f79cb5-28j8n" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-28j8n webserver-deployment-d9f79cb5- deployment-6359  b34e1e4a-1bd3-4096-aef5-c4e5adcf7e3e 102989 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc005809677 0xc005809678}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5t8zd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5t8zd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.725: INFO: Pod "webserver-deployment-d9f79cb5-5vcwj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5vcwj webserver-deployment-d9f79cb5- deployment-6359  c2337e6d-96c9-45ec-8d74-51398dca94e6 102999 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0058097bf 0xc0058097d0}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vgpv2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vgpv2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.725: INFO: Pod "webserver-deployment-d9f79cb5-6bf2d" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6bf2d webserver-deployment-d9f79cb5- deployment-6359  6d68e27c-947d-47de-896c-30eea6d3f93a 102988 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc00580991f 0xc005809930}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wcj8r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wcj8r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-6w98q" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6w98q webserver-deployment-d9f79cb5- deployment-6359  e4532d68-f62e-4f3a-a5fd-665dfa140607 102915 0 2023-06-09 11:26:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:cdf9bbab37e6ef1617ee97c2d3fb90b49f9d71df741f72affcf4293122d4926c cni.projectcalico.org/podIP:172.27.53.102/32 cni.projectcalico.org/podIPs:172.27.53.102/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc005809a6f 0xc005809aa0}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-09 11:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfn7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfn7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-09 11:26:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-82lqn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-82lqn webserver-deployment-d9f79cb5- deployment-6359  b82557db-9fa5-46ff-bba0-12aa7f672dd1 102907 0 2023-06-09 11:26:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9d6cd02ba9fc373a358ee1b0a4051240ab33756f144116eeefeb3db0e49c390e cni.projectcalico.org/podIP:172.27.53.106/32 cni.projectcalico.org/podIPs:172.27.53.106/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc005809cb7 0xc005809cb8}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-09 11:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ldnbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ldnbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-09 11:26:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-d5rkm" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d5rkm webserver-deployment-d9f79cb5- deployment-6359  837b7134-d099-4896-bde3-fc3540a52d90 102904 0 2023-06-09 11:26:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:0b58755478e518b184e708490668b1d5b5b1d5951b2e6d2c662a42c4b4429c5c cni.projectcalico.org/podIP:172.30.17.187/32 cni.projectcalico.org/podIPs:172.30.17.187/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc005809ed7 0xc005809ed8}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-09 11:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pwl8g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pwl8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-09 11:26:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-d6gkk" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d6gkk webserver-deployment-d9f79cb5- deployment-6359  1ab07f35-6f7d-4ac4-99d2-66c3b79a03cb 102994 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0029bc0d7 0xc0029bc0d8}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-krx4s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krx4s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-fkq2m" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fkq2m webserver-deployment-d9f79cb5- deployment-6359  1ef297f9-a3b3-4c9b-ad6c-f3536650601b 102993 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0029bc22f 0xc0029bc240}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j9gxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j9gxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-k2p5q" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-k2p5q webserver-deployment-d9f79cb5- deployment-6359  b3941a84-0aa9-40ae-851c-154dde24ca52 102914 0 2023-06-09 11:26:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3291f95b7ec8e75ce8e725454dc68db69af3491441a89bd38e59f6ff9282f7fa cni.projectcalico.org/podIP:172.30.17.186/32 cni.projectcalico.org/podIPs:172.30.17.186/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0029bc38f 0xc0029bc3c0}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-09 11:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-654wq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-654wq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-09 11:26:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-r25n5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-r25n5 webserver-deployment-d9f79cb5- deployment-6359  0d6bc1a9-93bc-4c22-9109-c78ac8359c91 102996 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0029bc667 0xc0029bc668}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ww4qm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ww4qm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-rt9fz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rt9fz webserver-deployment-d9f79cb5- deployment-6359  78125897-0906-4bfe-a849-0efc9f9eb3ca 102987 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0029bc7af 0xc0029bc7c0}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kf8vp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kf8vp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.731: INFO: Pod "webserver-deployment-d9f79cb5-tm4bl" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tm4bl webserver-deployment-d9f79cb5- deployment-6359  b2ce26c1-7fdd-42ed-abd9-a5cf03cd3e85 102908 0 2023-06-09 11:26:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:984ab275f7f7c822e8c2df159910d5194b6025d94be060d4fcf0ee10072ef363 cni.projectcalico.org/podIP:172.26.90.49/32 cni.projectcalico.org/podIPs:172.26.90.49/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0029bc8ff 0xc0029bc930}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-09 11:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmkbt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmkbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-09 11:26:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  9 11:26:22.731: INFO: Pod "webserver-deployment-d9f79cb5-xj7j9" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xj7j9 webserver-deployment-d9f79cb5- deployment-6359  40764766-4b52-4f75-bdfb-402d0c508fed 102968 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0029bcb27 0xc0029bcb28}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-spdgz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-spdgz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  9 11:26:22.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6359" for this suite. 06/09/23 11:26:22.772
------------------------------
• [SLOW TEST] [6.529 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:26:16.265
    Jun  9 11:26:16.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename deployment 06/09/23 11:26:16.266
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:26:16.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:26:16.316
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jun  9 11:26:16.320: INFO: Creating deployment "webserver-deployment"
    Jun  9 11:26:16.334: INFO: Waiting for observed generation 1
    Jun  9 11:26:18.353: INFO: Waiting for all required pods to come up
    Jun  9 11:26:18.360: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 06/09/23 11:26:18.36
    Jun  9 11:26:18.360: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4xk9x" in namespace "deployment-6359" to be "running"
    Jun  9 11:26:18.360: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6r922" in namespace "deployment-6359" to be "running"
    Jun  9 11:26:18.360: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-7trkd" in namespace "deployment-6359" to be "running"
    Jun  9 11:26:18.360: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-cbzdf" in namespace "deployment-6359" to be "running"
    Jun  9 11:26:18.361: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jhzbl" in namespace "deployment-6359" to be "running"
    Jun  9 11:26:18.361: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-stp76" in namespace "deployment-6359" to be "running"
    Jun  9 11:26:18.361: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-ttp9m" in namespace "deployment-6359" to be "running"
    Jun  9 11:26:18.366: INFO: Pod "webserver-deployment-7f5969cbc7-6r922": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061578ms
    Jun  9 11:26:18.367: INFO: Pod "webserver-deployment-7f5969cbc7-7trkd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.094381ms
    Jun  9 11:26:18.368: INFO: Pod "webserver-deployment-7f5969cbc7-stp76": Phase="Pending", Reason="", readiness=false. Elapsed: 6.9602ms
    Jun  9 11:26:18.368: INFO: Pod "webserver-deployment-7f5969cbc7-ttp9m": Phase="Pending", Reason="", readiness=false. Elapsed: 7.008597ms
    Jun  9 11:26:18.368: INFO: Pod "webserver-deployment-7f5969cbc7-jhzbl": Phase="Pending", Reason="", readiness=false. Elapsed: 7.278923ms
    Jun  9 11:26:18.367: INFO: Pod "webserver-deployment-7f5969cbc7-cbzdf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.503618ms
    Jun  9 11:26:18.369: INFO: Pod "webserver-deployment-7f5969cbc7-4xk9x": Phase="Pending", Reason="", readiness=false. Elapsed: 9.363028ms
    Jun  9 11:26:20.380: INFO: Pod "webserver-deployment-7f5969cbc7-cbzdf": Phase="Running", Reason="", readiness=true. Elapsed: 2.019397673s
    Jun  9 11:26:20.380: INFO: Pod "webserver-deployment-7f5969cbc7-cbzdf" satisfied condition "running"
    Jun  9 11:26:20.381: INFO: Pod "webserver-deployment-7f5969cbc7-4xk9x": Phase="Running", Reason="", readiness=true. Elapsed: 2.020953571s
    Jun  9 11:26:20.381: INFO: Pod "webserver-deployment-7f5969cbc7-4xk9x" satisfied condition "running"
    Jun  9 11:26:20.381: INFO: Pod "webserver-deployment-7f5969cbc7-jhzbl": Phase="Running", Reason="", readiness=true. Elapsed: 2.020419748s
    Jun  9 11:26:20.381: INFO: Pod "webserver-deployment-7f5969cbc7-jhzbl" satisfied condition "running"
    Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-stp76": Phase="Running", Reason="", readiness=true. Elapsed: 2.021012268s
    Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-stp76" satisfied condition "running"
    Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-6r922": Phase="Running", Reason="", readiness=true. Elapsed: 2.021700191s
    Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-6r922" satisfied condition "running"
    Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-7trkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.021921198s
    Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-7trkd" satisfied condition "running"
    Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-ttp9m": Phase="Running", Reason="", readiness=true. Elapsed: 2.021725099s
    Jun  9 11:26:20.382: INFO: Pod "webserver-deployment-7f5969cbc7-ttp9m" satisfied condition "running"
    Jun  9 11:26:20.383: INFO: Waiting for deployment "webserver-deployment" to complete
    Jun  9 11:26:20.406: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jun  9 11:26:20.437: INFO: Updating deployment webserver-deployment
    Jun  9 11:26:20.437: INFO: Waiting for observed generation 2
    Jun  9 11:26:22.457: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jun  9 11:26:22.469: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jun  9 11:26:22.479: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jun  9 11:26:22.514: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jun  9 11:26:22.514: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jun  9 11:26:22.522: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jun  9 11:26:22.541: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jun  9 11:26:22.541: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jun  9 11:26:22.564: INFO: Updating deployment webserver-deployment
    Jun  9 11:26:22.564: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jun  9 11:26:22.575: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jun  9 11:26:22.585: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  9 11:26:22.627: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-6359  af5a43cd-9b78-4982-ba83-1b2eff8a10c4 102976 3 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e13e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-06-09 11:26:20 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-09 11:26:22 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jun  9 11:26:22.653: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-6359  e2d1105f-758f-4773-881d-b4c4cc859589 102958 3 2023-06-09 11:26:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment af5a43cd-9b78-4982-ba83-1b2eff8a10c4 0xc0033673c7 0xc0033673c8}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af5a43cd-9b78-4982-ba83-1b2eff8a10c4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003367468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun  9 11:26:22.653: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jun  9 11:26:22.653: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-6359  e88f6fd8-c52c-445d-8620-22b874bda7fd 102955 3 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment af5a43cd-9b78-4982-ba83-1b2eff8a10c4 0xc0033672c7 0xc0033672c8}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af5a43cd-9b78-4982-ba83-1b2eff8a10c4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003367368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jun  9 11:26:22.717: INFO: Pod "webserver-deployment-7f5969cbc7-2stft" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2stft webserver-deployment-7f5969cbc7- deployment-6359  539bb424-0fad-4f8d-9ce0-c900370f7aa5 102985 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc003367927 0xc003367928}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4g2zz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4g2zz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-09 11:26:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.718: INFO: Pod "webserver-deployment-7f5969cbc7-6r922" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6r922 webserver-deployment-7f5969cbc7- deployment-6359  a8d4a833-64dc-43be-81f7-496549992146 102808 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f2f617d4462ca22264ef18625855b42f3bb3294b299a9af95189edf898db43a5 cni.projectcalico.org/podIP:172.27.53.90/32 cni.projectcalico.org/podIPs:172.27.53.90/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc003367b27 0xc003367b28}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.27.53.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mgr25,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mgr25,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.27.53.90,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://587850ef382e8a9865b70f9a732d9092901714f1a47a2edca1ab6215fd88c252,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.27.53.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.718: INFO: Pod "webserver-deployment-7f5969cbc7-6vgz6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6vgz6 webserver-deployment-7f5969cbc7- deployment-6359  1ae76f76-cd16-408a-8c63-8906e84bdbe3 102990 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc003367f77 0xc003367f78}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fv7gr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fv7gr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.719: INFO: Pod "webserver-deployment-7f5969cbc7-86zrn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-86zrn webserver-deployment-7f5969cbc7- deployment-6359  5c74c132-b4f7-4f9d-8d76-9f8317d51b0a 102991 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0009c8b20 0xc0009c8b21}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-svjdv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-svjdv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.719: INFO: Pod "webserver-deployment-7f5969cbc7-bff6x" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bff6x webserver-deployment-7f5969cbc7- deployment-6359  3a361a60-ddc7-4341-a11f-01c6255f68a4 102978 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0009c9390 0xc0009c9391}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nqqbm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nqqbm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.720: INFO: Pod "webserver-deployment-7f5969cbc7-cbzdf" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cbzdf webserver-deployment-7f5969cbc7- deployment-6359  13947265-6f4f-4bc7-abf4-2fbbba927dba 102821 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8b0fe5ca4f9bc718ed5571ce6a5a73848bc9f5ce7f4bc130fc8346a67689fd5c cni.projectcalico.org/podIP:172.26.90.56/32 cni.projectcalico.org/podIPs:172.26.90.56/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0009c9500 0xc0009c9501}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.90.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qfqfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qfqfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.26.90.56,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a9e943c4d4e1f72ac32468db74a7bdfda89a7cb6827f1fb7f0bdf61dbb0714d6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.90.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.720: INFO: Pod "webserver-deployment-7f5969cbc7-cgkvl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cgkvl webserver-deployment-7f5969cbc7- deployment-6359  fd84428c-c81b-4715-b16e-120d5cd938bb 102966 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0009c9707 0xc0009c9708}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94g6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94g6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.720: INFO: Pod "webserver-deployment-7f5969cbc7-d9fpq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d9fpq webserver-deployment-7f5969cbc7- deployment-6359  4e65767f-c1da-476c-8b37-b2eb9484047b 102984 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0009c9860 0xc0009c9861}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hjz9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hjz9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.720: INFO: Pod "webserver-deployment-7f5969cbc7-fdcrj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fdcrj webserver-deployment-7f5969cbc7- deployment-6359  d8a5e704-0d26-4a96-80af-47cf0c7bb7bc 102986 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0009c99a0 0xc0009c99a1}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rx25z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rx25z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.720: INFO: Pod "webserver-deployment-7f5969cbc7-hgp9q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hgp9q webserver-deployment-7f5969cbc7- deployment-6359  f13d6914-3deb-461b-891b-ee1ccee5e6a5 102980 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0058082c0 0xc0058082c1}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lnqvq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lnqvq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.720: INFO: Pod "webserver-deployment-7f5969cbc7-hzkcj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hzkcj webserver-deployment-7f5969cbc7- deployment-6359  13c15c61-113e-4687-94b4-61f6882cafd9 102992 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005808400 0xc005808401}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-prhkm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-prhkm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-09 11:26:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.720: INFO: Pod "webserver-deployment-7f5969cbc7-j88f9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-j88f9 webserver-deployment-7f5969cbc7- deployment-6359  e3b30cdd-6888-4695-99e8-c7c31123d91b 102997 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc0058085b7 0xc0058085b8}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnw96,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnw96,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.721: INFO: Pod "webserver-deployment-7f5969cbc7-jhzbl" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jhzbl webserver-deployment-7f5969cbc7- deployment-6359  98c72a48-f2ee-480e-ad7f-1d6ccd2f5d83 102828 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1cb3d2558c5ee05a92b9de967ab3801f1069e987ce94a7a411a5ed9d6461b4f4 cni.projectcalico.org/podIP:172.30.17.184/32 cni.projectcalico.org/podIPs:172.30.17.184/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005808730 0xc005808731}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.17.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zz6q9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zz6q9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.17.184,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c7e62bb37a83b4582218cfd0a6a9adceff120dac57bf540b7fffd791cd2fc51b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.17.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.721: INFO: Pod "webserver-deployment-7f5969cbc7-kxtj4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kxtj4 webserver-deployment-7f5969cbc7- deployment-6359  b691eb66-9eeb-42f9-9ece-5bf797ba2e2e 102995 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005808937 0xc005808938}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x9p6r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x9p6r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.721: INFO: Pod "webserver-deployment-7f5969cbc7-lf6g5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lf6g5 webserver-deployment-7f5969cbc7- deployment-6359  e9034244-6d9a-4f3c-a20e-98e9695e07ca 102983 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005808a90 0xc005808a91}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jc4gr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jc4gr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.721: INFO: Pod "webserver-deployment-7f5969cbc7-nmqc4" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nmqc4 webserver-deployment-7f5969cbc7- deployment-6359  97729e95-dc69-46f1-81fd-efd462f454f2 102786 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8a2f43d10da55fe9c6c2175f4d6785decdee681de686359d5ab6fc2200fcbd12 cni.projectcalico.org/podIP:172.26.90.55/32 cni.projectcalico.org/podIPs:172.26.90.55/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005808bf0 0xc005808bf1}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.90.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hhp5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hhp5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.26.90.55,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://43bce70471d3197df7086f9a76bd9dbca1d93d72d9af630d16ead781ca0296f9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.90.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.722: INFO: Pod "webserver-deployment-7f5969cbc7-stp76" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-stp76 webserver-deployment-7f5969cbc7- deployment-6359  61052726-1712-4efc-a574-87de0bb6c62a 102840 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9af9bc73846105f72cf63804d6cb6242c9ac371e3ff3490a68afc03ca9dcfa50 cni.projectcalico.org/podIP:172.27.53.121/32 cni.projectcalico.org/podIPs:172.27.53.121/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005808e07 0xc005808e08}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.27.53.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4p4vj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4p4vj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.27.53.121,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2561eae39473ff09cd2b1426b0e8dd1f19dfd95f4b98ba5e8d6bc4455073fc96,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.27.53.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.722: INFO: Pod "webserver-deployment-7f5969cbc7-ttp9m" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ttp9m webserver-deployment-7f5969cbc7- deployment-6359  91a059b6-547e-43e3-b6a5-726d4fe95f7e 102826 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b33c56013d5fa2537aed9e20438ef2e763d0693930cd018971378744d16ad117 cni.projectcalico.org/podIP:172.30.17.185/32 cni.projectcalico.org/podIPs:172.30.17.185/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005809027 0xc005809028}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.17.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fh4k4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fh4k4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.17.185,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://13bf05e3f9f2bf48111c3129c94a7e089e14b78f191afff8cd3fb6b17ca9c66d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.17.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.722: INFO: Pod "webserver-deployment-7f5969cbc7-w2qkh" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-w2qkh webserver-deployment-7f5969cbc7- deployment-6359  ce76df2f-5332-4ffd-940d-1fab0f8ccd42 102784 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:fe8c70d7e51f42c90996c6efc5f451bea644f0eaa230d229f07eba68d22ee280 cni.projectcalico.org/podIP:172.26.90.58/32 cni.projectcalico.org/podIPs:172.26.90.58/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005809247 0xc005809248}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.90.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p89z7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p89z7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.26.90.58,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e236f0aa9b720b3846291bba9cd8957751e89a4793a9a2ffb003b9170f957778,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.90.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.722: INFO: Pod "webserver-deployment-7f5969cbc7-xshsh" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xshsh webserver-deployment-7f5969cbc7- deployment-6359  45794c0f-d035-4d1b-a225-99d6aa7fd275 102793 0 2023-06-09 11:26:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a28481bdba923bd0260741f98edfedfb8e85567dd5cbedf0bea91aec5d44ee40 cni.projectcalico.org/podIP:172.30.17.183/32 cni.projectcalico.org/podIPs:172.30.17.183/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 e88f6fd8-c52c-445d-8620-22b874bda7fd 0xc005809467 0xc005809468}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e88f6fd8-c52c-445d-8620-22b874bda7fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:26:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.17.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4swlq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4swlq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.17.183,StartTime:2023-06-09 11:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:26:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://28d81acb7fc6f138e3c69826f867cf2f228cc8a1196fba93cc1cbfa4fc9c9ba5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.17.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.725: INFO: Pod "webserver-deployment-d9f79cb5-28j8n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-28j8n webserver-deployment-d9f79cb5- deployment-6359  b34e1e4a-1bd3-4096-aef5-c4e5adcf7e3e 102989 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc005809677 0xc005809678}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5t8zd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5t8zd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.725: INFO: Pod "webserver-deployment-d9f79cb5-5vcwj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5vcwj webserver-deployment-d9f79cb5- deployment-6359  c2337e6d-96c9-45ec-8d74-51398dca94e6 102999 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0058097bf 0xc0058097d0}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vgpv2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vgpv2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.725: INFO: Pod "webserver-deployment-d9f79cb5-6bf2d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6bf2d webserver-deployment-d9f79cb5- deployment-6359  6d68e27c-947d-47de-896c-30eea6d3f93a 102988 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc00580991f 0xc005809930}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wcj8r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wcj8r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-6w98q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6w98q webserver-deployment-d9f79cb5- deployment-6359  e4532d68-f62e-4f3a-a5fd-665dfa140607 102915 0 2023-06-09 11:26:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:cdf9bbab37e6ef1617ee97c2d3fb90b49f9d71df741f72affcf4293122d4926c cni.projectcalico.org/podIP:172.27.53.102/32 cni.projectcalico.org/podIPs:172.27.53.102/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc005809a6f 0xc005809aa0}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-09 11:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfn7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfn7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-09 11:26:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-82lqn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-82lqn webserver-deployment-d9f79cb5- deployment-6359  b82557db-9fa5-46ff-bba0-12aa7f672dd1 102907 0 2023-06-09 11:26:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9d6cd02ba9fc373a358ee1b0a4051240ab33756f144116eeefeb3db0e49c390e cni.projectcalico.org/podIP:172.27.53.106/32 cni.projectcalico.org/podIPs:172.27.53.106/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc005809cb7 0xc005809cb8}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-09 11:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ldnbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ldnbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-09 11:26:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-d5rkm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d5rkm webserver-deployment-d9f79cb5- deployment-6359  837b7134-d099-4896-bde3-fc3540a52d90 102904 0 2023-06-09 11:26:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:0b58755478e518b184e708490668b1d5b5b1d5951b2e6d2c662a42c4b4429c5c cni.projectcalico.org/podIP:172.30.17.187/32 cni.projectcalico.org/podIPs:172.30.17.187/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc005809ed7 0xc005809ed8}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-09 11:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pwl8g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pwl8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-09 11:26:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-d6gkk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d6gkk webserver-deployment-d9f79cb5- deployment-6359  1ab07f35-6f7d-4ac4-99d2-66c3b79a03cb 102994 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0029bc0d7 0xc0029bc0d8}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-krx4s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krx4s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-fkq2m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fkq2m webserver-deployment-d9f79cb5- deployment-6359  1ef297f9-a3b3-4c9b-ad6c-f3536650601b 102993 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0029bc22f 0xc0029bc240}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j9gxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j9gxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-k2p5q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-k2p5q webserver-deployment-d9f79cb5- deployment-6359  b3941a84-0aa9-40ae-851c-154dde24ca52 102914 0 2023-06-09 11:26:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3291f95b7ec8e75ce8e725454dc68db69af3491441a89bd38e59f6ff9282f7fa cni.projectcalico.org/podIP:172.30.17.186/32 cni.projectcalico.org/podIPs:172.30.17.186/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0029bc38f 0xc0029bc3c0}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-09 11:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-654wq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-654wq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-q5bjm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-09 11:26:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-r25n5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-r25n5 webserver-deployment-d9f79cb5- deployment-6359  0d6bc1a9-93bc-4c22-9109-c78ac8359c91 102996 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0029bc667 0xc0029bc668}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ww4qm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ww4qm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.726: INFO: Pod "webserver-deployment-d9f79cb5-rt9fz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rt9fz webserver-deployment-d9f79cb5- deployment-6359  78125897-0906-4bfe-a849-0efc9f9eb3ca 102987 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0029bc7af 0xc0029bc7c0}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kf8vp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kf8vp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.731: INFO: Pod "webserver-deployment-d9f79cb5-tm4bl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tm4bl webserver-deployment-d9f79cb5- deployment-6359  b2ce26c1-7fdd-42ed-abd9-a5cf03cd3e85 102908 0 2023-06-09 11:26:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:984ab275f7f7c822e8c2df159910d5194b6025d94be060d4fcf0ee10072ef363 cni.projectcalico.org/podIP:172.26.90.49/32 cni.projectcalico.org/podIPs:172.26.90.49/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0029bc8ff 0xc0029bc930}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-09 11:26:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-09 11:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmkbt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmkbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-09 11:26:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun  9 11:26:22.731: INFO: Pod "webserver-deployment-d9f79cb5-xj7j9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xj7j9 webserver-deployment-d9f79cb5- deployment-6359  40764766-4b52-4f75-bdfb-402d0c508fed 102968 0 2023-06-09 11:26:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 e2d1105f-758f-4773-881d-b4c4cc859589 0xc0029bcb27 0xc0029bcb28}] [] [{kube-controller-manager Update v1 2023-06-09 11:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2d1105f-758f-4773-881d-b4c4cc859589\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-spdgz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-spdgz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:26:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:26:22.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6359" for this suite. 06/09/23 11:26:22.772
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:26:22.797
Jun  9 11:26:22.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename hostport 06/09/23 11:26:22.799
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:26:22.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:26:22.835
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 06/09/23 11:26:22.869
Jun  9 11:26:22.882: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1279" to be "running and ready"
Jun  9 11:26:22.893: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.372355ms
Jun  9 11:26:22.893: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:26:24.900: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018577496s
Jun  9 11:26:24.900: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:26:26.912: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030147626s
Jun  9 11:26:26.912: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:26:28.906: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 6.023941263s
Jun  9 11:26:28.906: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun  9 11:26:28.906: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.255.64.102 on the node which pod1 resides and expect scheduled 06/09/23 11:26:28.906
Jun  9 11:26:28.928: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1279" to be "running and ready"
Jun  9 11:26:28.936: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.305759ms
Jun  9 11:26:28.936: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:26:30.943: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015194839s
Jun  9 11:26:30.943: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:26:32.943: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.014635931s
Jun  9 11:26:32.943: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun  9 11:26:32.943: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.255.64.102 but use UDP protocol on the node which pod2 resides 06/09/23 11:26:32.943
Jun  9 11:26:32.951: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1279" to be "running and ready"
Jun  9 11:26:32.955: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069412ms
Jun  9 11:26:32.955: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:26:34.962: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011418454s
Jun  9 11:26:34.963: INFO: The phase of Pod pod3 is Running (Ready = true)
Jun  9 11:26:34.963: INFO: Pod "pod3" satisfied condition "running and ready"
Jun  9 11:26:34.971: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1279" to be "running and ready"
Jun  9 11:26:34.976: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.596352ms
Jun  9 11:26:34.976: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:26:36.983: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.012285311s
Jun  9 11:26:36.983: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jun  9 11:26:36.983: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 06/09/23 11:26:36.988
Jun  9 11:26:36.988: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.255.64.102 http://127.0.0.1:54323/hostname] Namespace:hostport-1279 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 11:26:36.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 11:26:36.989: INFO: ExecWithOptions: Clientset creation
Jun  9 11:26:36.989: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1279/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.255.64.102+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.255.64.102, port: 54323 06/09/23 11:26:37.102
Jun  9 11:26:37.103: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.255.64.102:54323/hostname] Namespace:hostport-1279 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 11:26:37.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 11:26:37.103: INFO: ExecWithOptions: Clientset creation
Jun  9 11:26:37.104: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1279/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.255.64.102%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.255.64.102, port: 54323 UDP 06/09/23 11:26:37.219
Jun  9 11:26:37.220: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.255.64.102 54323] Namespace:hostport-1279 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 11:26:37.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 11:26:37.221: INFO: ExecWithOptions: Clientset creation
Jun  9 11:26:37.221: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1279/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.255.64.102+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jun  9 11:26:42.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-1279" for this suite. 06/09/23 11:26:42.329
------------------------------
• [SLOW TEST] [19.539 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:26:22.797
    Jun  9 11:26:22.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename hostport 06/09/23 11:26:22.799
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:26:22.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:26:22.835
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 06/09/23 11:26:22.869
    Jun  9 11:26:22.882: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1279" to be "running and ready"
    Jun  9 11:26:22.893: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.372355ms
    Jun  9 11:26:22.893: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:26:24.900: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018577496s
    Jun  9 11:26:24.900: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:26:26.912: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030147626s
    Jun  9 11:26:26.912: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:26:28.906: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 6.023941263s
    Jun  9 11:26:28.906: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun  9 11:26:28.906: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.255.64.102 on the node which pod1 resides and expect scheduled 06/09/23 11:26:28.906
    Jun  9 11:26:28.928: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1279" to be "running and ready"
    Jun  9 11:26:28.936: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.305759ms
    Jun  9 11:26:28.936: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:26:30.943: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015194839s
    Jun  9 11:26:30.943: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:26:32.943: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.014635931s
    Jun  9 11:26:32.943: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun  9 11:26:32.943: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.255.64.102 but use UDP protocol on the node which pod2 resides 06/09/23 11:26:32.943
    Jun  9 11:26:32.951: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1279" to be "running and ready"
    Jun  9 11:26:32.955: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069412ms
    Jun  9 11:26:32.955: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:26:34.962: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011418454s
    Jun  9 11:26:34.963: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jun  9 11:26:34.963: INFO: Pod "pod3" satisfied condition "running and ready"
    Jun  9 11:26:34.971: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1279" to be "running and ready"
    Jun  9 11:26:34.976: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.596352ms
    Jun  9 11:26:34.976: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:26:36.983: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.012285311s
    Jun  9 11:26:36.983: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jun  9 11:26:36.983: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 06/09/23 11:26:36.988
    Jun  9 11:26:36.988: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.255.64.102 http://127.0.0.1:54323/hostname] Namespace:hostport-1279 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 11:26:36.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 11:26:36.989: INFO: ExecWithOptions: Clientset creation
    Jun  9 11:26:36.989: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1279/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.255.64.102+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.255.64.102, port: 54323 06/09/23 11:26:37.102
    Jun  9 11:26:37.103: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.255.64.102:54323/hostname] Namespace:hostport-1279 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 11:26:37.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 11:26:37.103: INFO: ExecWithOptions: Clientset creation
    Jun  9 11:26:37.104: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1279/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.255.64.102%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.255.64.102, port: 54323 UDP 06/09/23 11:26:37.219
    Jun  9 11:26:37.220: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.255.64.102 54323] Namespace:hostport-1279 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 11:26:37.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 11:26:37.221: INFO: ExecWithOptions: Clientset creation
    Jun  9 11:26:37.221: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1279/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.255.64.102+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:26:42.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-1279" for this suite. 06/09/23 11:26:42.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:26:42.339
Jun  9 11:26:42.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 11:26:42.34
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:26:42.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:26:42.362
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-bb80039f-b91e-479c-b674-c40fcb5766ac 06/09/23 11:26:42.369
STEP: Creating a pod to test consume configMaps 06/09/23 11:26:42.377
Jun  9 11:26:42.393: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5" in namespace "projected-2212" to be "Succeeded or Failed"
Jun  9 11:26:42.398: INFO: Pod "pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.388425ms
Jun  9 11:26:44.405: INFO: Pod "pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011413417s
Jun  9 11:26:46.405: INFO: Pod "pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011850743s
STEP: Saw pod success 06/09/23 11:26:46.405
Jun  9 11:26:46.406: INFO: Pod "pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5" satisfied condition "Succeeded or Failed"
Jun  9 11:26:46.421: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5 container agnhost-container: <nil>
STEP: delete the pod 06/09/23 11:26:46.478
Jun  9 11:26:46.494: INFO: Waiting for pod pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5 to disappear
Jun  9 11:26:46.499: INFO: Pod pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  9 11:26:46.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2212" for this suite. 06/09/23 11:26:46.514
------------------------------
• [4.187 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:26:42.339
    Jun  9 11:26:42.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 11:26:42.34
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:26:42.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:26:42.362
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-bb80039f-b91e-479c-b674-c40fcb5766ac 06/09/23 11:26:42.369
    STEP: Creating a pod to test consume configMaps 06/09/23 11:26:42.377
    Jun  9 11:26:42.393: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5" in namespace "projected-2212" to be "Succeeded or Failed"
    Jun  9 11:26:42.398: INFO: Pod "pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.388425ms
    Jun  9 11:26:44.405: INFO: Pod "pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011413417s
    Jun  9 11:26:46.405: INFO: Pod "pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011850743s
    STEP: Saw pod success 06/09/23 11:26:46.405
    Jun  9 11:26:46.406: INFO: Pod "pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5" satisfied condition "Succeeded or Failed"
    Jun  9 11:26:46.421: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5 container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 11:26:46.478
    Jun  9 11:26:46.494: INFO: Waiting for pod pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5 to disappear
    Jun  9 11:26:46.499: INFO: Pod pod-projected-configmaps-0e63458f-e08d-466b-b295-5366b7d5c0f5 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:26:46.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2212" for this suite. 06/09/23 11:26:46.514
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:26:46.526
Jun  9 11:26:46.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename resourcequota 06/09/23 11:26:46.528
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:26:46.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:26:46.555
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 06/09/23 11:26:46.566
STEP: Ensuring ResourceQuota status is calculated 06/09/23 11:26:46.574
STEP: Creating a ResourceQuota with not terminating scope 06/09/23 11:26:48.581
STEP: Ensuring ResourceQuota status is calculated 06/09/23 11:26:48.607
STEP: Creating a long running pod 06/09/23 11:26:50.613
STEP: Ensuring resource quota with not terminating scope captures the pod usage 06/09/23 11:26:50.634
STEP: Ensuring resource quota with terminating scope ignored the pod usage 06/09/23 11:26:52.645
STEP: Deleting the pod 06/09/23 11:26:54.651
STEP: Ensuring resource quota status released the pod usage 06/09/23 11:26:54.729
STEP: Creating a terminating pod 06/09/23 11:26:56.737
STEP: Ensuring resource quota with terminating scope captures the pod usage 06/09/23 11:26:56.802
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 06/09/23 11:26:58.808
STEP: Deleting the pod 06/09/23 11:27:00.817
STEP: Ensuring resource quota status released the pod usage 06/09/23 11:27:00.838
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  9 11:27:02.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2029" for this suite. 06/09/23 11:27:02.859
------------------------------
• [SLOW TEST] [16.349 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:26:46.526
    Jun  9 11:26:46.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename resourcequota 06/09/23 11:26:46.528
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:26:46.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:26:46.555
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 06/09/23 11:26:46.566
    STEP: Ensuring ResourceQuota status is calculated 06/09/23 11:26:46.574
    STEP: Creating a ResourceQuota with not terminating scope 06/09/23 11:26:48.581
    STEP: Ensuring ResourceQuota status is calculated 06/09/23 11:26:48.607
    STEP: Creating a long running pod 06/09/23 11:26:50.613
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 06/09/23 11:26:50.634
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 06/09/23 11:26:52.645
    STEP: Deleting the pod 06/09/23 11:26:54.651
    STEP: Ensuring resource quota status released the pod usage 06/09/23 11:26:54.729
    STEP: Creating a terminating pod 06/09/23 11:26:56.737
    STEP: Ensuring resource quota with terminating scope captures the pod usage 06/09/23 11:26:56.802
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 06/09/23 11:26:58.808
    STEP: Deleting the pod 06/09/23 11:27:00.817
    STEP: Ensuring resource quota status released the pod usage 06/09/23 11:27:00.838
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:27:02.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2029" for this suite. 06/09/23 11:27:02.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:27:02.876
Jun  9 11:27:02.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 11:27:02.879
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:27:02.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:27:02.911
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 06/09/23 11:27:02.919
Jun  9 11:27:02.932: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495" in namespace "downward-api-8648" to be "Succeeded or Failed"
Jun  9 11:27:02.938: INFO: Pod "downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495": Phase="Pending", Reason="", readiness=false. Elapsed: 6.250331ms
Jun  9 11:27:04.949: INFO: Pod "downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017335992s
Jun  9 11:27:06.946: INFO: Pod "downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014482048s
STEP: Saw pod success 06/09/23 11:27:06.946
Jun  9 11:27:06.947: INFO: Pod "downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495" satisfied condition "Succeeded or Failed"
Jun  9 11:27:06.955: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495 container client-container: <nil>
STEP: delete the pod 06/09/23 11:27:06.969
Jun  9 11:27:06.990: INFO: Waiting for pod downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495 to disappear
Jun  9 11:27:06.996: INFO: Pod downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  9 11:27:06.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8648" for this suite. 06/09/23 11:27:07.004
------------------------------
• [4.139 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:27:02.876
    Jun  9 11:27:02.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 11:27:02.879
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:27:02.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:27:02.911
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 06/09/23 11:27:02.919
    Jun  9 11:27:02.932: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495" in namespace "downward-api-8648" to be "Succeeded or Failed"
    Jun  9 11:27:02.938: INFO: Pod "downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495": Phase="Pending", Reason="", readiness=false. Elapsed: 6.250331ms
    Jun  9 11:27:04.949: INFO: Pod "downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017335992s
    Jun  9 11:27:06.946: INFO: Pod "downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014482048s
    STEP: Saw pod success 06/09/23 11:27:06.946
    Jun  9 11:27:06.947: INFO: Pod "downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495" satisfied condition "Succeeded or Failed"
    Jun  9 11:27:06.955: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495 container client-container: <nil>
    STEP: delete the pod 06/09/23 11:27:06.969
    Jun  9 11:27:06.990: INFO: Waiting for pod downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495 to disappear
    Jun  9 11:27:06.996: INFO: Pod downwardapi-volume-aacecff7-26ad-4187-879c-3866ca77a495 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:27:06.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8648" for this suite. 06/09/23 11:27:07.004
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:27:07.015
Jun  9 11:27:07.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename dns 06/09/23 11:27:07.018
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:27:07.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:27:07.045
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 06/09/23 11:27:07.055
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local;sleep 1; done
 06/09/23 11:27:07.069
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local;sleep 1; done
 06/09/23 11:27:07.069
STEP: creating a pod to probe DNS 06/09/23 11:27:07.069
STEP: submitting the pod to kubernetes 06/09/23 11:27:07.069
Jun  9 11:27:07.086: INFO: Waiting up to 15m0s for pod "dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51" in namespace "dns-5893" to be "running"
Jun  9 11:27:07.093: INFO: Pod "dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51": Phase="Pending", Reason="", readiness=false. Elapsed: 6.323031ms
Jun  9 11:27:09.155: INFO: Pod "dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51": Phase="Running", Reason="", readiness=true. Elapsed: 2.068871511s
Jun  9 11:27:09.155: INFO: Pod "dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51" satisfied condition "running"
STEP: retrieving the pod 06/09/23 11:27:09.155
STEP: looking for the results for each expected name from probers 06/09/23 11:27:09.169
Jun  9 11:27:09.179: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:09.188: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:09.201: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:09.215: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:09.223: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:09.234: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:09.242: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:09.249: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:09.249: INFO: Lookups using dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local]

Jun  9 11:27:14.313: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:14.338: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:14.409: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:14.437: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:14.437: INFO: Lookups using dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51 failed for: [wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local]

Jun  9 11:27:19.275: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:19.282: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:19.301: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:19.310: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:19.310: INFO: Lookups using dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51 failed for: [wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local]

Jun  9 11:27:24.289: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:24.298: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:24.333: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:24.341: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:24.341: INFO: Lookups using dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51 failed for: [wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local]

Jun  9 11:27:29.374: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:29.382: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:29.422: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:29.443: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:29.443: INFO: Lookups using dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51 failed for: [wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local]

Jun  9 11:27:34.367: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:34.404: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:34.517: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:34.541: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
Jun  9 11:27:34.541: INFO: Lookups using dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51 failed for: [wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local]

Jun  9 11:27:39.307: INFO: DNS probes using dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51 succeeded

STEP: deleting the pod 06/09/23 11:27:39.307
STEP: deleting the test headless service 06/09/23 11:27:39.328
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  9 11:27:39.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5893" for this suite. 06/09/23 11:27:39.37
------------------------------
• [SLOW TEST] [32.369 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:27:07.015
    Jun  9 11:27:07.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename dns 06/09/23 11:27:07.018
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:27:07.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:27:07.045
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 06/09/23 11:27:07.055
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local;sleep 1; done
     06/09/23 11:27:07.069
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5893.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local;sleep 1; done
     06/09/23 11:27:07.069
    STEP: creating a pod to probe DNS 06/09/23 11:27:07.069
    STEP: submitting the pod to kubernetes 06/09/23 11:27:07.069
    Jun  9 11:27:07.086: INFO: Waiting up to 15m0s for pod "dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51" in namespace "dns-5893" to be "running"
    Jun  9 11:27:07.093: INFO: Pod "dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51": Phase="Pending", Reason="", readiness=false. Elapsed: 6.323031ms
    Jun  9 11:27:09.155: INFO: Pod "dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51": Phase="Running", Reason="", readiness=true. Elapsed: 2.068871511s
    Jun  9 11:27:09.155: INFO: Pod "dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51" satisfied condition "running"
    STEP: retrieving the pod 06/09/23 11:27:09.155
    STEP: looking for the results for each expected name from probers 06/09/23 11:27:09.169
    Jun  9 11:27:09.179: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:09.188: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:09.201: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:09.215: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:09.223: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:09.234: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:09.242: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:09.249: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:09.249: INFO: Lookups using dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5893.svc.cluster.local jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local]

    Jun  9 11:27:14.313: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:14.338: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:14.409: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:14.437: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:14.437: INFO: Lookups using dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51 failed for: [wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local]

    Jun  9 11:27:19.275: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:19.282: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:19.301: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:19.310: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:19.310: INFO: Lookups using dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51 failed for: [wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local]

    Jun  9 11:27:24.289: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:24.298: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:24.333: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:24.341: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:24.341: INFO: Lookups using dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51 failed for: [wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local]

    Jun  9 11:27:29.374: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:29.382: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:29.422: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:29.443: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:29.443: INFO: Lookups using dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51 failed for: [wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local]

    Jun  9 11:27:34.367: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:34.404: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:34.517: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:34.541: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local from pod dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51: the server could not find the requested resource (get pods dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51)
    Jun  9 11:27:34.541: INFO: Lookups using dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51 failed for: [wheezy_udp@dns-test-service-2.dns-5893.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5893.svc.cluster.local jessie_udp@dns-test-service-2.dns-5893.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5893.svc.cluster.local]

    Jun  9 11:27:39.307: INFO: DNS probes using dns-5893/dns-test-1dbcd749-1b5f-47b2-9c8e-2cf698706b51 succeeded

    STEP: deleting the pod 06/09/23 11:27:39.307
    STEP: deleting the test headless service 06/09/23 11:27:39.328
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:27:39.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5893" for this suite. 06/09/23 11:27:39.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:27:39.387
Jun  9 11:27:39.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 11:27:39.389
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:27:39.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:27:39.446
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 06/09/23 11:27:39.451
Jun  9 11:27:39.476: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4" in namespace "downward-api-6826" to be "Succeeded or Failed"
Jun  9 11:27:39.496: INFO: Pod "downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.220116ms
Jun  9 11:27:41.503: INFO: Pod "downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026671129s
Jun  9 11:27:43.502: INFO: Pod "downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026252388s
STEP: Saw pod success 06/09/23 11:27:43.503
Jun  9 11:27:43.503: INFO: Pod "downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4" satisfied condition "Succeeded or Failed"
Jun  9 11:27:43.508: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4 container client-container: <nil>
STEP: delete the pod 06/09/23 11:27:43.517
Jun  9 11:27:43.534: INFO: Waiting for pod downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4 to disappear
Jun  9 11:27:43.539: INFO: Pod downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  9 11:27:43.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6826" for this suite. 06/09/23 11:27:43.547
------------------------------
• [4.174 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:27:39.387
    Jun  9 11:27:39.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 11:27:39.389
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:27:39.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:27:39.446
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 06/09/23 11:27:39.451
    Jun  9 11:27:39.476: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4" in namespace "downward-api-6826" to be "Succeeded or Failed"
    Jun  9 11:27:39.496: INFO: Pod "downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.220116ms
    Jun  9 11:27:41.503: INFO: Pod "downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026671129s
    Jun  9 11:27:43.502: INFO: Pod "downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026252388s
    STEP: Saw pod success 06/09/23 11:27:43.503
    Jun  9 11:27:43.503: INFO: Pod "downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4" satisfied condition "Succeeded or Failed"
    Jun  9 11:27:43.508: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4 container client-container: <nil>
    STEP: delete the pod 06/09/23 11:27:43.517
    Jun  9 11:27:43.534: INFO: Waiting for pod downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4 to disappear
    Jun  9 11:27:43.539: INFO: Pod downwardapi-volume-4c4017be-fc87-41e4-a381-d0be7e0633d4 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:27:43.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6826" for this suite. 06/09/23 11:27:43.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:27:43.562
Jun  9 11:27:43.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 11:27:43.563
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:27:43.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:27:43.588
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 06/09/23 11:27:43.595
Jun  9 11:27:43.611: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a" in namespace "downward-api-3922" to be "Succeeded or Failed"
Jun  9 11:27:43.636: INFO: Pod "downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a": Phase="Pending", Reason="", readiness=false. Elapsed: 25.074723ms
Jun  9 11:27:45.643: INFO: Pod "downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031835943s
Jun  9 11:27:47.644: INFO: Pod "downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033441054s
STEP: Saw pod success 06/09/23 11:27:47.644
Jun  9 11:27:47.645: INFO: Pod "downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a" satisfied condition "Succeeded or Failed"
Jun  9 11:27:47.655: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a container client-container: <nil>
STEP: delete the pod 06/09/23 11:27:47.67
Jun  9 11:27:47.692: INFO: Waiting for pod downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a to disappear
Jun  9 11:27:47.698: INFO: Pod downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun  9 11:27:47.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3922" for this suite. 06/09/23 11:27:47.713
------------------------------
• [4.168 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:27:43.562
    Jun  9 11:27:43.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 11:27:43.563
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:27:43.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:27:43.588
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 06/09/23 11:27:43.595
    Jun  9 11:27:43.611: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a" in namespace "downward-api-3922" to be "Succeeded or Failed"
    Jun  9 11:27:43.636: INFO: Pod "downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a": Phase="Pending", Reason="", readiness=false. Elapsed: 25.074723ms
    Jun  9 11:27:45.643: INFO: Pod "downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031835943s
    Jun  9 11:27:47.644: INFO: Pod "downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033441054s
    STEP: Saw pod success 06/09/23 11:27:47.644
    Jun  9 11:27:47.645: INFO: Pod "downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a" satisfied condition "Succeeded or Failed"
    Jun  9 11:27:47.655: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a container client-container: <nil>
    STEP: delete the pod 06/09/23 11:27:47.67
    Jun  9 11:27:47.692: INFO: Waiting for pod downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a to disappear
    Jun  9 11:27:47.698: INFO: Pod downwardapi-volume-6ad4272e-60f5-4004-9621-8f8fa6e7b90a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:27:47.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3922" for this suite. 06/09/23 11:27:47.713
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:27:47.732
Jun  9 11:27:47.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename disruption 06/09/23 11:27:47.734
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:27:47.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:27:47.776
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:27:47.784
Jun  9 11:27:47.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename disruption-2 06/09/23 11:27:47.785
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:27:47.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:27:47.83
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 06/09/23 11:27:47.847
STEP: Waiting for the pdb to be processed 06/09/23 11:27:49.871
STEP: Waiting for the pdb to be processed 06/09/23 11:27:51.903
STEP: listing a collection of PDBs across all namespaces 06/09/23 11:27:53.917
STEP: listing a collection of PDBs in namespace disruption-137 06/09/23 11:27:53.926
STEP: deleting a collection of PDBs 06/09/23 11:27:53.934
STEP: Waiting for the PDB collection to be deleted 06/09/23 11:27:53.961
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jun  9 11:27:53.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun  9 11:27:53.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-5177" for this suite. 06/09/23 11:27:53.998
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-137" for this suite. 06/09/23 11:27:54.012
------------------------------
• [SLOW TEST] [6.294 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:27:47.732
    Jun  9 11:27:47.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename disruption 06/09/23 11:27:47.734
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:27:47.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:27:47.776
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:27:47.784
    Jun  9 11:27:47.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename disruption-2 06/09/23 11:27:47.785
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:27:47.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:27:47.83
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 06/09/23 11:27:47.847
    STEP: Waiting for the pdb to be processed 06/09/23 11:27:49.871
    STEP: Waiting for the pdb to be processed 06/09/23 11:27:51.903
    STEP: listing a collection of PDBs across all namespaces 06/09/23 11:27:53.917
    STEP: listing a collection of PDBs in namespace disruption-137 06/09/23 11:27:53.926
    STEP: deleting a collection of PDBs 06/09/23 11:27:53.934
    STEP: Waiting for the PDB collection to be deleted 06/09/23 11:27:53.961
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:27:53.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:27:53.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-5177" for this suite. 06/09/23 11:27:53.998
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-137" for this suite. 06/09/23 11:27:54.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:27:54.03
Jun  9 11:27:54.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename gc 06/09/23 11:27:54.031
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:27:54.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:27:54.064
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 06/09/23 11:27:54.079
STEP: create the rc2 06/09/23 11:27:54.145
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 06/09/23 11:27:59.194
STEP: delete the rc simpletest-rc-to-be-deleted 06/09/23 11:28:01.876
STEP: wait for the rc to be deleted 06/09/23 11:28:02.201
Jun  9 11:28:07.405: INFO: 70 pods remaining
Jun  9 11:28:07.405: INFO: 70 pods has nil DeletionTimestamp
Jun  9 11:28:07.405: INFO: 
STEP: Gathering metrics 06/09/23 11:28:12.235
Jun  9 11:28:12.296: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" in namespace "kube-system" to be "running and ready"
Jun  9 11:28:12.305: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh": Phase="Running", Reason="", readiness=true. Elapsed: 9.500288ms
Jun  9 11:28:12.305: INFO: The phase of Pod kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh is Running (Ready = true)
Jun  9 11:28:12.306: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" satisfied condition "running and ready"
Jun  9 11:28:12.430: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun  9 11:28:12.430: INFO: Deleting pod "simpletest-rc-to-be-deleted-22m7w" in namespace "gc-720"
Jun  9 11:28:12.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-27fh9" in namespace "gc-720"
Jun  9 11:28:12.488: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dlsw" in namespace "gc-720"
Jun  9 11:28:12.532: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gnhz" in namespace "gc-720"
Jun  9 11:28:12.564: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rrrb" in namespace "gc-720"
Jun  9 11:28:12.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-47sll" in namespace "gc-720"
Jun  9 11:28:12.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-4b8qt" in namespace "gc-720"
Jun  9 11:28:12.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-4krkw" in namespace "gc-720"
Jun  9 11:28:12.743: INFO: Deleting pod "simpletest-rc-to-be-deleted-525mz" in namespace "gc-720"
Jun  9 11:28:12.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cmnd" in namespace "gc-720"
Jun  9 11:28:12.845: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lx4j" in namespace "gc-720"
Jun  9 11:28:12.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tnt7" in namespace "gc-720"
Jun  9 11:28:12.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-62rbj" in namespace "gc-720"
Jun  9 11:28:13.075: INFO: Deleting pod "simpletest-rc-to-be-deleted-695ss" in namespace "gc-720"
Jun  9 11:28:13.154: INFO: Deleting pod "simpletest-rc-to-be-deleted-69gft" in namespace "gc-720"
Jun  9 11:28:13.265: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jjvm" in namespace "gc-720"
Jun  9 11:28:13.311: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kr72" in namespace "gc-720"
Jun  9 11:28:13.352: INFO: Deleting pod "simpletest-rc-to-be-deleted-6m4rt" in namespace "gc-720"
Jun  9 11:28:13.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pnvp" in namespace "gc-720"
Jun  9 11:28:13.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qh2w" in namespace "gc-720"
Jun  9 11:28:13.507: INFO: Deleting pod "simpletest-rc-to-be-deleted-6z58f" in namespace "gc-720"
Jun  9 11:28:13.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-7m654" in namespace "gc-720"
Jun  9 11:28:13.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qjc5" in namespace "gc-720"
Jun  9 11:28:13.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-87q7f" in namespace "gc-720"
Jun  9 11:28:13.780: INFO: Deleting pod "simpletest-rc-to-be-deleted-8m66n" in namespace "gc-720"
Jun  9 11:28:13.861: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t5v7" in namespace "gc-720"
Jun  9 11:28:13.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-8w5zl" in namespace "gc-720"
Jun  9 11:28:14.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gz48" in namespace "gc-720"
Jun  9 11:28:14.078: INFO: Deleting pod "simpletest-rc-to-be-deleted-b57lv" in namespace "gc-720"
Jun  9 11:28:14.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-bltz2" in namespace "gc-720"
Jun  9 11:28:14.328: INFO: Deleting pod "simpletest-rc-to-be-deleted-blx9r" in namespace "gc-720"
Jun  9 11:28:14.380: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2ptr" in namespace "gc-720"
Jun  9 11:28:14.423: INFO: Deleting pod "simpletest-rc-to-be-deleted-chlw5" in namespace "gc-720"
Jun  9 11:28:14.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwn4w" in namespace "gc-720"
Jun  9 11:28:14.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-czdbz" in namespace "gc-720"
Jun  9 11:28:14.803: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfbsq" in namespace "gc-720"
Jun  9 11:28:14.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmvqw" in namespace "gc-720"
Jun  9 11:28:14.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsf8s" in namespace "gc-720"
Jun  9 11:28:14.893: INFO: Deleting pod "simpletest-rc-to-be-deleted-fp2bn" in namespace "gc-720"
Jun  9 11:28:15.008: INFO: Deleting pod "simpletest-rc-to-be-deleted-ft62j" in namespace "gc-720"
Jun  9 11:28:15.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftvmh" in namespace "gc-720"
Jun  9 11:28:15.060: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8z4f" in namespace "gc-720"
Jun  9 11:28:15.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9578" in namespace "gc-720"
Jun  9 11:28:15.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-gms4b" in namespace "gc-720"
Jun  9 11:28:15.235: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzt4w" in namespace "gc-720"
Jun  9 11:28:15.273: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7fvv" in namespace "gc-720"
Jun  9 11:28:15.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhbxs" in namespace "gc-720"
Jun  9 11:28:15.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjvdx" in namespace "gc-720"
Jun  9 11:28:15.377: INFO: Deleting pod "simpletest-rc-to-be-deleted-hrt5q" in namespace "gc-720"
Jun  9 11:28:15.408: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvsxw" in namespace "gc-720"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun  9 11:28:15.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-720" for this suite. 06/09/23 11:28:15.592
------------------------------
• [SLOW TEST] [21.579 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:27:54.03
    Jun  9 11:27:54.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename gc 06/09/23 11:27:54.031
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:27:54.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:27:54.064
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 06/09/23 11:27:54.079
    STEP: create the rc2 06/09/23 11:27:54.145
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 06/09/23 11:27:59.194
    STEP: delete the rc simpletest-rc-to-be-deleted 06/09/23 11:28:01.876
    STEP: wait for the rc to be deleted 06/09/23 11:28:02.201
    Jun  9 11:28:07.405: INFO: 70 pods remaining
    Jun  9 11:28:07.405: INFO: 70 pods has nil DeletionTimestamp
    Jun  9 11:28:07.405: INFO: 
    STEP: Gathering metrics 06/09/23 11:28:12.235
    Jun  9 11:28:12.296: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" in namespace "kube-system" to be "running and ready"
    Jun  9 11:28:12.305: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh": Phase="Running", Reason="", readiness=true. Elapsed: 9.500288ms
    Jun  9 11:28:12.305: INFO: The phase of Pod kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh is Running (Ready = true)
    Jun  9 11:28:12.306: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" satisfied condition "running and ready"
    Jun  9 11:28:12.430: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jun  9 11:28:12.430: INFO: Deleting pod "simpletest-rc-to-be-deleted-22m7w" in namespace "gc-720"
    Jun  9 11:28:12.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-27fh9" in namespace "gc-720"
    Jun  9 11:28:12.488: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dlsw" in namespace "gc-720"
    Jun  9 11:28:12.532: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gnhz" in namespace "gc-720"
    Jun  9 11:28:12.564: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rrrb" in namespace "gc-720"
    Jun  9 11:28:12.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-47sll" in namespace "gc-720"
    Jun  9 11:28:12.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-4b8qt" in namespace "gc-720"
    Jun  9 11:28:12.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-4krkw" in namespace "gc-720"
    Jun  9 11:28:12.743: INFO: Deleting pod "simpletest-rc-to-be-deleted-525mz" in namespace "gc-720"
    Jun  9 11:28:12.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cmnd" in namespace "gc-720"
    Jun  9 11:28:12.845: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lx4j" in namespace "gc-720"
    Jun  9 11:28:12.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tnt7" in namespace "gc-720"
    Jun  9 11:28:12.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-62rbj" in namespace "gc-720"
    Jun  9 11:28:13.075: INFO: Deleting pod "simpletest-rc-to-be-deleted-695ss" in namespace "gc-720"
    Jun  9 11:28:13.154: INFO: Deleting pod "simpletest-rc-to-be-deleted-69gft" in namespace "gc-720"
    Jun  9 11:28:13.265: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jjvm" in namespace "gc-720"
    Jun  9 11:28:13.311: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kr72" in namespace "gc-720"
    Jun  9 11:28:13.352: INFO: Deleting pod "simpletest-rc-to-be-deleted-6m4rt" in namespace "gc-720"
    Jun  9 11:28:13.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pnvp" in namespace "gc-720"
    Jun  9 11:28:13.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qh2w" in namespace "gc-720"
    Jun  9 11:28:13.507: INFO: Deleting pod "simpletest-rc-to-be-deleted-6z58f" in namespace "gc-720"
    Jun  9 11:28:13.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-7m654" in namespace "gc-720"
    Jun  9 11:28:13.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qjc5" in namespace "gc-720"
    Jun  9 11:28:13.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-87q7f" in namespace "gc-720"
    Jun  9 11:28:13.780: INFO: Deleting pod "simpletest-rc-to-be-deleted-8m66n" in namespace "gc-720"
    Jun  9 11:28:13.861: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t5v7" in namespace "gc-720"
    Jun  9 11:28:13.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-8w5zl" in namespace "gc-720"
    Jun  9 11:28:14.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gz48" in namespace "gc-720"
    Jun  9 11:28:14.078: INFO: Deleting pod "simpletest-rc-to-be-deleted-b57lv" in namespace "gc-720"
    Jun  9 11:28:14.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-bltz2" in namespace "gc-720"
    Jun  9 11:28:14.328: INFO: Deleting pod "simpletest-rc-to-be-deleted-blx9r" in namespace "gc-720"
    Jun  9 11:28:14.380: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2ptr" in namespace "gc-720"
    Jun  9 11:28:14.423: INFO: Deleting pod "simpletest-rc-to-be-deleted-chlw5" in namespace "gc-720"
    Jun  9 11:28:14.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwn4w" in namespace "gc-720"
    Jun  9 11:28:14.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-czdbz" in namespace "gc-720"
    Jun  9 11:28:14.803: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfbsq" in namespace "gc-720"
    Jun  9 11:28:14.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmvqw" in namespace "gc-720"
    Jun  9 11:28:14.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsf8s" in namespace "gc-720"
    Jun  9 11:28:14.893: INFO: Deleting pod "simpletest-rc-to-be-deleted-fp2bn" in namespace "gc-720"
    Jun  9 11:28:15.008: INFO: Deleting pod "simpletest-rc-to-be-deleted-ft62j" in namespace "gc-720"
    Jun  9 11:28:15.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftvmh" in namespace "gc-720"
    Jun  9 11:28:15.060: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8z4f" in namespace "gc-720"
    Jun  9 11:28:15.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9578" in namespace "gc-720"
    Jun  9 11:28:15.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-gms4b" in namespace "gc-720"
    Jun  9 11:28:15.235: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzt4w" in namespace "gc-720"
    Jun  9 11:28:15.273: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7fvv" in namespace "gc-720"
    Jun  9 11:28:15.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhbxs" in namespace "gc-720"
    Jun  9 11:28:15.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjvdx" in namespace "gc-720"
    Jun  9 11:28:15.377: INFO: Deleting pod "simpletest-rc-to-be-deleted-hrt5q" in namespace "gc-720"
    Jun  9 11:28:15.408: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvsxw" in namespace "gc-720"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:28:15.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-720" for this suite. 06/09/23 11:28:15.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:28:15.61
Jun  9 11:28:15.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 11:28:15.612
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:28:15.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:28:15.869
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 06/09/23 11:28:15.879
Jun  9 11:28:15.894: INFO: Waiting up to 5m0s for pod "pod-7e16ad82-b600-4138-aa24-b6648c24a51c" in namespace "emptydir-6374" to be "Succeeded or Failed"
Jun  9 11:28:16.036: INFO: Pod "pod-7e16ad82-b600-4138-aa24-b6648c24a51c": Phase="Pending", Reason="", readiness=false. Elapsed: 141.542024ms
Jun  9 11:28:18.045: INFO: Pod "pod-7e16ad82-b600-4138-aa24-b6648c24a51c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.150508411s
Jun  9 11:28:20.050: INFO: Pod "pod-7e16ad82-b600-4138-aa24-b6648c24a51c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.155946115s
Jun  9 11:28:22.047: INFO: Pod "pod-7e16ad82-b600-4138-aa24-b6648c24a51c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.153104887s
STEP: Saw pod success 06/09/23 11:28:22.048
Jun  9 11:28:22.048: INFO: Pod "pod-7e16ad82-b600-4138-aa24-b6648c24a51c" satisfied condition "Succeeded or Failed"
Jun  9 11:28:22.055: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-7e16ad82-b600-4138-aa24-b6648c24a51c container test-container: <nil>
STEP: delete the pod 06/09/23 11:28:22.08
Jun  9 11:28:22.098: INFO: Waiting for pod pod-7e16ad82-b600-4138-aa24-b6648c24a51c to disappear
Jun  9 11:28:22.107: INFO: Pod pod-7e16ad82-b600-4138-aa24-b6648c24a51c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 11:28:22.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6374" for this suite. 06/09/23 11:28:22.127
------------------------------
• [SLOW TEST] [6.540 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:28:15.61
    Jun  9 11:28:15.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 11:28:15.612
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:28:15.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:28:15.869
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 06/09/23 11:28:15.879
    Jun  9 11:28:15.894: INFO: Waiting up to 5m0s for pod "pod-7e16ad82-b600-4138-aa24-b6648c24a51c" in namespace "emptydir-6374" to be "Succeeded or Failed"
    Jun  9 11:28:16.036: INFO: Pod "pod-7e16ad82-b600-4138-aa24-b6648c24a51c": Phase="Pending", Reason="", readiness=false. Elapsed: 141.542024ms
    Jun  9 11:28:18.045: INFO: Pod "pod-7e16ad82-b600-4138-aa24-b6648c24a51c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.150508411s
    Jun  9 11:28:20.050: INFO: Pod "pod-7e16ad82-b600-4138-aa24-b6648c24a51c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.155946115s
    Jun  9 11:28:22.047: INFO: Pod "pod-7e16ad82-b600-4138-aa24-b6648c24a51c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.153104887s
    STEP: Saw pod success 06/09/23 11:28:22.048
    Jun  9 11:28:22.048: INFO: Pod "pod-7e16ad82-b600-4138-aa24-b6648c24a51c" satisfied condition "Succeeded or Failed"
    Jun  9 11:28:22.055: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-7e16ad82-b600-4138-aa24-b6648c24a51c container test-container: <nil>
    STEP: delete the pod 06/09/23 11:28:22.08
    Jun  9 11:28:22.098: INFO: Waiting for pod pod-7e16ad82-b600-4138-aa24-b6648c24a51c to disappear
    Jun  9 11:28:22.107: INFO: Pod pod-7e16ad82-b600-4138-aa24-b6648c24a51c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:28:22.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6374" for this suite. 06/09/23 11:28:22.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:28:22.151
Jun  9 11:28:22.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename svc-latency 06/09/23 11:28:22.153
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:28:22.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:28:22.208
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jun  9 11:28:22.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7147 06/09/23 11:28:22.217
I0609 11:28:22.251861      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7147, replica count: 1
I0609 11:28:23.302593      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0609 11:28:24.303766      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0609 11:28:25.304486      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 11:28:25.463: INFO: Created: latency-svc-nnjxc
Jun  9 11:28:25.475: INFO: Got endpoints: latency-svc-nnjxc [70.628343ms]
Jun  9 11:28:25.521: INFO: Created: latency-svc-j747r
Jun  9 11:28:25.533: INFO: Got endpoints: latency-svc-j747r [57.003076ms]
Jun  9 11:28:25.535: INFO: Created: latency-svc-gs8rk
Jun  9 11:28:25.561: INFO: Got endpoints: latency-svc-gs8rk [84.499ms]
Jun  9 11:28:25.572: INFO: Created: latency-svc-2j2xt
Jun  9 11:28:25.604: INFO: Got endpoints: latency-svc-2j2xt [126.898651ms]
Jun  9 11:28:25.604: INFO: Created: latency-svc-vz4dq
Jun  9 11:28:25.640: INFO: Got endpoints: latency-svc-vz4dq [162.987275ms]
Jun  9 11:28:25.660: INFO: Created: latency-svc-wv92r
Jun  9 11:28:25.679: INFO: Got endpoints: latency-svc-wv92r [201.834322ms]
Jun  9 11:28:25.701: INFO: Created: latency-svc-4hn4d
Jun  9 11:28:25.701: INFO: Got endpoints: latency-svc-4hn4d [223.418609ms]
Jun  9 11:28:25.705: INFO: Created: latency-svc-xj25m
Jun  9 11:28:25.725: INFO: Got endpoints: latency-svc-xj25m [246.473708ms]
Jun  9 11:28:25.725: INFO: Created: latency-svc-d5px7
Jun  9 11:28:25.738: INFO: Got endpoints: latency-svc-d5px7 [259.539471ms]
Jun  9 11:28:25.745: INFO: Created: latency-svc-kzzpx
Jun  9 11:28:25.759: INFO: Got endpoints: latency-svc-kzzpx [280.502587ms]
Jun  9 11:28:25.767: INFO: Created: latency-svc-8pbtj
Jun  9 11:28:25.778: INFO: Got endpoints: latency-svc-8pbtj [299.425769ms]
Jun  9 11:28:25.795: INFO: Created: latency-svc-vzzvc
Jun  9 11:28:25.806: INFO: Got endpoints: latency-svc-vzzvc [327.05829ms]
Jun  9 11:28:25.817: INFO: Created: latency-svc-8d47b
Jun  9 11:28:25.828: INFO: Got endpoints: latency-svc-8d47b [349.931808ms]
Jun  9 11:28:25.844: INFO: Created: latency-svc-pzn8t
Jun  9 11:28:25.857: INFO: Got endpoints: latency-svc-pzn8t [378.83913ms]
Jun  9 11:28:25.874: INFO: Created: latency-svc-z49nb
Jun  9 11:28:25.891: INFO: Got endpoints: latency-svc-z49nb [413.440936ms]
Jun  9 11:28:25.892: INFO: Created: latency-svc-ptcj7
Jun  9 11:28:25.906: INFO: Got endpoints: latency-svc-ptcj7 [427.993497ms]
Jun  9 11:28:25.913: INFO: Created: latency-svc-qhd9m
Jun  9 11:28:25.946: INFO: Got endpoints: latency-svc-qhd9m [413.012441ms]
Jun  9 11:28:25.947: INFO: Created: latency-svc-lccfw
Jun  9 11:28:25.957: INFO: Got endpoints: latency-svc-lccfw [396.092829ms]
Jun  9 11:28:25.964: INFO: Created: latency-svc-mxgjj
Jun  9 11:28:25.975: INFO: Got endpoints: latency-svc-mxgjj [371.229105ms]
Jun  9 11:28:25.987: INFO: Created: latency-svc-q9bbs
Jun  9 11:28:25.994: INFO: Got endpoints: latency-svc-q9bbs [354.056889ms]
Jun  9 11:28:26.007: INFO: Created: latency-svc-jk9t2
Jun  9 11:28:26.022: INFO: Got endpoints: latency-svc-jk9t2 [343.124073ms]
Jun  9 11:28:26.023: INFO: Created: latency-svc-r2bhp
Jun  9 11:28:26.043: INFO: Got endpoints: latency-svc-r2bhp [341.944387ms]
Jun  9 11:28:26.056: INFO: Created: latency-svc-nrlzj
Jun  9 11:28:26.065: INFO: Got endpoints: latency-svc-nrlzj [340.586205ms]
Jun  9 11:28:26.089: INFO: Created: latency-svc-2qgks
Jun  9 11:28:26.090: INFO: Got endpoints: latency-svc-2qgks [351.263436ms]
Jun  9 11:28:26.090: INFO: Created: latency-svc-l69hz
Jun  9 11:28:26.097: INFO: Got endpoints: latency-svc-l69hz [337.905431ms]
Jun  9 11:28:26.111: INFO: Created: latency-svc-rgmq7
Jun  9 11:28:26.125: INFO: Got endpoints: latency-svc-rgmq7 [347.574231ms]
Jun  9 11:28:26.131: INFO: Created: latency-svc-rsjck
Jun  9 11:28:26.143: INFO: Got endpoints: latency-svc-rsjck [337.333152ms]
Jun  9 11:28:26.152: INFO: Created: latency-svc-hlm6g
Jun  9 11:28:26.159: INFO: Got endpoints: latency-svc-hlm6g [330.397136ms]
Jun  9 11:28:26.174: INFO: Created: latency-svc-wmkf4
Jun  9 11:28:26.191: INFO: Got endpoints: latency-svc-wmkf4 [334.186028ms]
Jun  9 11:28:26.202: INFO: Created: latency-svc-qn4pl
Jun  9 11:28:26.212: INFO: Got endpoints: latency-svc-qn4pl [320.804338ms]
Jun  9 11:28:26.225: INFO: Created: latency-svc-49dz2
Jun  9 11:28:26.240: INFO: Got endpoints: latency-svc-49dz2 [333.831481ms]
Jun  9 11:28:26.241: INFO: Created: latency-svc-tln8f
Jun  9 11:28:26.250: INFO: Got endpoints: latency-svc-tln8f [304.047868ms]
Jun  9 11:28:26.268: INFO: Created: latency-svc-5cf9c
Jun  9 11:28:26.278: INFO: Got endpoints: latency-svc-5cf9c [320.940646ms]
Jun  9 11:28:26.283: INFO: Created: latency-svc-b2m4q
Jun  9 11:28:26.300: INFO: Got endpoints: latency-svc-b2m4q [325.138931ms]
Jun  9 11:28:26.306: INFO: Created: latency-svc-nsq8w
Jun  9 11:28:26.324: INFO: Got endpoints: latency-svc-nsq8w [329.544839ms]
Jun  9 11:28:26.362: INFO: Created: latency-svc-mlkff
Jun  9 11:28:26.362: INFO: Created: latency-svc-4h98l
Jun  9 11:28:26.362: INFO: Got endpoints: latency-svc-4h98l [340.020515ms]
Jun  9 11:28:26.373: INFO: Got endpoints: latency-svc-mlkff [329.977666ms]
Jun  9 11:28:26.374: INFO: Created: latency-svc-j7qdm
Jun  9 11:28:26.396: INFO: Created: latency-svc-4wj24
Jun  9 11:28:26.396: INFO: Got endpoints: latency-svc-j7qdm [331.224963ms]
Jun  9 11:28:26.409: INFO: Got endpoints: latency-svc-4wj24 [319.433792ms]
Jun  9 11:28:26.419: INFO: Created: latency-svc-qp5ck
Jun  9 11:28:26.443: INFO: Got endpoints: latency-svc-qp5ck [345.798979ms]
Jun  9 11:28:26.447: INFO: Created: latency-svc-m6x5b
Jun  9 11:28:26.458: INFO: Got endpoints: latency-svc-m6x5b [332.513839ms]
Jun  9 11:28:26.478: INFO: Created: latency-svc-mjr6n
Jun  9 11:28:26.480: INFO: Got endpoints: latency-svc-mjr6n [336.986872ms]
Jun  9 11:28:26.486: INFO: Created: latency-svc-tnltd
Jun  9 11:28:26.527: INFO: Got endpoints: latency-svc-tnltd [368.419128ms]
Jun  9 11:28:26.528: INFO: Created: latency-svc-r2vct
Jun  9 11:28:26.542: INFO: Got endpoints: latency-svc-r2vct [351.361414ms]
Jun  9 11:28:26.548: INFO: Created: latency-svc-hfbsk
Jun  9 11:28:26.597: INFO: Got endpoints: latency-svc-hfbsk [384.30437ms]
Jun  9 11:28:26.612: INFO: Created: latency-svc-wmf8k
Jun  9 11:28:26.625: INFO: Got endpoints: latency-svc-wmf8k [384.170851ms]
Jun  9 11:28:26.638: INFO: Created: latency-svc-8dczn
Jun  9 11:28:26.664: INFO: Got endpoints: latency-svc-8dczn [413.668154ms]
Jun  9 11:28:26.665: INFO: Created: latency-svc-jbtdh
Jun  9 11:28:26.679: INFO: Got endpoints: latency-svc-jbtdh [400.586654ms]
Jun  9 11:28:26.683: INFO: Created: latency-svc-bbk9c
Jun  9 11:28:26.697: INFO: Got endpoints: latency-svc-bbk9c [396.953048ms]
Jun  9 11:28:26.707: INFO: Created: latency-svc-sg96m
Jun  9 11:28:26.719: INFO: Got endpoints: latency-svc-sg96m [395.58649ms]
Jun  9 11:28:26.732: INFO: Created: latency-svc-dvrb2
Jun  9 11:28:26.744: INFO: Got endpoints: latency-svc-dvrb2 [381.792429ms]
Jun  9 11:28:26.752: INFO: Created: latency-svc-4gv2h
Jun  9 11:28:26.762: INFO: Got endpoints: latency-svc-4gv2h [388.727718ms]
Jun  9 11:28:26.771: INFO: Created: latency-svc-vqx8m
Jun  9 11:28:26.781: INFO: Got endpoints: latency-svc-vqx8m [384.10101ms]
Jun  9 11:28:26.792: INFO: Created: latency-svc-6r4wx
Jun  9 11:28:26.804: INFO: Got endpoints: latency-svc-6r4wx [395.202663ms]
Jun  9 11:28:26.819: INFO: Created: latency-svc-9jfgh
Jun  9 11:28:26.828: INFO: Got endpoints: latency-svc-9jfgh [385.125006ms]
Jun  9 11:28:26.846: INFO: Created: latency-svc-z849t
Jun  9 11:28:26.848: INFO: Got endpoints: latency-svc-z849t [390.062515ms]
Jun  9 11:28:26.854: INFO: Created: latency-svc-hxfkn
Jun  9 11:28:26.862: INFO: Got endpoints: latency-svc-hxfkn [381.78948ms]
Jun  9 11:28:26.867: INFO: Created: latency-svc-lf9rg
Jun  9 11:28:26.877: INFO: Got endpoints: latency-svc-lf9rg [350.123714ms]
Jun  9 11:28:26.880: INFO: Created: latency-svc-rmszz
Jun  9 11:28:26.897: INFO: Created: latency-svc-d8k4f
Jun  9 11:28:26.936: INFO: Got endpoints: latency-svc-rmszz [393.243308ms]
Jun  9 11:28:26.945: INFO: Created: latency-svc-mgxgr
Jun  9 11:28:26.972: INFO: Created: latency-svc-bnr4l
Jun  9 11:28:26.981: INFO: Got endpoints: latency-svc-d8k4f [384.629914ms]
Jun  9 11:28:26.995: INFO: Created: latency-svc-nqpr2
Jun  9 11:28:27.010: INFO: Created: latency-svc-9fqbc
Jun  9 11:28:27.035: INFO: Got endpoints: latency-svc-mgxgr [410.464398ms]
Jun  9 11:28:27.036: INFO: Created: latency-svc-r4phk
Jun  9 11:28:27.059: INFO: Created: latency-svc-j4lcz
Jun  9 11:28:27.114: INFO: Got endpoints: latency-svc-bnr4l [449.546529ms]
Jun  9 11:28:27.117: INFO: Created: latency-svc-jmnjh
Jun  9 11:28:27.136: INFO: Got endpoints: latency-svc-nqpr2 [456.886204ms]
Jun  9 11:28:27.141: INFO: Created: latency-svc-7kqnc
Jun  9 11:28:27.159: INFO: Created: latency-svc-k4nsr
Jun  9 11:28:27.176: INFO: Got endpoints: latency-svc-9fqbc [479.155269ms]
Jun  9 11:28:27.182: INFO: Created: latency-svc-gsfkd
Jun  9 11:28:27.201: INFO: Created: latency-svc-5fqsw
Jun  9 11:28:27.223: INFO: Created: latency-svc-nbhvb
Jun  9 11:28:27.228: INFO: Got endpoints: latency-svc-r4phk [508.781753ms]
Jun  9 11:28:27.235: INFO: Created: latency-svc-2rttz
Jun  9 11:28:27.250: INFO: Created: latency-svc-fvnph
Jun  9 11:28:27.273: INFO: Created: latency-svc-qg4j4
Jun  9 11:28:27.275: INFO: Got endpoints: latency-svc-j4lcz [531.143466ms]
Jun  9 11:28:27.292: INFO: Created: latency-svc-6d98h
Jun  9 11:28:27.313: INFO: Created: latency-svc-62b5p
Jun  9 11:28:27.329: INFO: Got endpoints: latency-svc-jmnjh [567.244566ms]
Jun  9 11:28:27.330: INFO: Created: latency-svc-ldl5v
Jun  9 11:28:27.357: INFO: Created: latency-svc-s2n8s
Jun  9 11:28:27.381: INFO: Got endpoints: latency-svc-7kqnc [600.761465ms]
Jun  9 11:28:27.382: INFO: Created: latency-svc-dvf6b
Jun  9 11:28:27.404: INFO: Created: latency-svc-gwg8g
Jun  9 11:28:27.433: INFO: Got endpoints: latency-svc-k4nsr [628.660971ms]
Jun  9 11:28:27.434: INFO: Created: latency-svc-pxbnf
Jun  9 11:28:27.474: INFO: Created: latency-svc-qvb28
Jun  9 11:28:27.512: INFO: Got endpoints: latency-svc-gsfkd [683.642923ms]
Jun  9 11:28:27.536: INFO: Got endpoints: latency-svc-5fqsw [687.743869ms]
Jun  9 11:28:27.540: INFO: Created: latency-svc-chl5f
Jun  9 11:28:27.568: INFO: Created: latency-svc-z9tpj
Jun  9 11:28:27.593: INFO: Got endpoints: latency-svc-nbhvb [730.770763ms]
Jun  9 11:28:27.594: INFO: Created: latency-svc-xlcmw
Jun  9 11:28:27.670: INFO: Got endpoints: latency-svc-2rttz [792.66246ms]
Jun  9 11:28:27.687: INFO: Created: latency-svc-l2cld
Jun  9 11:28:27.691: INFO: Got endpoints: latency-svc-fvnph [755.33776ms]
Jun  9 11:28:27.746: INFO: Got endpoints: latency-svc-qg4j4 [764.159342ms]
Jun  9 11:28:27.747: INFO: Created: latency-svc-z86bj
Jun  9 11:28:27.769: INFO: Created: latency-svc-bqj47
Jun  9 11:28:27.786: INFO: Got endpoints: latency-svc-6d98h [751.135821ms]
Jun  9 11:28:27.797: INFO: Created: latency-svc-9rfln
Jun  9 11:28:27.828: INFO: Got endpoints: latency-svc-62b5p [714.200806ms]
Jun  9 11:28:27.831: INFO: Created: latency-svc-fk2xd
Jun  9 11:28:27.867: INFO: Created: latency-svc-njt8j
Jun  9 11:28:27.877: INFO: Got endpoints: latency-svc-ldl5v [740.776915ms]
Jun  9 11:28:27.938: INFO: Created: latency-svc-89l2q
Jun  9 11:28:27.945: INFO: Got endpoints: latency-svc-s2n8s [768.961833ms]
Jun  9 11:28:27.976: INFO: Got endpoints: latency-svc-dvf6b [747.949137ms]
Jun  9 11:28:27.983: INFO: Created: latency-svc-xzpmg
Jun  9 11:28:28.019: INFO: Created: latency-svc-wg5wt
Jun  9 11:28:28.027: INFO: Got endpoints: latency-svc-gwg8g [751.665117ms]
Jun  9 11:28:28.061: INFO: Created: latency-svc-bmbdg
Jun  9 11:28:28.077: INFO: Got endpoints: latency-svc-pxbnf [747.66453ms]
Jun  9 11:28:28.104: INFO: Created: latency-svc-vx4rs
Jun  9 11:28:28.144: INFO: Got endpoints: latency-svc-qvb28 [762.116048ms]
Jun  9 11:28:28.190: INFO: Got endpoints: latency-svc-chl5f [757.225922ms]
Jun  9 11:28:28.193: INFO: Created: latency-svc-kw5rf
Jun  9 11:28:28.216: INFO: Created: latency-svc-qbq8x
Jun  9 11:28:28.223: INFO: Got endpoints: latency-svc-z9tpj [710.921016ms]
Jun  9 11:28:28.246: INFO: Created: latency-svc-ff5th
Jun  9 11:28:28.276: INFO: Got endpoints: latency-svc-xlcmw [740.04116ms]
Jun  9 11:28:28.300: INFO: Created: latency-svc-hglp9
Jun  9 11:28:28.324: INFO: Got endpoints: latency-svc-l2cld [731.30483ms]
Jun  9 11:28:28.348: INFO: Created: latency-svc-44zf9
Jun  9 11:28:28.374: INFO: Got endpoints: latency-svc-z86bj [704.419522ms]
Jun  9 11:28:28.402: INFO: Created: latency-svc-j2bbm
Jun  9 11:28:28.423: INFO: Got endpoints: latency-svc-bqj47 [731.561556ms]
Jun  9 11:28:28.443: INFO: Created: latency-svc-mrtws
Jun  9 11:28:28.510: INFO: Got endpoints: latency-svc-9rfln [764.617546ms]
Jun  9 11:28:28.533: INFO: Got endpoints: latency-svc-fk2xd [746.881433ms]
Jun  9 11:28:28.559: INFO: Created: latency-svc-xpjtg
Jun  9 11:28:28.588: INFO: Got endpoints: latency-svc-njt8j [759.561705ms]
Jun  9 11:28:28.588: INFO: Created: latency-svc-rsd94
Jun  9 11:28:28.634: INFO: Got endpoints: latency-svc-89l2q [757.031953ms]
Jun  9 11:28:28.635: INFO: Created: latency-svc-p8m8k
Jun  9 11:28:28.662: INFO: Created: latency-svc-bchrb
Jun  9 11:28:28.674: INFO: Got endpoints: latency-svc-xzpmg [728.697452ms]
Jun  9 11:28:28.715: INFO: Created: latency-svc-b4x25
Jun  9 11:28:28.723: INFO: Got endpoints: latency-svc-wg5wt [746.515025ms]
Jun  9 11:28:28.752: INFO: Created: latency-svc-zddzj
Jun  9 11:28:28.777: INFO: Got endpoints: latency-svc-bmbdg [749.742209ms]
Jun  9 11:28:28.807: INFO: Created: latency-svc-2kwfm
Jun  9 11:28:28.829: INFO: Got endpoints: latency-svc-vx4rs [752.375379ms]
Jun  9 11:28:28.858: INFO: Created: latency-svc-v2x8x
Jun  9 11:28:28.874: INFO: Got endpoints: latency-svc-kw5rf [730.274231ms]
Jun  9 11:28:28.903: INFO: Created: latency-svc-v9vrk
Jun  9 11:28:28.925: INFO: Got endpoints: latency-svc-qbq8x [734.585447ms]
Jun  9 11:28:28.951: INFO: Created: latency-svc-llz8j
Jun  9 11:28:28.974: INFO: Got endpoints: latency-svc-ff5th [750.698005ms]
Jun  9 11:28:28.998: INFO: Created: latency-svc-55nx4
Jun  9 11:28:29.027: INFO: Got endpoints: latency-svc-hglp9 [751.08236ms]
Jun  9 11:28:29.064: INFO: Created: latency-svc-h9rhf
Jun  9 11:28:29.075: INFO: Got endpoints: latency-svc-44zf9 [751.052723ms]
Jun  9 11:28:29.111: INFO: Created: latency-svc-m29n9
Jun  9 11:28:29.128: INFO: Got endpoints: latency-svc-j2bbm [753.70208ms]
Jun  9 11:28:29.163: INFO: Created: latency-svc-pjw68
Jun  9 11:28:29.177: INFO: Got endpoints: latency-svc-mrtws [754.420471ms]
Jun  9 11:28:29.208: INFO: Created: latency-svc-d4nnj
Jun  9 11:28:29.241: INFO: Got endpoints: latency-svc-xpjtg [730.943502ms]
Jun  9 11:28:29.271: INFO: Created: latency-svc-sq4hm
Jun  9 11:28:29.274: INFO: Got endpoints: latency-svc-rsd94 [740.711871ms]
Jun  9 11:28:29.313: INFO: Created: latency-svc-zdjmz
Jun  9 11:28:29.328: INFO: Got endpoints: latency-svc-p8m8k [740.56603ms]
Jun  9 11:28:29.358: INFO: Created: latency-svc-8ppr4
Jun  9 11:28:29.381: INFO: Got endpoints: latency-svc-bchrb [746.573673ms]
Jun  9 11:28:29.406: INFO: Created: latency-svc-bzn9k
Jun  9 11:28:29.433: INFO: Got endpoints: latency-svc-b4x25 [759.004942ms]
Jun  9 11:28:29.453: INFO: Created: latency-svc-7fl58
Jun  9 11:28:29.501: INFO: Got endpoints: latency-svc-zddzj [778.001586ms]
Jun  9 11:28:29.531: INFO: Got endpoints: latency-svc-2kwfm [753.629004ms]
Jun  9 11:28:29.533: INFO: Created: latency-svc-hfvm6
Jun  9 11:28:29.567: INFO: Created: latency-svc-czjqw
Jun  9 11:28:29.576: INFO: Got endpoints: latency-svc-v2x8x [746.574022ms]
Jun  9 11:28:29.612: INFO: Created: latency-svc-khpv4
Jun  9 11:28:29.641: INFO: Got endpoints: latency-svc-v9vrk [767.148405ms]
Jun  9 11:28:29.668: INFO: Created: latency-svc-8ktnf
Jun  9 11:28:29.685: INFO: Got endpoints: latency-svc-llz8j [760.050761ms]
Jun  9 11:28:29.733: INFO: Created: latency-svc-s422j
Jun  9 11:28:29.744: INFO: Got endpoints: latency-svc-55nx4 [769.898407ms]
Jun  9 11:28:29.904: INFO: Got endpoints: latency-svc-h9rhf [876.830095ms]
Jun  9 11:28:29.912: INFO: Created: latency-svc-bfq7b
Jun  9 11:28:29.913: INFO: Got endpoints: latency-svc-pjw68 [784.178628ms]
Jun  9 11:28:29.913: INFO: Got endpoints: latency-svc-m29n9 [837.565413ms]
Jun  9 11:28:29.953: INFO: Got endpoints: latency-svc-d4nnj [776.031654ms]
Jun  9 11:28:29.963: INFO: Created: latency-svc-nrh5m
Jun  9 11:28:29.984: INFO: Got endpoints: latency-svc-sq4hm [742.417447ms]
Jun  9 11:28:29.994: INFO: Created: latency-svc-5d4hv
Jun  9 11:28:30.054: INFO: Got endpoints: latency-svc-zdjmz [779.409177ms]
Jun  9 11:28:30.054: INFO: Created: latency-svc-lhq7r
Jun  9 11:28:30.090: INFO: Got endpoints: latency-svc-8ppr4 [761.656557ms]
Jun  9 11:28:30.101: INFO: Created: latency-svc-s9z66
Jun  9 11:28:30.164: INFO: Got endpoints: latency-svc-bzn9k [783.524272ms]
Jun  9 11:28:30.187: INFO: Created: latency-svc-nfzgx
Jun  9 11:28:30.187: INFO: Got endpoints: latency-svc-7fl58 [754.069996ms]
Jun  9 11:28:30.272: INFO: Got endpoints: latency-svc-hfvm6 [770.789561ms]
Jun  9 11:28:30.294: INFO: Got endpoints: latency-svc-czjqw [763.142849ms]
Jun  9 11:28:30.295: INFO: Created: latency-svc-wx5ts
Jun  9 11:28:30.388: INFO: Got endpoints: latency-svc-8ktnf [746.874125ms]
Jun  9 11:28:30.388: INFO: Created: latency-svc-wgm4q
Jun  9 11:28:30.388: INFO: Got endpoints: latency-svc-khpv4 [812.405579ms]
Jun  9 11:28:30.433: INFO: Created: latency-svc-kf7cd
Jun  9 11:28:30.439: INFO: Got endpoints: latency-svc-s422j [754.218917ms]
Jun  9 11:28:30.489: INFO: Created: latency-svc-kk844
Jun  9 11:28:30.504: INFO: Got endpoints: latency-svc-bfq7b [759.971805ms]
Jun  9 11:28:30.545: INFO: Created: latency-svc-f2dl6
Jun  9 11:28:30.584: INFO: Got endpoints: latency-svc-nrh5m [680.024587ms]
Jun  9 11:28:30.635: INFO: Got endpoints: latency-svc-lhq7r [722.505914ms]
Jun  9 11:28:30.659: INFO: Created: latency-svc-rj9vd
Jun  9 11:28:30.665: INFO: Got endpoints: latency-svc-5d4hv [752.079289ms]
Jun  9 11:28:30.700: INFO: Got endpoints: latency-svc-s9z66 [746.75918ms]
Jun  9 11:28:30.754: INFO: Got endpoints: latency-svc-nfzgx [769.886146ms]
Jun  9 11:28:30.785: INFO: Created: latency-svc-xpdbn
Jun  9 11:28:30.800: INFO: Got endpoints: latency-svc-wx5ts [746.195158ms]
Jun  9 11:28:30.847: INFO: Got endpoints: latency-svc-wgm4q [756.856744ms]
Jun  9 11:28:30.876: INFO: Created: latency-svc-mggml
Jun  9 11:28:30.892: INFO: Got endpoints: latency-svc-kf7cd [727.86752ms]
Jun  9 11:28:30.936: INFO: Got endpoints: latency-svc-kk844 [748.0918ms]
Jun  9 11:28:30.936: INFO: Created: latency-svc-2kqvd
Jun  9 11:28:30.950: INFO: Created: latency-svc-gzk5t
Jun  9 11:28:31.000: INFO: Got endpoints: latency-svc-f2dl6 [728.487512ms]
Jun  9 11:28:31.006: INFO: Created: latency-svc-jgqq7
Jun  9 11:28:31.034: INFO: Got endpoints: latency-svc-rj9vd [740.283207ms]
Jun  9 11:28:31.061: INFO: Created: latency-svc-49z48
Jun  9 11:28:31.174: INFO: Got endpoints: latency-svc-mggml [785.893882ms]
Jun  9 11:28:31.176: INFO: Got endpoints: latency-svc-xpdbn [788.091521ms]
Jun  9 11:28:31.180: INFO: Got endpoints: latency-svc-2kqvd [740.85044ms]
Jun  9 11:28:31.186: INFO: Created: latency-svc-4b4hw
Jun  9 11:28:31.199: INFO: Created: latency-svc-75zn4
Jun  9 11:28:31.239: INFO: Created: latency-svc-972lq
Jun  9 11:28:31.245: INFO: Got endpoints: latency-svc-gzk5t [741.187261ms]
Jun  9 11:28:31.309: INFO: Got endpoints: latency-svc-jgqq7 [724.920685ms]
Jun  9 11:28:31.310: INFO: Created: latency-svc-b6m5m
Jun  9 11:28:31.329: INFO: Got endpoints: latency-svc-49z48 [694.023551ms]
Jun  9 11:28:31.336: INFO: Created: latency-svc-86z9s
Jun  9 11:28:31.357: INFO: Created: latency-svc-8xszz
Jun  9 11:28:31.401: INFO: Got endpoints: latency-svc-4b4hw [736.259822ms]
Jun  9 11:28:31.409: INFO: Created: latency-svc-79htq
Jun  9 11:28:31.424: INFO: Got endpoints: latency-svc-75zn4 [724.154976ms]
Jun  9 11:28:31.424: INFO: Created: latency-svc-56pkt
Jun  9 11:28:31.441: INFO: Created: latency-svc-pmwzh
Jun  9 11:28:31.461: INFO: Created: latency-svc-kzs4l
Jun  9 11:28:31.497: INFO: Got endpoints: latency-svc-972lq [743.079958ms]
Jun  9 11:28:31.510: INFO: Created: latency-svc-rp2qx
Jun  9 11:28:31.530: INFO: Got endpoints: latency-svc-b6m5m [730.042137ms]
Jun  9 11:28:31.536: INFO: Created: latency-svc-xfbsd
Jun  9 11:28:31.602: INFO: Got endpoints: latency-svc-86z9s [755.271079ms]
Jun  9 11:28:31.608: INFO: Created: latency-svc-7d6tc
Jun  9 11:28:31.647: INFO: Got endpoints: latency-svc-8xszz [754.946133ms]
Jun  9 11:28:31.648: INFO: Created: latency-svc-5t9rf
Jun  9 11:28:31.679: INFO: Created: latency-svc-wzb6q
Jun  9 11:28:31.679: INFO: Got endpoints: latency-svc-79htq [743.269147ms]
Jun  9 11:28:31.711: INFO: Created: latency-svc-5kgxl
Jun  9 11:28:31.730: INFO: Got endpoints: latency-svc-56pkt [730.003717ms]
Jun  9 11:28:31.732: INFO: Created: latency-svc-5lklt
Jun  9 11:28:31.757: INFO: Created: latency-svc-mm8pf
Jun  9 11:28:31.763: INFO: Created: latency-svc-2cftj
Jun  9 11:28:31.776: INFO: Got endpoints: latency-svc-pmwzh [741.401968ms]
Jun  9 11:28:31.781: INFO: Created: latency-svc-nfnfd
Jun  9 11:28:31.804: INFO: Created: latency-svc-r5ngg
Jun  9 11:28:31.822: INFO: Created: latency-svc-7q6r9
Jun  9 11:28:31.830: INFO: Got endpoints: latency-svc-kzs4l [655.920029ms]
Jun  9 11:28:31.836: INFO: Created: latency-svc-fm7q6
Jun  9 11:28:31.855: INFO: Created: latency-svc-dfj5p
Jun  9 11:28:31.869: INFO: Created: latency-svc-26gvc
Jun  9 11:28:31.875: INFO: Got endpoints: latency-svc-rp2qx [698.240736ms]
Jun  9 11:28:31.903: INFO: Created: latency-svc-vrqgg
Jun  9 11:28:31.927: INFO: Got endpoints: latency-svc-xfbsd [746.77212ms]
Jun  9 11:28:31.948: INFO: Created: latency-svc-dzxfp
Jun  9 11:28:31.975: INFO: Got endpoints: latency-svc-7d6tc [730.309599ms]
Jun  9 11:28:32.014: INFO: Created: latency-svc-dhvsq
Jun  9 11:28:32.025: INFO: Got endpoints: latency-svc-5t9rf [715.779281ms]
Jun  9 11:28:32.048: INFO: Created: latency-svc-7dkgn
Jun  9 11:28:32.073: INFO: Got endpoints: latency-svc-wzb6q [743.395851ms]
Jun  9 11:28:32.097: INFO: Created: latency-svc-8m9wl
Jun  9 11:28:32.123: INFO: Got endpoints: latency-svc-5kgxl [722.054469ms]
Jun  9 11:28:32.208: INFO: Got endpoints: latency-svc-5lklt [783.411951ms]
Jun  9 11:28:32.208: INFO: Created: latency-svc-cxmxw
Jun  9 11:28:32.269: INFO: Got endpoints: latency-svc-mm8pf [771.958639ms]
Jun  9 11:28:32.272: INFO: Created: latency-svc-tnzrh
Jun  9 11:28:32.296: INFO: Got endpoints: latency-svc-2cftj [766.24888ms]
Jun  9 11:28:32.327: INFO: Got endpoints: latency-svc-nfnfd [724.654708ms]
Jun  9 11:28:32.328: INFO: Created: latency-svc-qx6w5
Jun  9 11:28:32.344: INFO: Created: latency-svc-mmjgx
Jun  9 11:28:32.348: INFO: Created: latency-svc-znl9g
Jun  9 11:28:32.437: INFO: Got endpoints: latency-svc-7q6r9 [758.326469ms]
Jun  9 11:28:32.438: INFO: Got endpoints: latency-svc-r5ngg [790.461737ms]
Jun  9 11:28:32.462: INFO: Created: latency-svc-b6qmj
Jun  9 11:28:32.489: INFO: Got endpoints: latency-svc-fm7q6 [758.711897ms]
Jun  9 11:28:32.490: INFO: Created: latency-svc-6tbpx
Jun  9 11:28:32.513: INFO: Created: latency-svc-kw82m
Jun  9 11:28:32.530: INFO: Got endpoints: latency-svc-dfj5p [753.693943ms]
Jun  9 11:28:32.554: INFO: Created: latency-svc-2f577
Jun  9 11:28:32.577: INFO: Got endpoints: latency-svc-26gvc [746.982107ms]
Jun  9 11:28:32.606: INFO: Created: latency-svc-89kkk
Jun  9 11:28:32.629: INFO: Got endpoints: latency-svc-vrqgg [754.111019ms]
Jun  9 11:28:32.658: INFO: Created: latency-svc-q8bww
Jun  9 11:28:32.676: INFO: Got endpoints: latency-svc-dzxfp [748.835839ms]
Jun  9 11:28:32.700: INFO: Created: latency-svc-2jlkp
Jun  9 11:28:32.724: INFO: Got endpoints: latency-svc-dhvsq [748.814357ms]
Jun  9 11:28:32.752: INFO: Created: latency-svc-zzrz2
Jun  9 11:28:32.776: INFO: Got endpoints: latency-svc-7dkgn [751.222784ms]
Jun  9 11:28:32.798: INFO: Created: latency-svc-mq569
Jun  9 11:28:32.822: INFO: Got endpoints: latency-svc-8m9wl [749.071292ms]
Jun  9 11:28:32.841: INFO: Created: latency-svc-qf8tx
Jun  9 11:28:32.873: INFO: Got endpoints: latency-svc-cxmxw [749.360732ms]
Jun  9 11:28:32.901: INFO: Created: latency-svc-cnnvg
Jun  9 11:28:32.925: INFO: Got endpoints: latency-svc-tnzrh [717.413331ms]
Jun  9 11:28:32.948: INFO: Created: latency-svc-dgcq9
Jun  9 11:28:32.980: INFO: Got endpoints: latency-svc-qx6w5 [711.267729ms]
Jun  9 11:28:33.019: INFO: Created: latency-svc-rbccf
Jun  9 11:28:33.029: INFO: Got endpoints: latency-svc-mmjgx [732.721751ms]
Jun  9 11:28:33.054: INFO: Created: latency-svc-45jnk
Jun  9 11:28:33.073: INFO: Got endpoints: latency-svc-znl9g [746.374191ms]
Jun  9 11:28:33.104: INFO: Created: latency-svc-czzc4
Jun  9 11:28:33.128: INFO: Got endpoints: latency-svc-b6qmj [690.502835ms]
Jun  9 11:28:33.150: INFO: Created: latency-svc-sjhz2
Jun  9 11:28:33.173: INFO: Got endpoints: latency-svc-6tbpx [735.461326ms]
Jun  9 11:28:33.199: INFO: Created: latency-svc-5xqfx
Jun  9 11:28:33.225: INFO: Got endpoints: latency-svc-kw82m [735.789164ms]
Jun  9 11:28:33.246: INFO: Created: latency-svc-4cdv4
Jun  9 11:28:33.276: INFO: Got endpoints: latency-svc-2f577 [746.578432ms]
Jun  9 11:28:33.308: INFO: Created: latency-svc-b82hs
Jun  9 11:28:33.328: INFO: Got endpoints: latency-svc-89kkk [750.746561ms]
Jun  9 11:28:33.380: INFO: Got endpoints: latency-svc-q8bww [750.629341ms]
Jun  9 11:28:33.431: INFO: Got endpoints: latency-svc-2jlkp [755.06725ms]
Jun  9 11:28:33.473: INFO: Got endpoints: latency-svc-zzrz2 [749.093122ms]
Jun  9 11:28:33.533: INFO: Got endpoints: latency-svc-mq569 [756.817555ms]
Jun  9 11:28:33.585: INFO: Got endpoints: latency-svc-qf8tx [763.28279ms]
Jun  9 11:28:33.631: INFO: Got endpoints: latency-svc-cnnvg [758.617773ms]
Jun  9 11:28:33.677: INFO: Got endpoints: latency-svc-dgcq9 [752.059411ms]
Jun  9 11:28:33.729: INFO: Got endpoints: latency-svc-rbccf [748.972885ms]
Jun  9 11:28:33.789: INFO: Got endpoints: latency-svc-45jnk [759.654626ms]
Jun  9 11:28:33.833: INFO: Got endpoints: latency-svc-czzc4 [759.274385ms]
Jun  9 11:28:33.887: INFO: Got endpoints: latency-svc-sjhz2 [758.823314ms]
Jun  9 11:28:33.932: INFO: Got endpoints: latency-svc-5xqfx [758.828631ms]
Jun  9 11:28:33.984: INFO: Got endpoints: latency-svc-4cdv4 [758.755745ms]
Jun  9 11:28:34.029: INFO: Got endpoints: latency-svc-b82hs [752.74795ms]
Jun  9 11:28:34.029: INFO: Latencies: [57.003076ms 84.499ms 126.898651ms 162.987275ms 201.834322ms 223.418609ms 246.473708ms 259.539471ms 280.502587ms 299.425769ms 304.047868ms 319.433792ms 320.804338ms 320.940646ms 325.138931ms 327.05829ms 329.544839ms 329.977666ms 330.397136ms 331.224963ms 332.513839ms 333.831481ms 334.186028ms 336.986872ms 337.333152ms 337.905431ms 340.020515ms 340.586205ms 341.944387ms 343.124073ms 345.798979ms 347.574231ms 349.931808ms 350.123714ms 351.263436ms 351.361414ms 354.056889ms 368.419128ms 371.229105ms 378.83913ms 381.78948ms 381.792429ms 384.10101ms 384.170851ms 384.30437ms 384.629914ms 385.125006ms 388.727718ms 390.062515ms 393.243308ms 395.202663ms 395.58649ms 396.092829ms 396.953048ms 400.586654ms 410.464398ms 413.012441ms 413.440936ms 413.668154ms 427.993497ms 449.546529ms 456.886204ms 479.155269ms 508.781753ms 531.143466ms 567.244566ms 600.761465ms 628.660971ms 655.920029ms 680.024587ms 683.642923ms 687.743869ms 690.502835ms 694.023551ms 698.240736ms 704.419522ms 710.921016ms 711.267729ms 714.200806ms 715.779281ms 717.413331ms 722.054469ms 722.505914ms 724.154976ms 724.654708ms 724.920685ms 727.86752ms 728.487512ms 728.697452ms 730.003717ms 730.042137ms 730.274231ms 730.309599ms 730.770763ms 730.943502ms 731.30483ms 731.561556ms 732.721751ms 734.585447ms 735.461326ms 735.789164ms 736.259822ms 740.04116ms 740.283207ms 740.56603ms 740.711871ms 740.776915ms 740.85044ms 741.187261ms 741.401968ms 742.417447ms 743.079958ms 743.269147ms 743.395851ms 746.195158ms 746.374191ms 746.515025ms 746.573673ms 746.574022ms 746.578432ms 746.75918ms 746.77212ms 746.874125ms 746.881433ms 746.982107ms 747.66453ms 747.949137ms 748.0918ms 748.814357ms 748.835839ms 748.972885ms 749.071292ms 749.093122ms 749.360732ms 749.742209ms 750.629341ms 750.698005ms 750.746561ms 751.052723ms 751.08236ms 751.135821ms 751.222784ms 751.665117ms 752.059411ms 752.079289ms 752.375379ms 752.74795ms 753.629004ms 753.693943ms 753.70208ms 754.069996ms 754.111019ms 754.218917ms 754.420471ms 754.946133ms 755.06725ms 755.271079ms 755.33776ms 756.817555ms 756.856744ms 757.031953ms 757.225922ms 758.326469ms 758.617773ms 758.711897ms 758.755745ms 758.823314ms 758.828631ms 759.004942ms 759.274385ms 759.561705ms 759.654626ms 759.971805ms 760.050761ms 761.656557ms 762.116048ms 763.142849ms 763.28279ms 764.159342ms 764.617546ms 766.24888ms 767.148405ms 768.961833ms 769.886146ms 769.898407ms 770.789561ms 771.958639ms 776.031654ms 778.001586ms 779.409177ms 783.411951ms 783.524272ms 784.178628ms 785.893882ms 788.091521ms 790.461737ms 792.66246ms 812.405579ms 837.565413ms 876.830095ms]
Jun  9 11:28:34.029: INFO: 50 %ile: 735.789164ms
Jun  9 11:28:34.029: INFO: 90 %ile: 766.24888ms
Jun  9 11:28:34.029: INFO: 99 %ile: 837.565413ms
Jun  9 11:28:34.029: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jun  9 11:28:34.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-7147" for this suite. 06/09/23 11:28:34.044
------------------------------
• [SLOW TEST] [11.903 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:28:22.151
    Jun  9 11:28:22.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename svc-latency 06/09/23 11:28:22.153
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:28:22.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:28:22.208
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jun  9 11:28:22.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7147 06/09/23 11:28:22.217
    I0609 11:28:22.251861      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7147, replica count: 1
    I0609 11:28:23.302593      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0609 11:28:24.303766      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0609 11:28:25.304486      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  9 11:28:25.463: INFO: Created: latency-svc-nnjxc
    Jun  9 11:28:25.475: INFO: Got endpoints: latency-svc-nnjxc [70.628343ms]
    Jun  9 11:28:25.521: INFO: Created: latency-svc-j747r
    Jun  9 11:28:25.533: INFO: Got endpoints: latency-svc-j747r [57.003076ms]
    Jun  9 11:28:25.535: INFO: Created: latency-svc-gs8rk
    Jun  9 11:28:25.561: INFO: Got endpoints: latency-svc-gs8rk [84.499ms]
    Jun  9 11:28:25.572: INFO: Created: latency-svc-2j2xt
    Jun  9 11:28:25.604: INFO: Got endpoints: latency-svc-2j2xt [126.898651ms]
    Jun  9 11:28:25.604: INFO: Created: latency-svc-vz4dq
    Jun  9 11:28:25.640: INFO: Got endpoints: latency-svc-vz4dq [162.987275ms]
    Jun  9 11:28:25.660: INFO: Created: latency-svc-wv92r
    Jun  9 11:28:25.679: INFO: Got endpoints: latency-svc-wv92r [201.834322ms]
    Jun  9 11:28:25.701: INFO: Created: latency-svc-4hn4d
    Jun  9 11:28:25.701: INFO: Got endpoints: latency-svc-4hn4d [223.418609ms]
    Jun  9 11:28:25.705: INFO: Created: latency-svc-xj25m
    Jun  9 11:28:25.725: INFO: Got endpoints: latency-svc-xj25m [246.473708ms]
    Jun  9 11:28:25.725: INFO: Created: latency-svc-d5px7
    Jun  9 11:28:25.738: INFO: Got endpoints: latency-svc-d5px7 [259.539471ms]
    Jun  9 11:28:25.745: INFO: Created: latency-svc-kzzpx
    Jun  9 11:28:25.759: INFO: Got endpoints: latency-svc-kzzpx [280.502587ms]
    Jun  9 11:28:25.767: INFO: Created: latency-svc-8pbtj
    Jun  9 11:28:25.778: INFO: Got endpoints: latency-svc-8pbtj [299.425769ms]
    Jun  9 11:28:25.795: INFO: Created: latency-svc-vzzvc
    Jun  9 11:28:25.806: INFO: Got endpoints: latency-svc-vzzvc [327.05829ms]
    Jun  9 11:28:25.817: INFO: Created: latency-svc-8d47b
    Jun  9 11:28:25.828: INFO: Got endpoints: latency-svc-8d47b [349.931808ms]
    Jun  9 11:28:25.844: INFO: Created: latency-svc-pzn8t
    Jun  9 11:28:25.857: INFO: Got endpoints: latency-svc-pzn8t [378.83913ms]
    Jun  9 11:28:25.874: INFO: Created: latency-svc-z49nb
    Jun  9 11:28:25.891: INFO: Got endpoints: latency-svc-z49nb [413.440936ms]
    Jun  9 11:28:25.892: INFO: Created: latency-svc-ptcj7
    Jun  9 11:28:25.906: INFO: Got endpoints: latency-svc-ptcj7 [427.993497ms]
    Jun  9 11:28:25.913: INFO: Created: latency-svc-qhd9m
    Jun  9 11:28:25.946: INFO: Got endpoints: latency-svc-qhd9m [413.012441ms]
    Jun  9 11:28:25.947: INFO: Created: latency-svc-lccfw
    Jun  9 11:28:25.957: INFO: Got endpoints: latency-svc-lccfw [396.092829ms]
    Jun  9 11:28:25.964: INFO: Created: latency-svc-mxgjj
    Jun  9 11:28:25.975: INFO: Got endpoints: latency-svc-mxgjj [371.229105ms]
    Jun  9 11:28:25.987: INFO: Created: latency-svc-q9bbs
    Jun  9 11:28:25.994: INFO: Got endpoints: latency-svc-q9bbs [354.056889ms]
    Jun  9 11:28:26.007: INFO: Created: latency-svc-jk9t2
    Jun  9 11:28:26.022: INFO: Got endpoints: latency-svc-jk9t2 [343.124073ms]
    Jun  9 11:28:26.023: INFO: Created: latency-svc-r2bhp
    Jun  9 11:28:26.043: INFO: Got endpoints: latency-svc-r2bhp [341.944387ms]
    Jun  9 11:28:26.056: INFO: Created: latency-svc-nrlzj
    Jun  9 11:28:26.065: INFO: Got endpoints: latency-svc-nrlzj [340.586205ms]
    Jun  9 11:28:26.089: INFO: Created: latency-svc-2qgks
    Jun  9 11:28:26.090: INFO: Got endpoints: latency-svc-2qgks [351.263436ms]
    Jun  9 11:28:26.090: INFO: Created: latency-svc-l69hz
    Jun  9 11:28:26.097: INFO: Got endpoints: latency-svc-l69hz [337.905431ms]
    Jun  9 11:28:26.111: INFO: Created: latency-svc-rgmq7
    Jun  9 11:28:26.125: INFO: Got endpoints: latency-svc-rgmq7 [347.574231ms]
    Jun  9 11:28:26.131: INFO: Created: latency-svc-rsjck
    Jun  9 11:28:26.143: INFO: Got endpoints: latency-svc-rsjck [337.333152ms]
    Jun  9 11:28:26.152: INFO: Created: latency-svc-hlm6g
    Jun  9 11:28:26.159: INFO: Got endpoints: latency-svc-hlm6g [330.397136ms]
    Jun  9 11:28:26.174: INFO: Created: latency-svc-wmkf4
    Jun  9 11:28:26.191: INFO: Got endpoints: latency-svc-wmkf4 [334.186028ms]
    Jun  9 11:28:26.202: INFO: Created: latency-svc-qn4pl
    Jun  9 11:28:26.212: INFO: Got endpoints: latency-svc-qn4pl [320.804338ms]
    Jun  9 11:28:26.225: INFO: Created: latency-svc-49dz2
    Jun  9 11:28:26.240: INFO: Got endpoints: latency-svc-49dz2 [333.831481ms]
    Jun  9 11:28:26.241: INFO: Created: latency-svc-tln8f
    Jun  9 11:28:26.250: INFO: Got endpoints: latency-svc-tln8f [304.047868ms]
    Jun  9 11:28:26.268: INFO: Created: latency-svc-5cf9c
    Jun  9 11:28:26.278: INFO: Got endpoints: latency-svc-5cf9c [320.940646ms]
    Jun  9 11:28:26.283: INFO: Created: latency-svc-b2m4q
    Jun  9 11:28:26.300: INFO: Got endpoints: latency-svc-b2m4q [325.138931ms]
    Jun  9 11:28:26.306: INFO: Created: latency-svc-nsq8w
    Jun  9 11:28:26.324: INFO: Got endpoints: latency-svc-nsq8w [329.544839ms]
    Jun  9 11:28:26.362: INFO: Created: latency-svc-mlkff
    Jun  9 11:28:26.362: INFO: Created: latency-svc-4h98l
    Jun  9 11:28:26.362: INFO: Got endpoints: latency-svc-4h98l [340.020515ms]
    Jun  9 11:28:26.373: INFO: Got endpoints: latency-svc-mlkff [329.977666ms]
    Jun  9 11:28:26.374: INFO: Created: latency-svc-j7qdm
    Jun  9 11:28:26.396: INFO: Created: latency-svc-4wj24
    Jun  9 11:28:26.396: INFO: Got endpoints: latency-svc-j7qdm [331.224963ms]
    Jun  9 11:28:26.409: INFO: Got endpoints: latency-svc-4wj24 [319.433792ms]
    Jun  9 11:28:26.419: INFO: Created: latency-svc-qp5ck
    Jun  9 11:28:26.443: INFO: Got endpoints: latency-svc-qp5ck [345.798979ms]
    Jun  9 11:28:26.447: INFO: Created: latency-svc-m6x5b
    Jun  9 11:28:26.458: INFO: Got endpoints: latency-svc-m6x5b [332.513839ms]
    Jun  9 11:28:26.478: INFO: Created: latency-svc-mjr6n
    Jun  9 11:28:26.480: INFO: Got endpoints: latency-svc-mjr6n [336.986872ms]
    Jun  9 11:28:26.486: INFO: Created: latency-svc-tnltd
    Jun  9 11:28:26.527: INFO: Got endpoints: latency-svc-tnltd [368.419128ms]
    Jun  9 11:28:26.528: INFO: Created: latency-svc-r2vct
    Jun  9 11:28:26.542: INFO: Got endpoints: latency-svc-r2vct [351.361414ms]
    Jun  9 11:28:26.548: INFO: Created: latency-svc-hfbsk
    Jun  9 11:28:26.597: INFO: Got endpoints: latency-svc-hfbsk [384.30437ms]
    Jun  9 11:28:26.612: INFO: Created: latency-svc-wmf8k
    Jun  9 11:28:26.625: INFO: Got endpoints: latency-svc-wmf8k [384.170851ms]
    Jun  9 11:28:26.638: INFO: Created: latency-svc-8dczn
    Jun  9 11:28:26.664: INFO: Got endpoints: latency-svc-8dczn [413.668154ms]
    Jun  9 11:28:26.665: INFO: Created: latency-svc-jbtdh
    Jun  9 11:28:26.679: INFO: Got endpoints: latency-svc-jbtdh [400.586654ms]
    Jun  9 11:28:26.683: INFO: Created: latency-svc-bbk9c
    Jun  9 11:28:26.697: INFO: Got endpoints: latency-svc-bbk9c [396.953048ms]
    Jun  9 11:28:26.707: INFO: Created: latency-svc-sg96m
    Jun  9 11:28:26.719: INFO: Got endpoints: latency-svc-sg96m [395.58649ms]
    Jun  9 11:28:26.732: INFO: Created: latency-svc-dvrb2
    Jun  9 11:28:26.744: INFO: Got endpoints: latency-svc-dvrb2 [381.792429ms]
    Jun  9 11:28:26.752: INFO: Created: latency-svc-4gv2h
    Jun  9 11:28:26.762: INFO: Got endpoints: latency-svc-4gv2h [388.727718ms]
    Jun  9 11:28:26.771: INFO: Created: latency-svc-vqx8m
    Jun  9 11:28:26.781: INFO: Got endpoints: latency-svc-vqx8m [384.10101ms]
    Jun  9 11:28:26.792: INFO: Created: latency-svc-6r4wx
    Jun  9 11:28:26.804: INFO: Got endpoints: latency-svc-6r4wx [395.202663ms]
    Jun  9 11:28:26.819: INFO: Created: latency-svc-9jfgh
    Jun  9 11:28:26.828: INFO: Got endpoints: latency-svc-9jfgh [385.125006ms]
    Jun  9 11:28:26.846: INFO: Created: latency-svc-z849t
    Jun  9 11:28:26.848: INFO: Got endpoints: latency-svc-z849t [390.062515ms]
    Jun  9 11:28:26.854: INFO: Created: latency-svc-hxfkn
    Jun  9 11:28:26.862: INFO: Got endpoints: latency-svc-hxfkn [381.78948ms]
    Jun  9 11:28:26.867: INFO: Created: latency-svc-lf9rg
    Jun  9 11:28:26.877: INFO: Got endpoints: latency-svc-lf9rg [350.123714ms]
    Jun  9 11:28:26.880: INFO: Created: latency-svc-rmszz
    Jun  9 11:28:26.897: INFO: Created: latency-svc-d8k4f
    Jun  9 11:28:26.936: INFO: Got endpoints: latency-svc-rmszz [393.243308ms]
    Jun  9 11:28:26.945: INFO: Created: latency-svc-mgxgr
    Jun  9 11:28:26.972: INFO: Created: latency-svc-bnr4l
    Jun  9 11:28:26.981: INFO: Got endpoints: latency-svc-d8k4f [384.629914ms]
    Jun  9 11:28:26.995: INFO: Created: latency-svc-nqpr2
    Jun  9 11:28:27.010: INFO: Created: latency-svc-9fqbc
    Jun  9 11:28:27.035: INFO: Got endpoints: latency-svc-mgxgr [410.464398ms]
    Jun  9 11:28:27.036: INFO: Created: latency-svc-r4phk
    Jun  9 11:28:27.059: INFO: Created: latency-svc-j4lcz
    Jun  9 11:28:27.114: INFO: Got endpoints: latency-svc-bnr4l [449.546529ms]
    Jun  9 11:28:27.117: INFO: Created: latency-svc-jmnjh
    Jun  9 11:28:27.136: INFO: Got endpoints: latency-svc-nqpr2 [456.886204ms]
    Jun  9 11:28:27.141: INFO: Created: latency-svc-7kqnc
    Jun  9 11:28:27.159: INFO: Created: latency-svc-k4nsr
    Jun  9 11:28:27.176: INFO: Got endpoints: latency-svc-9fqbc [479.155269ms]
    Jun  9 11:28:27.182: INFO: Created: latency-svc-gsfkd
    Jun  9 11:28:27.201: INFO: Created: latency-svc-5fqsw
    Jun  9 11:28:27.223: INFO: Created: latency-svc-nbhvb
    Jun  9 11:28:27.228: INFO: Got endpoints: latency-svc-r4phk [508.781753ms]
    Jun  9 11:28:27.235: INFO: Created: latency-svc-2rttz
    Jun  9 11:28:27.250: INFO: Created: latency-svc-fvnph
    Jun  9 11:28:27.273: INFO: Created: latency-svc-qg4j4
    Jun  9 11:28:27.275: INFO: Got endpoints: latency-svc-j4lcz [531.143466ms]
    Jun  9 11:28:27.292: INFO: Created: latency-svc-6d98h
    Jun  9 11:28:27.313: INFO: Created: latency-svc-62b5p
    Jun  9 11:28:27.329: INFO: Got endpoints: latency-svc-jmnjh [567.244566ms]
    Jun  9 11:28:27.330: INFO: Created: latency-svc-ldl5v
    Jun  9 11:28:27.357: INFO: Created: latency-svc-s2n8s
    Jun  9 11:28:27.381: INFO: Got endpoints: latency-svc-7kqnc [600.761465ms]
    Jun  9 11:28:27.382: INFO: Created: latency-svc-dvf6b
    Jun  9 11:28:27.404: INFO: Created: latency-svc-gwg8g
    Jun  9 11:28:27.433: INFO: Got endpoints: latency-svc-k4nsr [628.660971ms]
    Jun  9 11:28:27.434: INFO: Created: latency-svc-pxbnf
    Jun  9 11:28:27.474: INFO: Created: latency-svc-qvb28
    Jun  9 11:28:27.512: INFO: Got endpoints: latency-svc-gsfkd [683.642923ms]
    Jun  9 11:28:27.536: INFO: Got endpoints: latency-svc-5fqsw [687.743869ms]
    Jun  9 11:28:27.540: INFO: Created: latency-svc-chl5f
    Jun  9 11:28:27.568: INFO: Created: latency-svc-z9tpj
    Jun  9 11:28:27.593: INFO: Got endpoints: latency-svc-nbhvb [730.770763ms]
    Jun  9 11:28:27.594: INFO: Created: latency-svc-xlcmw
    Jun  9 11:28:27.670: INFO: Got endpoints: latency-svc-2rttz [792.66246ms]
    Jun  9 11:28:27.687: INFO: Created: latency-svc-l2cld
    Jun  9 11:28:27.691: INFO: Got endpoints: latency-svc-fvnph [755.33776ms]
    Jun  9 11:28:27.746: INFO: Got endpoints: latency-svc-qg4j4 [764.159342ms]
    Jun  9 11:28:27.747: INFO: Created: latency-svc-z86bj
    Jun  9 11:28:27.769: INFO: Created: latency-svc-bqj47
    Jun  9 11:28:27.786: INFO: Got endpoints: latency-svc-6d98h [751.135821ms]
    Jun  9 11:28:27.797: INFO: Created: latency-svc-9rfln
    Jun  9 11:28:27.828: INFO: Got endpoints: latency-svc-62b5p [714.200806ms]
    Jun  9 11:28:27.831: INFO: Created: latency-svc-fk2xd
    Jun  9 11:28:27.867: INFO: Created: latency-svc-njt8j
    Jun  9 11:28:27.877: INFO: Got endpoints: latency-svc-ldl5v [740.776915ms]
    Jun  9 11:28:27.938: INFO: Created: latency-svc-89l2q
    Jun  9 11:28:27.945: INFO: Got endpoints: latency-svc-s2n8s [768.961833ms]
    Jun  9 11:28:27.976: INFO: Got endpoints: latency-svc-dvf6b [747.949137ms]
    Jun  9 11:28:27.983: INFO: Created: latency-svc-xzpmg
    Jun  9 11:28:28.019: INFO: Created: latency-svc-wg5wt
    Jun  9 11:28:28.027: INFO: Got endpoints: latency-svc-gwg8g [751.665117ms]
    Jun  9 11:28:28.061: INFO: Created: latency-svc-bmbdg
    Jun  9 11:28:28.077: INFO: Got endpoints: latency-svc-pxbnf [747.66453ms]
    Jun  9 11:28:28.104: INFO: Created: latency-svc-vx4rs
    Jun  9 11:28:28.144: INFO: Got endpoints: latency-svc-qvb28 [762.116048ms]
    Jun  9 11:28:28.190: INFO: Got endpoints: latency-svc-chl5f [757.225922ms]
    Jun  9 11:28:28.193: INFO: Created: latency-svc-kw5rf
    Jun  9 11:28:28.216: INFO: Created: latency-svc-qbq8x
    Jun  9 11:28:28.223: INFO: Got endpoints: latency-svc-z9tpj [710.921016ms]
    Jun  9 11:28:28.246: INFO: Created: latency-svc-ff5th
    Jun  9 11:28:28.276: INFO: Got endpoints: latency-svc-xlcmw [740.04116ms]
    Jun  9 11:28:28.300: INFO: Created: latency-svc-hglp9
    Jun  9 11:28:28.324: INFO: Got endpoints: latency-svc-l2cld [731.30483ms]
    Jun  9 11:28:28.348: INFO: Created: latency-svc-44zf9
    Jun  9 11:28:28.374: INFO: Got endpoints: latency-svc-z86bj [704.419522ms]
    Jun  9 11:28:28.402: INFO: Created: latency-svc-j2bbm
    Jun  9 11:28:28.423: INFO: Got endpoints: latency-svc-bqj47 [731.561556ms]
    Jun  9 11:28:28.443: INFO: Created: latency-svc-mrtws
    Jun  9 11:28:28.510: INFO: Got endpoints: latency-svc-9rfln [764.617546ms]
    Jun  9 11:28:28.533: INFO: Got endpoints: latency-svc-fk2xd [746.881433ms]
    Jun  9 11:28:28.559: INFO: Created: latency-svc-xpjtg
    Jun  9 11:28:28.588: INFO: Got endpoints: latency-svc-njt8j [759.561705ms]
    Jun  9 11:28:28.588: INFO: Created: latency-svc-rsd94
    Jun  9 11:28:28.634: INFO: Got endpoints: latency-svc-89l2q [757.031953ms]
    Jun  9 11:28:28.635: INFO: Created: latency-svc-p8m8k
    Jun  9 11:28:28.662: INFO: Created: latency-svc-bchrb
    Jun  9 11:28:28.674: INFO: Got endpoints: latency-svc-xzpmg [728.697452ms]
    Jun  9 11:28:28.715: INFO: Created: latency-svc-b4x25
    Jun  9 11:28:28.723: INFO: Got endpoints: latency-svc-wg5wt [746.515025ms]
    Jun  9 11:28:28.752: INFO: Created: latency-svc-zddzj
    Jun  9 11:28:28.777: INFO: Got endpoints: latency-svc-bmbdg [749.742209ms]
    Jun  9 11:28:28.807: INFO: Created: latency-svc-2kwfm
    Jun  9 11:28:28.829: INFO: Got endpoints: latency-svc-vx4rs [752.375379ms]
    Jun  9 11:28:28.858: INFO: Created: latency-svc-v2x8x
    Jun  9 11:28:28.874: INFO: Got endpoints: latency-svc-kw5rf [730.274231ms]
    Jun  9 11:28:28.903: INFO: Created: latency-svc-v9vrk
    Jun  9 11:28:28.925: INFO: Got endpoints: latency-svc-qbq8x [734.585447ms]
    Jun  9 11:28:28.951: INFO: Created: latency-svc-llz8j
    Jun  9 11:28:28.974: INFO: Got endpoints: latency-svc-ff5th [750.698005ms]
    Jun  9 11:28:28.998: INFO: Created: latency-svc-55nx4
    Jun  9 11:28:29.027: INFO: Got endpoints: latency-svc-hglp9 [751.08236ms]
    Jun  9 11:28:29.064: INFO: Created: latency-svc-h9rhf
    Jun  9 11:28:29.075: INFO: Got endpoints: latency-svc-44zf9 [751.052723ms]
    Jun  9 11:28:29.111: INFO: Created: latency-svc-m29n9
    Jun  9 11:28:29.128: INFO: Got endpoints: latency-svc-j2bbm [753.70208ms]
    Jun  9 11:28:29.163: INFO: Created: latency-svc-pjw68
    Jun  9 11:28:29.177: INFO: Got endpoints: latency-svc-mrtws [754.420471ms]
    Jun  9 11:28:29.208: INFO: Created: latency-svc-d4nnj
    Jun  9 11:28:29.241: INFO: Got endpoints: latency-svc-xpjtg [730.943502ms]
    Jun  9 11:28:29.271: INFO: Created: latency-svc-sq4hm
    Jun  9 11:28:29.274: INFO: Got endpoints: latency-svc-rsd94 [740.711871ms]
    Jun  9 11:28:29.313: INFO: Created: latency-svc-zdjmz
    Jun  9 11:28:29.328: INFO: Got endpoints: latency-svc-p8m8k [740.56603ms]
    Jun  9 11:28:29.358: INFO: Created: latency-svc-8ppr4
    Jun  9 11:28:29.381: INFO: Got endpoints: latency-svc-bchrb [746.573673ms]
    Jun  9 11:28:29.406: INFO: Created: latency-svc-bzn9k
    Jun  9 11:28:29.433: INFO: Got endpoints: latency-svc-b4x25 [759.004942ms]
    Jun  9 11:28:29.453: INFO: Created: latency-svc-7fl58
    Jun  9 11:28:29.501: INFO: Got endpoints: latency-svc-zddzj [778.001586ms]
    Jun  9 11:28:29.531: INFO: Got endpoints: latency-svc-2kwfm [753.629004ms]
    Jun  9 11:28:29.533: INFO: Created: latency-svc-hfvm6
    Jun  9 11:28:29.567: INFO: Created: latency-svc-czjqw
    Jun  9 11:28:29.576: INFO: Got endpoints: latency-svc-v2x8x [746.574022ms]
    Jun  9 11:28:29.612: INFO: Created: latency-svc-khpv4
    Jun  9 11:28:29.641: INFO: Got endpoints: latency-svc-v9vrk [767.148405ms]
    Jun  9 11:28:29.668: INFO: Created: latency-svc-8ktnf
    Jun  9 11:28:29.685: INFO: Got endpoints: latency-svc-llz8j [760.050761ms]
    Jun  9 11:28:29.733: INFO: Created: latency-svc-s422j
    Jun  9 11:28:29.744: INFO: Got endpoints: latency-svc-55nx4 [769.898407ms]
    Jun  9 11:28:29.904: INFO: Got endpoints: latency-svc-h9rhf [876.830095ms]
    Jun  9 11:28:29.912: INFO: Created: latency-svc-bfq7b
    Jun  9 11:28:29.913: INFO: Got endpoints: latency-svc-pjw68 [784.178628ms]
    Jun  9 11:28:29.913: INFO: Got endpoints: latency-svc-m29n9 [837.565413ms]
    Jun  9 11:28:29.953: INFO: Got endpoints: latency-svc-d4nnj [776.031654ms]
    Jun  9 11:28:29.963: INFO: Created: latency-svc-nrh5m
    Jun  9 11:28:29.984: INFO: Got endpoints: latency-svc-sq4hm [742.417447ms]
    Jun  9 11:28:29.994: INFO: Created: latency-svc-5d4hv
    Jun  9 11:28:30.054: INFO: Got endpoints: latency-svc-zdjmz [779.409177ms]
    Jun  9 11:28:30.054: INFO: Created: latency-svc-lhq7r
    Jun  9 11:28:30.090: INFO: Got endpoints: latency-svc-8ppr4 [761.656557ms]
    Jun  9 11:28:30.101: INFO: Created: latency-svc-s9z66
    Jun  9 11:28:30.164: INFO: Got endpoints: latency-svc-bzn9k [783.524272ms]
    Jun  9 11:28:30.187: INFO: Created: latency-svc-nfzgx
    Jun  9 11:28:30.187: INFO: Got endpoints: latency-svc-7fl58 [754.069996ms]
    Jun  9 11:28:30.272: INFO: Got endpoints: latency-svc-hfvm6 [770.789561ms]
    Jun  9 11:28:30.294: INFO: Got endpoints: latency-svc-czjqw [763.142849ms]
    Jun  9 11:28:30.295: INFO: Created: latency-svc-wx5ts
    Jun  9 11:28:30.388: INFO: Got endpoints: latency-svc-8ktnf [746.874125ms]
    Jun  9 11:28:30.388: INFO: Created: latency-svc-wgm4q
    Jun  9 11:28:30.388: INFO: Got endpoints: latency-svc-khpv4 [812.405579ms]
    Jun  9 11:28:30.433: INFO: Created: latency-svc-kf7cd
    Jun  9 11:28:30.439: INFO: Got endpoints: latency-svc-s422j [754.218917ms]
    Jun  9 11:28:30.489: INFO: Created: latency-svc-kk844
    Jun  9 11:28:30.504: INFO: Got endpoints: latency-svc-bfq7b [759.971805ms]
    Jun  9 11:28:30.545: INFO: Created: latency-svc-f2dl6
    Jun  9 11:28:30.584: INFO: Got endpoints: latency-svc-nrh5m [680.024587ms]
    Jun  9 11:28:30.635: INFO: Got endpoints: latency-svc-lhq7r [722.505914ms]
    Jun  9 11:28:30.659: INFO: Created: latency-svc-rj9vd
    Jun  9 11:28:30.665: INFO: Got endpoints: latency-svc-5d4hv [752.079289ms]
    Jun  9 11:28:30.700: INFO: Got endpoints: latency-svc-s9z66 [746.75918ms]
    Jun  9 11:28:30.754: INFO: Got endpoints: latency-svc-nfzgx [769.886146ms]
    Jun  9 11:28:30.785: INFO: Created: latency-svc-xpdbn
    Jun  9 11:28:30.800: INFO: Got endpoints: latency-svc-wx5ts [746.195158ms]
    Jun  9 11:28:30.847: INFO: Got endpoints: latency-svc-wgm4q [756.856744ms]
    Jun  9 11:28:30.876: INFO: Created: latency-svc-mggml
    Jun  9 11:28:30.892: INFO: Got endpoints: latency-svc-kf7cd [727.86752ms]
    Jun  9 11:28:30.936: INFO: Got endpoints: latency-svc-kk844 [748.0918ms]
    Jun  9 11:28:30.936: INFO: Created: latency-svc-2kqvd
    Jun  9 11:28:30.950: INFO: Created: latency-svc-gzk5t
    Jun  9 11:28:31.000: INFO: Got endpoints: latency-svc-f2dl6 [728.487512ms]
    Jun  9 11:28:31.006: INFO: Created: latency-svc-jgqq7
    Jun  9 11:28:31.034: INFO: Got endpoints: latency-svc-rj9vd [740.283207ms]
    Jun  9 11:28:31.061: INFO: Created: latency-svc-49z48
    Jun  9 11:28:31.174: INFO: Got endpoints: latency-svc-mggml [785.893882ms]
    Jun  9 11:28:31.176: INFO: Got endpoints: latency-svc-xpdbn [788.091521ms]
    Jun  9 11:28:31.180: INFO: Got endpoints: latency-svc-2kqvd [740.85044ms]
    Jun  9 11:28:31.186: INFO: Created: latency-svc-4b4hw
    Jun  9 11:28:31.199: INFO: Created: latency-svc-75zn4
    Jun  9 11:28:31.239: INFO: Created: latency-svc-972lq
    Jun  9 11:28:31.245: INFO: Got endpoints: latency-svc-gzk5t [741.187261ms]
    Jun  9 11:28:31.309: INFO: Got endpoints: latency-svc-jgqq7 [724.920685ms]
    Jun  9 11:28:31.310: INFO: Created: latency-svc-b6m5m
    Jun  9 11:28:31.329: INFO: Got endpoints: latency-svc-49z48 [694.023551ms]
    Jun  9 11:28:31.336: INFO: Created: latency-svc-86z9s
    Jun  9 11:28:31.357: INFO: Created: latency-svc-8xszz
    Jun  9 11:28:31.401: INFO: Got endpoints: latency-svc-4b4hw [736.259822ms]
    Jun  9 11:28:31.409: INFO: Created: latency-svc-79htq
    Jun  9 11:28:31.424: INFO: Got endpoints: latency-svc-75zn4 [724.154976ms]
    Jun  9 11:28:31.424: INFO: Created: latency-svc-56pkt
    Jun  9 11:28:31.441: INFO: Created: latency-svc-pmwzh
    Jun  9 11:28:31.461: INFO: Created: latency-svc-kzs4l
    Jun  9 11:28:31.497: INFO: Got endpoints: latency-svc-972lq [743.079958ms]
    Jun  9 11:28:31.510: INFO: Created: latency-svc-rp2qx
    Jun  9 11:28:31.530: INFO: Got endpoints: latency-svc-b6m5m [730.042137ms]
    Jun  9 11:28:31.536: INFO: Created: latency-svc-xfbsd
    Jun  9 11:28:31.602: INFO: Got endpoints: latency-svc-86z9s [755.271079ms]
    Jun  9 11:28:31.608: INFO: Created: latency-svc-7d6tc
    Jun  9 11:28:31.647: INFO: Got endpoints: latency-svc-8xszz [754.946133ms]
    Jun  9 11:28:31.648: INFO: Created: latency-svc-5t9rf
    Jun  9 11:28:31.679: INFO: Created: latency-svc-wzb6q
    Jun  9 11:28:31.679: INFO: Got endpoints: latency-svc-79htq [743.269147ms]
    Jun  9 11:28:31.711: INFO: Created: latency-svc-5kgxl
    Jun  9 11:28:31.730: INFO: Got endpoints: latency-svc-56pkt [730.003717ms]
    Jun  9 11:28:31.732: INFO: Created: latency-svc-5lklt
    Jun  9 11:28:31.757: INFO: Created: latency-svc-mm8pf
    Jun  9 11:28:31.763: INFO: Created: latency-svc-2cftj
    Jun  9 11:28:31.776: INFO: Got endpoints: latency-svc-pmwzh [741.401968ms]
    Jun  9 11:28:31.781: INFO: Created: latency-svc-nfnfd
    Jun  9 11:28:31.804: INFO: Created: latency-svc-r5ngg
    Jun  9 11:28:31.822: INFO: Created: latency-svc-7q6r9
    Jun  9 11:28:31.830: INFO: Got endpoints: latency-svc-kzs4l [655.920029ms]
    Jun  9 11:28:31.836: INFO: Created: latency-svc-fm7q6
    Jun  9 11:28:31.855: INFO: Created: latency-svc-dfj5p
    Jun  9 11:28:31.869: INFO: Created: latency-svc-26gvc
    Jun  9 11:28:31.875: INFO: Got endpoints: latency-svc-rp2qx [698.240736ms]
    Jun  9 11:28:31.903: INFO: Created: latency-svc-vrqgg
    Jun  9 11:28:31.927: INFO: Got endpoints: latency-svc-xfbsd [746.77212ms]
    Jun  9 11:28:31.948: INFO: Created: latency-svc-dzxfp
    Jun  9 11:28:31.975: INFO: Got endpoints: latency-svc-7d6tc [730.309599ms]
    Jun  9 11:28:32.014: INFO: Created: latency-svc-dhvsq
    Jun  9 11:28:32.025: INFO: Got endpoints: latency-svc-5t9rf [715.779281ms]
    Jun  9 11:28:32.048: INFO: Created: latency-svc-7dkgn
    Jun  9 11:28:32.073: INFO: Got endpoints: latency-svc-wzb6q [743.395851ms]
    Jun  9 11:28:32.097: INFO: Created: latency-svc-8m9wl
    Jun  9 11:28:32.123: INFO: Got endpoints: latency-svc-5kgxl [722.054469ms]
    Jun  9 11:28:32.208: INFO: Got endpoints: latency-svc-5lklt [783.411951ms]
    Jun  9 11:28:32.208: INFO: Created: latency-svc-cxmxw
    Jun  9 11:28:32.269: INFO: Got endpoints: latency-svc-mm8pf [771.958639ms]
    Jun  9 11:28:32.272: INFO: Created: latency-svc-tnzrh
    Jun  9 11:28:32.296: INFO: Got endpoints: latency-svc-2cftj [766.24888ms]
    Jun  9 11:28:32.327: INFO: Got endpoints: latency-svc-nfnfd [724.654708ms]
    Jun  9 11:28:32.328: INFO: Created: latency-svc-qx6w5
    Jun  9 11:28:32.344: INFO: Created: latency-svc-mmjgx
    Jun  9 11:28:32.348: INFO: Created: latency-svc-znl9g
    Jun  9 11:28:32.437: INFO: Got endpoints: latency-svc-7q6r9 [758.326469ms]
    Jun  9 11:28:32.438: INFO: Got endpoints: latency-svc-r5ngg [790.461737ms]
    Jun  9 11:28:32.462: INFO: Created: latency-svc-b6qmj
    Jun  9 11:28:32.489: INFO: Got endpoints: latency-svc-fm7q6 [758.711897ms]
    Jun  9 11:28:32.490: INFO: Created: latency-svc-6tbpx
    Jun  9 11:28:32.513: INFO: Created: latency-svc-kw82m
    Jun  9 11:28:32.530: INFO: Got endpoints: latency-svc-dfj5p [753.693943ms]
    Jun  9 11:28:32.554: INFO: Created: latency-svc-2f577
    Jun  9 11:28:32.577: INFO: Got endpoints: latency-svc-26gvc [746.982107ms]
    Jun  9 11:28:32.606: INFO: Created: latency-svc-89kkk
    Jun  9 11:28:32.629: INFO: Got endpoints: latency-svc-vrqgg [754.111019ms]
    Jun  9 11:28:32.658: INFO: Created: latency-svc-q8bww
    Jun  9 11:28:32.676: INFO: Got endpoints: latency-svc-dzxfp [748.835839ms]
    Jun  9 11:28:32.700: INFO: Created: latency-svc-2jlkp
    Jun  9 11:28:32.724: INFO: Got endpoints: latency-svc-dhvsq [748.814357ms]
    Jun  9 11:28:32.752: INFO: Created: latency-svc-zzrz2
    Jun  9 11:28:32.776: INFO: Got endpoints: latency-svc-7dkgn [751.222784ms]
    Jun  9 11:28:32.798: INFO: Created: latency-svc-mq569
    Jun  9 11:28:32.822: INFO: Got endpoints: latency-svc-8m9wl [749.071292ms]
    Jun  9 11:28:32.841: INFO: Created: latency-svc-qf8tx
    Jun  9 11:28:32.873: INFO: Got endpoints: latency-svc-cxmxw [749.360732ms]
    Jun  9 11:28:32.901: INFO: Created: latency-svc-cnnvg
    Jun  9 11:28:32.925: INFO: Got endpoints: latency-svc-tnzrh [717.413331ms]
    Jun  9 11:28:32.948: INFO: Created: latency-svc-dgcq9
    Jun  9 11:28:32.980: INFO: Got endpoints: latency-svc-qx6w5 [711.267729ms]
    Jun  9 11:28:33.019: INFO: Created: latency-svc-rbccf
    Jun  9 11:28:33.029: INFO: Got endpoints: latency-svc-mmjgx [732.721751ms]
    Jun  9 11:28:33.054: INFO: Created: latency-svc-45jnk
    Jun  9 11:28:33.073: INFO: Got endpoints: latency-svc-znl9g [746.374191ms]
    Jun  9 11:28:33.104: INFO: Created: latency-svc-czzc4
    Jun  9 11:28:33.128: INFO: Got endpoints: latency-svc-b6qmj [690.502835ms]
    Jun  9 11:28:33.150: INFO: Created: latency-svc-sjhz2
    Jun  9 11:28:33.173: INFO: Got endpoints: latency-svc-6tbpx [735.461326ms]
    Jun  9 11:28:33.199: INFO: Created: latency-svc-5xqfx
    Jun  9 11:28:33.225: INFO: Got endpoints: latency-svc-kw82m [735.789164ms]
    Jun  9 11:28:33.246: INFO: Created: latency-svc-4cdv4
    Jun  9 11:28:33.276: INFO: Got endpoints: latency-svc-2f577 [746.578432ms]
    Jun  9 11:28:33.308: INFO: Created: latency-svc-b82hs
    Jun  9 11:28:33.328: INFO: Got endpoints: latency-svc-89kkk [750.746561ms]
    Jun  9 11:28:33.380: INFO: Got endpoints: latency-svc-q8bww [750.629341ms]
    Jun  9 11:28:33.431: INFO: Got endpoints: latency-svc-2jlkp [755.06725ms]
    Jun  9 11:28:33.473: INFO: Got endpoints: latency-svc-zzrz2 [749.093122ms]
    Jun  9 11:28:33.533: INFO: Got endpoints: latency-svc-mq569 [756.817555ms]
    Jun  9 11:28:33.585: INFO: Got endpoints: latency-svc-qf8tx [763.28279ms]
    Jun  9 11:28:33.631: INFO: Got endpoints: latency-svc-cnnvg [758.617773ms]
    Jun  9 11:28:33.677: INFO: Got endpoints: latency-svc-dgcq9 [752.059411ms]
    Jun  9 11:28:33.729: INFO: Got endpoints: latency-svc-rbccf [748.972885ms]
    Jun  9 11:28:33.789: INFO: Got endpoints: latency-svc-45jnk [759.654626ms]
    Jun  9 11:28:33.833: INFO: Got endpoints: latency-svc-czzc4 [759.274385ms]
    Jun  9 11:28:33.887: INFO: Got endpoints: latency-svc-sjhz2 [758.823314ms]
    Jun  9 11:28:33.932: INFO: Got endpoints: latency-svc-5xqfx [758.828631ms]
    Jun  9 11:28:33.984: INFO: Got endpoints: latency-svc-4cdv4 [758.755745ms]
    Jun  9 11:28:34.029: INFO: Got endpoints: latency-svc-b82hs [752.74795ms]
    Jun  9 11:28:34.029: INFO: Latencies: [57.003076ms 84.499ms 126.898651ms 162.987275ms 201.834322ms 223.418609ms 246.473708ms 259.539471ms 280.502587ms 299.425769ms 304.047868ms 319.433792ms 320.804338ms 320.940646ms 325.138931ms 327.05829ms 329.544839ms 329.977666ms 330.397136ms 331.224963ms 332.513839ms 333.831481ms 334.186028ms 336.986872ms 337.333152ms 337.905431ms 340.020515ms 340.586205ms 341.944387ms 343.124073ms 345.798979ms 347.574231ms 349.931808ms 350.123714ms 351.263436ms 351.361414ms 354.056889ms 368.419128ms 371.229105ms 378.83913ms 381.78948ms 381.792429ms 384.10101ms 384.170851ms 384.30437ms 384.629914ms 385.125006ms 388.727718ms 390.062515ms 393.243308ms 395.202663ms 395.58649ms 396.092829ms 396.953048ms 400.586654ms 410.464398ms 413.012441ms 413.440936ms 413.668154ms 427.993497ms 449.546529ms 456.886204ms 479.155269ms 508.781753ms 531.143466ms 567.244566ms 600.761465ms 628.660971ms 655.920029ms 680.024587ms 683.642923ms 687.743869ms 690.502835ms 694.023551ms 698.240736ms 704.419522ms 710.921016ms 711.267729ms 714.200806ms 715.779281ms 717.413331ms 722.054469ms 722.505914ms 724.154976ms 724.654708ms 724.920685ms 727.86752ms 728.487512ms 728.697452ms 730.003717ms 730.042137ms 730.274231ms 730.309599ms 730.770763ms 730.943502ms 731.30483ms 731.561556ms 732.721751ms 734.585447ms 735.461326ms 735.789164ms 736.259822ms 740.04116ms 740.283207ms 740.56603ms 740.711871ms 740.776915ms 740.85044ms 741.187261ms 741.401968ms 742.417447ms 743.079958ms 743.269147ms 743.395851ms 746.195158ms 746.374191ms 746.515025ms 746.573673ms 746.574022ms 746.578432ms 746.75918ms 746.77212ms 746.874125ms 746.881433ms 746.982107ms 747.66453ms 747.949137ms 748.0918ms 748.814357ms 748.835839ms 748.972885ms 749.071292ms 749.093122ms 749.360732ms 749.742209ms 750.629341ms 750.698005ms 750.746561ms 751.052723ms 751.08236ms 751.135821ms 751.222784ms 751.665117ms 752.059411ms 752.079289ms 752.375379ms 752.74795ms 753.629004ms 753.693943ms 753.70208ms 754.069996ms 754.111019ms 754.218917ms 754.420471ms 754.946133ms 755.06725ms 755.271079ms 755.33776ms 756.817555ms 756.856744ms 757.031953ms 757.225922ms 758.326469ms 758.617773ms 758.711897ms 758.755745ms 758.823314ms 758.828631ms 759.004942ms 759.274385ms 759.561705ms 759.654626ms 759.971805ms 760.050761ms 761.656557ms 762.116048ms 763.142849ms 763.28279ms 764.159342ms 764.617546ms 766.24888ms 767.148405ms 768.961833ms 769.886146ms 769.898407ms 770.789561ms 771.958639ms 776.031654ms 778.001586ms 779.409177ms 783.411951ms 783.524272ms 784.178628ms 785.893882ms 788.091521ms 790.461737ms 792.66246ms 812.405579ms 837.565413ms 876.830095ms]
    Jun  9 11:28:34.029: INFO: 50 %ile: 735.789164ms
    Jun  9 11:28:34.029: INFO: 90 %ile: 766.24888ms
    Jun  9 11:28:34.029: INFO: 99 %ile: 837.565413ms
    Jun  9 11:28:34.029: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:28:34.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-7147" for this suite. 06/09/23 11:28:34.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:28:34.056
Jun  9 11:28:34.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 11:28:34.058
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:28:34.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:28:34.097
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 11:28:34.133
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:28:34.56
STEP: Deploying the webhook pod 06/09/23 11:28:34.581
STEP: Wait for the deployment to be ready 06/09/23 11:28:34.613
Jun  9 11:28:34.645: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 11:28:36.666
STEP: Verifying the service has paired with the endpoint 06/09/23 11:28:36.693
Jun  9 11:28:37.693: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 06/09/23 11:28:37.699
STEP: Creating a configMap that does not comply to the validation webhook rules 06/09/23 11:28:37.723
STEP: Updating a validating webhook configuration's rules to not include the create operation 06/09/23 11:28:37.738
STEP: Creating a configMap that does not comply to the validation webhook rules 06/09/23 11:28:37.756
STEP: Patching a validating webhook configuration's rules to include the create operation 06/09/23 11:28:37.782
STEP: Creating a configMap that does not comply to the validation webhook rules 06/09/23 11:28:37.798
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:28:37.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4348" for this suite. 06/09/23 11:28:37.934
STEP: Destroying namespace "webhook-4348-markers" for this suite. 06/09/23 11:28:37.962
------------------------------
• [3.920 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:28:34.056
    Jun  9 11:28:34.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 11:28:34.058
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:28:34.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:28:34.097
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 11:28:34.133
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:28:34.56
    STEP: Deploying the webhook pod 06/09/23 11:28:34.581
    STEP: Wait for the deployment to be ready 06/09/23 11:28:34.613
    Jun  9 11:28:34.645: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 11:28:36.666
    STEP: Verifying the service has paired with the endpoint 06/09/23 11:28:36.693
    Jun  9 11:28:37.693: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 06/09/23 11:28:37.699
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/09/23 11:28:37.723
    STEP: Updating a validating webhook configuration's rules to not include the create operation 06/09/23 11:28:37.738
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/09/23 11:28:37.756
    STEP: Patching a validating webhook configuration's rules to include the create operation 06/09/23 11:28:37.782
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/09/23 11:28:37.798
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:28:37.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4348" for this suite. 06/09/23 11:28:37.934
    STEP: Destroying namespace "webhook-4348-markers" for this suite. 06/09/23 11:28:37.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:28:37.977
Jun  9 11:28:37.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename job 06/09/23 11:28:37.978
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:28:38.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:28:38.036
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 06/09/23 11:28:38.049
STEP: Ensuring job reaches completions 06/09/23 11:28:38.06
STEP: Ensuring pods with index for job exist 06/09/23 11:28:50.088
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun  9 11:28:50.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2416" for this suite. 06/09/23 11:28:50.148
------------------------------
• [SLOW TEST] [12.253 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:28:37.977
    Jun  9 11:28:37.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename job 06/09/23 11:28:37.978
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:28:38.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:28:38.036
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 06/09/23 11:28:38.049
    STEP: Ensuring job reaches completions 06/09/23 11:28:38.06
    STEP: Ensuring pods with index for job exist 06/09/23 11:28:50.088
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:28:50.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2416" for this suite. 06/09/23 11:28:50.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:28:50.231
Jun  9 11:28:50.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename cronjob 06/09/23 11:28:50.233
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:28:50.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:28:50.397
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 06/09/23 11:28:50.405
STEP: Ensuring a job is scheduled 06/09/23 11:28:50.42
STEP: Ensuring exactly one is scheduled 06/09/23 11:29:00.432
STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/09/23 11:29:00.484
STEP: Ensuring the job is replaced with a new one 06/09/23 11:29:00.516
STEP: Removing cronjob 06/09/23 11:30:00.568
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun  9 11:30:00.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5816" for this suite. 06/09/23 11:30:00.665
------------------------------
• [SLOW TEST] [70.631 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:28:50.231
    Jun  9 11:28:50.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename cronjob 06/09/23 11:28:50.233
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:28:50.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:28:50.397
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 06/09/23 11:28:50.405
    STEP: Ensuring a job is scheduled 06/09/23 11:28:50.42
    STEP: Ensuring exactly one is scheduled 06/09/23 11:29:00.432
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/09/23 11:29:00.484
    STEP: Ensuring the job is replaced with a new one 06/09/23 11:29:00.516
    STEP: Removing cronjob 06/09/23 11:30:00.568
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:30:00.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5816" for this suite. 06/09/23 11:30:00.665
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:30:00.862
Jun  9 11:30:00.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename cronjob 06/09/23 11:30:00.866
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:30:01.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:30:01.165
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 06/09/23 11:30:01.172
STEP: Ensuring no jobs are scheduled 06/09/23 11:30:01.246
STEP: Ensuring no job exists by listing jobs explicitly 06/09/23 11:35:01.258
STEP: Removing cronjob 06/09/23 11:35:01.265
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun  9 11:35:01.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5172" for this suite. 06/09/23 11:35:01.287
------------------------------
• [SLOW TEST] [300.439 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:30:00.862
    Jun  9 11:30:00.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename cronjob 06/09/23 11:30:00.866
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:30:01.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:30:01.165
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 06/09/23 11:30:01.172
    STEP: Ensuring no jobs are scheduled 06/09/23 11:30:01.246
    STEP: Ensuring no job exists by listing jobs explicitly 06/09/23 11:35:01.258
    STEP: Removing cronjob 06/09/23 11:35:01.265
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:35:01.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5172" for this suite. 06/09/23 11:35:01.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:35:01.305
Jun  9 11:35:01.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename deployment 06/09/23 11:35:01.307
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:01.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:01.347
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 06/09/23 11:35:01.36
STEP: waiting for Deployment to be created 06/09/23 11:35:01.37
STEP: waiting for all Replicas to be Ready 06/09/23 11:35:01.374
Jun  9 11:35:01.376: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  9 11:35:01.376: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  9 11:35:01.387: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  9 11:35:01.387: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  9 11:35:01.415: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  9 11:35:01.415: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  9 11:35:01.468: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  9 11:35:01.468: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  9 11:35:02.722: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun  9 11:35:02.722: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun  9 11:35:03.321: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 06/09/23 11:35:03.321
W0609 11:35:03.338637      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun  9 11:35:03.341: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 06/09/23 11:35:03.342
Jun  9 11:35:03.344: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
Jun  9 11:35:03.344: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
Jun  9 11:35:03.344: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
Jun  9 11:35:03.344: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
Jun  9 11:35:03.363: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
Jun  9 11:35:03.363: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
Jun  9 11:35:03.415: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
Jun  9 11:35:03.415: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
Jun  9 11:35:03.422: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
Jun  9 11:35:03.422: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
Jun  9 11:35:03.431: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
Jun  9 11:35:03.431: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
Jun  9 11:35:04.780: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
Jun  9 11:35:04.780: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
Jun  9 11:35:04.992: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
STEP: listing Deployments 06/09/23 11:35:04.992
Jun  9 11:35:05.001: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 06/09/23 11:35:05.001
Jun  9 11:35:05.035: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 06/09/23 11:35:05.035
Jun  9 11:35:05.064: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  9 11:35:05.096: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  9 11:35:05.188: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  9 11:35:05.223: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  9 11:35:05.298: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  9 11:35:06.465: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun  9 11:35:07.336: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jun  9 11:35:07.383: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun  9 11:35:07.413: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun  9 11:35:09.780: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 06/09/23 11:35:09.812
STEP: fetching the DeploymentStatus 06/09/23 11:35:09.832
Jun  9 11:35:09.844: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
Jun  9 11:35:09.844: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
Jun  9 11:35:09.844: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
Jun  9 11:35:09.844: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
Jun  9 11:35:09.844: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
Jun  9 11:35:09.844: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
Jun  9 11:35:09.845: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 3
Jun  9 11:35:09.845: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
Jun  9 11:35:09.845: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
Jun  9 11:35:09.845: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 3
STEP: deleting the Deployment 06/09/23 11:35:09.845
Jun  9 11:35:09.866: INFO: observed event type MODIFIED
Jun  9 11:35:09.866: INFO: observed event type MODIFIED
Jun  9 11:35:09.866: INFO: observed event type MODIFIED
Jun  9 11:35:09.866: INFO: observed event type MODIFIED
Jun  9 11:35:09.866: INFO: observed event type MODIFIED
Jun  9 11:35:09.866: INFO: observed event type MODIFIED
Jun  9 11:35:09.866: INFO: observed event type MODIFIED
Jun  9 11:35:09.866: INFO: observed event type MODIFIED
Jun  9 11:35:09.867: INFO: observed event type MODIFIED
Jun  9 11:35:09.867: INFO: observed event type MODIFIED
Jun  9 11:35:09.867: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  9 11:35:09.881: INFO: Log out all the ReplicaSets if there is no deployment created
Jun  9 11:35:09.888: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-8951  e0a206e4-2114-40a4-a819-aaace9cd3bde 110446 2 2023-06-09 11:35:05 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 99d885f1-9b4c-42f4-9659-26049355ac03 0xc003c3da67 0xc003c3da68}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d885f1-9b4c-42f4-9659-26049355ac03\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:35:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c3dc00 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jun  9 11:35:09.899: INFO: pod: "test-deployment-7b7876f9d6-mcl55":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-mcl55 test-deployment-7b7876f9d6- deployment-8951  64c3cfb8-b643-48ac-abb8-f9f8c35e8b95 110445 0 2023-06-09 11:35:07 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:cb72ead30c3197b9c5984f33b4522493074e11e707eb30e55b819ce0ba20404c cni.projectcalico.org/podIP:172.27.53.83/32 cni.projectcalico.org/podIPs:172.27.53.83/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 e0a206e4-2114-40a4-a819-aaace9cd3bde 0xc005f34477 0xc005f34478}] [] [{kube-controller-manager Update v1 2023-06-09 11:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a206e4-2114-40a4-a819-aaace9cd3bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.27.53.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-25dcj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-25dcj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.27.53.83,StartTime:2023-06-09 11:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:35:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://315bb787c38ead299af7b407a99627fa0241a2b9e5f4df8aafe7397bdda09c85,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.27.53.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun  9 11:35:09.899: INFO: pod: "test-deployment-7b7876f9d6-ml8f7":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-ml8f7 test-deployment-7b7876f9d6- deployment-8951  e2c98313-931c-4c3e-ab1a-93f98ae85946 110406 0 2023-06-09 11:35:05 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:bb417b9e935e3add0de1ade7dfb73aee9bd80908f49a4a9da2f8e8a00c364108 cni.projectcalico.org/podIP:172.26.90.25/32 cni.projectcalico.org/podIPs:172.26.90.25/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 e0a206e4-2114-40a4-a819-aaace9cd3bde 0xc005f34777 0xc005f34778}] [] [{kube-controller-manager Update v1 2023-06-09 11:35:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a206e4-2114-40a4-a819-aaace9cd3bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:35:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.90.25\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8bsd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8bsd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.26.90.25,StartTime:2023-06-09 11:35:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:35:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://53abb86537941aa8e39a54db38a2dff338523747f1f858749a66345470833323,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.90.25,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun  9 11:35:09.899: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-8951  1ccf76fe-41d8-4c1d-8a2c-b9dc23a7e3f4 110455 4 2023-06-09 11:35:03 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 99d885f1-9b4c-42f4-9659-26049355ac03 0xc003c3de17 0xc003c3de18}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d885f1-9b4c-42f4-9659-26049355ac03\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:35:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c3dfc0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun  9 11:35:09.911: INFO: pod: "test-deployment-7df74c55ff-ztr79":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-ztr79 test-deployment-7df74c55ff- deployment-8951  ca866737-dda9-45b2-aa44-965626d4a3c8 110450 0 2023-06-09 11:35:03 +0000 UTC 2023-06-09 11:35:10 +0000 UTC 0xc000e125f8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:914a82d143b2f2c63cee949a819b767a5b25444e659969f70e4e8ce190cccd65 cni.projectcalico.org/podIP:172.27.53.80/32 cni.projectcalico.org/podIPs:172.27.53.80/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1ccf76fe-41d8-4c1d-8a2c-b9dc23a7e3f4 0xc000e12847 0xc000e12848}] [] [{kube-controller-manager Update v1 2023-06-09 11:35:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ccf76fe-41d8-4c1d-8a2c-b9dc23a7e3f4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:35:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:35:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.27.53.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h6z6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h6z6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.27.53.80,StartTime:2023-06-09 11:35:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:35:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://eadb8a4386ae961a196661a640d268aa9cd95d9ab8fddb867593bf295109a25f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.27.53.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun  9 11:35:09.911: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-8951  64117825-6c5c-4add-b187-4c0bd272c6d8 110334 3 2023-06-09 11:35:01 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 99d885f1-9b4c-42f4-9659-26049355ac03 0xc000e120e7 0xc000e120e8}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:35:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d885f1-9b4c-42f4-9659-26049355ac03\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:35:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e12200 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  9 11:35:09.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8951" for this suite. 06/09/23 11:35:09.935
------------------------------
• [SLOW TEST] [8.646 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:35:01.305
    Jun  9 11:35:01.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename deployment 06/09/23 11:35:01.307
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:01.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:01.347
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 06/09/23 11:35:01.36
    STEP: waiting for Deployment to be created 06/09/23 11:35:01.37
    STEP: waiting for all Replicas to be Ready 06/09/23 11:35:01.374
    Jun  9 11:35:01.376: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  9 11:35:01.376: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  9 11:35:01.387: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  9 11:35:01.387: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  9 11:35:01.415: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  9 11:35:01.415: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  9 11:35:01.468: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  9 11:35:01.468: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun  9 11:35:02.722: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jun  9 11:35:02.722: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jun  9 11:35:03.321: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 06/09/23 11:35:03.321
    W0609 11:35:03.338637      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun  9 11:35:03.341: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 06/09/23 11:35:03.342
    Jun  9 11:35:03.344: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
    Jun  9 11:35:03.344: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
    Jun  9 11:35:03.344: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
    Jun  9 11:35:03.344: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
    Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
    Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
    Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
    Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 0
    Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
    Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
    Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
    Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
    Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
    Jun  9 11:35:03.345: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
    Jun  9 11:35:03.363: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
    Jun  9 11:35:03.363: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
    Jun  9 11:35:03.415: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
    Jun  9 11:35:03.415: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
    Jun  9 11:35:03.422: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
    Jun  9 11:35:03.422: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
    Jun  9 11:35:03.431: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
    Jun  9 11:35:03.431: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
    Jun  9 11:35:04.780: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
    Jun  9 11:35:04.780: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
    Jun  9 11:35:04.992: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
    STEP: listing Deployments 06/09/23 11:35:04.992
    Jun  9 11:35:05.001: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 06/09/23 11:35:05.001
    Jun  9 11:35:05.035: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 06/09/23 11:35:05.035
    Jun  9 11:35:05.064: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  9 11:35:05.096: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  9 11:35:05.188: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  9 11:35:05.223: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  9 11:35:05.298: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  9 11:35:06.465: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  9 11:35:07.336: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  9 11:35:07.383: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  9 11:35:07.413: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun  9 11:35:09.780: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 06/09/23 11:35:09.812
    STEP: fetching the DeploymentStatus 06/09/23 11:35:09.832
    Jun  9 11:35:09.844: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
    Jun  9 11:35:09.844: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
    Jun  9 11:35:09.844: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
    Jun  9 11:35:09.844: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
    Jun  9 11:35:09.844: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 1
    Jun  9 11:35:09.844: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
    Jun  9 11:35:09.845: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 3
    Jun  9 11:35:09.845: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
    Jun  9 11:35:09.845: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 2
    Jun  9 11:35:09.845: INFO: observed Deployment test-deployment in namespace deployment-8951 with ReadyReplicas 3
    STEP: deleting the Deployment 06/09/23 11:35:09.845
    Jun  9 11:35:09.866: INFO: observed event type MODIFIED
    Jun  9 11:35:09.866: INFO: observed event type MODIFIED
    Jun  9 11:35:09.866: INFO: observed event type MODIFIED
    Jun  9 11:35:09.866: INFO: observed event type MODIFIED
    Jun  9 11:35:09.866: INFO: observed event type MODIFIED
    Jun  9 11:35:09.866: INFO: observed event type MODIFIED
    Jun  9 11:35:09.866: INFO: observed event type MODIFIED
    Jun  9 11:35:09.866: INFO: observed event type MODIFIED
    Jun  9 11:35:09.867: INFO: observed event type MODIFIED
    Jun  9 11:35:09.867: INFO: observed event type MODIFIED
    Jun  9 11:35:09.867: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  9 11:35:09.881: INFO: Log out all the ReplicaSets if there is no deployment created
    Jun  9 11:35:09.888: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-8951  e0a206e4-2114-40a4-a819-aaace9cd3bde 110446 2 2023-06-09 11:35:05 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 99d885f1-9b4c-42f4-9659-26049355ac03 0xc003c3da67 0xc003c3da68}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d885f1-9b4c-42f4-9659-26049355ac03\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:35:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c3dc00 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jun  9 11:35:09.899: INFO: pod: "test-deployment-7b7876f9d6-mcl55":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-mcl55 test-deployment-7b7876f9d6- deployment-8951  64c3cfb8-b643-48ac-abb8-f9f8c35e8b95 110445 0 2023-06-09 11:35:07 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:cb72ead30c3197b9c5984f33b4522493074e11e707eb30e55b819ce0ba20404c cni.projectcalico.org/podIP:172.27.53.83/32 cni.projectcalico.org/podIPs:172.27.53.83/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 e0a206e4-2114-40a4-a819-aaace9cd3bde 0xc005f34477 0xc005f34478}] [] [{kube-controller-manager Update v1 2023-06-09 11:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a206e4-2114-40a4-a819-aaace9cd3bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.27.53.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-25dcj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-25dcj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.27.53.83,StartTime:2023-06-09 11:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:35:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://315bb787c38ead299af7b407a99627fa0241a2b9e5f4df8aafe7397bdda09c85,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.27.53.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun  9 11:35:09.899: INFO: pod: "test-deployment-7b7876f9d6-ml8f7":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-ml8f7 test-deployment-7b7876f9d6- deployment-8951  e2c98313-931c-4c3e-ab1a-93f98ae85946 110406 0 2023-06-09 11:35:05 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:bb417b9e935e3add0de1ade7dfb73aee9bd80908f49a4a9da2f8e8a00c364108 cni.projectcalico.org/podIP:172.26.90.25/32 cni.projectcalico.org/podIPs:172.26.90.25/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 e0a206e4-2114-40a4-a819-aaace9cd3bde 0xc005f34777 0xc005f34778}] [] [{kube-controller-manager Update v1 2023-06-09 11:35:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0a206e4-2114-40a4-a819-aaace9cd3bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:35:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.90.25\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8bsd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8bsd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.26.90.25,StartTime:2023-06-09 11:35:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:35:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://53abb86537941aa8e39a54db38a2dff338523747f1f858749a66345470833323,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.90.25,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun  9 11:35:09.899: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-8951  1ccf76fe-41d8-4c1d-8a2c-b9dc23a7e3f4 110455 4 2023-06-09 11:35:03 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 99d885f1-9b4c-42f4-9659-26049355ac03 0xc003c3de17 0xc003c3de18}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d885f1-9b4c-42f4-9659-26049355ac03\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:35:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c3dfc0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jun  9 11:35:09.911: INFO: pod: "test-deployment-7df74c55ff-ztr79":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-ztr79 test-deployment-7df74c55ff- deployment-8951  ca866737-dda9-45b2-aa44-965626d4a3c8 110450 0 2023-06-09 11:35:03 +0000 UTC 2023-06-09 11:35:10 +0000 UTC 0xc000e125f8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:914a82d143b2f2c63cee949a819b767a5b25444e659969f70e4e8ce190cccd65 cni.projectcalico.org/podIP:172.27.53.80/32 cni.projectcalico.org/podIPs:172.27.53.80/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1ccf76fe-41d8-4c1d-8a2c-b9dc23a7e3f4 0xc000e12847 0xc000e12848}] [] [{kube-controller-manager Update v1 2023-06-09 11:35:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ccf76fe-41d8-4c1d-8a2c-b9dc23a7e3f4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:35:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:35:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.27.53.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h6z6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h6z6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-qdprq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:35:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.27.53.80,StartTime:2023-06-09 11:35:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:35:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://eadb8a4386ae961a196661a640d268aa9cd95d9ab8fddb867593bf295109a25f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.27.53.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun  9 11:35:09.911: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-8951  64117825-6c5c-4add-b187-4c0bd272c6d8 110334 3 2023-06-09 11:35:01 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 99d885f1-9b4c-42f4-9659-26049355ac03 0xc000e120e7 0xc000e120e8}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:35:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99d885f1-9b4c-42f4-9659-26049355ac03\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:35:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e12200 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:35:09.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8951" for this suite. 06/09/23 11:35:09.935
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:35:09.952
Jun  9 11:35:09.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-lifecycle-hook 06/09/23 11:35:09.954
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:09.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:09.988
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 06/09/23 11:35:10.01
Jun  9 11:35:10.025: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4177" to be "running and ready"
Jun  9 11:35:10.036: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.04038ms
Jun  9 11:35:10.036: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:35:12.048: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.023000956s
Jun  9 11:35:12.048: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun  9 11:35:12.048: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 06/09/23 11:35:12.054
Jun  9 11:35:12.093: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4177" to be "running and ready"
Jun  9 11:35:12.105: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.923372ms
Jun  9 11:35:12.105: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:35:14.111: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018016155s
Jun  9 11:35:14.111: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jun  9 11:35:14.111: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 06/09/23 11:35:14.116
STEP: delete the pod with lifecycle hook 06/09/23 11:35:14.146
Jun  9 11:35:14.158: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  9 11:35:14.165: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  9 11:35:16.167: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  9 11:35:16.179: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  9 11:35:18.166: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  9 11:35:18.174: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jun  9 11:35:18.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4177" for this suite. 06/09/23 11:35:18.187
------------------------------
• [SLOW TEST] [8.266 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:35:09.952
    Jun  9 11:35:09.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/09/23 11:35:09.954
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:09.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:09.988
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 06/09/23 11:35:10.01
    Jun  9 11:35:10.025: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4177" to be "running and ready"
    Jun  9 11:35:10.036: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.04038ms
    Jun  9 11:35:10.036: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:35:12.048: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.023000956s
    Jun  9 11:35:12.048: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun  9 11:35:12.048: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 06/09/23 11:35:12.054
    Jun  9 11:35:12.093: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4177" to be "running and ready"
    Jun  9 11:35:12.105: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.923372ms
    Jun  9 11:35:12.105: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:35:14.111: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018016155s
    Jun  9 11:35:14.111: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jun  9 11:35:14.111: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 06/09/23 11:35:14.116
    STEP: delete the pod with lifecycle hook 06/09/23 11:35:14.146
    Jun  9 11:35:14.158: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun  9 11:35:14.165: INFO: Pod pod-with-poststart-exec-hook still exists
    Jun  9 11:35:16.167: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun  9 11:35:16.179: INFO: Pod pod-with-poststart-exec-hook still exists
    Jun  9 11:35:18.166: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun  9 11:35:18.174: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:35:18.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4177" for this suite. 06/09/23 11:35:18.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:35:18.222
Jun  9 11:35:18.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 11:35:18.225
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:18.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:18.351
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-df42941b-83d3-4777-ab9f-07399875a3d4 06/09/23 11:35:18.379
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 11:35:18.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-843" for this suite. 06/09/23 11:35:18.395
------------------------------
• [0.185 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:35:18.222
    Jun  9 11:35:18.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 11:35:18.225
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:18.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:18.351
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-df42941b-83d3-4777-ab9f-07399875a3d4 06/09/23 11:35:18.379
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:35:18.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-843" for this suite. 06/09/23 11:35:18.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:35:18.409
Jun  9 11:35:18.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename sysctl 06/09/23 11:35:18.412
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:18.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:18.438
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 06/09/23 11:35:18.444
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:35:18.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-5212" for this suite. 06/09/23 11:35:18.464
------------------------------
• [0.068 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:35:18.409
    Jun  9 11:35:18.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename sysctl 06/09/23 11:35:18.412
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:18.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:18.438
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 06/09/23 11:35:18.444
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:35:18.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-5212" for this suite. 06/09/23 11:35:18.464
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:35:18.477
Jun  9 11:35:18.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename svcaccounts 06/09/23 11:35:18.479
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:18.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:18.534
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jun  9 11:35:18.546: INFO: Got root ca configmap in namespace "svcaccounts-1454"
Jun  9 11:35:18.556: INFO: Deleted root ca configmap in namespace "svcaccounts-1454"
STEP: waiting for a new root ca configmap created 06/09/23 11:35:19.058
Jun  9 11:35:19.069: INFO: Recreated root ca configmap in namespace "svcaccounts-1454"
Jun  9 11:35:19.077: INFO: Updated root ca configmap in namespace "svcaccounts-1454"
STEP: waiting for the root ca configmap reconciled 06/09/23 11:35:19.578
Jun  9 11:35:19.586: INFO: Reconciled root ca configmap in namespace "svcaccounts-1454"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun  9 11:35:19.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1454" for this suite. 06/09/23 11:35:19.596
------------------------------
• [1.130 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:35:18.477
    Jun  9 11:35:18.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename svcaccounts 06/09/23 11:35:18.479
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:18.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:18.534
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jun  9 11:35:18.546: INFO: Got root ca configmap in namespace "svcaccounts-1454"
    Jun  9 11:35:18.556: INFO: Deleted root ca configmap in namespace "svcaccounts-1454"
    STEP: waiting for a new root ca configmap created 06/09/23 11:35:19.058
    Jun  9 11:35:19.069: INFO: Recreated root ca configmap in namespace "svcaccounts-1454"
    Jun  9 11:35:19.077: INFO: Updated root ca configmap in namespace "svcaccounts-1454"
    STEP: waiting for the root ca configmap reconciled 06/09/23 11:35:19.578
    Jun  9 11:35:19.586: INFO: Reconciled root ca configmap in namespace "svcaccounts-1454"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:35:19.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1454" for this suite. 06/09/23 11:35:19.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:35:19.621
Jun  9 11:35:19.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 11:35:19.622
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:19.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:19.671
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 11:35:19.709
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:35:20.275
STEP: Deploying the webhook pod 06/09/23 11:35:20.291
STEP: Wait for the deployment to be ready 06/09/23 11:35:20.315
Jun  9 11:35:20.327: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 11:35:22.348
STEP: Verifying the service has paired with the endpoint 06/09/23 11:35:22.376
Jun  9 11:35:23.377: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/09/23 11:35:23.381
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/09/23 11:35:23.412
STEP: Creating a dummy validating-webhook-configuration object 06/09/23 11:35:23.445
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 06/09/23 11:35:23.468
STEP: Creating a dummy mutating-webhook-configuration object 06/09/23 11:35:23.481
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 06/09/23 11:35:23.504
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:35:23.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1705" for this suite. 06/09/23 11:35:23.785
STEP: Destroying namespace "webhook-1705-markers" for this suite. 06/09/23 11:35:23.813
------------------------------
• [4.211 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:35:19.621
    Jun  9 11:35:19.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 11:35:19.622
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:19.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:19.671
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 11:35:19.709
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:35:20.275
    STEP: Deploying the webhook pod 06/09/23 11:35:20.291
    STEP: Wait for the deployment to be ready 06/09/23 11:35:20.315
    Jun  9 11:35:20.327: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 11:35:22.348
    STEP: Verifying the service has paired with the endpoint 06/09/23 11:35:22.376
    Jun  9 11:35:23.377: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/09/23 11:35:23.381
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/09/23 11:35:23.412
    STEP: Creating a dummy validating-webhook-configuration object 06/09/23 11:35:23.445
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 06/09/23 11:35:23.468
    STEP: Creating a dummy mutating-webhook-configuration object 06/09/23 11:35:23.481
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 06/09/23 11:35:23.504
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:35:23.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1705" for this suite. 06/09/23 11:35:23.785
    STEP: Destroying namespace "webhook-1705-markers" for this suite. 06/09/23 11:35:23.813
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:35:23.832
Jun  9 11:35:23.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename secrets 06/09/23 11:35:23.837
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:23.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:23.882
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 06/09/23 11:35:23.897
STEP: listing secrets in all namespaces to ensure that there are more than zero 06/09/23 11:35:23.907
STEP: patching the secret 06/09/23 11:35:23.916
STEP: deleting the secret using a LabelSelector 06/09/23 11:35:23.992
STEP: listing secrets in all namespaces, searching for label name and value in patch 06/09/23 11:35:24.009
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  9 11:35:24.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-191" for this suite. 06/09/23 11:35:24.025
------------------------------
• [0.212 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:35:23.832
    Jun  9 11:35:23.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename secrets 06/09/23 11:35:23.837
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:23.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:23.882
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 06/09/23 11:35:23.897
    STEP: listing secrets in all namespaces to ensure that there are more than zero 06/09/23 11:35:23.907
    STEP: patching the secret 06/09/23 11:35:23.916
    STEP: deleting the secret using a LabelSelector 06/09/23 11:35:23.992
    STEP: listing secrets in all namespaces, searching for label name and value in patch 06/09/23 11:35:24.009
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:35:24.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-191" for this suite. 06/09/23 11:35:24.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:35:24.045
Jun  9 11:35:24.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename secrets 06/09/23 11:35:24.047
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:24.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:24.091
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-3dbb2c6f-77cd-4367-a5d9-d71dc9dbe31a 06/09/23 11:35:24.107
STEP: Creating a pod to test consume secrets 06/09/23 11:35:24.121
Jun  9 11:35:24.140: INFO: Waiting up to 5m0s for pod "pod-secrets-73504109-a4e7-4b38-9349-25a881e53741" in namespace "secrets-4094" to be "Succeeded or Failed"
Jun  9 11:35:24.150: INFO: Pod "pod-secrets-73504109-a4e7-4b38-9349-25a881e53741": Phase="Pending", Reason="", readiness=false. Elapsed: 9.310564ms
Jun  9 11:35:26.159: INFO: Pod "pod-secrets-73504109-a4e7-4b38-9349-25a881e53741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018874978s
Jun  9 11:35:28.156: INFO: Pod "pod-secrets-73504109-a4e7-4b38-9349-25a881e53741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015603551s
STEP: Saw pod success 06/09/23 11:35:28.156
Jun  9 11:35:28.156: INFO: Pod "pod-secrets-73504109-a4e7-4b38-9349-25a881e53741" satisfied condition "Succeeded or Failed"
Jun  9 11:35:28.162: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-secrets-73504109-a4e7-4b38-9349-25a881e53741 container secret-env-test: <nil>
STEP: delete the pod 06/09/23 11:35:28.198
Jun  9 11:35:28.258: INFO: Waiting for pod pod-secrets-73504109-a4e7-4b38-9349-25a881e53741 to disappear
Jun  9 11:35:28.265: INFO: Pod pod-secrets-73504109-a4e7-4b38-9349-25a881e53741 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  9 11:35:28.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4094" for this suite. 06/09/23 11:35:28.278
------------------------------
• [4.267 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:35:24.045
    Jun  9 11:35:24.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename secrets 06/09/23 11:35:24.047
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:24.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:24.091
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-3dbb2c6f-77cd-4367-a5d9-d71dc9dbe31a 06/09/23 11:35:24.107
    STEP: Creating a pod to test consume secrets 06/09/23 11:35:24.121
    Jun  9 11:35:24.140: INFO: Waiting up to 5m0s for pod "pod-secrets-73504109-a4e7-4b38-9349-25a881e53741" in namespace "secrets-4094" to be "Succeeded or Failed"
    Jun  9 11:35:24.150: INFO: Pod "pod-secrets-73504109-a4e7-4b38-9349-25a881e53741": Phase="Pending", Reason="", readiness=false. Elapsed: 9.310564ms
    Jun  9 11:35:26.159: INFO: Pod "pod-secrets-73504109-a4e7-4b38-9349-25a881e53741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018874978s
    Jun  9 11:35:28.156: INFO: Pod "pod-secrets-73504109-a4e7-4b38-9349-25a881e53741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015603551s
    STEP: Saw pod success 06/09/23 11:35:28.156
    Jun  9 11:35:28.156: INFO: Pod "pod-secrets-73504109-a4e7-4b38-9349-25a881e53741" satisfied condition "Succeeded or Failed"
    Jun  9 11:35:28.162: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-secrets-73504109-a4e7-4b38-9349-25a881e53741 container secret-env-test: <nil>
    STEP: delete the pod 06/09/23 11:35:28.198
    Jun  9 11:35:28.258: INFO: Waiting for pod pod-secrets-73504109-a4e7-4b38-9349-25a881e53741 to disappear
    Jun  9 11:35:28.265: INFO: Pod pod-secrets-73504109-a4e7-4b38-9349-25a881e53741 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:35:28.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4094" for this suite. 06/09/23 11:35:28.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:35:28.314
Jun  9 11:35:28.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-probe 06/09/23 11:35:28.315
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:28.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:28.375
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff in namespace container-probe-8293 06/09/23 11:35:28.381
Jun  9 11:35:28.404: INFO: Waiting up to 5m0s for pod "liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff" in namespace "container-probe-8293" to be "not pending"
Jun  9 11:35:28.437: INFO: Pod "liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff": Phase="Pending", Reason="", readiness=false. Elapsed: 33.313266ms
Jun  9 11:35:30.446: INFO: Pod "liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.042065209s
Jun  9 11:35:30.446: INFO: Pod "liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff" satisfied condition "not pending"
Jun  9 11:35:30.446: INFO: Started pod liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff in namespace container-probe-8293
STEP: checking the pod's current state and verifying that restartCount is present 06/09/23 11:35:30.446
Jun  9 11:35:30.452: INFO: Initial restart count of pod liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff is 0
Jun  9 11:35:50.567: INFO: Restart count of pod container-probe-8293/liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff is now 1 (20.115190111s elapsed)
STEP: deleting the pod 06/09/23 11:35:50.567
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun  9 11:35:50.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8293" for this suite. 06/09/23 11:35:50.627
------------------------------
• [SLOW TEST] [22.335 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:35:28.314
    Jun  9 11:35:28.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-probe 06/09/23 11:35:28.315
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:28.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:28.375
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff in namespace container-probe-8293 06/09/23 11:35:28.381
    Jun  9 11:35:28.404: INFO: Waiting up to 5m0s for pod "liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff" in namespace "container-probe-8293" to be "not pending"
    Jun  9 11:35:28.437: INFO: Pod "liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff": Phase="Pending", Reason="", readiness=false. Elapsed: 33.313266ms
    Jun  9 11:35:30.446: INFO: Pod "liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.042065209s
    Jun  9 11:35:30.446: INFO: Pod "liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff" satisfied condition "not pending"
    Jun  9 11:35:30.446: INFO: Started pod liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff in namespace container-probe-8293
    STEP: checking the pod's current state and verifying that restartCount is present 06/09/23 11:35:30.446
    Jun  9 11:35:30.452: INFO: Initial restart count of pod liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff is 0
    Jun  9 11:35:50.567: INFO: Restart count of pod container-probe-8293/liveness-ee410a7b-d27a-4106-9250-ce656c9d50ff is now 1 (20.115190111s elapsed)
    STEP: deleting the pod 06/09/23 11:35:50.567
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:35:50.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8293" for this suite. 06/09/23 11:35:50.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:35:50.651
Jun  9 11:35:50.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename subpath 06/09/23 11:35:50.653
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:50.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:50.743
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/09/23 11:35:50.754
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-n9mc 06/09/23 11:35:50.843
STEP: Creating a pod to test atomic-volume-subpath 06/09/23 11:35:50.843
Jun  9 11:35:50.876: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-n9mc" in namespace "subpath-6369" to be "Succeeded or Failed"
Jun  9 11:35:50.903: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Pending", Reason="", readiness=false. Elapsed: 27.670762ms
Jun  9 11:35:52.910: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 2.034151893s
Jun  9 11:35:54.923: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 4.04783533s
Jun  9 11:35:56.911: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 6.035687116s
Jun  9 11:35:58.939: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 8.063042458s
Jun  9 11:36:00.912: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 10.036036093s
Jun  9 11:36:02.915: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 12.039682512s
Jun  9 11:36:04.911: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 14.035541773s
Jun  9 11:36:06.911: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 16.035823793s
Jun  9 11:36:08.911: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 18.03515789s
Jun  9 11:36:10.912: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 20.035903954s
Jun  9 11:36:12.912: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=false. Elapsed: 22.035965714s
Jun  9 11:36:14.911: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.035481563s
STEP: Saw pod success 06/09/23 11:36:14.911
Jun  9 11:36:14.911: INFO: Pod "pod-subpath-test-downwardapi-n9mc" satisfied condition "Succeeded or Failed"
Jun  9 11:36:14.918: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-subpath-test-downwardapi-n9mc container test-container-subpath-downwardapi-n9mc: <nil>
STEP: delete the pod 06/09/23 11:36:14.93
Jun  9 11:36:15.033: INFO: Waiting for pod pod-subpath-test-downwardapi-n9mc to disappear
Jun  9 11:36:15.047: INFO: Pod pod-subpath-test-downwardapi-n9mc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-n9mc 06/09/23 11:36:15.047
Jun  9 11:36:15.047: INFO: Deleting pod "pod-subpath-test-downwardapi-n9mc" in namespace "subpath-6369"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun  9 11:36:15.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6369" for this suite. 06/09/23 11:36:15.064
------------------------------
• [SLOW TEST] [24.425 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:35:50.651
    Jun  9 11:35:50.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename subpath 06/09/23 11:35:50.653
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:35:50.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:35:50.743
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/09/23 11:35:50.754
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-n9mc 06/09/23 11:35:50.843
    STEP: Creating a pod to test atomic-volume-subpath 06/09/23 11:35:50.843
    Jun  9 11:35:50.876: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-n9mc" in namespace "subpath-6369" to be "Succeeded or Failed"
    Jun  9 11:35:50.903: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Pending", Reason="", readiness=false. Elapsed: 27.670762ms
    Jun  9 11:35:52.910: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 2.034151893s
    Jun  9 11:35:54.923: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 4.04783533s
    Jun  9 11:35:56.911: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 6.035687116s
    Jun  9 11:35:58.939: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 8.063042458s
    Jun  9 11:36:00.912: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 10.036036093s
    Jun  9 11:36:02.915: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 12.039682512s
    Jun  9 11:36:04.911: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 14.035541773s
    Jun  9 11:36:06.911: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 16.035823793s
    Jun  9 11:36:08.911: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 18.03515789s
    Jun  9 11:36:10.912: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=true. Elapsed: 20.035903954s
    Jun  9 11:36:12.912: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Running", Reason="", readiness=false. Elapsed: 22.035965714s
    Jun  9 11:36:14.911: INFO: Pod "pod-subpath-test-downwardapi-n9mc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.035481563s
    STEP: Saw pod success 06/09/23 11:36:14.911
    Jun  9 11:36:14.911: INFO: Pod "pod-subpath-test-downwardapi-n9mc" satisfied condition "Succeeded or Failed"
    Jun  9 11:36:14.918: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-subpath-test-downwardapi-n9mc container test-container-subpath-downwardapi-n9mc: <nil>
    STEP: delete the pod 06/09/23 11:36:14.93
    Jun  9 11:36:15.033: INFO: Waiting for pod pod-subpath-test-downwardapi-n9mc to disappear
    Jun  9 11:36:15.047: INFO: Pod pod-subpath-test-downwardapi-n9mc no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-n9mc 06/09/23 11:36:15.047
    Jun  9 11:36:15.047: INFO: Deleting pod "pod-subpath-test-downwardapi-n9mc" in namespace "subpath-6369"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:36:15.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6369" for this suite. 06/09/23 11:36:15.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:36:15.077
Jun  9 11:36:15.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 11:36:15.08
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:36:15.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:36:15.111
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 11:36:15.138
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:36:16.14
STEP: Deploying the webhook pod 06/09/23 11:36:16.158
STEP: Wait for the deployment to be ready 06/09/23 11:36:16.183
Jun  9 11:36:16.200: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 11:36:18.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 36, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 36, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 36, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 36, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/09/23 11:36:20.242
STEP: Verifying the service has paired with the endpoint 06/09/23 11:36:20.269
Jun  9 11:36:21.270: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 06/09/23 11:36:21.277
STEP: create a configmap that should be updated by the webhook 06/09/23 11:36:21.302
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:36:21.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8296" for this suite. 06/09/23 11:36:21.445
STEP: Destroying namespace "webhook-8296-markers" for this suite. 06/09/23 11:36:21.481
------------------------------
• [SLOW TEST] [6.435 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:36:15.077
    Jun  9 11:36:15.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 11:36:15.08
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:36:15.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:36:15.111
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 11:36:15.138
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:36:16.14
    STEP: Deploying the webhook pod 06/09/23 11:36:16.158
    STEP: Wait for the deployment to be ready 06/09/23 11:36:16.183
    Jun  9 11:36:16.200: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun  9 11:36:18.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 36, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 36, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 36, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 36, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/09/23 11:36:20.242
    STEP: Verifying the service has paired with the endpoint 06/09/23 11:36:20.269
    Jun  9 11:36:21.270: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 06/09/23 11:36:21.277
    STEP: create a configmap that should be updated by the webhook 06/09/23 11:36:21.302
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:36:21.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8296" for this suite. 06/09/23 11:36:21.445
    STEP: Destroying namespace "webhook-8296-markers" for this suite. 06/09/23 11:36:21.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:36:21.515
Jun  9 11:36:21.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename resourcequota 06/09/23 11:36:21.516
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:36:21.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:36:21.555
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 06/09/23 11:36:21.568
STEP: Creating a ResourceQuota 06/09/23 11:36:26.572
STEP: Ensuring resource quota status is calculated 06/09/23 11:36:26.582
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  9 11:36:28.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-531" for this suite. 06/09/23 11:36:28.611
------------------------------
• [SLOW TEST] [7.171 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:36:21.515
    Jun  9 11:36:21.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename resourcequota 06/09/23 11:36:21.516
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:36:21.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:36:21.555
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 06/09/23 11:36:21.568
    STEP: Creating a ResourceQuota 06/09/23 11:36:26.572
    STEP: Ensuring resource quota status is calculated 06/09/23 11:36:26.582
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:36:28.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-531" for this suite. 06/09/23 11:36:28.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:36:28.687
Jun  9 11:36:28.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename replicaset 06/09/23 11:36:28.688
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:36:28.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:36:28.775
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jun  9 11:36:28.780: INFO: Creating ReplicaSet my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387
Jun  9 11:36:28.813: INFO: Pod name my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387: Found 0 pods out of 1
Jun  9 11:36:33.820: INFO: Pod name my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387: Found 1 pods out of 1
Jun  9 11:36:33.820: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387" is running
Jun  9 11:36:33.820: INFO: Waiting up to 5m0s for pod "my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387-gzgmb" in namespace "replicaset-7254" to be "running"
Jun  9 11:36:33.828: INFO: Pod "my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387-gzgmb": Phase="Running", Reason="", readiness=true. Elapsed: 7.918901ms
Jun  9 11:36:33.828: INFO: Pod "my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387-gzgmb" satisfied condition "running"
Jun  9 11:36:33.828: INFO: Pod "my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387-gzgmb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-09 11:36:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-09 11:36:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-09 11:36:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-09 11:36:28 +0000 UTC Reason: Message:}])
Jun  9 11:36:33.828: INFO: Trying to dial the pod
Jun  9 11:36:38.846: INFO: Controller my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387: Got expected result from replica 1 [my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387-gzgmb]: "my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387-gzgmb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun  9 11:36:38.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7254" for this suite. 06/09/23 11:36:38.855
------------------------------
• [SLOW TEST] [10.189 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:36:28.687
    Jun  9 11:36:28.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename replicaset 06/09/23 11:36:28.688
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:36:28.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:36:28.775
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jun  9 11:36:28.780: INFO: Creating ReplicaSet my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387
    Jun  9 11:36:28.813: INFO: Pod name my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387: Found 0 pods out of 1
    Jun  9 11:36:33.820: INFO: Pod name my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387: Found 1 pods out of 1
    Jun  9 11:36:33.820: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387" is running
    Jun  9 11:36:33.820: INFO: Waiting up to 5m0s for pod "my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387-gzgmb" in namespace "replicaset-7254" to be "running"
    Jun  9 11:36:33.828: INFO: Pod "my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387-gzgmb": Phase="Running", Reason="", readiness=true. Elapsed: 7.918901ms
    Jun  9 11:36:33.828: INFO: Pod "my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387-gzgmb" satisfied condition "running"
    Jun  9 11:36:33.828: INFO: Pod "my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387-gzgmb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-09 11:36:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-09 11:36:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-09 11:36:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-09 11:36:28 +0000 UTC Reason: Message:}])
    Jun  9 11:36:33.828: INFO: Trying to dial the pod
    Jun  9 11:36:38.846: INFO: Controller my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387: Got expected result from replica 1 [my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387-gzgmb]: "my-hostname-basic-300bb302-298e-45b8-9dfb-b906cd5c8387-gzgmb", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:36:38.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7254" for this suite. 06/09/23 11:36:38.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:36:38.877
Jun  9 11:36:38.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 11:36:38.878
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:36:38.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:36:38.917
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-74a03cdd-8a22-4c44-9b60-6aa9623530aa 06/09/23 11:36:38.931
STEP: Creating a pod to test consume configMaps 06/09/23 11:36:38.954
Jun  9 11:36:38.975: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530" in namespace "projected-1022" to be "Succeeded or Failed"
Jun  9 11:36:38.982: INFO: Pod "pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530": Phase="Pending", Reason="", readiness=false. Elapsed: 7.174216ms
Jun  9 11:36:40.989: INFO: Pod "pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014099068s
Jun  9 11:36:42.992: INFO: Pod "pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016862371s
STEP: Saw pod success 06/09/23 11:36:42.992
Jun  9 11:36:42.992: INFO: Pod "pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530" satisfied condition "Succeeded or Failed"
Jun  9 11:36:42.998: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530 container agnhost-container: <nil>
STEP: delete the pod 06/09/23 11:36:43.019
Jun  9 11:36:43.063: INFO: Waiting for pod pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530 to disappear
Jun  9 11:36:43.069: INFO: Pod pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  9 11:36:43.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1022" for this suite. 06/09/23 11:36:43.077
------------------------------
• [4.225 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:36:38.877
    Jun  9 11:36:38.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 11:36:38.878
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:36:38.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:36:38.917
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-74a03cdd-8a22-4c44-9b60-6aa9623530aa 06/09/23 11:36:38.931
    STEP: Creating a pod to test consume configMaps 06/09/23 11:36:38.954
    Jun  9 11:36:38.975: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530" in namespace "projected-1022" to be "Succeeded or Failed"
    Jun  9 11:36:38.982: INFO: Pod "pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530": Phase="Pending", Reason="", readiness=false. Elapsed: 7.174216ms
    Jun  9 11:36:40.989: INFO: Pod "pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014099068s
    Jun  9 11:36:42.992: INFO: Pod "pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016862371s
    STEP: Saw pod success 06/09/23 11:36:42.992
    Jun  9 11:36:42.992: INFO: Pod "pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530" satisfied condition "Succeeded or Failed"
    Jun  9 11:36:42.998: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530 container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 11:36:43.019
    Jun  9 11:36:43.063: INFO: Waiting for pod pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530 to disappear
    Jun  9 11:36:43.069: INFO: Pod pod-projected-configmaps-d81b2e93-10b5-47af-94fb-e4b568335530 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:36:43.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1022" for this suite. 06/09/23 11:36:43.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:36:43.103
Jun  9 11:36:43.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename resourcequota 06/09/23 11:36:43.105
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:36:43.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:36:43.148
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 06/09/23 11:37:00.17
STEP: Creating a ResourceQuota 06/09/23 11:37:05.176
STEP: Ensuring resource quota status is calculated 06/09/23 11:37:05.217
STEP: Creating a ConfigMap 06/09/23 11:37:07.224
STEP: Ensuring resource quota status captures configMap creation 06/09/23 11:37:07.292
STEP: Deleting a ConfigMap 06/09/23 11:37:09.299
STEP: Ensuring resource quota status released usage 06/09/23 11:37:09.309
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  9 11:37:11.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9284" for this suite. 06/09/23 11:37:11.324
------------------------------
• [SLOW TEST] [28.248 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:36:43.103
    Jun  9 11:36:43.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename resourcequota 06/09/23 11:36:43.105
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:36:43.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:36:43.148
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 06/09/23 11:37:00.17
    STEP: Creating a ResourceQuota 06/09/23 11:37:05.176
    STEP: Ensuring resource quota status is calculated 06/09/23 11:37:05.217
    STEP: Creating a ConfigMap 06/09/23 11:37:07.224
    STEP: Ensuring resource quota status captures configMap creation 06/09/23 11:37:07.292
    STEP: Deleting a ConfigMap 06/09/23 11:37:09.299
    STEP: Ensuring resource quota status released usage 06/09/23 11:37:09.309
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:37:11.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9284" for this suite. 06/09/23 11:37:11.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:37:11.352
Jun  9 11:37:11.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename containers 06/09/23 11:37:11.354
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:11.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:11.385
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 06/09/23 11:37:11.392
Jun  9 11:37:11.408: INFO: Waiting up to 5m0s for pod "client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59" in namespace "containers-7843" to be "Succeeded or Failed"
Jun  9 11:37:11.415: INFO: Pod "client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59": Phase="Pending", Reason="", readiness=false. Elapsed: 7.741145ms
Jun  9 11:37:13.425: INFO: Pod "client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017283485s
Jun  9 11:37:15.423: INFO: Pod "client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015145249s
STEP: Saw pod success 06/09/23 11:37:15.423
Jun  9 11:37:15.423: INFO: Pod "client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59" satisfied condition "Succeeded or Failed"
Jun  9 11:37:15.429: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59 container agnhost-container: <nil>
STEP: delete the pod 06/09/23 11:37:15.444
Jun  9 11:37:15.464: INFO: Waiting for pod client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59 to disappear
Jun  9 11:37:15.472: INFO: Pod client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jun  9 11:37:15.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7843" for this suite. 06/09/23 11:37:15.484
------------------------------
• [4.261 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:37:11.352
    Jun  9 11:37:11.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename containers 06/09/23 11:37:11.354
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:11.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:11.385
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 06/09/23 11:37:11.392
    Jun  9 11:37:11.408: INFO: Waiting up to 5m0s for pod "client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59" in namespace "containers-7843" to be "Succeeded or Failed"
    Jun  9 11:37:11.415: INFO: Pod "client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59": Phase="Pending", Reason="", readiness=false. Elapsed: 7.741145ms
    Jun  9 11:37:13.425: INFO: Pod "client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017283485s
    Jun  9 11:37:15.423: INFO: Pod "client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015145249s
    STEP: Saw pod success 06/09/23 11:37:15.423
    Jun  9 11:37:15.423: INFO: Pod "client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59" satisfied condition "Succeeded or Failed"
    Jun  9 11:37:15.429: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59 container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 11:37:15.444
    Jun  9 11:37:15.464: INFO: Waiting for pod client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59 to disappear
    Jun  9 11:37:15.472: INFO: Pod client-containers-c8ec4ce0-260a-416f-811d-968b94f82c59 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:37:15.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7843" for this suite. 06/09/23 11:37:15.484
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:37:15.615
Jun  9 11:37:15.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename runtimeclass 06/09/23 11:37:15.617
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:15.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:15.667
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-2748-delete-me 06/09/23 11:37:15.81
STEP: Waiting for the RuntimeClass to disappear 06/09/23 11:37:15.828
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun  9 11:37:15.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2748" for this suite. 06/09/23 11:37:15.874
------------------------------
• [0.277 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:37:15.615
    Jun  9 11:37:15.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename runtimeclass 06/09/23 11:37:15.617
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:15.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:15.667
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-2748-delete-me 06/09/23 11:37:15.81
    STEP: Waiting for the RuntimeClass to disappear 06/09/23 11:37:15.828
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:37:15.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2748" for this suite. 06/09/23 11:37:15.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:37:15.898
Jun  9 11:37:15.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename runtimeclass 06/09/23 11:37:15.9
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:15.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:15.959
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun  9 11:37:15.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4022" for this suite. 06/09/23 11:37:15.999
------------------------------
• [0.122 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:37:15.898
    Jun  9 11:37:15.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename runtimeclass 06/09/23 11:37:15.9
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:15.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:15.959
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:37:15.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4022" for this suite. 06/09/23 11:37:15.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:37:16.021
Jun  9 11:37:16.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 11:37:16.023
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:16.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:16.055
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 11:37:16.097
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:37:16.672
STEP: Deploying the webhook pod 06/09/23 11:37:16.76
STEP: Wait for the deployment to be ready 06/09/23 11:37:16.799
Jun  9 11:37:16.836: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 11:37:18.87
STEP: Verifying the service has paired with the endpoint 06/09/23 11:37:18.9
Jun  9 11:37:19.901: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 06/09/23 11:37:19.908
STEP: create a pod that should be updated by the webhook 06/09/23 11:37:19.97
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:37:20.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6235" for this suite. 06/09/23 11:37:20.41
STEP: Destroying namespace "webhook-6235-markers" for this suite. 06/09/23 11:37:20.442
------------------------------
• [4.488 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:37:16.021
    Jun  9 11:37:16.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 11:37:16.023
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:16.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:16.055
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 11:37:16.097
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:37:16.672
    STEP: Deploying the webhook pod 06/09/23 11:37:16.76
    STEP: Wait for the deployment to be ready 06/09/23 11:37:16.799
    Jun  9 11:37:16.836: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 11:37:18.87
    STEP: Verifying the service has paired with the endpoint 06/09/23 11:37:18.9
    Jun  9 11:37:19.901: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 06/09/23 11:37:19.908
    STEP: create a pod that should be updated by the webhook 06/09/23 11:37:19.97
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:37:20.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6235" for this suite. 06/09/23 11:37:20.41
    STEP: Destroying namespace "webhook-6235-markers" for this suite. 06/09/23 11:37:20.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:37:20.511
Jun  9 11:37:20.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 11:37:20.513
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:20.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:20.561
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-2a867ae2-82de-422c-b4e0-0dc894b8c77a 06/09/23 11:37:20.582
STEP: Creating the pod 06/09/23 11:37:20.61
Jun  9 11:37:20.751: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-043d2958-f353-4b82-ace7-bbdc9e11bd43" in namespace "projected-442" to be "running and ready"
Jun  9 11:37:20.785: INFO: Pod "pod-projected-configmaps-043d2958-f353-4b82-ace7-bbdc9e11bd43": Phase="Pending", Reason="", readiness=false. Elapsed: 33.581512ms
Jun  9 11:37:20.785: INFO: The phase of Pod pod-projected-configmaps-043d2958-f353-4b82-ace7-bbdc9e11bd43 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:37:22.792: INFO: Pod "pod-projected-configmaps-043d2958-f353-4b82-ace7-bbdc9e11bd43": Phase="Running", Reason="", readiness=true. Elapsed: 2.040785627s
Jun  9 11:37:22.792: INFO: The phase of Pod pod-projected-configmaps-043d2958-f353-4b82-ace7-bbdc9e11bd43 is Running (Ready = true)
Jun  9 11:37:22.792: INFO: Pod "pod-projected-configmaps-043d2958-f353-4b82-ace7-bbdc9e11bd43" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-2a867ae2-82de-422c-b4e0-0dc894b8c77a 06/09/23 11:37:22.807
STEP: waiting to observe update in volume 06/09/23 11:37:22.817
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  9 11:37:24.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-442" for this suite. 06/09/23 11:37:24.855
------------------------------
• [4.357 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:37:20.511
    Jun  9 11:37:20.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 11:37:20.513
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:20.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:20.561
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-2a867ae2-82de-422c-b4e0-0dc894b8c77a 06/09/23 11:37:20.582
    STEP: Creating the pod 06/09/23 11:37:20.61
    Jun  9 11:37:20.751: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-043d2958-f353-4b82-ace7-bbdc9e11bd43" in namespace "projected-442" to be "running and ready"
    Jun  9 11:37:20.785: INFO: Pod "pod-projected-configmaps-043d2958-f353-4b82-ace7-bbdc9e11bd43": Phase="Pending", Reason="", readiness=false. Elapsed: 33.581512ms
    Jun  9 11:37:20.785: INFO: The phase of Pod pod-projected-configmaps-043d2958-f353-4b82-ace7-bbdc9e11bd43 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:37:22.792: INFO: Pod "pod-projected-configmaps-043d2958-f353-4b82-ace7-bbdc9e11bd43": Phase="Running", Reason="", readiness=true. Elapsed: 2.040785627s
    Jun  9 11:37:22.792: INFO: The phase of Pod pod-projected-configmaps-043d2958-f353-4b82-ace7-bbdc9e11bd43 is Running (Ready = true)
    Jun  9 11:37:22.792: INFO: Pod "pod-projected-configmaps-043d2958-f353-4b82-ace7-bbdc9e11bd43" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-2a867ae2-82de-422c-b4e0-0dc894b8c77a 06/09/23 11:37:22.807
    STEP: waiting to observe update in volume 06/09/23 11:37:22.817
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:37:24.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-442" for this suite. 06/09/23 11:37:24.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:37:24.87
Jun  9 11:37:24.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 11:37:24.872
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:24.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:24.904
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-5867 06/09/23 11:37:24.91
STEP: creating service affinity-nodeport-transition in namespace services-5867 06/09/23 11:37:24.91
STEP: creating replication controller affinity-nodeport-transition in namespace services-5867 06/09/23 11:37:24.94
I0609 11:37:24.952851      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5867, replica count: 3
I0609 11:37:28.004989      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 11:37:28.032: INFO: Creating new exec pod
Jun  9 11:37:28.044: INFO: Waiting up to 5m0s for pod "execpod-affinity58vkm" in namespace "services-5867" to be "running"
Jun  9 11:37:28.051: INFO: Pod "execpod-affinity58vkm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.360167ms
Jun  9 11:37:30.059: INFO: Pod "execpod-affinity58vkm": Phase="Running", Reason="", readiness=true. Elapsed: 2.014450244s
Jun  9 11:37:30.059: INFO: Pod "execpod-affinity58vkm" satisfied condition "running"
Jun  9 11:37:31.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-5867 exec execpod-affinity58vkm -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jun  9 11:37:31.367: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun  9 11:37:31.367: INFO: stdout: ""
Jun  9 11:37:31.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-5867 exec execpod-affinity58vkm -- /bin/sh -x -c nc -v -z -w 2 10.103.16.38 80'
Jun  9 11:37:31.550: INFO: stderr: "+ nc -v -z -w 2 10.103.16.38 80\nConnection to 10.103.16.38 80 port [tcp/http] succeeded!\n"
Jun  9 11:37:31.550: INFO: stdout: ""
Jun  9 11:37:31.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-5867 exec execpod-affinity58vkm -- /bin/sh -x -c nc -v -z -w 2 10.255.64.103 30478'
Jun  9 11:37:31.750: INFO: stderr: "+ nc -v -z -w 2 10.255.64.103 30478\nConnection to 10.255.64.103 30478 port [tcp/*] succeeded!\n"
Jun  9 11:37:31.750: INFO: stdout: ""
Jun  9 11:37:31.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-5867 exec execpod-affinity58vkm -- /bin/sh -x -c nc -v -z -w 2 10.255.64.104 30478'
Jun  9 11:37:31.953: INFO: stderr: "+ nc -v -z -w 2 10.255.64.104 30478\nConnection to 10.255.64.104 30478 port [tcp/*] succeeded!\n"
Jun  9 11:37:31.953: INFO: stdout: ""
Jun  9 11:37:31.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-5867 exec execpod-affinity58vkm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.64.104:30478/ ; done'
Jun  9 11:37:32.290: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n"
Jun  9 11:37:32.290: INFO: stdout: "\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-qrl46\naffinity-nodeport-transition-b2stk\naffinity-nodeport-transition-b2stk\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-b2stk\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-b2stk\naffinity-nodeport-transition-qrl46\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-b2stk\naffinity-nodeport-transition-qrl46\naffinity-nodeport-transition-qrl46\naffinity-nodeport-transition-qrl46\naffinity-nodeport-transition-qrl46"
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-qrl46
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-b2stk
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-b2stk
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-b2stk
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-b2stk
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-qrl46
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-b2stk
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-qrl46
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-qrl46
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-qrl46
Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-qrl46
Jun  9 11:37:32.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-5867 exec execpod-affinity58vkm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.64.104:30478/ ; done'
Jun  9 11:37:32.637: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n"
Jun  9 11:37:32.637: INFO: stdout: "\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h"
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
Jun  9 11:37:32.637: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5867, will wait for the garbage collector to delete the pods 06/09/23 11:37:32.677
Jun  9 11:37:32.745: INFO: Deleting ReplicationController affinity-nodeport-transition took: 10.186211ms
Jun  9 11:37:32.846: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.557977ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 11:37:35.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5867" for this suite. 06/09/23 11:37:35.214
------------------------------
• [SLOW TEST] [10.355 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:37:24.87
    Jun  9 11:37:24.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 11:37:24.872
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:24.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:24.904
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-5867 06/09/23 11:37:24.91
    STEP: creating service affinity-nodeport-transition in namespace services-5867 06/09/23 11:37:24.91
    STEP: creating replication controller affinity-nodeport-transition in namespace services-5867 06/09/23 11:37:24.94
    I0609 11:37:24.952851      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5867, replica count: 3
    I0609 11:37:28.004989      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  9 11:37:28.032: INFO: Creating new exec pod
    Jun  9 11:37:28.044: INFO: Waiting up to 5m0s for pod "execpod-affinity58vkm" in namespace "services-5867" to be "running"
    Jun  9 11:37:28.051: INFO: Pod "execpod-affinity58vkm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.360167ms
    Jun  9 11:37:30.059: INFO: Pod "execpod-affinity58vkm": Phase="Running", Reason="", readiness=true. Elapsed: 2.014450244s
    Jun  9 11:37:30.059: INFO: Pod "execpod-affinity58vkm" satisfied condition "running"
    Jun  9 11:37:31.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-5867 exec execpod-affinity58vkm -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jun  9 11:37:31.367: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jun  9 11:37:31.367: INFO: stdout: ""
    Jun  9 11:37:31.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-5867 exec execpod-affinity58vkm -- /bin/sh -x -c nc -v -z -w 2 10.103.16.38 80'
    Jun  9 11:37:31.550: INFO: stderr: "+ nc -v -z -w 2 10.103.16.38 80\nConnection to 10.103.16.38 80 port [tcp/http] succeeded!\n"
    Jun  9 11:37:31.550: INFO: stdout: ""
    Jun  9 11:37:31.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-5867 exec execpod-affinity58vkm -- /bin/sh -x -c nc -v -z -w 2 10.255.64.103 30478'
    Jun  9 11:37:31.750: INFO: stderr: "+ nc -v -z -w 2 10.255.64.103 30478\nConnection to 10.255.64.103 30478 port [tcp/*] succeeded!\n"
    Jun  9 11:37:31.750: INFO: stdout: ""
    Jun  9 11:37:31.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-5867 exec execpod-affinity58vkm -- /bin/sh -x -c nc -v -z -w 2 10.255.64.104 30478'
    Jun  9 11:37:31.953: INFO: stderr: "+ nc -v -z -w 2 10.255.64.104 30478\nConnection to 10.255.64.104 30478 port [tcp/*] succeeded!\n"
    Jun  9 11:37:31.953: INFO: stdout: ""
    Jun  9 11:37:31.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-5867 exec execpod-affinity58vkm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.64.104:30478/ ; done'
    Jun  9 11:37:32.290: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n"
    Jun  9 11:37:32.290: INFO: stdout: "\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-qrl46\naffinity-nodeport-transition-b2stk\naffinity-nodeport-transition-b2stk\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-b2stk\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-b2stk\naffinity-nodeport-transition-qrl46\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-b2stk\naffinity-nodeport-transition-qrl46\naffinity-nodeport-transition-qrl46\naffinity-nodeport-transition-qrl46\naffinity-nodeport-transition-qrl46"
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-qrl46
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-b2stk
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-b2stk
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-b2stk
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-b2stk
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-qrl46
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-b2stk
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-qrl46
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-qrl46
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-qrl46
    Jun  9 11:37:32.290: INFO: Received response from host: affinity-nodeport-transition-qrl46
    Jun  9 11:37:32.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-5867 exec execpod-affinity58vkm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.64.104:30478/ ; done'
    Jun  9 11:37:32.637: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.104:30478/\n"
    Jun  9 11:37:32.637: INFO: stdout: "\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h\naffinity-nodeport-transition-4c66h"
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Received response from host: affinity-nodeport-transition-4c66h
    Jun  9 11:37:32.637: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5867, will wait for the garbage collector to delete the pods 06/09/23 11:37:32.677
    Jun  9 11:37:32.745: INFO: Deleting ReplicationController affinity-nodeport-transition took: 10.186211ms
    Jun  9 11:37:32.846: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.557977ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:37:35.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5867" for this suite. 06/09/23 11:37:35.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:37:35.226
Jun  9 11:37:35.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 11:37:35.228
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:35.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:35.265
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jun  9 11:37:35.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/09/23 11:37:38.594
Jun  9 11:37:38.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-9823 --namespace=crd-publish-openapi-9823 create -f -'
Jun  9 11:37:39.953: INFO: stderr: ""
Jun  9 11:37:39.953: INFO: stdout: "e2e-test-crd-publish-openapi-5554-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun  9 11:37:39.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-9823 --namespace=crd-publish-openapi-9823 delete e2e-test-crd-publish-openapi-5554-crds test-cr'
Jun  9 11:37:40.090: INFO: stderr: ""
Jun  9 11:37:40.090: INFO: stdout: "e2e-test-crd-publish-openapi-5554-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun  9 11:37:40.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-9823 --namespace=crd-publish-openapi-9823 apply -f -'
Jun  9 11:37:41.125: INFO: stderr: ""
Jun  9 11:37:41.125: INFO: stdout: "e2e-test-crd-publish-openapi-5554-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun  9 11:37:41.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-9823 --namespace=crd-publish-openapi-9823 delete e2e-test-crd-publish-openapi-5554-crds test-cr'
Jun  9 11:37:41.345: INFO: stderr: ""
Jun  9 11:37:41.345: INFO: stdout: "e2e-test-crd-publish-openapi-5554-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 06/09/23 11:37:41.345
Jun  9 11:37:41.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-9823 explain e2e-test-crd-publish-openapi-5554-crds'
Jun  9 11:37:41.708: INFO: stderr: ""
Jun  9 11:37:41.708: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5554-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:37:44.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9823" for this suite. 06/09/23 11:37:44.589
------------------------------
• [SLOW TEST] [9.376 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:37:35.226
    Jun  9 11:37:35.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 11:37:35.228
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:35.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:35.265
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jun  9 11:37:35.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/09/23 11:37:38.594
    Jun  9 11:37:38.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-9823 --namespace=crd-publish-openapi-9823 create -f -'
    Jun  9 11:37:39.953: INFO: stderr: ""
    Jun  9 11:37:39.953: INFO: stdout: "e2e-test-crd-publish-openapi-5554-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jun  9 11:37:39.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-9823 --namespace=crd-publish-openapi-9823 delete e2e-test-crd-publish-openapi-5554-crds test-cr'
    Jun  9 11:37:40.090: INFO: stderr: ""
    Jun  9 11:37:40.090: INFO: stdout: "e2e-test-crd-publish-openapi-5554-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jun  9 11:37:40.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-9823 --namespace=crd-publish-openapi-9823 apply -f -'
    Jun  9 11:37:41.125: INFO: stderr: ""
    Jun  9 11:37:41.125: INFO: stdout: "e2e-test-crd-publish-openapi-5554-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jun  9 11:37:41.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-9823 --namespace=crd-publish-openapi-9823 delete e2e-test-crd-publish-openapi-5554-crds test-cr'
    Jun  9 11:37:41.345: INFO: stderr: ""
    Jun  9 11:37:41.345: INFO: stdout: "e2e-test-crd-publish-openapi-5554-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 06/09/23 11:37:41.345
    Jun  9 11:37:41.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-9823 explain e2e-test-crd-publish-openapi-5554-crds'
    Jun  9 11:37:41.708: INFO: stderr: ""
    Jun  9 11:37:41.708: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5554-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:37:44.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9823" for this suite. 06/09/23 11:37:44.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:37:44.603
Jun  9 11:37:44.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename sched-pred 06/09/23 11:37:44.604
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:44.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:44.632
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jun  9 11:37:44.637: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  9 11:37:44.655: INFO: Waiting for terminating namespaces to be deleted...
Jun  9 11:37:44.669: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-4hkw9 before test
Jun  9 11:37:44.687: INFO: calico-kube-controllers-75b856575b-4vmmr from calico-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 11:37:44.687: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun  9 11:37:44.687: INFO: calico-node-7bzns from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
Jun  9 11:37:44.687: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 11:37:44.687: INFO: csi-node-driver-qw6d7 from calico-system started at 2023-06-09 06:03:15 +0000 UTC (2 container statuses recorded)
Jun  9 11:37:44.687: INFO: 	Container calico-csi ready: true, restart count 0
Jun  9 11:37:44.687: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun  9 11:37:44.687: INFO: coredns-5c4bbd897-mndk4 from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 11:37:44.687: INFO: 	Container coredns ready: true, restart count 0
Jun  9 11:37:44.687: INFO: coredns-5c4bbd897-vswmt from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 11:37:44.687: INFO: 	Container coredns ready: true, restart count 0
Jun  9 11:37:44.687: INFO: kube-proxy-f8glt from kube-system started at 2023-06-09 06:01:45 +0000 UTC (1 container statuses recorded)
Jun  9 11:37:44.687: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 11:37:44.687: INFO: snapshot-controller-df74f7b6c-pdcpl from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 11:37:44.687: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun  9 11:37:44.687: INFO: smtx-elf-csi-driver-node-plugin-dbgrm from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
Jun  9 11:37:44.687: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 11:37:44.687: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  9 11:37:44.687: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 11:37:44.687: INFO: tigera-operator-5948566997-9t6st from sks-system started at 2023-06-09 06:02:52 +0000 UTC (1 container statuses recorded)
Jun  9 11:37:44.687: INFO: 	Container tigera-operator ready: true, restart count 0
Jun  9 11:37:44.687: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-4r82f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 11:37:44.687: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 11:37:44.687: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 11:37:44.687: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-q5bjm before test
Jun  9 11:37:44.705: INFO: calico-node-vpfwq from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
Jun  9 11:37:44.706: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 11:37:44.706: INFO: calico-typha-866dbffd5c-r6nf8 from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
Jun  9 11:37:44.706: INFO: 	Container calico-typha ready: true, restart count 0
Jun  9 11:37:44.706: INFO: csi-node-driver-k2xdl from calico-system started at 2023-06-09 06:04:17 +0000 UTC (2 container statuses recorded)
Jun  9 11:37:44.706: INFO: 	Container calico-csi ready: true, restart count 0
Jun  9 11:37:44.706: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun  9 11:37:44.706: INFO: kube-proxy-b8r9m from kube-system started at 2023-06-09 06:02:23 +0000 UTC (1 container statuses recorded)
Jun  9 11:37:44.706: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 11:37:44.706: INFO: smtx-elf-csi-driver-controller-plugin-7bb788b57-vhsgd from sks-system started at 2023-06-09 06:04:22 +0000 UTC (6 container statuses recorded)
Jun  9 11:37:44.706: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  9 11:37:44.706: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 11:37:44.706: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun  9 11:37:44.706: INFO: 	Container csi-resizer ready: true, restart count 0
Jun  9 11:37:44.706: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jun  9 11:37:44.706: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 11:37:44.706: INFO: smtx-elf-csi-driver-node-plugin-cs9pp from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
Jun  9 11:37:44.706: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 11:37:44.706: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  9 11:37:44.706: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 11:37:44.706: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-crx45 from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 11:37:44.706: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 11:37:44.706: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 11:37:44.706: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-qdprq before test
Jun  9 11:37:44.722: INFO: calico-node-5cjkc from calico-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
Jun  9 11:37:44.722: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 11:37:44.722: INFO: calico-typha-866dbffd5c-n5p5b from calico-system started at 2023-06-09 06:03:07 +0000 UTC (1 container statuses recorded)
Jun  9 11:37:44.722: INFO: 	Container calico-typha ready: true, restart count 0
Jun  9 11:37:44.722: INFO: csi-node-driver-f9pz9 from calico-system started at 2023-06-09 11:24:43 +0000 UTC (2 container statuses recorded)
Jun  9 11:37:44.722: INFO: 	Container calico-csi ready: true, restart count 0
Jun  9 11:37:44.722: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun  9 11:37:44.722: INFO: kube-proxy-gpttb from kube-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
Jun  9 11:37:44.722: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 11:37:44.722: INFO: smtx-elf-csi-driver-node-plugin-9plgj from sks-system started at 2023-06-09 06:04:22 +0000 UTC (3 container statuses recorded)
Jun  9 11:37:44.722: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 11:37:44.722: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  9 11:37:44.722: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 11:37:44.722: INFO: sonobuoy from sonobuoy started at 2023-06-09 10:18:23 +0000 UTC (1 container statuses recorded)
Jun  9 11:37:44.722: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  9 11:37:44.722: INFO: sonobuoy-e2e-job-09e3c2818710427f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 11:37:44.722: INFO: 	Container e2e ready: true, restart count 0
Jun  9 11:37:44.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 11:37:44.722: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-f89bf from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 11:37:44.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 11:37:44.722: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/09/23 11:37:44.722
Jun  9 11:37:44.739: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9090" to be "running"
Jun  9 11:37:44.747: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031778ms
Jun  9 11:37:46.757: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.017495247s
Jun  9 11:37:46.757: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/09/23 11:37:46.765
STEP: Trying to apply a random label on the found node. 06/09/23 11:37:46.905
STEP: verifying the node has the label kubernetes.io/e2e-1dbcf36b-bac7-4dd0-9f4b-df697d7d6538 42 06/09/23 11:37:46.935
STEP: Trying to relaunch the pod, now with labels. 06/09/23 11:37:46.942
Jun  9 11:37:46.955: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-9090" to be "not pending"
Jun  9 11:37:46.967: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 11.849591ms
Jun  9 11:37:48.979: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.023968511s
Jun  9 11:37:48.980: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-1dbcf36b-bac7-4dd0-9f4b-df697d7d6538 off the node sks-test-v1-26.4-workergroup-qdprq 06/09/23 11:37:48.987
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1dbcf36b-bac7-4dd0-9f4b-df697d7d6538 06/09/23 11:37:49.02
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:37:49.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9090" for this suite. 06/09/23 11:37:49.034
------------------------------
• [4.447 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:37:44.603
    Jun  9 11:37:44.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename sched-pred 06/09/23 11:37:44.604
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:44.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:44.632
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jun  9 11:37:44.637: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun  9 11:37:44.655: INFO: Waiting for terminating namespaces to be deleted...
    Jun  9 11:37:44.669: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-4hkw9 before test
    Jun  9 11:37:44.687: INFO: calico-kube-controllers-75b856575b-4vmmr from calico-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 11:37:44.687: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jun  9 11:37:44.687: INFO: calico-node-7bzns from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
    Jun  9 11:37:44.687: INFO: 	Container calico-node ready: true, restart count 0
    Jun  9 11:37:44.687: INFO: csi-node-driver-qw6d7 from calico-system started at 2023-06-09 06:03:15 +0000 UTC (2 container statuses recorded)
    Jun  9 11:37:44.687: INFO: 	Container calico-csi ready: true, restart count 0
    Jun  9 11:37:44.687: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun  9 11:37:44.687: INFO: coredns-5c4bbd897-mndk4 from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 11:37:44.687: INFO: 	Container coredns ready: true, restart count 0
    Jun  9 11:37:44.687: INFO: coredns-5c4bbd897-vswmt from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 11:37:44.687: INFO: 	Container coredns ready: true, restart count 0
    Jun  9 11:37:44.687: INFO: kube-proxy-f8glt from kube-system started at 2023-06-09 06:01:45 +0000 UTC (1 container statuses recorded)
    Jun  9 11:37:44.687: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  9 11:37:44.687: INFO: snapshot-controller-df74f7b6c-pdcpl from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 11:37:44.687: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun  9 11:37:44.687: INFO: smtx-elf-csi-driver-node-plugin-dbgrm from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
    Jun  9 11:37:44.687: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 11:37:44.687: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun  9 11:37:44.687: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 11:37:44.687: INFO: tigera-operator-5948566997-9t6st from sks-system started at 2023-06-09 06:02:52 +0000 UTC (1 container statuses recorded)
    Jun  9 11:37:44.687: INFO: 	Container tigera-operator ready: true, restart count 0
    Jun  9 11:37:44.687: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-4r82f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 11:37:44.687: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 11:37:44.687: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  9 11:37:44.687: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-q5bjm before test
    Jun  9 11:37:44.705: INFO: calico-node-vpfwq from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
    Jun  9 11:37:44.706: INFO: 	Container calico-node ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: calico-typha-866dbffd5c-r6nf8 from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
    Jun  9 11:37:44.706: INFO: 	Container calico-typha ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: csi-node-driver-k2xdl from calico-system started at 2023-06-09 06:04:17 +0000 UTC (2 container statuses recorded)
    Jun  9 11:37:44.706: INFO: 	Container calico-csi ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: kube-proxy-b8r9m from kube-system started at 2023-06-09 06:02:23 +0000 UTC (1 container statuses recorded)
    Jun  9 11:37:44.706: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: smtx-elf-csi-driver-controller-plugin-7bb788b57-vhsgd from sks-system started at 2023-06-09 06:04:22 +0000 UTC (6 container statuses recorded)
    Jun  9 11:37:44.706: INFO: 	Container csi-attacher ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: 	Container csi-resizer ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: smtx-elf-csi-driver-node-plugin-cs9pp from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
    Jun  9 11:37:44.706: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-crx45 from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 11:37:44.706: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  9 11:37:44.706: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-qdprq before test
    Jun  9 11:37:44.722: INFO: calico-node-5cjkc from calico-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
    Jun  9 11:37:44.722: INFO: 	Container calico-node ready: true, restart count 0
    Jun  9 11:37:44.722: INFO: calico-typha-866dbffd5c-n5p5b from calico-system started at 2023-06-09 06:03:07 +0000 UTC (1 container statuses recorded)
    Jun  9 11:37:44.722: INFO: 	Container calico-typha ready: true, restart count 0
    Jun  9 11:37:44.722: INFO: csi-node-driver-f9pz9 from calico-system started at 2023-06-09 11:24:43 +0000 UTC (2 container statuses recorded)
    Jun  9 11:37:44.722: INFO: 	Container calico-csi ready: true, restart count 0
    Jun  9 11:37:44.722: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun  9 11:37:44.722: INFO: kube-proxy-gpttb from kube-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
    Jun  9 11:37:44.722: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  9 11:37:44.722: INFO: smtx-elf-csi-driver-node-plugin-9plgj from sks-system started at 2023-06-09 06:04:22 +0000 UTC (3 container statuses recorded)
    Jun  9 11:37:44.722: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 11:37:44.722: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun  9 11:37:44.722: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 11:37:44.722: INFO: sonobuoy from sonobuoy started at 2023-06-09 10:18:23 +0000 UTC (1 container statuses recorded)
    Jun  9 11:37:44.722: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun  9 11:37:44.722: INFO: sonobuoy-e2e-job-09e3c2818710427f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 11:37:44.722: INFO: 	Container e2e ready: true, restart count 0
    Jun  9 11:37:44.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 11:37:44.722: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-f89bf from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 11:37:44.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 11:37:44.722: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/09/23 11:37:44.722
    Jun  9 11:37:44.739: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9090" to be "running"
    Jun  9 11:37:44.747: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031778ms
    Jun  9 11:37:46.757: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.017495247s
    Jun  9 11:37:46.757: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/09/23 11:37:46.765
    STEP: Trying to apply a random label on the found node. 06/09/23 11:37:46.905
    STEP: verifying the node has the label kubernetes.io/e2e-1dbcf36b-bac7-4dd0-9f4b-df697d7d6538 42 06/09/23 11:37:46.935
    STEP: Trying to relaunch the pod, now with labels. 06/09/23 11:37:46.942
    Jun  9 11:37:46.955: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-9090" to be "not pending"
    Jun  9 11:37:46.967: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 11.849591ms
    Jun  9 11:37:48.979: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.023968511s
    Jun  9 11:37:48.980: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-1dbcf36b-bac7-4dd0-9f4b-df697d7d6538 off the node sks-test-v1-26.4-workergroup-qdprq 06/09/23 11:37:48.987
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-1dbcf36b-bac7-4dd0-9f4b-df697d7d6538 06/09/23 11:37:49.02
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:37:49.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9090" for this suite. 06/09/23 11:37:49.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:37:49.051
Jun  9 11:37:49.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir-wrapper 06/09/23 11:37:49.053
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:49.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:49.1
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jun  9 11:37:49.144: INFO: Waiting up to 5m0s for pod "pod-secrets-2e1376a4-47c2-4739-ac21-0e1c3d76af8c" in namespace "emptydir-wrapper-8985" to be "running and ready"
Jun  9 11:37:49.156: INFO: Pod "pod-secrets-2e1376a4-47c2-4739-ac21-0e1c3d76af8c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.618027ms
Jun  9 11:37:49.156: INFO: The phase of Pod pod-secrets-2e1376a4-47c2-4739-ac21-0e1c3d76af8c is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:37:51.163: INFO: Pod "pod-secrets-2e1376a4-47c2-4739-ac21-0e1c3d76af8c": Phase="Running", Reason="", readiness=true. Elapsed: 2.018593389s
Jun  9 11:37:51.163: INFO: The phase of Pod pod-secrets-2e1376a4-47c2-4739-ac21-0e1c3d76af8c is Running (Ready = true)
Jun  9 11:37:51.163: INFO: Pod "pod-secrets-2e1376a4-47c2-4739-ac21-0e1c3d76af8c" satisfied condition "running and ready"
STEP: Cleaning up the secret 06/09/23 11:37:51.17
STEP: Cleaning up the configmap 06/09/23 11:37:51.182
STEP: Cleaning up the pod 06/09/23 11:37:51.195
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 11:37:51.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8985" for this suite. 06/09/23 11:37:51.242
------------------------------
• [2.201 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:37:49.051
    Jun  9 11:37:49.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir-wrapper 06/09/23 11:37:49.053
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:49.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:49.1
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jun  9 11:37:49.144: INFO: Waiting up to 5m0s for pod "pod-secrets-2e1376a4-47c2-4739-ac21-0e1c3d76af8c" in namespace "emptydir-wrapper-8985" to be "running and ready"
    Jun  9 11:37:49.156: INFO: Pod "pod-secrets-2e1376a4-47c2-4739-ac21-0e1c3d76af8c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.618027ms
    Jun  9 11:37:49.156: INFO: The phase of Pod pod-secrets-2e1376a4-47c2-4739-ac21-0e1c3d76af8c is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:37:51.163: INFO: Pod "pod-secrets-2e1376a4-47c2-4739-ac21-0e1c3d76af8c": Phase="Running", Reason="", readiness=true. Elapsed: 2.018593389s
    Jun  9 11:37:51.163: INFO: The phase of Pod pod-secrets-2e1376a4-47c2-4739-ac21-0e1c3d76af8c is Running (Ready = true)
    Jun  9 11:37:51.163: INFO: Pod "pod-secrets-2e1376a4-47c2-4739-ac21-0e1c3d76af8c" satisfied condition "running and ready"
    STEP: Cleaning up the secret 06/09/23 11:37:51.17
    STEP: Cleaning up the configmap 06/09/23 11:37:51.182
    STEP: Cleaning up the pod 06/09/23 11:37:51.195
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:37:51.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8985" for this suite. 06/09/23 11:37:51.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:37:51.253
Jun  9 11:37:51.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 11:37:51.254
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:51.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:51.277
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 06/09/23 11:37:51.283
Jun  9 11:37:51.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: rename a version 06/09/23 11:37:57.14
STEP: check the new version name is served 06/09/23 11:37:57.173
STEP: check the old version name is removed 06/09/23 11:38:00.542
STEP: check the other version is not changed 06/09/23 11:38:01.431
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:38:06.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-56" for this suite. 06/09/23 11:38:06.635
------------------------------
• [SLOW TEST] [15.407 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:37:51.253
    Jun  9 11:37:51.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 11:37:51.254
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:37:51.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:37:51.277
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 06/09/23 11:37:51.283
    Jun  9 11:37:51.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: rename a version 06/09/23 11:37:57.14
    STEP: check the new version name is served 06/09/23 11:37:57.173
    STEP: check the old version name is removed 06/09/23 11:38:00.542
    STEP: check the other version is not changed 06/09/23 11:38:01.431
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:38:06.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-56" for this suite. 06/09/23 11:38:06.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:38:06.661
Jun  9 11:38:06.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 11:38:06.663
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:38:06.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:38:06.752
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 06/09/23 11:38:06.764
STEP: waiting for available Endpoint 06/09/23 11:38:06.778
STEP: listing all Endpoints 06/09/23 11:38:06.78
STEP: updating the Endpoint 06/09/23 11:38:06.785
STEP: fetching the Endpoint 06/09/23 11:38:06.797
STEP: patching the Endpoint 06/09/23 11:38:06.803
STEP: fetching the Endpoint 06/09/23 11:38:06.817
STEP: deleting the Endpoint by Collection 06/09/23 11:38:06.824
STEP: waiting for Endpoint deletion 06/09/23 11:38:06.841
STEP: fetching the Endpoint 06/09/23 11:38:06.846
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 11:38:06.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7310" for this suite. 06/09/23 11:38:06.864
------------------------------
• [0.214 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:38:06.661
    Jun  9 11:38:06.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 11:38:06.663
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:38:06.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:38:06.752
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 06/09/23 11:38:06.764
    STEP: waiting for available Endpoint 06/09/23 11:38:06.778
    STEP: listing all Endpoints 06/09/23 11:38:06.78
    STEP: updating the Endpoint 06/09/23 11:38:06.785
    STEP: fetching the Endpoint 06/09/23 11:38:06.797
    STEP: patching the Endpoint 06/09/23 11:38:06.803
    STEP: fetching the Endpoint 06/09/23 11:38:06.817
    STEP: deleting the Endpoint by Collection 06/09/23 11:38:06.824
    STEP: waiting for Endpoint deletion 06/09/23 11:38:06.841
    STEP: fetching the Endpoint 06/09/23 11:38:06.846
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:38:06.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7310" for this suite. 06/09/23 11:38:06.864
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:38:06.876
Jun  9 11:38:06.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 11:38:06.877
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:38:06.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:38:06.908
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 11:38:06.957
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:38:07.309
STEP: Deploying the webhook pod 06/09/23 11:38:07.325
STEP: Wait for the deployment to be ready 06/09/23 11:38:07.355
Jun  9 11:38:07.368: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 11:38:09.402
STEP: Verifying the service has paired with the endpoint 06/09/23 11:38:09.443
Jun  9 11:38:10.443: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 06/09/23 11:38:10.452
STEP: create a pod that should be denied by the webhook 06/09/23 11:38:10.482
STEP: create a pod that causes the webhook to hang 06/09/23 11:38:10.511
STEP: create a configmap that should be denied by the webhook 06/09/23 11:38:20.525
STEP: create a configmap that should be admitted by the webhook 06/09/23 11:38:20.543
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 06/09/23 11:38:20.561
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 06/09/23 11:38:20.583
STEP: create a namespace that bypass the webhook 06/09/23 11:38:20.595
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 06/09/23 11:38:20.61
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:38:20.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3032" for this suite. 06/09/23 11:38:20.84
STEP: Destroying namespace "webhook-3032-markers" for this suite. 06/09/23 11:38:20.856
------------------------------
• [SLOW TEST] [14.002 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:38:06.876
    Jun  9 11:38:06.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 11:38:06.877
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:38:06.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:38:06.908
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 11:38:06.957
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:38:07.309
    STEP: Deploying the webhook pod 06/09/23 11:38:07.325
    STEP: Wait for the deployment to be ready 06/09/23 11:38:07.355
    Jun  9 11:38:07.368: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 11:38:09.402
    STEP: Verifying the service has paired with the endpoint 06/09/23 11:38:09.443
    Jun  9 11:38:10.443: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 06/09/23 11:38:10.452
    STEP: create a pod that should be denied by the webhook 06/09/23 11:38:10.482
    STEP: create a pod that causes the webhook to hang 06/09/23 11:38:10.511
    STEP: create a configmap that should be denied by the webhook 06/09/23 11:38:20.525
    STEP: create a configmap that should be admitted by the webhook 06/09/23 11:38:20.543
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 06/09/23 11:38:20.561
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 06/09/23 11:38:20.583
    STEP: create a namespace that bypass the webhook 06/09/23 11:38:20.595
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 06/09/23 11:38:20.61
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:38:20.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3032" for this suite. 06/09/23 11:38:20.84
    STEP: Destroying namespace "webhook-3032-markers" for this suite. 06/09/23 11:38:20.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:38:20.88
Jun  9 11:38:20.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename subpath 06/09/23 11:38:20.882
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:38:20.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:38:20.939
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/09/23 11:38:20.954
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-wx69 06/09/23 11:38:20.973
STEP: Creating a pod to test atomic-volume-subpath 06/09/23 11:38:20.973
Jun  9 11:38:20.987: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wx69" in namespace "subpath-8214" to be "Succeeded or Failed"
Jun  9 11:38:20.997: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Pending", Reason="", readiness=false. Elapsed: 9.931538ms
Jun  9 11:38:23.006: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018876458s
Jun  9 11:38:25.006: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 4.018675555s
Jun  9 11:38:27.005: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 6.018198969s
Jun  9 11:38:29.004: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 8.016675994s
Jun  9 11:38:31.018: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 10.031032939s
Jun  9 11:38:33.005: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 12.018060201s
Jun  9 11:38:35.005: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 14.018463936s
Jun  9 11:38:37.007: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 16.019979413s
Jun  9 11:38:39.004: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 18.017490863s
Jun  9 11:38:41.011: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 20.023958553s
Jun  9 11:38:43.003: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 22.016391855s
Jun  9 11:38:45.006: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=false. Elapsed: 24.018735986s
Jun  9 11:38:47.004: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.01685566s
STEP: Saw pod success 06/09/23 11:38:47.004
Jun  9 11:38:47.004: INFO: Pod "pod-subpath-test-secret-wx69" satisfied condition "Succeeded or Failed"
Jun  9 11:38:47.013: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-subpath-test-secret-wx69 container test-container-subpath-secret-wx69: <nil>
STEP: delete the pod 06/09/23 11:38:47.026
Jun  9 11:38:47.059: INFO: Waiting for pod pod-subpath-test-secret-wx69 to disappear
Jun  9 11:38:47.065: INFO: Pod pod-subpath-test-secret-wx69 no longer exists
STEP: Deleting pod pod-subpath-test-secret-wx69 06/09/23 11:38:47.065
Jun  9 11:38:47.066: INFO: Deleting pod "pod-subpath-test-secret-wx69" in namespace "subpath-8214"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun  9 11:38:47.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8214" for this suite. 06/09/23 11:38:47.088
------------------------------
• [SLOW TEST] [26.251 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:38:20.88
    Jun  9 11:38:20.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename subpath 06/09/23 11:38:20.882
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:38:20.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:38:20.939
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/09/23 11:38:20.954
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-wx69 06/09/23 11:38:20.973
    STEP: Creating a pod to test atomic-volume-subpath 06/09/23 11:38:20.973
    Jun  9 11:38:20.987: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wx69" in namespace "subpath-8214" to be "Succeeded or Failed"
    Jun  9 11:38:20.997: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Pending", Reason="", readiness=false. Elapsed: 9.931538ms
    Jun  9 11:38:23.006: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018876458s
    Jun  9 11:38:25.006: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 4.018675555s
    Jun  9 11:38:27.005: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 6.018198969s
    Jun  9 11:38:29.004: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 8.016675994s
    Jun  9 11:38:31.018: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 10.031032939s
    Jun  9 11:38:33.005: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 12.018060201s
    Jun  9 11:38:35.005: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 14.018463936s
    Jun  9 11:38:37.007: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 16.019979413s
    Jun  9 11:38:39.004: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 18.017490863s
    Jun  9 11:38:41.011: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 20.023958553s
    Jun  9 11:38:43.003: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=true. Elapsed: 22.016391855s
    Jun  9 11:38:45.006: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Running", Reason="", readiness=false. Elapsed: 24.018735986s
    Jun  9 11:38:47.004: INFO: Pod "pod-subpath-test-secret-wx69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.01685566s
    STEP: Saw pod success 06/09/23 11:38:47.004
    Jun  9 11:38:47.004: INFO: Pod "pod-subpath-test-secret-wx69" satisfied condition "Succeeded or Failed"
    Jun  9 11:38:47.013: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-subpath-test-secret-wx69 container test-container-subpath-secret-wx69: <nil>
    STEP: delete the pod 06/09/23 11:38:47.026
    Jun  9 11:38:47.059: INFO: Waiting for pod pod-subpath-test-secret-wx69 to disappear
    Jun  9 11:38:47.065: INFO: Pod pod-subpath-test-secret-wx69 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-wx69 06/09/23 11:38:47.065
    Jun  9 11:38:47.066: INFO: Deleting pod "pod-subpath-test-secret-wx69" in namespace "subpath-8214"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:38:47.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8214" for this suite. 06/09/23 11:38:47.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:38:47.133
Jun  9 11:38:47.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename controllerrevisions 06/09/23 11:38:47.135
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:38:47.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:38:47.173
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-k9flm-daemon-set" 06/09/23 11:38:47.218
STEP: Check that daemon pods launch on every node of the cluster. 06/09/23 11:38:47.232
Jun  9 11:38:47.244: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:38:47.244: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:38:47.244: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:38:47.260: INFO: Number of nodes with available pods controlled by daemonset e2e-k9flm-daemon-set: 0
Jun  9 11:38:47.260: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 11:38:48.421: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:38:48.421: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:38:48.421: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:38:48.431: INFO: Number of nodes with available pods controlled by daemonset e2e-k9flm-daemon-set: 0
Jun  9 11:38:48.431: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 11:38:49.274: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:38:49.274: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:38:49.274: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:38:49.282: INFO: Number of nodes with available pods controlled by daemonset e2e-k9flm-daemon-set: 2
Jun  9 11:38:49.282: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
Jun  9 11:38:50.267: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:38:50.267: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:38:50.267: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  9 11:38:50.273: INFO: Number of nodes with available pods controlled by daemonset e2e-k9flm-daemon-set: 3
Jun  9 11:38:50.273: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-k9flm-daemon-set
STEP: Confirm DaemonSet "e2e-k9flm-daemon-set" successfully created with "daemonset-name=e2e-k9flm-daemon-set" label 06/09/23 11:38:50.28
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-k9flm-daemon-set" 06/09/23 11:38:50.295
Jun  9 11:38:50.302: INFO: Located ControllerRevision: "e2e-k9flm-daemon-set-675cc4f6f8"
STEP: Patching ControllerRevision "e2e-k9flm-daemon-set-675cc4f6f8" 06/09/23 11:38:50.308
Jun  9 11:38:50.322: INFO: e2e-k9flm-daemon-set-675cc4f6f8 has been patched
STEP: Create a new ControllerRevision 06/09/23 11:38:50.322
Jun  9 11:38:50.338: INFO: Created ControllerRevision: e2e-k9flm-daemon-set-698c9cd496
STEP: Confirm that there are two ControllerRevisions 06/09/23 11:38:50.338
Jun  9 11:38:50.338: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun  9 11:38:50.344: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-k9flm-daemon-set-675cc4f6f8" 06/09/23 11:38:50.344
STEP: Confirm that there is only one ControllerRevision 06/09/23 11:38:50.382
Jun  9 11:38:50.382: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun  9 11:38:50.388: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-k9flm-daemon-set-698c9cd496" 06/09/23 11:38:50.394
Jun  9 11:38:50.431: INFO: e2e-k9flm-daemon-set-698c9cd496 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 06/09/23 11:38:50.431
W0609 11:38:50.475954      18 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 06/09/23 11:38:50.476
Jun  9 11:38:50.476: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun  9 11:38:51.485: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun  9 11:38:51.525: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-k9flm-daemon-set-698c9cd496=updated" 06/09/23 11:38:51.525
STEP: Confirm that there is only one ControllerRevision 06/09/23 11:38:51.699
Jun  9 11:38:51.699: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun  9 11:38:51.706: INFO: Found 1 ControllerRevisions
Jun  9 11:38:51.759: INFO: ControllerRevision "e2e-k9flm-daemon-set-5c45f8c8f7" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-k9flm-daemon-set" 06/09/23 11:38:51.782
STEP: deleting DaemonSet.extensions e2e-k9flm-daemon-set in namespace controllerrevisions-1192, will wait for the garbage collector to delete the pods 06/09/23 11:38:51.783
Jun  9 11:38:51.878: INFO: Deleting DaemonSet.extensions e2e-k9flm-daemon-set took: 31.45808ms
Jun  9 11:38:52.079: INFO: Terminating DaemonSet.extensions e2e-k9flm-daemon-set pods took: 200.785194ms
Jun  9 11:38:53.986: INFO: Number of nodes with available pods controlled by daemonset e2e-k9flm-daemon-set: 0
Jun  9 11:38:53.986: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-k9flm-daemon-set
Jun  9 11:38:53.992: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"112591"},"items":null}

Jun  9 11:38:53.999: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"112591"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:38:54.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-1192" for this suite. 06/09/23 11:38:54.038
------------------------------
• [SLOW TEST] [6.917 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:38:47.133
    Jun  9 11:38:47.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename controllerrevisions 06/09/23 11:38:47.135
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:38:47.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:38:47.173
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-k9flm-daemon-set" 06/09/23 11:38:47.218
    STEP: Check that daemon pods launch on every node of the cluster. 06/09/23 11:38:47.232
    Jun  9 11:38:47.244: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:38:47.244: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:38:47.244: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:38:47.260: INFO: Number of nodes with available pods controlled by daemonset e2e-k9flm-daemon-set: 0
    Jun  9 11:38:47.260: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 11:38:48.421: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:38:48.421: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:38:48.421: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:38:48.431: INFO: Number of nodes with available pods controlled by daemonset e2e-k9flm-daemon-set: 0
    Jun  9 11:38:48.431: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 11:38:49.274: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:38:49.274: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:38:49.274: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:38:49.282: INFO: Number of nodes with available pods controlled by daemonset e2e-k9flm-daemon-set: 2
    Jun  9 11:38:49.282: INFO: Node sks-test-v1-26.4-workergroup-4hkw9 is running 0 daemon pod, expected 1
    Jun  9 11:38:50.267: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-hvp4c with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:38:50.267: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-ncc4t with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:38:50.267: INFO: DaemonSet pods can't tolerate node sks-test-v1-26.4-controlplane-qtlkh with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun  9 11:38:50.273: INFO: Number of nodes with available pods controlled by daemonset e2e-k9flm-daemon-set: 3
    Jun  9 11:38:50.273: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-k9flm-daemon-set
    STEP: Confirm DaemonSet "e2e-k9flm-daemon-set" successfully created with "daemonset-name=e2e-k9flm-daemon-set" label 06/09/23 11:38:50.28
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-k9flm-daemon-set" 06/09/23 11:38:50.295
    Jun  9 11:38:50.302: INFO: Located ControllerRevision: "e2e-k9flm-daemon-set-675cc4f6f8"
    STEP: Patching ControllerRevision "e2e-k9flm-daemon-set-675cc4f6f8" 06/09/23 11:38:50.308
    Jun  9 11:38:50.322: INFO: e2e-k9flm-daemon-set-675cc4f6f8 has been patched
    STEP: Create a new ControllerRevision 06/09/23 11:38:50.322
    Jun  9 11:38:50.338: INFO: Created ControllerRevision: e2e-k9flm-daemon-set-698c9cd496
    STEP: Confirm that there are two ControllerRevisions 06/09/23 11:38:50.338
    Jun  9 11:38:50.338: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun  9 11:38:50.344: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-k9flm-daemon-set-675cc4f6f8" 06/09/23 11:38:50.344
    STEP: Confirm that there is only one ControllerRevision 06/09/23 11:38:50.382
    Jun  9 11:38:50.382: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun  9 11:38:50.388: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-k9flm-daemon-set-698c9cd496" 06/09/23 11:38:50.394
    Jun  9 11:38:50.431: INFO: e2e-k9flm-daemon-set-698c9cd496 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 06/09/23 11:38:50.431
    W0609 11:38:50.475954      18 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 06/09/23 11:38:50.476
    Jun  9 11:38:50.476: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun  9 11:38:51.485: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun  9 11:38:51.525: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-k9flm-daemon-set-698c9cd496=updated" 06/09/23 11:38:51.525
    STEP: Confirm that there is only one ControllerRevision 06/09/23 11:38:51.699
    Jun  9 11:38:51.699: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun  9 11:38:51.706: INFO: Found 1 ControllerRevisions
    Jun  9 11:38:51.759: INFO: ControllerRevision "e2e-k9flm-daemon-set-5c45f8c8f7" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-k9flm-daemon-set" 06/09/23 11:38:51.782
    STEP: deleting DaemonSet.extensions e2e-k9flm-daemon-set in namespace controllerrevisions-1192, will wait for the garbage collector to delete the pods 06/09/23 11:38:51.783
    Jun  9 11:38:51.878: INFO: Deleting DaemonSet.extensions e2e-k9flm-daemon-set took: 31.45808ms
    Jun  9 11:38:52.079: INFO: Terminating DaemonSet.extensions e2e-k9flm-daemon-set pods took: 200.785194ms
    Jun  9 11:38:53.986: INFO: Number of nodes with available pods controlled by daemonset e2e-k9flm-daemon-set: 0
    Jun  9 11:38:53.986: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-k9flm-daemon-set
    Jun  9 11:38:53.992: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"112591"},"items":null}

    Jun  9 11:38:53.999: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"112591"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:38:54.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-1192" for this suite. 06/09/23 11:38:54.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:38:54.051
Jun  9 11:38:54.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename resourcequota 06/09/23 11:38:54.052
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:38:54.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:38:54.083
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 06/09/23 11:38:54.091
STEP: Creating a ResourceQuota 06/09/23 11:38:59.097
STEP: Ensuring resource quota status is calculated 06/09/23 11:38:59.11
STEP: Creating a ReplicationController 06/09/23 11:39:01.119
STEP: Ensuring resource quota status captures replication controller creation 06/09/23 11:39:01.225
STEP: Deleting a ReplicationController 06/09/23 11:39:03.237
STEP: Ensuring resource quota status released usage 06/09/23 11:39:03.253
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  9 11:39:05.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5864" for this suite. 06/09/23 11:39:05.275
------------------------------
• [SLOW TEST] [11.240 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:38:54.051
    Jun  9 11:38:54.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename resourcequota 06/09/23 11:38:54.052
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:38:54.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:38:54.083
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 06/09/23 11:38:54.091
    STEP: Creating a ResourceQuota 06/09/23 11:38:59.097
    STEP: Ensuring resource quota status is calculated 06/09/23 11:38:59.11
    STEP: Creating a ReplicationController 06/09/23 11:39:01.119
    STEP: Ensuring resource quota status captures replication controller creation 06/09/23 11:39:01.225
    STEP: Deleting a ReplicationController 06/09/23 11:39:03.237
    STEP: Ensuring resource quota status released usage 06/09/23 11:39:03.253
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:39:05.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5864" for this suite. 06/09/23 11:39:05.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:39:05.291
Jun  9 11:39:05.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 11:39:05.293
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:39:05.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:39:05.419
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 11:39:05.452
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:39:05.775
STEP: Deploying the webhook pod 06/09/23 11:39:05.823
STEP: Wait for the deployment to be ready 06/09/23 11:39:05.855
Jun  9 11:39:05.944: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  9 11:39:07.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 39, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 39, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/09/23 11:39:10.03
STEP: Verifying the service has paired with the endpoint 06/09/23 11:39:10.134
Jun  9 11:39:11.135: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jun  9 11:39:11.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2513-crds.webhook.example.com via the AdmissionRegistration API 06/09/23 11:39:11.663
STEP: Creating a custom resource while v1 is storage version 06/09/23 11:39:11.698
STEP: Patching Custom Resource Definition to set v2 as storage 06/09/23 11:39:13.786
STEP: Patching the custom resource while v2 is storage version 06/09/23 11:39:13.813
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:39:14.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4530" for this suite. 06/09/23 11:39:14.596
STEP: Destroying namespace "webhook-4530-markers" for this suite. 06/09/23 11:39:14.616
------------------------------
• [SLOW TEST] [9.355 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:39:05.291
    Jun  9 11:39:05.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 11:39:05.293
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:39:05.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:39:05.419
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 11:39:05.452
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:39:05.775
    STEP: Deploying the webhook pod 06/09/23 11:39:05.823
    STEP: Wait for the deployment to be ready 06/09/23 11:39:05.855
    Jun  9 11:39:05.944: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun  9 11:39:07.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 9, 11, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 39, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 39, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/09/23 11:39:10.03
    STEP: Verifying the service has paired with the endpoint 06/09/23 11:39:10.134
    Jun  9 11:39:11.135: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jun  9 11:39:11.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2513-crds.webhook.example.com via the AdmissionRegistration API 06/09/23 11:39:11.663
    STEP: Creating a custom resource while v1 is storage version 06/09/23 11:39:11.698
    STEP: Patching Custom Resource Definition to set v2 as storage 06/09/23 11:39:13.786
    STEP: Patching the custom resource while v2 is storage version 06/09/23 11:39:13.813
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:39:14.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4530" for this suite. 06/09/23 11:39:14.596
    STEP: Destroying namespace "webhook-4530-markers" for this suite. 06/09/23 11:39:14.616
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:39:14.647
Jun  9 11:39:14.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename secrets 06/09/23 11:39:14.648
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:39:14.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:39:14.718
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  9 11:39:14.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4193" for this suite. 06/09/23 11:39:14.843
------------------------------
• [0.223 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:39:14.647
    Jun  9 11:39:14.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename secrets 06/09/23 11:39:14.648
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:39:14.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:39:14.718
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:39:14.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4193" for this suite. 06/09/23 11:39:14.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:39:14.872
Jun  9 11:39:14.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename certificates 06/09/23 11:39:14.873
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:39:14.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:39:14.942
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 06/09/23 11:39:15.409
STEP: getting /apis/certificates.k8s.io 06/09/23 11:39:15.428
STEP: getting /apis/certificates.k8s.io/v1 06/09/23 11:39:15.438
STEP: creating 06/09/23 11:39:15.45
STEP: getting 06/09/23 11:39:15.516
STEP: listing 06/09/23 11:39:15.528
STEP: watching 06/09/23 11:39:15.541
Jun  9 11:39:15.541: INFO: starting watch
STEP: patching 06/09/23 11:39:15.551
STEP: updating 06/09/23 11:39:15.572
Jun  9 11:39:15.594: INFO: waiting for watch events with expected annotations
Jun  9 11:39:15.594: INFO: saw patched and updated annotations
STEP: getting /approval 06/09/23 11:39:15.595
STEP: patching /approval 06/09/23 11:39:15.61
STEP: updating /approval 06/09/23 11:39:15.632
STEP: getting /status 06/09/23 11:39:15.653
STEP: patching /status 06/09/23 11:39:15.668
STEP: updating /status 06/09/23 11:39:15.695
STEP: deleting 06/09/23 11:39:15.723
STEP: deleting a collection 06/09/23 11:39:15.787
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:39:15.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-7726" for this suite. 06/09/23 11:39:15.942
------------------------------
• [1.115 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:39:14.872
    Jun  9 11:39:14.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename certificates 06/09/23 11:39:14.873
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:39:14.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:39:14.942
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 06/09/23 11:39:15.409
    STEP: getting /apis/certificates.k8s.io 06/09/23 11:39:15.428
    STEP: getting /apis/certificates.k8s.io/v1 06/09/23 11:39:15.438
    STEP: creating 06/09/23 11:39:15.45
    STEP: getting 06/09/23 11:39:15.516
    STEP: listing 06/09/23 11:39:15.528
    STEP: watching 06/09/23 11:39:15.541
    Jun  9 11:39:15.541: INFO: starting watch
    STEP: patching 06/09/23 11:39:15.551
    STEP: updating 06/09/23 11:39:15.572
    Jun  9 11:39:15.594: INFO: waiting for watch events with expected annotations
    Jun  9 11:39:15.594: INFO: saw patched and updated annotations
    STEP: getting /approval 06/09/23 11:39:15.595
    STEP: patching /approval 06/09/23 11:39:15.61
    STEP: updating /approval 06/09/23 11:39:15.632
    STEP: getting /status 06/09/23 11:39:15.653
    STEP: patching /status 06/09/23 11:39:15.668
    STEP: updating /status 06/09/23 11:39:15.695
    STEP: deleting 06/09/23 11:39:15.723
    STEP: deleting a collection 06/09/23 11:39:15.787
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:39:15.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-7726" for this suite. 06/09/23 11:39:15.942
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:39:15.987
Jun  9 11:39:15.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename watch 06/09/23 11:39:15.989
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:39:16.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:39:16.142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 06/09/23 11:39:16.147
STEP: creating a new configmap 06/09/23 11:39:16.149
STEP: modifying the configmap once 06/09/23 11:39:16.185
STEP: changing the label value of the configmap 06/09/23 11:39:16.264
STEP: Expecting to observe a delete notification for the watched object 06/09/23 11:39:16.302
Jun  9 11:39:16.302: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-251  8c28f89d-0e90-412f-bbb5-4d7bb4c243fe 112818 0 2023-06-09 11:39:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-09 11:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  9 11:39:16.302: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-251  8c28f89d-0e90-412f-bbb5-4d7bb4c243fe 112819 0 2023-06-09 11:39:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-09 11:39:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  9 11:39:16.302: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-251  8c28f89d-0e90-412f-bbb5-4d7bb4c243fe 112820 0 2023-06-09 11:39:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-09 11:39:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 06/09/23 11:39:16.303
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 06/09/23 11:39:16.331
STEP: changing the label value of the configmap back 06/09/23 11:39:26.332
STEP: modifying the configmap a third time 06/09/23 11:39:26.377
STEP: deleting the configmap 06/09/23 11:39:26.407
STEP: Expecting to observe an add notification for the watched object when the label value was restored 06/09/23 11:39:26.427
Jun  9 11:39:26.427: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-251  8c28f89d-0e90-412f-bbb5-4d7bb4c243fe 112887 0 2023-06-09 11:39:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-09 11:39:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  9 11:39:26.428: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-251  8c28f89d-0e90-412f-bbb5-4d7bb4c243fe 112888 0 2023-06-09 11:39:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-09 11:39:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  9 11:39:26.428: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-251  8c28f89d-0e90-412f-bbb5-4d7bb4c243fe 112889 0 2023-06-09 11:39:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-09 11:39:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun  9 11:39:26.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-251" for this suite. 06/09/23 11:39:26.465
------------------------------
• [SLOW TEST] [10.489 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:39:15.987
    Jun  9 11:39:15.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename watch 06/09/23 11:39:15.989
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:39:16.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:39:16.142
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 06/09/23 11:39:16.147
    STEP: creating a new configmap 06/09/23 11:39:16.149
    STEP: modifying the configmap once 06/09/23 11:39:16.185
    STEP: changing the label value of the configmap 06/09/23 11:39:16.264
    STEP: Expecting to observe a delete notification for the watched object 06/09/23 11:39:16.302
    Jun  9 11:39:16.302: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-251  8c28f89d-0e90-412f-bbb5-4d7bb4c243fe 112818 0 2023-06-09 11:39:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-09 11:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  9 11:39:16.302: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-251  8c28f89d-0e90-412f-bbb5-4d7bb4c243fe 112819 0 2023-06-09 11:39:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-09 11:39:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  9 11:39:16.302: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-251  8c28f89d-0e90-412f-bbb5-4d7bb4c243fe 112820 0 2023-06-09 11:39:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-09 11:39:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 06/09/23 11:39:16.303
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 06/09/23 11:39:16.331
    STEP: changing the label value of the configmap back 06/09/23 11:39:26.332
    STEP: modifying the configmap a third time 06/09/23 11:39:26.377
    STEP: deleting the configmap 06/09/23 11:39:26.407
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 06/09/23 11:39:26.427
    Jun  9 11:39:26.427: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-251  8c28f89d-0e90-412f-bbb5-4d7bb4c243fe 112887 0 2023-06-09 11:39:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-09 11:39:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  9 11:39:26.428: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-251  8c28f89d-0e90-412f-bbb5-4d7bb4c243fe 112888 0 2023-06-09 11:39:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-09 11:39:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  9 11:39:26.428: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-251  8c28f89d-0e90-412f-bbb5-4d7bb4c243fe 112889 0 2023-06-09 11:39:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-09 11:39:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:39:26.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-251" for this suite. 06/09/23 11:39:26.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:39:26.479
Jun  9 11:39:26.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename dns 06/09/23 11:39:26.481
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:39:26.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:39:26.529
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 06/09/23 11:39:26.535
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9130 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9130;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9130 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9130;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9130.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9130.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9130.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9130.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9130.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9130.svc;check="$$(dig +notcp +noall +answer +search 91.250.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.250.91_udp@PTR;check="$$(dig +tcp +noall +answer +search 91.250.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.250.91_tcp@PTR;sleep 1; done
 06/09/23 11:39:26.613
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9130 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9130;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9130 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9130;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9130.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9130.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9130.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9130.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9130.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9130.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9130.svc;check="$$(dig +notcp +noall +answer +search 91.250.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.250.91_udp@PTR;check="$$(dig +tcp +noall +answer +search 91.250.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.250.91_tcp@PTR;sleep 1; done
 06/09/23 11:39:26.613
STEP: creating a pod to probe DNS 06/09/23 11:39:26.613
STEP: submitting the pod to kubernetes 06/09/23 11:39:26.613
Jun  9 11:39:26.642: INFO: Waiting up to 15m0s for pod "dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a" in namespace "dns-9130" to be "running"
Jun  9 11:39:26.771: INFO: Pod "dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a": Phase="Pending", Reason="", readiness=false. Elapsed: 128.938559ms
Jun  9 11:39:28.780: INFO: Pod "dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138269097s
Jun  9 11:39:30.778: INFO: Pod "dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a": Phase="Running", Reason="", readiness=true. Elapsed: 4.13555988s
Jun  9 11:39:30.778: INFO: Pod "dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a" satisfied condition "running"
STEP: retrieving the pod 06/09/23 11:39:30.778
STEP: looking for the results for each expected name from probers 06/09/23 11:39:30.784
Jun  9 11:39:30.798: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.805: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.813: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.819: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.825: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.831: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.837: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.842: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.873: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.878: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.885: INFO: Unable to read jessie_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.890: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.898: INFO: Unable to read jessie_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.909: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.915: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.922: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:30.949: INFO: Lookups using dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9130 wheezy_tcp@dns-test-service.dns-9130 wheezy_udp@dns-test-service.dns-9130.svc wheezy_tcp@dns-test-service.dns-9130.svc wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9130 jessie_tcp@dns-test-service.dns-9130 jessie_udp@dns-test-service.dns-9130.svc jessie_tcp@dns-test-service.dns-9130.svc jessie_udp@_http._tcp.dns-test-service.dns-9130.svc jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc]

Jun  9 11:39:35.959: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:35.966: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:35.974: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:35.983: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:35.989: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:35.995: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:36.003: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:36.010: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:36.046: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:36.054: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:36.065: INFO: Unable to read jessie_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:36.072: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:36.080: INFO: Unable to read jessie_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:36.096: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:36.103: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:36.109: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:36.141: INFO: Lookups using dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9130 wheezy_tcp@dns-test-service.dns-9130 wheezy_udp@dns-test-service.dns-9130.svc wheezy_tcp@dns-test-service.dns-9130.svc wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9130 jessie_tcp@dns-test-service.dns-9130 jessie_udp@dns-test-service.dns-9130.svc jessie_tcp@dns-test-service.dns-9130.svc jessie_udp@_http._tcp.dns-test-service.dns-9130.svc jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc]

Jun  9 11:39:40.958: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:40.964: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:40.970: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:40.976: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:40.981: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:40.987: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:40.993: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:41.000: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:41.038: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:41.044: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:41.049: INFO: Unable to read jessie_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:41.054: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:41.061: INFO: Unable to read jessie_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:41.068: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:41.075: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:41.082: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:41.113: INFO: Lookups using dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9130 wheezy_tcp@dns-test-service.dns-9130 wheezy_udp@dns-test-service.dns-9130.svc wheezy_tcp@dns-test-service.dns-9130.svc wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9130 jessie_tcp@dns-test-service.dns-9130 jessie_udp@dns-test-service.dns-9130.svc jessie_tcp@dns-test-service.dns-9130.svc jessie_udp@_http._tcp.dns-test-service.dns-9130.svc jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc]

Jun  9 11:39:45.960: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:45.969: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:45.983: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:45.991: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:46.000: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:46.007: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:46.014: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:46.028: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:46.083: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:46.103: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:46.115: INFO: Unable to read jessie_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:46.125: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:46.136: INFO: Unable to read jessie_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:46.146: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:46.153: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:46.163: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:46.202: INFO: Lookups using dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9130 wheezy_tcp@dns-test-service.dns-9130 wheezy_udp@dns-test-service.dns-9130.svc wheezy_tcp@dns-test-service.dns-9130.svc wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9130 jessie_tcp@dns-test-service.dns-9130 jessie_udp@dns-test-service.dns-9130.svc jessie_tcp@dns-test-service.dns-9130.svc jessie_udp@_http._tcp.dns-test-service.dns-9130.svc jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc]

Jun  9 11:39:50.960: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:50.965: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:50.971: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:50.977: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:50.985: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:50.996: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:51.003: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:51.008: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:51.040: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:51.047: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:51.055: INFO: Unable to read jessie_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:51.063: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:51.070: INFO: Unable to read jessie_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:51.080: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:51.090: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:51.102: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:51.128: INFO: Lookups using dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9130 wheezy_tcp@dns-test-service.dns-9130 wheezy_udp@dns-test-service.dns-9130.svc wheezy_tcp@dns-test-service.dns-9130.svc wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9130 jessie_tcp@dns-test-service.dns-9130 jessie_udp@dns-test-service.dns-9130.svc jessie_tcp@dns-test-service.dns-9130.svc jessie_udp@_http._tcp.dns-test-service.dns-9130.svc jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc]

Jun  9 11:39:55.961: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:55.967: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:55.973: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:55.980: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:55.989: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:55.995: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:56.002: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:56.013: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:56.045: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:56.052: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:56.060: INFO: Unable to read jessie_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:56.068: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:56.075: INFO: Unable to read jessie_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:56.091: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:56.101: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:56.117: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
Jun  9 11:39:56.147: INFO: Lookups using dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9130 wheezy_tcp@dns-test-service.dns-9130 wheezy_udp@dns-test-service.dns-9130.svc wheezy_tcp@dns-test-service.dns-9130.svc wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9130 jessie_tcp@dns-test-service.dns-9130 jessie_udp@dns-test-service.dns-9130.svc jessie_tcp@dns-test-service.dns-9130.svc jessie_udp@_http._tcp.dns-test-service.dns-9130.svc jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc]

Jun  9 11:40:01.123: INFO: DNS probes using dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a succeeded

STEP: deleting the pod 06/09/23 11:40:01.123
STEP: deleting the test service 06/09/23 11:40:01.16
STEP: deleting the test headless service 06/09/23 11:40:01.219
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  9 11:40:01.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9130" for this suite. 06/09/23 11:40:01.257
------------------------------
• [SLOW TEST] [34.792 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:39:26.479
    Jun  9 11:39:26.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename dns 06/09/23 11:39:26.481
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:39:26.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:39:26.529
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 06/09/23 11:39:26.535
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9130 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9130;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9130 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9130;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9130.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9130.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9130.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9130.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9130.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9130.svc;check="$$(dig +notcp +noall +answer +search 91.250.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.250.91_udp@PTR;check="$$(dig +tcp +noall +answer +search 91.250.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.250.91_tcp@PTR;sleep 1; done
     06/09/23 11:39:26.613
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9130 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9130;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9130 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9130;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9130.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9130.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9130.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9130.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9130.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9130.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9130.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9130.svc;check="$$(dig +notcp +noall +answer +search 91.250.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.250.91_udp@PTR;check="$$(dig +tcp +noall +answer +search 91.250.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.250.91_tcp@PTR;sleep 1; done
     06/09/23 11:39:26.613
    STEP: creating a pod to probe DNS 06/09/23 11:39:26.613
    STEP: submitting the pod to kubernetes 06/09/23 11:39:26.613
    Jun  9 11:39:26.642: INFO: Waiting up to 15m0s for pod "dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a" in namespace "dns-9130" to be "running"
    Jun  9 11:39:26.771: INFO: Pod "dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a": Phase="Pending", Reason="", readiness=false. Elapsed: 128.938559ms
    Jun  9 11:39:28.780: INFO: Pod "dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138269097s
    Jun  9 11:39:30.778: INFO: Pod "dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a": Phase="Running", Reason="", readiness=true. Elapsed: 4.13555988s
    Jun  9 11:39:30.778: INFO: Pod "dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a" satisfied condition "running"
    STEP: retrieving the pod 06/09/23 11:39:30.778
    STEP: looking for the results for each expected name from probers 06/09/23 11:39:30.784
    Jun  9 11:39:30.798: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.805: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.813: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.819: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.825: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.831: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.837: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.842: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.873: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.878: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.885: INFO: Unable to read jessie_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.890: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.898: INFO: Unable to read jessie_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.909: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.915: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.922: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:30.949: INFO: Lookups using dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9130 wheezy_tcp@dns-test-service.dns-9130 wheezy_udp@dns-test-service.dns-9130.svc wheezy_tcp@dns-test-service.dns-9130.svc wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9130 jessie_tcp@dns-test-service.dns-9130 jessie_udp@dns-test-service.dns-9130.svc jessie_tcp@dns-test-service.dns-9130.svc jessie_udp@_http._tcp.dns-test-service.dns-9130.svc jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc]

    Jun  9 11:39:35.959: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:35.966: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:35.974: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:35.983: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:35.989: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:35.995: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:36.003: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:36.010: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:36.046: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:36.054: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:36.065: INFO: Unable to read jessie_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:36.072: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:36.080: INFO: Unable to read jessie_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:36.096: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:36.103: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:36.109: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:36.141: INFO: Lookups using dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9130 wheezy_tcp@dns-test-service.dns-9130 wheezy_udp@dns-test-service.dns-9130.svc wheezy_tcp@dns-test-service.dns-9130.svc wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9130 jessie_tcp@dns-test-service.dns-9130 jessie_udp@dns-test-service.dns-9130.svc jessie_tcp@dns-test-service.dns-9130.svc jessie_udp@_http._tcp.dns-test-service.dns-9130.svc jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc]

    Jun  9 11:39:40.958: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:40.964: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:40.970: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:40.976: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:40.981: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:40.987: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:40.993: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:41.000: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:41.038: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:41.044: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:41.049: INFO: Unable to read jessie_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:41.054: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:41.061: INFO: Unable to read jessie_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:41.068: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:41.075: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:41.082: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:41.113: INFO: Lookups using dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9130 wheezy_tcp@dns-test-service.dns-9130 wheezy_udp@dns-test-service.dns-9130.svc wheezy_tcp@dns-test-service.dns-9130.svc wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9130 jessie_tcp@dns-test-service.dns-9130 jessie_udp@dns-test-service.dns-9130.svc jessie_tcp@dns-test-service.dns-9130.svc jessie_udp@_http._tcp.dns-test-service.dns-9130.svc jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc]

    Jun  9 11:39:45.960: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:45.969: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:45.983: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:45.991: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:46.000: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:46.007: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:46.014: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:46.028: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:46.083: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:46.103: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:46.115: INFO: Unable to read jessie_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:46.125: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:46.136: INFO: Unable to read jessie_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:46.146: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:46.153: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:46.163: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:46.202: INFO: Lookups using dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9130 wheezy_tcp@dns-test-service.dns-9130 wheezy_udp@dns-test-service.dns-9130.svc wheezy_tcp@dns-test-service.dns-9130.svc wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9130 jessie_tcp@dns-test-service.dns-9130 jessie_udp@dns-test-service.dns-9130.svc jessie_tcp@dns-test-service.dns-9130.svc jessie_udp@_http._tcp.dns-test-service.dns-9130.svc jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc]

    Jun  9 11:39:50.960: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:50.965: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:50.971: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:50.977: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:50.985: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:50.996: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:51.003: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:51.008: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:51.040: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:51.047: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:51.055: INFO: Unable to read jessie_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:51.063: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:51.070: INFO: Unable to read jessie_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:51.080: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:51.090: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:51.102: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:51.128: INFO: Lookups using dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9130 wheezy_tcp@dns-test-service.dns-9130 wheezy_udp@dns-test-service.dns-9130.svc wheezy_tcp@dns-test-service.dns-9130.svc wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9130 jessie_tcp@dns-test-service.dns-9130 jessie_udp@dns-test-service.dns-9130.svc jessie_tcp@dns-test-service.dns-9130.svc jessie_udp@_http._tcp.dns-test-service.dns-9130.svc jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc]

    Jun  9 11:39:55.961: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:55.967: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:55.973: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:55.980: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:55.989: INFO: Unable to read wheezy_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:55.995: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:56.002: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:56.013: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:56.045: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:56.052: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:56.060: INFO: Unable to read jessie_udp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:56.068: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130 from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:56.075: INFO: Unable to read jessie_udp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:56.091: INFO: Unable to read jessie_tcp@dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:56.101: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:56.117: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc from pod dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a: the server could not find the requested resource (get pods dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a)
    Jun  9 11:39:56.147: INFO: Lookups using dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9130 wheezy_tcp@dns-test-service.dns-9130 wheezy_udp@dns-test-service.dns-9130.svc wheezy_tcp@dns-test-service.dns-9130.svc wheezy_udp@_http._tcp.dns-test-service.dns-9130.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9130.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9130 jessie_tcp@dns-test-service.dns-9130 jessie_udp@dns-test-service.dns-9130.svc jessie_tcp@dns-test-service.dns-9130.svc jessie_udp@_http._tcp.dns-test-service.dns-9130.svc jessie_tcp@_http._tcp.dns-test-service.dns-9130.svc]

    Jun  9 11:40:01.123: INFO: DNS probes using dns-9130/dns-test-e1ecea73-fea0-40e1-8c84-0a67df0c469a succeeded

    STEP: deleting the pod 06/09/23 11:40:01.123
    STEP: deleting the test service 06/09/23 11:40:01.16
    STEP: deleting the test headless service 06/09/23 11:40:01.219
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:40:01.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9130" for this suite. 06/09/23 11:40:01.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:40:01.273
Jun  9 11:40:01.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 11:40:01.274
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:01.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:01.313
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 06/09/23 11:40:01.322
Jun  9 11:40:01.336: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2" in namespace "projected-1265" to be "Succeeded or Failed"
Jun  9 11:40:01.342: INFO: Pod "downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.309534ms
Jun  9 11:40:03.349: INFO: Pod "downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012452673s
Jun  9 11:40:05.349: INFO: Pod "downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012252263s
STEP: Saw pod success 06/09/23 11:40:05.349
Jun  9 11:40:05.349: INFO: Pod "downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2" satisfied condition "Succeeded or Failed"
Jun  9 11:40:05.354: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2 container client-container: <nil>
STEP: delete the pod 06/09/23 11:40:05.364
Jun  9 11:40:05.380: INFO: Waiting for pod downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2 to disappear
Jun  9 11:40:05.386: INFO: Pod downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  9 11:40:05.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1265" for this suite. 06/09/23 11:40:05.395
------------------------------
• [4.137 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:40:01.273
    Jun  9 11:40:01.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 11:40:01.274
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:01.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:01.313
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 06/09/23 11:40:01.322
    Jun  9 11:40:01.336: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2" in namespace "projected-1265" to be "Succeeded or Failed"
    Jun  9 11:40:01.342: INFO: Pod "downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.309534ms
    Jun  9 11:40:03.349: INFO: Pod "downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012452673s
    Jun  9 11:40:05.349: INFO: Pod "downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012252263s
    STEP: Saw pod success 06/09/23 11:40:05.349
    Jun  9 11:40:05.349: INFO: Pod "downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2" satisfied condition "Succeeded or Failed"
    Jun  9 11:40:05.354: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2 container client-container: <nil>
    STEP: delete the pod 06/09/23 11:40:05.364
    Jun  9 11:40:05.380: INFO: Waiting for pod downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2 to disappear
    Jun  9 11:40:05.386: INFO: Pod downwardapi-volume-8dfd23c6-42ba-4958-8806-de3e15ee71b2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:40:05.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1265" for this suite. 06/09/23 11:40:05.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:40:05.411
Jun  9 11:40:05.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename svcaccounts 06/09/23 11:40:05.413
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:05.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:05.445
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Jun  9 11:40:05.530: INFO: Waiting up to 5m0s for pod "pod-service-account-c146ff33-de45-4a3e-a4ea-20d3aeb81453" in namespace "svcaccounts-4097" to be "running"
Jun  9 11:40:05.546: INFO: Pod "pod-service-account-c146ff33-de45-4a3e-a4ea-20d3aeb81453": Phase="Pending", Reason="", readiness=false. Elapsed: 16.146247ms
Jun  9 11:40:07.556: INFO: Pod "pod-service-account-c146ff33-de45-4a3e-a4ea-20d3aeb81453": Phase="Running", Reason="", readiness=true. Elapsed: 2.025698156s
Jun  9 11:40:07.556: INFO: Pod "pod-service-account-c146ff33-de45-4a3e-a4ea-20d3aeb81453" satisfied condition "running"
STEP: reading a file in the container 06/09/23 11:40:07.556
Jun  9 11:40:07.556: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4097 pod-service-account-c146ff33-de45-4a3e-a4ea-20d3aeb81453 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 06/09/23 11:40:07.759
Jun  9 11:40:07.759: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4097 pod-service-account-c146ff33-de45-4a3e-a4ea-20d3aeb81453 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 06/09/23 11:40:07.946
Jun  9 11:40:07.947: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4097 pod-service-account-c146ff33-de45-4a3e-a4ea-20d3aeb81453 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jun  9 11:40:08.153: INFO: Got root ca configmap in namespace "svcaccounts-4097"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun  9 11:40:08.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4097" for this suite. 06/09/23 11:40:08.166
------------------------------
• [2.765 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:40:05.411
    Jun  9 11:40:05.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename svcaccounts 06/09/23 11:40:05.413
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:05.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:05.445
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Jun  9 11:40:05.530: INFO: Waiting up to 5m0s for pod "pod-service-account-c146ff33-de45-4a3e-a4ea-20d3aeb81453" in namespace "svcaccounts-4097" to be "running"
    Jun  9 11:40:05.546: INFO: Pod "pod-service-account-c146ff33-de45-4a3e-a4ea-20d3aeb81453": Phase="Pending", Reason="", readiness=false. Elapsed: 16.146247ms
    Jun  9 11:40:07.556: INFO: Pod "pod-service-account-c146ff33-de45-4a3e-a4ea-20d3aeb81453": Phase="Running", Reason="", readiness=true. Elapsed: 2.025698156s
    Jun  9 11:40:07.556: INFO: Pod "pod-service-account-c146ff33-de45-4a3e-a4ea-20d3aeb81453" satisfied condition "running"
    STEP: reading a file in the container 06/09/23 11:40:07.556
    Jun  9 11:40:07.556: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4097 pod-service-account-c146ff33-de45-4a3e-a4ea-20d3aeb81453 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 06/09/23 11:40:07.759
    Jun  9 11:40:07.759: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4097 pod-service-account-c146ff33-de45-4a3e-a4ea-20d3aeb81453 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 06/09/23 11:40:07.946
    Jun  9 11:40:07.947: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4097 pod-service-account-c146ff33-de45-4a3e-a4ea-20d3aeb81453 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jun  9 11:40:08.153: INFO: Got root ca configmap in namespace "svcaccounts-4097"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:40:08.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4097" for this suite. 06/09/23 11:40:08.166
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:40:08.176
Jun  9 11:40:08.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 11:40:08.179
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:08.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:08.211
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 06/09/23 11:40:08.218
Jun  9 11:40:08.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-7759 create -f -'
Jun  9 11:40:10.039: INFO: stderr: ""
Jun  9 11:40:10.040: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/09/23 11:40:10.04
Jun  9 11:40:11.118: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 11:40:11.118: INFO: Found 0 / 1
Jun  9 11:40:12.047: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 11:40:12.047: INFO: Found 0 / 1
Jun  9 11:40:13.047: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 11:40:13.047: INFO: Found 1 / 1
Jun  9 11:40:13.047: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 06/09/23 11:40:13.047
Jun  9 11:40:13.054: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 11:40:13.055: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  9 11:40:13.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-7759 patch pod agnhost-primary-q828l -p {"metadata":{"annotations":{"x":"y"}}}'
Jun  9 11:40:13.171: INFO: stderr: ""
Jun  9 11:40:13.171: INFO: stdout: "pod/agnhost-primary-q828l patched\n"
STEP: checking annotations 06/09/23 11:40:13.171
Jun  9 11:40:13.176: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  9 11:40:13.176: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 11:40:13.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7759" for this suite. 06/09/23 11:40:13.19
------------------------------
• [SLOW TEST] [5.038 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:40:08.176
    Jun  9 11:40:08.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 11:40:08.179
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:08.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:08.211
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 06/09/23 11:40:08.218
    Jun  9 11:40:08.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-7759 create -f -'
    Jun  9 11:40:10.039: INFO: stderr: ""
    Jun  9 11:40:10.040: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/09/23 11:40:10.04
    Jun  9 11:40:11.118: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  9 11:40:11.118: INFO: Found 0 / 1
    Jun  9 11:40:12.047: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  9 11:40:12.047: INFO: Found 0 / 1
    Jun  9 11:40:13.047: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  9 11:40:13.047: INFO: Found 1 / 1
    Jun  9 11:40:13.047: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 06/09/23 11:40:13.047
    Jun  9 11:40:13.054: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  9 11:40:13.055: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun  9 11:40:13.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-7759 patch pod agnhost-primary-q828l -p {"metadata":{"annotations":{"x":"y"}}}'
    Jun  9 11:40:13.171: INFO: stderr: ""
    Jun  9 11:40:13.171: INFO: stdout: "pod/agnhost-primary-q828l patched\n"
    STEP: checking annotations 06/09/23 11:40:13.171
    Jun  9 11:40:13.176: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun  9 11:40:13.176: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:40:13.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7759" for this suite. 06/09/23 11:40:13.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:40:13.215
Jun  9 11:40:13.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename gc 06/09/23 11:40:13.218
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:13.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:13.251
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 06/09/23 11:40:13.275
STEP: Wait for the Deployment to create new ReplicaSet 06/09/23 11:40:13.334
STEP: delete the deployment 06/09/23 11:40:13.373
STEP: wait for all rs to be garbage collected 06/09/23 11:40:13.461
STEP: expected 0 pods, got 2 pods 06/09/23 11:40:13.511
STEP: expected 0 rs, got 1 rs 06/09/23 11:40:13.54
STEP: Gathering metrics 06/09/23 11:40:14.078
Jun  9 11:40:14.112: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" in namespace "kube-system" to be "running and ready"
Jun  9 11:40:14.118: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh": Phase="Running", Reason="", readiness=true. Elapsed: 6.19592ms
Jun  9 11:40:14.118: INFO: The phase of Pod kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh is Running (Ready = true)
Jun  9 11:40:14.118: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" satisfied condition "running and ready"
Jun  9 11:40:14.199: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun  9 11:40:14.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4674" for this suite. 06/09/23 11:40:14.208
------------------------------
• [1.001 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:40:13.215
    Jun  9 11:40:13.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename gc 06/09/23 11:40:13.218
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:13.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:13.251
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 06/09/23 11:40:13.275
    STEP: Wait for the Deployment to create new ReplicaSet 06/09/23 11:40:13.334
    STEP: delete the deployment 06/09/23 11:40:13.373
    STEP: wait for all rs to be garbage collected 06/09/23 11:40:13.461
    STEP: expected 0 pods, got 2 pods 06/09/23 11:40:13.511
    STEP: expected 0 rs, got 1 rs 06/09/23 11:40:13.54
    STEP: Gathering metrics 06/09/23 11:40:14.078
    Jun  9 11:40:14.112: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" in namespace "kube-system" to be "running and ready"
    Jun  9 11:40:14.118: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh": Phase="Running", Reason="", readiness=true. Elapsed: 6.19592ms
    Jun  9 11:40:14.118: INFO: The phase of Pod kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh is Running (Ready = true)
    Jun  9 11:40:14.118: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" satisfied condition "running and ready"
    Jun  9 11:40:14.199: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:40:14.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4674" for this suite. 06/09/23 11:40:14.208
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:40:14.218
Jun  9 11:40:14.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pod-network-test 06/09/23 11:40:14.219
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:14.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:14.248
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-8018 06/09/23 11:40:14.252
STEP: creating a selector 06/09/23 11:40:14.253
STEP: Creating the service pods in kubernetes 06/09/23 11:40:14.253
Jun  9 11:40:14.253: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  9 11:40:14.302: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8018" to be "running and ready"
Jun  9 11:40:14.310: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.255483ms
Jun  9 11:40:14.310: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  9 11:40:16.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015483473s
Jun  9 11:40:16.318: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 11:40:18.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017874041s
Jun  9 11:40:18.320: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 11:40:20.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015370035s
Jun  9 11:40:20.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 11:40:22.321: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019115567s
Jun  9 11:40:22.321: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 11:40:24.322: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.020327856s
Jun  9 11:40:24.322: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 11:40:26.330: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.028290124s
Jun  9 11:40:26.330: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 11:40:28.318: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015853419s
Jun  9 11:40:28.318: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 11:40:30.328: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.026236965s
Jun  9 11:40:30.328: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 11:40:32.318: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.016312024s
Jun  9 11:40:32.318: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 11:40:34.321: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.018960484s
Jun  9 11:40:34.321: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun  9 11:40:36.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014545614s
Jun  9 11:40:36.317: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun  9 11:40:36.317: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun  9 11:40:36.326: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8018" to be "running and ready"
Jun  9 11:40:36.344: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 17.543924ms
Jun  9 11:40:36.344: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun  9 11:40:36.344: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun  9 11:40:36.360: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8018" to be "running and ready"
Jun  9 11:40:36.374: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 14.640851ms
Jun  9 11:40:36.374: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun  9 11:40:36.374: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/09/23 11:40:36.392
Jun  9 11:40:36.437: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8018" to be "running"
Jun  9 11:40:36.475: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 37.394774ms
Jun  9 11:40:38.484: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.046868335s
Jun  9 11:40:38.484: INFO: Pod "test-container-pod" satisfied condition "running"
Jun  9 11:40:38.494: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8018" to be "running"
Jun  9 11:40:38.503: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 9.556506ms
Jun  9 11:40:38.503: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jun  9 11:40:38.509: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun  9 11:40:38.509: INFO: Going to poll 172.26.90.9 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun  9 11:40:38.515: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.26.90.9:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8018 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 11:40:38.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 11:40:38.516: INFO: ExecWithOptions: Clientset creation
Jun  9 11:40:38.516: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8018/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.26.90.9%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  9 11:40:38.636: INFO: Found all 1 expected endpoints: [netserver-0]
Jun  9 11:40:38.636: INFO: Going to poll 172.30.17.154 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun  9 11:40:38.644: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.17.154:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8018 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 11:40:38.644: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 11:40:38.645: INFO: ExecWithOptions: Clientset creation
Jun  9 11:40:38.645: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8018/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.17.154%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  9 11:40:38.783: INFO: Found all 1 expected endpoints: [netserver-1]
Jun  9 11:40:38.783: INFO: Going to poll 172.27.53.75 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun  9 11:40:38.793: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.27.53.75:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8018 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 11:40:38.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 11:40:38.794: INFO: ExecWithOptions: Clientset creation
Jun  9 11:40:38.794: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8018/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.27.53.75%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun  9 11:40:38.914: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jun  9 11:40:38.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-8018" for this suite. 06/09/23 11:40:38.934
------------------------------
• [SLOW TEST] [24.734 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:40:14.218
    Jun  9 11:40:14.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pod-network-test 06/09/23 11:40:14.219
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:14.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:14.248
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-8018 06/09/23 11:40:14.252
    STEP: creating a selector 06/09/23 11:40:14.253
    STEP: Creating the service pods in kubernetes 06/09/23 11:40:14.253
    Jun  9 11:40:14.253: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun  9 11:40:14.302: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8018" to be "running and ready"
    Jun  9 11:40:14.310: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.255483ms
    Jun  9 11:40:14.310: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun  9 11:40:16.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015483473s
    Jun  9 11:40:16.318: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 11:40:18.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017874041s
    Jun  9 11:40:18.320: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 11:40:20.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015370035s
    Jun  9 11:40:20.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 11:40:22.321: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019115567s
    Jun  9 11:40:22.321: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 11:40:24.322: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.020327856s
    Jun  9 11:40:24.322: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 11:40:26.330: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.028290124s
    Jun  9 11:40:26.330: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 11:40:28.318: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015853419s
    Jun  9 11:40:28.318: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 11:40:30.328: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.026236965s
    Jun  9 11:40:30.328: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 11:40:32.318: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.016312024s
    Jun  9 11:40:32.318: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 11:40:34.321: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.018960484s
    Jun  9 11:40:34.321: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun  9 11:40:36.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014545614s
    Jun  9 11:40:36.317: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun  9 11:40:36.317: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun  9 11:40:36.326: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8018" to be "running and ready"
    Jun  9 11:40:36.344: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 17.543924ms
    Jun  9 11:40:36.344: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun  9 11:40:36.344: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun  9 11:40:36.360: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8018" to be "running and ready"
    Jun  9 11:40:36.374: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 14.640851ms
    Jun  9 11:40:36.374: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun  9 11:40:36.374: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/09/23 11:40:36.392
    Jun  9 11:40:36.437: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8018" to be "running"
    Jun  9 11:40:36.475: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 37.394774ms
    Jun  9 11:40:38.484: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.046868335s
    Jun  9 11:40:38.484: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun  9 11:40:38.494: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8018" to be "running"
    Jun  9 11:40:38.503: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 9.556506ms
    Jun  9 11:40:38.503: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jun  9 11:40:38.509: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun  9 11:40:38.509: INFO: Going to poll 172.26.90.9 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun  9 11:40:38.515: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.26.90.9:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8018 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 11:40:38.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 11:40:38.516: INFO: ExecWithOptions: Clientset creation
    Jun  9 11:40:38.516: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8018/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.26.90.9%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  9 11:40:38.636: INFO: Found all 1 expected endpoints: [netserver-0]
    Jun  9 11:40:38.636: INFO: Going to poll 172.30.17.154 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun  9 11:40:38.644: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.17.154:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8018 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 11:40:38.644: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 11:40:38.645: INFO: ExecWithOptions: Clientset creation
    Jun  9 11:40:38.645: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8018/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.17.154%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  9 11:40:38.783: INFO: Found all 1 expected endpoints: [netserver-1]
    Jun  9 11:40:38.783: INFO: Going to poll 172.27.53.75 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun  9 11:40:38.793: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.27.53.75:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8018 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 11:40:38.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 11:40:38.794: INFO: ExecWithOptions: Clientset creation
    Jun  9 11:40:38.794: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8018/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.27.53.75%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun  9 11:40:38.914: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:40:38.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-8018" for this suite. 06/09/23 11:40:38.934
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:40:38.952
Jun  9 11:40:38.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 11:40:38.954
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:38.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:38.995
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 06/09/23 11:40:39.013
Jun  9 11:40:39.030: INFO: Waiting up to 5m0s for pod "pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1" in namespace "emptydir-9792" to be "Succeeded or Failed"
Jun  9 11:40:39.044: INFO: Pod "pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.955656ms
Jun  9 11:40:41.056: INFO: Pod "pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02538276s
Jun  9 11:40:43.053: INFO: Pod "pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022226675s
STEP: Saw pod success 06/09/23 11:40:43.053
Jun  9 11:40:43.053: INFO: Pod "pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1" satisfied condition "Succeeded or Failed"
Jun  9 11:40:43.060: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1 container test-container: <nil>
STEP: delete the pod 06/09/23 11:40:43.073
Jun  9 11:40:43.099: INFO: Waiting for pod pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1 to disappear
Jun  9 11:40:43.115: INFO: Pod pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 11:40:43.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9792" for this suite. 06/09/23 11:40:43.126
------------------------------
• [4.191 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:40:38.952
    Jun  9 11:40:38.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 11:40:38.954
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:38.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:38.995
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 06/09/23 11:40:39.013
    Jun  9 11:40:39.030: INFO: Waiting up to 5m0s for pod "pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1" in namespace "emptydir-9792" to be "Succeeded or Failed"
    Jun  9 11:40:39.044: INFO: Pod "pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.955656ms
    Jun  9 11:40:41.056: INFO: Pod "pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02538276s
    Jun  9 11:40:43.053: INFO: Pod "pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022226675s
    STEP: Saw pod success 06/09/23 11:40:43.053
    Jun  9 11:40:43.053: INFO: Pod "pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1" satisfied condition "Succeeded or Failed"
    Jun  9 11:40:43.060: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1 container test-container: <nil>
    STEP: delete the pod 06/09/23 11:40:43.073
    Jun  9 11:40:43.099: INFO: Waiting for pod pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1 to disappear
    Jun  9 11:40:43.115: INFO: Pod pod-c6659241-fc5b-49d3-892f-b7fb2e9792c1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:40:43.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9792" for this suite. 06/09/23 11:40:43.126
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:40:43.144
Jun  9 11:40:43.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename secrets 06/09/23 11:40:43.151
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:43.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:43.186
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-652971ff-dfcf-493e-90c4-186446a69513 06/09/23 11:40:43.192
STEP: Creating a pod to test consume secrets 06/09/23 11:40:43.201
Jun  9 11:40:43.233: INFO: Waiting up to 5m0s for pod "pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878" in namespace "secrets-6362" to be "Succeeded or Failed"
Jun  9 11:40:43.255: INFO: Pod "pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878": Phase="Pending", Reason="", readiness=false. Elapsed: 22.043187ms
Jun  9 11:40:45.263: INFO: Pod "pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029325257s
Jun  9 11:40:47.264: INFO: Pod "pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030541021s
Jun  9 11:40:49.262: INFO: Pod "pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028903618s
STEP: Saw pod success 06/09/23 11:40:49.262
Jun  9 11:40:49.263: INFO: Pod "pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878" satisfied condition "Succeeded or Failed"
Jun  9 11:40:49.277: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878 container secret-volume-test: <nil>
STEP: delete the pod 06/09/23 11:40:49.29
Jun  9 11:40:49.340: INFO: Waiting for pod pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878 to disappear
Jun  9 11:40:49.344: INFO: Pod pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  9 11:40:49.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6362" for this suite. 06/09/23 11:40:49.354
------------------------------
• [SLOW TEST] [6.245 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:40:43.144
    Jun  9 11:40:43.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename secrets 06/09/23 11:40:43.151
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:43.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:43.186
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-652971ff-dfcf-493e-90c4-186446a69513 06/09/23 11:40:43.192
    STEP: Creating a pod to test consume secrets 06/09/23 11:40:43.201
    Jun  9 11:40:43.233: INFO: Waiting up to 5m0s for pod "pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878" in namespace "secrets-6362" to be "Succeeded or Failed"
    Jun  9 11:40:43.255: INFO: Pod "pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878": Phase="Pending", Reason="", readiness=false. Elapsed: 22.043187ms
    Jun  9 11:40:45.263: INFO: Pod "pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029325257s
    Jun  9 11:40:47.264: INFO: Pod "pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030541021s
    Jun  9 11:40:49.262: INFO: Pod "pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028903618s
    STEP: Saw pod success 06/09/23 11:40:49.262
    Jun  9 11:40:49.263: INFO: Pod "pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878" satisfied condition "Succeeded or Failed"
    Jun  9 11:40:49.277: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878 container secret-volume-test: <nil>
    STEP: delete the pod 06/09/23 11:40:49.29
    Jun  9 11:40:49.340: INFO: Waiting for pod pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878 to disappear
    Jun  9 11:40:49.344: INFO: Pod pod-secrets-ae50dd94-518e-4f0f-bbc1-e895b6966878 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:40:49.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6362" for this suite. 06/09/23 11:40:49.354
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:40:49.389
Jun  9 11:40:49.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename custom-resource-definition 06/09/23 11:40:49.395
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:49.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:49.458
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jun  9 11:40:49.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:40:50.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3550" for this suite. 06/09/23 11:40:50.055
------------------------------
• [0.685 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:40:49.389
    Jun  9 11:40:49.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename custom-resource-definition 06/09/23 11:40:49.395
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:49.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:49.458
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jun  9 11:40:49.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:40:50.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3550" for this suite. 06/09/23 11:40:50.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:40:50.075
Jun  9 11:40:50.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename resourcequota 06/09/23 11:40:50.076
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:50.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:50.102
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 06/09/23 11:40:50.108
STEP: Creating a ResourceQuota 06/09/23 11:40:55.118
STEP: Ensuring resource quota status is calculated 06/09/23 11:40:55.14
STEP: Creating a Pod that fits quota 06/09/23 11:40:57.152
STEP: Ensuring ResourceQuota status captures the pod usage 06/09/23 11:40:57.177
STEP: Not allowing a pod to be created that exceeds remaining quota 06/09/23 11:40:59.183
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 06/09/23 11:40:59.193
STEP: Ensuring a pod cannot update its resource requirements 06/09/23 11:40:59.199
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 06/09/23 11:40:59.206
STEP: Deleting the pod 06/09/23 11:41:01.214
STEP: Ensuring resource quota status released the pod usage 06/09/23 11:41:01.232
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun  9 11:41:03.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9045" for this suite. 06/09/23 11:41:03.254
------------------------------
• [SLOW TEST] [13.196 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:40:50.075
    Jun  9 11:40:50.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename resourcequota 06/09/23 11:40:50.076
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:40:50.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:40:50.102
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 06/09/23 11:40:50.108
    STEP: Creating a ResourceQuota 06/09/23 11:40:55.118
    STEP: Ensuring resource quota status is calculated 06/09/23 11:40:55.14
    STEP: Creating a Pod that fits quota 06/09/23 11:40:57.152
    STEP: Ensuring ResourceQuota status captures the pod usage 06/09/23 11:40:57.177
    STEP: Not allowing a pod to be created that exceeds remaining quota 06/09/23 11:40:59.183
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 06/09/23 11:40:59.193
    STEP: Ensuring a pod cannot update its resource requirements 06/09/23 11:40:59.199
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 06/09/23 11:40:59.206
    STEP: Deleting the pod 06/09/23 11:41:01.214
    STEP: Ensuring resource quota status released the pod usage 06/09/23 11:41:01.232
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:41:03.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9045" for this suite. 06/09/23 11:41:03.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:41:03.275
Jun  9 11:41:03.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 11:41:03.276
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:03.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:03.372
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 06/09/23 11:41:03.378
Jun  9 11:41:03.436: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802" in namespace "projected-8346" to be "Succeeded or Failed"
Jun  9 11:41:03.466: INFO: Pod "downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802": Phase="Pending", Reason="", readiness=false. Elapsed: 30.32999ms
Jun  9 11:41:05.474: INFO: Pod "downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038377566s
Jun  9 11:41:07.477: INFO: Pod "downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040880805s
Jun  9 11:41:09.473: INFO: Pod "downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037738854s
STEP: Saw pod success 06/09/23 11:41:09.473
Jun  9 11:41:09.474: INFO: Pod "downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802" satisfied condition "Succeeded or Failed"
Jun  9 11:41:09.479: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802 container client-container: <nil>
STEP: delete the pod 06/09/23 11:41:09.49
Jun  9 11:41:09.510: INFO: Waiting for pod downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802 to disappear
Jun  9 11:41:09.517: INFO: Pod downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun  9 11:41:09.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8346" for this suite. 06/09/23 11:41:09.525
------------------------------
• [SLOW TEST] [6.261 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:41:03.275
    Jun  9 11:41:03.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 11:41:03.276
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:03.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:03.372
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 06/09/23 11:41:03.378
    Jun  9 11:41:03.436: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802" in namespace "projected-8346" to be "Succeeded or Failed"
    Jun  9 11:41:03.466: INFO: Pod "downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802": Phase="Pending", Reason="", readiness=false. Elapsed: 30.32999ms
    Jun  9 11:41:05.474: INFO: Pod "downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038377566s
    Jun  9 11:41:07.477: INFO: Pod "downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040880805s
    Jun  9 11:41:09.473: INFO: Pod "downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037738854s
    STEP: Saw pod success 06/09/23 11:41:09.473
    Jun  9 11:41:09.474: INFO: Pod "downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802" satisfied condition "Succeeded or Failed"
    Jun  9 11:41:09.479: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802 container client-container: <nil>
    STEP: delete the pod 06/09/23 11:41:09.49
    Jun  9 11:41:09.510: INFO: Waiting for pod downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802 to disappear
    Jun  9 11:41:09.517: INFO: Pod downwardapi-volume-2b943cd5-31df-4240-a3c9-3c6da6729802 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:41:09.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8346" for this suite. 06/09/23 11:41:09.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:41:09.537
Jun  9 11:41:09.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 11:41:09.539
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:09.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:09.564
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-8c88dbe0-dcb7-44c4-b467-6a6281843afe 06/09/23 11:41:09.578
STEP: Creating the pod 06/09/23 11:41:09.586
Jun  9 11:41:09.603: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ad9ecd9-d385-4791-b499-63a55a8548e8" in namespace "configmap-6365" to be "running"
Jun  9 11:41:09.609: INFO: Pod "pod-configmaps-0ad9ecd9-d385-4791-b499-63a55a8548e8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.882126ms
Jun  9 11:41:11.616: INFO: Pod "pod-configmaps-0ad9ecd9-d385-4791-b499-63a55a8548e8": Phase="Running", Reason="", readiness=false. Elapsed: 2.013423506s
Jun  9 11:41:11.616: INFO: Pod "pod-configmaps-0ad9ecd9-d385-4791-b499-63a55a8548e8" satisfied condition "running"
STEP: Waiting for pod with text data 06/09/23 11:41:11.616
STEP: Waiting for pod with binary data 06/09/23 11:41:11.63
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 11:41:11.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6365" for this suite. 06/09/23 11:41:11.65
------------------------------
• [2.130 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:41:09.537
    Jun  9 11:41:09.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 11:41:09.539
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:09.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:09.564
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-8c88dbe0-dcb7-44c4-b467-6a6281843afe 06/09/23 11:41:09.578
    STEP: Creating the pod 06/09/23 11:41:09.586
    Jun  9 11:41:09.603: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ad9ecd9-d385-4791-b499-63a55a8548e8" in namespace "configmap-6365" to be "running"
    Jun  9 11:41:09.609: INFO: Pod "pod-configmaps-0ad9ecd9-d385-4791-b499-63a55a8548e8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.882126ms
    Jun  9 11:41:11.616: INFO: Pod "pod-configmaps-0ad9ecd9-d385-4791-b499-63a55a8548e8": Phase="Running", Reason="", readiness=false. Elapsed: 2.013423506s
    Jun  9 11:41:11.616: INFO: Pod "pod-configmaps-0ad9ecd9-d385-4791-b499-63a55a8548e8" satisfied condition "running"
    STEP: Waiting for pod with text data 06/09/23 11:41:11.616
    STEP: Waiting for pod with binary data 06/09/23 11:41:11.63
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:41:11.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6365" for this suite. 06/09/23 11:41:11.65
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:41:11.667
Jun  9 11:41:11.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pods 06/09/23 11:41:11.669
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:11.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:11.701
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 06/09/23 11:41:11.707
STEP: submitting the pod to kubernetes 06/09/23 11:41:11.708
STEP: verifying QOS class is set on the pod 06/09/23 11:41:11.719
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jun  9 11:41:11.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3924" for this suite. 06/09/23 11:41:11.744
------------------------------
• [0.099 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:41:11.667
    Jun  9 11:41:11.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pods 06/09/23 11:41:11.669
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:11.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:11.701
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 06/09/23 11:41:11.707
    STEP: submitting the pod to kubernetes 06/09/23 11:41:11.708
    STEP: verifying QOS class is set on the pod 06/09/23 11:41:11.719
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:41:11.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3924" for this suite. 06/09/23 11:41:11.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:41:11.767
Jun  9 11:41:11.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 11:41:11.769
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:11.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:11.8
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 06/09/23 11:41:11.806
Jun  9 11:41:11.853: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-21c2a2f6-b533-462c-840b-8d7ab80c49e8" in namespace "emptydir-1174" to be "running"
Jun  9 11:41:11.860: INFO: Pod "pod-sharedvolume-21c2a2f6-b533-462c-840b-8d7ab80c49e8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.11701ms
Jun  9 11:41:13.868: INFO: Pod "pod-sharedvolume-21c2a2f6-b533-462c-840b-8d7ab80c49e8": Phase="Running", Reason="", readiness=false. Elapsed: 2.014726586s
Jun  9 11:41:13.868: INFO: Pod "pod-sharedvolume-21c2a2f6-b533-462c-840b-8d7ab80c49e8" satisfied condition "running"
STEP: Reading file content from the nginx-container 06/09/23 11:41:13.868
Jun  9 11:41:13.868: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1174 PodName:pod-sharedvolume-21c2a2f6-b533-462c-840b-8d7ab80c49e8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  9 11:41:13.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
Jun  9 11:41:13.870: INFO: ExecWithOptions: Clientset creation
Jun  9 11:41:13.870: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-1174/pods/pod-sharedvolume-21c2a2f6-b533-462c-840b-8d7ab80c49e8/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jun  9 11:41:13.984: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 11:41:13.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1174" for this suite. 06/09/23 11:41:14.002
------------------------------
• [2.249 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:41:11.767
    Jun  9 11:41:11.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 11:41:11.769
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:11.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:11.8
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 06/09/23 11:41:11.806
    Jun  9 11:41:11.853: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-21c2a2f6-b533-462c-840b-8d7ab80c49e8" in namespace "emptydir-1174" to be "running"
    Jun  9 11:41:11.860: INFO: Pod "pod-sharedvolume-21c2a2f6-b533-462c-840b-8d7ab80c49e8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.11701ms
    Jun  9 11:41:13.868: INFO: Pod "pod-sharedvolume-21c2a2f6-b533-462c-840b-8d7ab80c49e8": Phase="Running", Reason="", readiness=false. Elapsed: 2.014726586s
    Jun  9 11:41:13.868: INFO: Pod "pod-sharedvolume-21c2a2f6-b533-462c-840b-8d7ab80c49e8" satisfied condition "running"
    STEP: Reading file content from the nginx-container 06/09/23 11:41:13.868
    Jun  9 11:41:13.868: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1174 PodName:pod-sharedvolume-21c2a2f6-b533-462c-840b-8d7ab80c49e8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun  9 11:41:13.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    Jun  9 11:41:13.870: INFO: ExecWithOptions: Clientset creation
    Jun  9 11:41:13.870: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-1174/pods/pod-sharedvolume-21c2a2f6-b533-462c-840b-8d7ab80c49e8/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jun  9 11:41:13.984: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:41:13.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1174" for this suite. 06/09/23 11:41:14.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:41:14.017
Jun  9 11:41:14.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 11:41:14.018
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:14.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:14.072
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 06/09/23 11:41:14.131
Jun  9 11:41:14.157: INFO: Waiting up to 5m0s for pod "pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae" in namespace "emptydir-9207" to be "Succeeded or Failed"
Jun  9 11:41:14.170: INFO: Pod "pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae": Phase="Pending", Reason="", readiness=false. Elapsed: 13.249045ms
Jun  9 11:41:16.179: INFO: Pod "pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae": Phase="Running", Reason="", readiness=true. Elapsed: 2.022473773s
Jun  9 11:41:18.179: INFO: Pod "pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae": Phase="Running", Reason="", readiness=false. Elapsed: 4.022314798s
Jun  9 11:41:20.177: INFO: Pod "pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020415052s
STEP: Saw pod success 06/09/23 11:41:20.177
Jun  9 11:41:20.178: INFO: Pod "pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae" satisfied condition "Succeeded or Failed"
Jun  9 11:41:20.184: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae container test-container: <nil>
STEP: delete the pod 06/09/23 11:41:20.206
Jun  9 11:41:20.226: INFO: Waiting for pod pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae to disappear
Jun  9 11:41:20.232: INFO: Pod pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 11:41:20.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9207" for this suite. 06/09/23 11:41:20.242
------------------------------
• [SLOW TEST] [6.241 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:41:14.017
    Jun  9 11:41:14.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 11:41:14.018
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:14.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:14.072
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 06/09/23 11:41:14.131
    Jun  9 11:41:14.157: INFO: Waiting up to 5m0s for pod "pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae" in namespace "emptydir-9207" to be "Succeeded or Failed"
    Jun  9 11:41:14.170: INFO: Pod "pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae": Phase="Pending", Reason="", readiness=false. Elapsed: 13.249045ms
    Jun  9 11:41:16.179: INFO: Pod "pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae": Phase="Running", Reason="", readiness=true. Elapsed: 2.022473773s
    Jun  9 11:41:18.179: INFO: Pod "pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae": Phase="Running", Reason="", readiness=false. Elapsed: 4.022314798s
    Jun  9 11:41:20.177: INFO: Pod "pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020415052s
    STEP: Saw pod success 06/09/23 11:41:20.177
    Jun  9 11:41:20.178: INFO: Pod "pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae" satisfied condition "Succeeded or Failed"
    Jun  9 11:41:20.184: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae container test-container: <nil>
    STEP: delete the pod 06/09/23 11:41:20.206
    Jun  9 11:41:20.226: INFO: Waiting for pod pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae to disappear
    Jun  9 11:41:20.232: INFO: Pod pod-511ab4cb-7a8c-4821-ba3d-54c251b255ae no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:41:20.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9207" for this suite. 06/09/23 11:41:20.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:41:20.259
Jun  9 11:41:20.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename statefulset 06/09/23 11:41:20.26
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:20.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:20.287
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4393 06/09/23 11:41:20.293
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Jun  9 11:41:20.319: INFO: Found 0 stateful pods, waiting for 1
Jun  9 11:41:30.329: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 06/09/23 11:41:30.343
W0609 11:41:30.359551      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun  9 11:41:30.376: INFO: Found 1 stateful pods, waiting for 2
Jun  9 11:41:40.389: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  9 11:41:40.389: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 06/09/23 11:41:40.417
STEP: Delete all of the StatefulSets 06/09/23 11:41:40.434
STEP: Verify that StatefulSets have been deleted 06/09/23 11:41:40.461
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  9 11:41:40.473: INFO: Deleting all statefulset in ns statefulset-4393
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  9 11:41:40.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4393" for this suite. 06/09/23 11:41:40.525
------------------------------
• [SLOW TEST] [20.289 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:41:20.259
    Jun  9 11:41:20.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename statefulset 06/09/23 11:41:20.26
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:20.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:20.287
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4393 06/09/23 11:41:20.293
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Jun  9 11:41:20.319: INFO: Found 0 stateful pods, waiting for 1
    Jun  9 11:41:30.329: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 06/09/23 11:41:30.343
    W0609 11:41:30.359551      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun  9 11:41:30.376: INFO: Found 1 stateful pods, waiting for 2
    Jun  9 11:41:40.389: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun  9 11:41:40.389: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 06/09/23 11:41:40.417
    STEP: Delete all of the StatefulSets 06/09/23 11:41:40.434
    STEP: Verify that StatefulSets have been deleted 06/09/23 11:41:40.461
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  9 11:41:40.473: INFO: Deleting all statefulset in ns statefulset-4393
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:41:40.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4393" for this suite. 06/09/23 11:41:40.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:41:40.563
Jun  9 11:41:40.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 11:41:40.564
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:40.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:40.644
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 11:41:40.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2176" for this suite. 06/09/23 11:41:40.683
------------------------------
• [0.133 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:41:40.563
    Jun  9 11:41:40.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 11:41:40.564
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:40.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:40.644
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:41:40.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2176" for this suite. 06/09/23 11:41:40.683
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:41:40.697
Jun  9 11:41:40.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename custom-resource-definition 06/09/23 11:41:40.698
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:40.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:40.775
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jun  9 11:41:40.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:41:44.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2911" for this suite. 06/09/23 11:41:44.194
------------------------------
• [3.515 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:41:40.697
    Jun  9 11:41:40.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename custom-resource-definition 06/09/23 11:41:40.698
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:40.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:40.775
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jun  9 11:41:40.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:41:44.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2911" for this suite. 06/09/23 11:41:44.194
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:41:44.212
Jun  9 11:41:44.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename events 06/09/23 11:41:44.213
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:44.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:44.246
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 06/09/23 11:41:44.252
STEP: listing events in all namespaces 06/09/23 11:41:44.282
STEP: listing events in test namespace 06/09/23 11:41:44.294
STEP: listing events with field selection filtering on source 06/09/23 11:41:44.299
STEP: listing events with field selection filtering on reportingController 06/09/23 11:41:44.307
STEP: getting the test event 06/09/23 11:41:44.317
STEP: patching the test event 06/09/23 11:41:44.325
STEP: getting the test event 06/09/23 11:41:44.345
STEP: updating the test event 06/09/23 11:41:44.35
STEP: getting the test event 06/09/23 11:41:44.364
STEP: deleting the test event 06/09/23 11:41:44.373
STEP: listing events in all namespaces 06/09/23 11:41:44.414
STEP: listing events in test namespace 06/09/23 11:41:44.42
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jun  9 11:41:44.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6101" for this suite. 06/09/23 11:41:44.433
------------------------------
• [0.230 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:41:44.212
    Jun  9 11:41:44.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename events 06/09/23 11:41:44.213
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:44.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:44.246
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 06/09/23 11:41:44.252
    STEP: listing events in all namespaces 06/09/23 11:41:44.282
    STEP: listing events in test namespace 06/09/23 11:41:44.294
    STEP: listing events with field selection filtering on source 06/09/23 11:41:44.299
    STEP: listing events with field selection filtering on reportingController 06/09/23 11:41:44.307
    STEP: getting the test event 06/09/23 11:41:44.317
    STEP: patching the test event 06/09/23 11:41:44.325
    STEP: getting the test event 06/09/23 11:41:44.345
    STEP: updating the test event 06/09/23 11:41:44.35
    STEP: getting the test event 06/09/23 11:41:44.364
    STEP: deleting the test event 06/09/23 11:41:44.373
    STEP: listing events in all namespaces 06/09/23 11:41:44.414
    STEP: listing events in test namespace 06/09/23 11:41:44.42
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:41:44.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6101" for this suite. 06/09/23 11:41:44.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:41:44.444
Jun  9 11:41:44.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename secrets 06/09/23 11:41:44.445
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:44.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:44.553
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-6221549a-258c-4874-84d5-7454e3eff9b5 06/09/23 11:41:44.684
STEP: Creating a pod to test consume secrets 06/09/23 11:41:44.697
Jun  9 11:41:44.722: INFO: Waiting up to 5m0s for pod "pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe" in namespace "secrets-9766" to be "Succeeded or Failed"
Jun  9 11:41:44.751: INFO: Pod "pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe": Phase="Pending", Reason="", readiness=false. Elapsed: 28.732406ms
Jun  9 11:41:46.758: INFO: Pod "pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036047648s
Jun  9 11:41:48.766: INFO: Pod "pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043309921s
Jun  9 11:41:50.762: INFO: Pod "pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039387577s
STEP: Saw pod success 06/09/23 11:41:50.762
Jun  9 11:41:50.762: INFO: Pod "pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe" satisfied condition "Succeeded or Failed"
Jun  9 11:41:50.780: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe container secret-volume-test: <nil>
STEP: delete the pod 06/09/23 11:41:50.794
Jun  9 11:41:50.836: INFO: Waiting for pod pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe to disappear
Jun  9 11:41:50.841: INFO: Pod pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun  9 11:41:50.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9766" for this suite. 06/09/23 11:41:50.851
STEP: Destroying namespace "secret-namespace-2828" for this suite. 06/09/23 11:41:50.909
------------------------------
• [SLOW TEST] [6.499 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:41:44.444
    Jun  9 11:41:44.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename secrets 06/09/23 11:41:44.445
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:44.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:44.553
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-6221549a-258c-4874-84d5-7454e3eff9b5 06/09/23 11:41:44.684
    STEP: Creating a pod to test consume secrets 06/09/23 11:41:44.697
    Jun  9 11:41:44.722: INFO: Waiting up to 5m0s for pod "pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe" in namespace "secrets-9766" to be "Succeeded or Failed"
    Jun  9 11:41:44.751: INFO: Pod "pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe": Phase="Pending", Reason="", readiness=false. Elapsed: 28.732406ms
    Jun  9 11:41:46.758: INFO: Pod "pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036047648s
    Jun  9 11:41:48.766: INFO: Pod "pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043309921s
    Jun  9 11:41:50.762: INFO: Pod "pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039387577s
    STEP: Saw pod success 06/09/23 11:41:50.762
    Jun  9 11:41:50.762: INFO: Pod "pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe" satisfied condition "Succeeded or Failed"
    Jun  9 11:41:50.780: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe container secret-volume-test: <nil>
    STEP: delete the pod 06/09/23 11:41:50.794
    Jun  9 11:41:50.836: INFO: Waiting for pod pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe to disappear
    Jun  9 11:41:50.841: INFO: Pod pod-secrets-f9c2c965-14c9-44aa-adbc-d56acc77dffe no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:41:50.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9766" for this suite. 06/09/23 11:41:50.851
    STEP: Destroying namespace "secret-namespace-2828" for this suite. 06/09/23 11:41:50.909
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:41:50.943
Jun  9 11:41:50.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename container-runtime 06/09/23 11:41:50.944
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:51.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:51.032
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 06/09/23 11:41:51.042
STEP: wait for the container to reach Succeeded 06/09/23 11:41:51.113
STEP: get the container status 06/09/23 11:41:55.178
STEP: the container should be terminated 06/09/23 11:41:55.183
STEP: the termination message should be set 06/09/23 11:41:55.183
Jun  9 11:41:55.183: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 06/09/23 11:41:55.183
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun  9 11:41:55.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4586" for this suite. 06/09/23 11:41:55.224
------------------------------
• [4.296 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:41:50.943
    Jun  9 11:41:50.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename container-runtime 06/09/23 11:41:50.944
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:51.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:51.032
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 06/09/23 11:41:51.042
    STEP: wait for the container to reach Succeeded 06/09/23 11:41:51.113
    STEP: get the container status 06/09/23 11:41:55.178
    STEP: the container should be terminated 06/09/23 11:41:55.183
    STEP: the termination message should be set 06/09/23 11:41:55.183
    Jun  9 11:41:55.183: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 06/09/23 11:41:55.183
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:41:55.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4586" for this suite. 06/09/23 11:41:55.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:41:55.24
Jun  9 11:41:55.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename var-expansion 06/09/23 11:41:55.241
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:55.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:55.27
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 06/09/23 11:41:55.276
Jun  9 11:41:55.289: INFO: Waiting up to 5m0s for pod "var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0" in namespace "var-expansion-3928" to be "Succeeded or Failed"
Jun  9 11:41:55.300: INFO: Pod "var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.56891ms
Jun  9 11:41:57.306: INFO: Pod "var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017424313s
Jun  9 11:41:59.307: INFO: Pod "var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018377238s
STEP: Saw pod success 06/09/23 11:41:59.307
Jun  9 11:41:59.308: INFO: Pod "var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0" satisfied condition "Succeeded or Failed"
Jun  9 11:41:59.314: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0 container dapi-container: <nil>
STEP: delete the pod 06/09/23 11:41:59.331
Jun  9 11:41:59.378: INFO: Waiting for pod var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0 to disappear
Jun  9 11:41:59.389: INFO: Pod var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  9 11:41:59.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3928" for this suite. 06/09/23 11:41:59.408
------------------------------
• [4.190 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:41:55.24
    Jun  9 11:41:55.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename var-expansion 06/09/23 11:41:55.241
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:55.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:55.27
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 06/09/23 11:41:55.276
    Jun  9 11:41:55.289: INFO: Waiting up to 5m0s for pod "var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0" in namespace "var-expansion-3928" to be "Succeeded or Failed"
    Jun  9 11:41:55.300: INFO: Pod "var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.56891ms
    Jun  9 11:41:57.306: INFO: Pod "var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017424313s
    Jun  9 11:41:59.307: INFO: Pod "var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018377238s
    STEP: Saw pod success 06/09/23 11:41:59.307
    Jun  9 11:41:59.308: INFO: Pod "var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0" satisfied condition "Succeeded or Failed"
    Jun  9 11:41:59.314: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0 container dapi-container: <nil>
    STEP: delete the pod 06/09/23 11:41:59.331
    Jun  9 11:41:59.378: INFO: Waiting for pod var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0 to disappear
    Jun  9 11:41:59.389: INFO: Pod var-expansion-8ca708f6-3627-430e-9f7d-5d130fb513e0 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:41:59.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3928" for this suite. 06/09/23 11:41:59.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:41:59.431
Jun  9 11:41:59.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename statefulset 06/09/23 11:41:59.433
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:59.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:59.475
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1558 06/09/23 11:41:59.48
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-1558 06/09/23 11:41:59.493
Jun  9 11:41:59.517: INFO: Found 0 stateful pods, waiting for 1
Jun  9 11:42:09.526: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 06/09/23 11:42:09.539
STEP: updating a scale subresource 06/09/23 11:42:09.544
STEP: verifying the statefulset Spec.Replicas was modified 06/09/23 11:42:09.556
STEP: Patch a scale subresource 06/09/23 11:42:09.563
STEP: verifying the statefulset Spec.Replicas was modified 06/09/23 11:42:09.617
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun  9 11:42:09.623: INFO: Deleting all statefulset in ns statefulset-1558
Jun  9 11:42:09.627: INFO: Scaling statefulset ss to 0
Jun  9 11:42:19.691: INFO: Waiting for statefulset status.replicas updated to 0
Jun  9 11:42:19.696: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun  9 11:42:19.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1558" for this suite. 06/09/23 11:42:19.726
------------------------------
• [SLOW TEST] [20.303 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:41:59.431
    Jun  9 11:41:59.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename statefulset 06/09/23 11:41:59.433
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:41:59.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:41:59.475
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1558 06/09/23 11:41:59.48
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-1558 06/09/23 11:41:59.493
    Jun  9 11:41:59.517: INFO: Found 0 stateful pods, waiting for 1
    Jun  9 11:42:09.526: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 06/09/23 11:42:09.539
    STEP: updating a scale subresource 06/09/23 11:42:09.544
    STEP: verifying the statefulset Spec.Replicas was modified 06/09/23 11:42:09.556
    STEP: Patch a scale subresource 06/09/23 11:42:09.563
    STEP: verifying the statefulset Spec.Replicas was modified 06/09/23 11:42:09.617
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun  9 11:42:09.623: INFO: Deleting all statefulset in ns statefulset-1558
    Jun  9 11:42:09.627: INFO: Scaling statefulset ss to 0
    Jun  9 11:42:19.691: INFO: Waiting for statefulset status.replicas updated to 0
    Jun  9 11:42:19.696: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:42:19.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1558" for this suite. 06/09/23 11:42:19.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:42:19.737
Jun  9 11:42:19.737: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename crd-watch 06/09/23 11:42:19.739
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:42:19.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:42:19.762
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jun  9 11:42:19.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Creating first CR  06/09/23 11:42:22.354
Jun  9 11:42:22.368: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-09T11:42:22Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-09T11:42:22Z]] name:name1 resourceVersion:114589 uid:8730860e-6d55-4f00-9ab6-b21af6c12248] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 06/09/23 11:42:32.369
Jun  9 11:42:32.378: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-09T11:42:32Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-09T11:42:32Z]] name:name2 resourceVersion:114654 uid:48e2ff05-e2df-4258-b23f-e3bbe2cfffed] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 06/09/23 11:42:42.378
Jun  9 11:42:42.402: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-09T11:42:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-09T11:42:42Z]] name:name1 resourceVersion:114697 uid:8730860e-6d55-4f00-9ab6-b21af6c12248] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 06/09/23 11:42:52.402
Jun  9 11:42:52.413: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-09T11:42:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-09T11:42:52Z]] name:name2 resourceVersion:114741 uid:48e2ff05-e2df-4258-b23f-e3bbe2cfffed] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 06/09/23 11:43:02.414
Jun  9 11:43:02.427: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-09T11:42:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-09T11:42:42Z]] name:name1 resourceVersion:114783 uid:8730860e-6d55-4f00-9ab6-b21af6c12248] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 06/09/23 11:43:12.427
Jun  9 11:43:12.439: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-09T11:42:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-09T11:42:52Z]] name:name2 resourceVersion:114827 uid:48e2ff05-e2df-4258-b23f-e3bbe2cfffed] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:43:22.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-470" for this suite. 06/09/23 11:43:22.977
------------------------------
• [SLOW TEST] [63.251 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:42:19.737
    Jun  9 11:42:19.737: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename crd-watch 06/09/23 11:42:19.739
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:42:19.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:42:19.762
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jun  9 11:42:19.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Creating first CR  06/09/23 11:42:22.354
    Jun  9 11:42:22.368: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-09T11:42:22Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-09T11:42:22Z]] name:name1 resourceVersion:114589 uid:8730860e-6d55-4f00-9ab6-b21af6c12248] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 06/09/23 11:42:32.369
    Jun  9 11:42:32.378: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-09T11:42:32Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-09T11:42:32Z]] name:name2 resourceVersion:114654 uid:48e2ff05-e2df-4258-b23f-e3bbe2cfffed] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 06/09/23 11:42:42.378
    Jun  9 11:42:42.402: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-09T11:42:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-09T11:42:42Z]] name:name1 resourceVersion:114697 uid:8730860e-6d55-4f00-9ab6-b21af6c12248] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 06/09/23 11:42:52.402
    Jun  9 11:42:52.413: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-09T11:42:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-09T11:42:52Z]] name:name2 resourceVersion:114741 uid:48e2ff05-e2df-4258-b23f-e3bbe2cfffed] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 06/09/23 11:43:02.414
    Jun  9 11:43:02.427: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-09T11:42:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-09T11:42:42Z]] name:name1 resourceVersion:114783 uid:8730860e-6d55-4f00-9ab6-b21af6c12248] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 06/09/23 11:43:12.427
    Jun  9 11:43:12.439: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-09T11:42:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-09T11:42:52Z]] name:name2 resourceVersion:114827 uid:48e2ff05-e2df-4258-b23f-e3bbe2cfffed] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:43:22.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-470" for this suite. 06/09/23 11:43:22.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:43:22.989
Jun  9 11:43:22.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename proxy 06/09/23 11:43:22.991
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:23.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:23.016
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jun  9 11:43:23.020: INFO: Creating pod...
Jun  9 11:43:23.033: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9064" to be "running"
Jun  9 11:43:23.039: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.778501ms
Jun  9 11:43:25.048: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01466374s
Jun  9 11:43:27.051: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.018012763s
Jun  9 11:43:27.051: INFO: Pod "agnhost" satisfied condition "running"
Jun  9 11:43:27.051: INFO: Creating service...
Jun  9 11:43:27.147: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/pods/agnhost/proxy?method=DELETE
Jun  9 11:43:27.169: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun  9 11:43:27.169: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/pods/agnhost/proxy?method=OPTIONS
Jun  9 11:43:27.186: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun  9 11:43:27.186: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/pods/agnhost/proxy?method=PATCH
Jun  9 11:43:27.201: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun  9 11:43:27.201: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/pods/agnhost/proxy?method=POST
Jun  9 11:43:27.217: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun  9 11:43:27.217: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/pods/agnhost/proxy?method=PUT
Jun  9 11:43:27.229: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun  9 11:43:27.229: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/services/e2e-proxy-test-service/proxy?method=DELETE
Jun  9 11:43:27.269: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun  9 11:43:27.269: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jun  9 11:43:27.295: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun  9 11:43:27.295: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/services/e2e-proxy-test-service/proxy?method=PATCH
Jun  9 11:43:27.320: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun  9 11:43:27.320: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/services/e2e-proxy-test-service/proxy?method=POST
Jun  9 11:43:27.341: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun  9 11:43:27.341: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/services/e2e-proxy-test-service/proxy?method=PUT
Jun  9 11:43:27.360: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun  9 11:43:27.361: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/pods/agnhost/proxy?method=GET
Jun  9 11:43:27.373: INFO: http.Client request:GET StatusCode:301
Jun  9 11:43:27.373: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/services/e2e-proxy-test-service/proxy?method=GET
Jun  9 11:43:27.387: INFO: http.Client request:GET StatusCode:301
Jun  9 11:43:27.387: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/pods/agnhost/proxy?method=HEAD
Jun  9 11:43:27.391: INFO: http.Client request:HEAD StatusCode:301
Jun  9 11:43:27.391: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/services/e2e-proxy-test-service/proxy?method=HEAD
Jun  9 11:43:27.404: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jun  9 11:43:27.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-9064" for this suite. 06/09/23 11:43:27.417
------------------------------
• [4.455 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:43:22.989
    Jun  9 11:43:22.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename proxy 06/09/23 11:43:22.991
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:23.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:23.016
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jun  9 11:43:23.020: INFO: Creating pod...
    Jun  9 11:43:23.033: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9064" to be "running"
    Jun  9 11:43:23.039: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.778501ms
    Jun  9 11:43:25.048: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01466374s
    Jun  9 11:43:27.051: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.018012763s
    Jun  9 11:43:27.051: INFO: Pod "agnhost" satisfied condition "running"
    Jun  9 11:43:27.051: INFO: Creating service...
    Jun  9 11:43:27.147: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/pods/agnhost/proxy?method=DELETE
    Jun  9 11:43:27.169: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun  9 11:43:27.169: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/pods/agnhost/proxy?method=OPTIONS
    Jun  9 11:43:27.186: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun  9 11:43:27.186: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/pods/agnhost/proxy?method=PATCH
    Jun  9 11:43:27.201: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun  9 11:43:27.201: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/pods/agnhost/proxy?method=POST
    Jun  9 11:43:27.217: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun  9 11:43:27.217: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/pods/agnhost/proxy?method=PUT
    Jun  9 11:43:27.229: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun  9 11:43:27.229: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/services/e2e-proxy-test-service/proxy?method=DELETE
    Jun  9 11:43:27.269: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun  9 11:43:27.269: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jun  9 11:43:27.295: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun  9 11:43:27.295: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/services/e2e-proxy-test-service/proxy?method=PATCH
    Jun  9 11:43:27.320: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun  9 11:43:27.320: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/services/e2e-proxy-test-service/proxy?method=POST
    Jun  9 11:43:27.341: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun  9 11:43:27.341: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/services/e2e-proxy-test-service/proxy?method=PUT
    Jun  9 11:43:27.360: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun  9 11:43:27.361: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/pods/agnhost/proxy?method=GET
    Jun  9 11:43:27.373: INFO: http.Client request:GET StatusCode:301
    Jun  9 11:43:27.373: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/services/e2e-proxy-test-service/proxy?method=GET
    Jun  9 11:43:27.387: INFO: http.Client request:GET StatusCode:301
    Jun  9 11:43:27.387: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/pods/agnhost/proxy?method=HEAD
    Jun  9 11:43:27.391: INFO: http.Client request:HEAD StatusCode:301
    Jun  9 11:43:27.391: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9064/services/e2e-proxy-test-service/proxy?method=HEAD
    Jun  9 11:43:27.404: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:43:27.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-9064" for this suite. 06/09/23 11:43:27.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:43:27.446
Jun  9 11:43:27.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 11:43:27.448
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:27.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:27.491
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 06/09/23 11:43:27.506
Jun  9 11:43:27.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun  9 11:43:27.645: INFO: stderr: ""
Jun  9 11:43:27.645: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 06/09/23 11:43:27.645
Jun  9 11:43:27.645: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun  9 11:43:27.645: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3539" to be "running and ready, or succeeded"
Jun  9 11:43:27.674: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 29.105676ms
Jun  9 11:43:27.674: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'sks-test-v1-26.4-workergroup-4hkw9' to be 'Running' but was 'Pending'
Jun  9 11:43:29.681: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035638189s
Jun  9 11:43:29.681: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'sks-test-v1-26.4-workergroup-4hkw9' to be 'Running' but was 'Pending'
Jun  9 11:43:31.681: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.036177041s
Jun  9 11:43:31.681: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun  9 11:43:31.681: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 06/09/23 11:43:31.681
Jun  9 11:43:31.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 logs logs-generator logs-generator'
Jun  9 11:43:31.803: INFO: stderr: ""
Jun  9 11:43:31.804: INFO: stdout: "I0609 11:43:29.012628       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/jj6x 489\nI0609 11:43:29.212875       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/cwj 457\nI0609 11:43:29.413142       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/pmcn 391\nI0609 11:43:29.615177       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/8q5p 249\nI0609 11:43:29.813647       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/fk9 300\nI0609 11:43:30.013167       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/m5hv 266\nI0609 11:43:30.213618       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/bgpr 294\nI0609 11:43:30.413112       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/x6l5 386\nI0609 11:43:30.613783       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/l75 404\nI0609 11:43:30.813407       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/mgb 382\nI0609 11:43:31.012943       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/gxhz 344\nI0609 11:43:31.213431       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/29n2 481\nI0609 11:43:31.412817       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/b49 291\nI0609 11:43:31.613282       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/7gvh 418\nI0609 11:43:31.813948       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/bt72 405\n"
STEP: limiting log lines 06/09/23 11:43:31.804
Jun  9 11:43:31.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 logs logs-generator logs-generator --tail=1'
Jun  9 11:43:31.915: INFO: stderr: ""
Jun  9 11:43:31.915: INFO: stdout: "I0609 11:43:31.813948       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/bt72 405\n"
Jun  9 11:43:31.915: INFO: got output "I0609 11:43:31.813948       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/bt72 405\n"
STEP: limiting log bytes 06/09/23 11:43:31.915
Jun  9 11:43:31.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 logs logs-generator logs-generator --limit-bytes=1'
Jun  9 11:43:32.039: INFO: stderr: ""
Jun  9 11:43:32.039: INFO: stdout: "I"
Jun  9 11:43:32.039: INFO: got output "I"
STEP: exposing timestamps 06/09/23 11:43:32.039
Jun  9 11:43:32.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 logs logs-generator logs-generator --tail=1 --timestamps'
Jun  9 11:43:32.174: INFO: stderr: ""
Jun  9 11:43:32.174: INFO: stdout: "2023-06-09T19:43:32.013624899+08:00 I0609 11:43:32.013404       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/5fx6 237\n"
Jun  9 11:43:32.174: INFO: got output "2023-06-09T19:43:32.013624899+08:00 I0609 11:43:32.013404       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/5fx6 237\n"
STEP: restricting to a time range 06/09/23 11:43:32.174
Jun  9 11:43:34.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 logs logs-generator logs-generator --since=1s'
Jun  9 11:43:34.774: INFO: stderr: ""
Jun  9 11:43:34.774: INFO: stdout: "I0609 11:43:33.812816       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/nl5v 391\nI0609 11:43:34.013310       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/b7p 312\nI0609 11:43:34.212658       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/7pr 288\nI0609 11:43:34.413248       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/r6h5 239\nI0609 11:43:34.613637       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/vpc6 488\n"
Jun  9 11:43:34.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 logs logs-generator logs-generator --since=24h'
Jun  9 11:43:34.878: INFO: stderr: ""
Jun  9 11:43:34.878: INFO: stdout: "I0609 11:43:29.012628       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/jj6x 489\nI0609 11:43:29.212875       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/cwj 457\nI0609 11:43:29.413142       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/pmcn 391\nI0609 11:43:29.615177       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/8q5p 249\nI0609 11:43:29.813647       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/fk9 300\nI0609 11:43:30.013167       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/m5hv 266\nI0609 11:43:30.213618       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/bgpr 294\nI0609 11:43:30.413112       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/x6l5 386\nI0609 11:43:30.613783       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/l75 404\nI0609 11:43:30.813407       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/mgb 382\nI0609 11:43:31.012943       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/gxhz 344\nI0609 11:43:31.213431       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/29n2 481\nI0609 11:43:31.412817       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/b49 291\nI0609 11:43:31.613282       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/7gvh 418\nI0609 11:43:31.813948       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/bt72 405\nI0609 11:43:32.013404       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/5fx6 237\nI0609 11:43:32.212734       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/xnlq 342\nI0609 11:43:32.413073       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/j8d2 348\nI0609 11:43:32.613348       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/dfsp 415\nI0609 11:43:32.812737       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/8dk4 538\nI0609 11:43:33.013552       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/kd45 358\nI0609 11:43:33.213132       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/gsr 348\nI0609 11:43:33.413492       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/mst 205\nI0609 11:43:33.613087       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/jds 462\nI0609 11:43:33.812816       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/nl5v 391\nI0609 11:43:34.013310       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/b7p 312\nI0609 11:43:34.212658       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/7pr 288\nI0609 11:43:34.413248       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/r6h5 239\nI0609 11:43:34.613637       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/vpc6 488\nI0609 11:43:34.813071       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/xztd 240\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jun  9 11:43:34.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 delete pod logs-generator'
Jun  9 11:43:36.001: INFO: stderr: ""
Jun  9 11:43:36.002: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 11:43:36.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3539" for this suite. 06/09/23 11:43:36.011
------------------------------
• [SLOW TEST] [8.575 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:43:27.446
    Jun  9 11:43:27.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 11:43:27.448
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:27.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:27.491
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 06/09/23 11:43:27.506
    Jun  9 11:43:27.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jun  9 11:43:27.645: INFO: stderr: ""
    Jun  9 11:43:27.645: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 06/09/23 11:43:27.645
    Jun  9 11:43:27.645: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jun  9 11:43:27.645: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3539" to be "running and ready, or succeeded"
    Jun  9 11:43:27.674: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 29.105676ms
    Jun  9 11:43:27.674: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'sks-test-v1-26.4-workergroup-4hkw9' to be 'Running' but was 'Pending'
    Jun  9 11:43:29.681: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035638189s
    Jun  9 11:43:29.681: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'sks-test-v1-26.4-workergroup-4hkw9' to be 'Running' but was 'Pending'
    Jun  9 11:43:31.681: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.036177041s
    Jun  9 11:43:31.681: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jun  9 11:43:31.681: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 06/09/23 11:43:31.681
    Jun  9 11:43:31.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 logs logs-generator logs-generator'
    Jun  9 11:43:31.803: INFO: stderr: ""
    Jun  9 11:43:31.804: INFO: stdout: "I0609 11:43:29.012628       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/jj6x 489\nI0609 11:43:29.212875       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/cwj 457\nI0609 11:43:29.413142       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/pmcn 391\nI0609 11:43:29.615177       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/8q5p 249\nI0609 11:43:29.813647       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/fk9 300\nI0609 11:43:30.013167       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/m5hv 266\nI0609 11:43:30.213618       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/bgpr 294\nI0609 11:43:30.413112       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/x6l5 386\nI0609 11:43:30.613783       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/l75 404\nI0609 11:43:30.813407       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/mgb 382\nI0609 11:43:31.012943       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/gxhz 344\nI0609 11:43:31.213431       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/29n2 481\nI0609 11:43:31.412817       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/b49 291\nI0609 11:43:31.613282       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/7gvh 418\nI0609 11:43:31.813948       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/bt72 405\n"
    STEP: limiting log lines 06/09/23 11:43:31.804
    Jun  9 11:43:31.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 logs logs-generator logs-generator --tail=1'
    Jun  9 11:43:31.915: INFO: stderr: ""
    Jun  9 11:43:31.915: INFO: stdout: "I0609 11:43:31.813948       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/bt72 405\n"
    Jun  9 11:43:31.915: INFO: got output "I0609 11:43:31.813948       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/bt72 405\n"
    STEP: limiting log bytes 06/09/23 11:43:31.915
    Jun  9 11:43:31.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 logs logs-generator logs-generator --limit-bytes=1'
    Jun  9 11:43:32.039: INFO: stderr: ""
    Jun  9 11:43:32.039: INFO: stdout: "I"
    Jun  9 11:43:32.039: INFO: got output "I"
    STEP: exposing timestamps 06/09/23 11:43:32.039
    Jun  9 11:43:32.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 logs logs-generator logs-generator --tail=1 --timestamps'
    Jun  9 11:43:32.174: INFO: stderr: ""
    Jun  9 11:43:32.174: INFO: stdout: "2023-06-09T19:43:32.013624899+08:00 I0609 11:43:32.013404       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/5fx6 237\n"
    Jun  9 11:43:32.174: INFO: got output "2023-06-09T19:43:32.013624899+08:00 I0609 11:43:32.013404       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/5fx6 237\n"
    STEP: restricting to a time range 06/09/23 11:43:32.174
    Jun  9 11:43:34.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 logs logs-generator logs-generator --since=1s'
    Jun  9 11:43:34.774: INFO: stderr: ""
    Jun  9 11:43:34.774: INFO: stdout: "I0609 11:43:33.812816       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/nl5v 391\nI0609 11:43:34.013310       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/b7p 312\nI0609 11:43:34.212658       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/7pr 288\nI0609 11:43:34.413248       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/r6h5 239\nI0609 11:43:34.613637       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/vpc6 488\n"
    Jun  9 11:43:34.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 logs logs-generator logs-generator --since=24h'
    Jun  9 11:43:34.878: INFO: stderr: ""
    Jun  9 11:43:34.878: INFO: stdout: "I0609 11:43:29.012628       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/jj6x 489\nI0609 11:43:29.212875       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/cwj 457\nI0609 11:43:29.413142       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/pmcn 391\nI0609 11:43:29.615177       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/8q5p 249\nI0609 11:43:29.813647       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/fk9 300\nI0609 11:43:30.013167       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/m5hv 266\nI0609 11:43:30.213618       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/bgpr 294\nI0609 11:43:30.413112       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/x6l5 386\nI0609 11:43:30.613783       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/l75 404\nI0609 11:43:30.813407       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/mgb 382\nI0609 11:43:31.012943       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/gxhz 344\nI0609 11:43:31.213431       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/29n2 481\nI0609 11:43:31.412817       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/b49 291\nI0609 11:43:31.613282       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/7gvh 418\nI0609 11:43:31.813948       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/bt72 405\nI0609 11:43:32.013404       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/5fx6 237\nI0609 11:43:32.212734       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/xnlq 342\nI0609 11:43:32.413073       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/j8d2 348\nI0609 11:43:32.613348       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/dfsp 415\nI0609 11:43:32.812737       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/8dk4 538\nI0609 11:43:33.013552       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/kd45 358\nI0609 11:43:33.213132       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/gsr 348\nI0609 11:43:33.413492       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/mst 205\nI0609 11:43:33.613087       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/jds 462\nI0609 11:43:33.812816       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/nl5v 391\nI0609 11:43:34.013310       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/b7p 312\nI0609 11:43:34.212658       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/7pr 288\nI0609 11:43:34.413248       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/r6h5 239\nI0609 11:43:34.613637       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/vpc6 488\nI0609 11:43:34.813071       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/xztd 240\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jun  9 11:43:34.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-3539 delete pod logs-generator'
    Jun  9 11:43:36.001: INFO: stderr: ""
    Jun  9 11:43:36.002: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:43:36.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3539" for this suite. 06/09/23 11:43:36.011
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:43:36.023
Jun  9 11:43:36.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename tables 06/09/23 11:43:36.024
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:36.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:36.048
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jun  9 11:43:36.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-4436" for this suite. 06/09/23 11:43:36.065
------------------------------
• [0.054 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:43:36.023
    Jun  9 11:43:36.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename tables 06/09/23 11:43:36.024
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:36.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:36.048
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:43:36.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-4436" for this suite. 06/09/23 11:43:36.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:43:36.081
Jun  9 11:43:36.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename svcaccounts 06/09/23 11:43:36.083
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:36.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:36.106
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Jun  9 11:43:36.179: INFO: created pod pod-service-account-defaultsa
Jun  9 11:43:36.179: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun  9 11:43:36.193: INFO: created pod pod-service-account-mountsa
Jun  9 11:43:36.193: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun  9 11:43:36.207: INFO: created pod pod-service-account-nomountsa
Jun  9 11:43:36.207: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun  9 11:43:36.219: INFO: created pod pod-service-account-defaultsa-mountspec
Jun  9 11:43:36.219: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun  9 11:43:36.233: INFO: created pod pod-service-account-mountsa-mountspec
Jun  9 11:43:36.233: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun  9 11:43:36.305: INFO: created pod pod-service-account-nomountsa-mountspec
Jun  9 11:43:36.305: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun  9 11:43:36.347: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun  9 11:43:36.347: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun  9 11:43:36.376: INFO: created pod pod-service-account-mountsa-nomountspec
Jun  9 11:43:36.376: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun  9 11:43:36.389: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun  9 11:43:36.389: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun  9 11:43:36.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8388" for this suite. 06/09/23 11:43:36.398
------------------------------
• [0.337 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:43:36.081
    Jun  9 11:43:36.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename svcaccounts 06/09/23 11:43:36.083
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:36.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:36.106
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Jun  9 11:43:36.179: INFO: created pod pod-service-account-defaultsa
    Jun  9 11:43:36.179: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jun  9 11:43:36.193: INFO: created pod pod-service-account-mountsa
    Jun  9 11:43:36.193: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jun  9 11:43:36.207: INFO: created pod pod-service-account-nomountsa
    Jun  9 11:43:36.207: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jun  9 11:43:36.219: INFO: created pod pod-service-account-defaultsa-mountspec
    Jun  9 11:43:36.219: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jun  9 11:43:36.233: INFO: created pod pod-service-account-mountsa-mountspec
    Jun  9 11:43:36.233: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jun  9 11:43:36.305: INFO: created pod pod-service-account-nomountsa-mountspec
    Jun  9 11:43:36.305: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jun  9 11:43:36.347: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jun  9 11:43:36.347: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jun  9 11:43:36.376: INFO: created pod pod-service-account-mountsa-nomountspec
    Jun  9 11:43:36.376: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jun  9 11:43:36.389: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jun  9 11:43:36.389: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:43:36.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8388" for this suite. 06/09/23 11:43:36.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:43:36.419
Jun  9 11:43:36.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename projected 06/09/23 11:43:36.42
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:36.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:36.515
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-37257f95-7767-4161-a707-c901914c1e4e 06/09/23 11:43:36.526
STEP: Creating a pod to test consume configMaps 06/09/23 11:43:36.539
Jun  9 11:43:36.564: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec" in namespace "projected-8191" to be "Succeeded or Failed"
Jun  9 11:43:36.570: INFO: Pod "pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.351946ms
Jun  9 11:43:38.579: INFO: Pod "pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015062077s
Jun  9 11:43:40.579: INFO: Pod "pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015346435s
STEP: Saw pod success 06/09/23 11:43:40.579
Jun  9 11:43:40.579: INFO: Pod "pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec" satisfied condition "Succeeded or Failed"
Jun  9 11:43:40.586: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec container agnhost-container: <nil>
STEP: delete the pod 06/09/23 11:43:40.61
Jun  9 11:43:40.642: INFO: Waiting for pod pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec to disappear
Jun  9 11:43:40.647: INFO: Pod pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun  9 11:43:40.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8191" for this suite. 06/09/23 11:43:40.659
------------------------------
• [4.253 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:43:36.419
    Jun  9 11:43:36.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename projected 06/09/23 11:43:36.42
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:36.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:36.515
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-37257f95-7767-4161-a707-c901914c1e4e 06/09/23 11:43:36.526
    STEP: Creating a pod to test consume configMaps 06/09/23 11:43:36.539
    Jun  9 11:43:36.564: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec" in namespace "projected-8191" to be "Succeeded or Failed"
    Jun  9 11:43:36.570: INFO: Pod "pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.351946ms
    Jun  9 11:43:38.579: INFO: Pod "pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015062077s
    Jun  9 11:43:40.579: INFO: Pod "pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015346435s
    STEP: Saw pod success 06/09/23 11:43:40.579
    Jun  9 11:43:40.579: INFO: Pod "pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec" satisfied condition "Succeeded or Failed"
    Jun  9 11:43:40.586: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-q5bjm pod pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 11:43:40.61
    Jun  9 11:43:40.642: INFO: Waiting for pod pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec to disappear
    Jun  9 11:43:40.647: INFO: Pod pod-projected-configmaps-4cb15b55-68c7-4c05-828c-d704f86c15ec no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:43:40.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8191" for this suite. 06/09/23 11:43:40.659
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:43:40.672
Jun  9 11:43:40.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename podtemplate 06/09/23 11:43:40.674
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:40.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:40.7
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 06/09/23 11:43:40.705
Jun  9 11:43:40.731: INFO: created test-podtemplate-1
Jun  9 11:43:40.742: INFO: created test-podtemplate-2
Jun  9 11:43:40.753: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 06/09/23 11:43:40.753
STEP: delete collection of pod templates 06/09/23 11:43:40.76
Jun  9 11:43:40.760: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 06/09/23 11:43:40.813
Jun  9 11:43:40.813: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jun  9 11:43:40.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-2622" for this suite. 06/09/23 11:43:40.826
------------------------------
• [0.170 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:43:40.672
    Jun  9 11:43:40.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename podtemplate 06/09/23 11:43:40.674
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:40.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:40.7
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 06/09/23 11:43:40.705
    Jun  9 11:43:40.731: INFO: created test-podtemplate-1
    Jun  9 11:43:40.742: INFO: created test-podtemplate-2
    Jun  9 11:43:40.753: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 06/09/23 11:43:40.753
    STEP: delete collection of pod templates 06/09/23 11:43:40.76
    Jun  9 11:43:40.760: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 06/09/23 11:43:40.813
    Jun  9 11:43:40.813: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:43:40.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-2622" for this suite. 06/09/23 11:43:40.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:43:40.844
Jun  9 11:43:40.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename init-container 06/09/23 11:43:40.845
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:40.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:40.917
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 06/09/23 11:43:40.924
Jun  9 11:43:40.924: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:43:45.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2908" for this suite. 06/09/23 11:43:45.248
------------------------------
• [4.413 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:43:40.844
    Jun  9 11:43:40.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename init-container 06/09/23 11:43:40.845
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:40.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:40.917
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 06/09/23 11:43:40.924
    Jun  9 11:43:40.924: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:43:45.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2908" for this suite. 06/09/23 11:43:45.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:43:45.258
Jun  9 11:43:45.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename gc 06/09/23 11:43:45.26
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:45.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:45.289
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jun  9 11:43:45.356: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"66ee9024-1c41-4ae5-9d1c-aab518816920", Controller:(*bool)(0xc0036c112a), BlockOwnerDeletion:(*bool)(0xc0036c112b)}}
Jun  9 11:43:45.376: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b2c422f7-4b84-4313-a780-907d6423d5ef", Controller:(*bool)(0xc0009c9362), BlockOwnerDeletion:(*bool)(0xc0009c9363)}}
Jun  9 11:43:45.388: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"79b64318-2442-4b79-9b6d-4b9d44fde459", Controller:(*bool)(0xc0036c13a2), BlockOwnerDeletion:(*bool)(0xc0036c13a3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun  9 11:43:50.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6862" for this suite. 06/09/23 11:43:50.438
------------------------------
• [SLOW TEST] [5.195 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:43:45.258
    Jun  9 11:43:45.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename gc 06/09/23 11:43:45.26
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:45.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:45.289
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jun  9 11:43:45.356: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"66ee9024-1c41-4ae5-9d1c-aab518816920", Controller:(*bool)(0xc0036c112a), BlockOwnerDeletion:(*bool)(0xc0036c112b)}}
    Jun  9 11:43:45.376: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b2c422f7-4b84-4313-a780-907d6423d5ef", Controller:(*bool)(0xc0009c9362), BlockOwnerDeletion:(*bool)(0xc0009c9363)}}
    Jun  9 11:43:45.388: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"79b64318-2442-4b79-9b6d-4b9d44fde459", Controller:(*bool)(0xc0036c13a2), BlockOwnerDeletion:(*bool)(0xc0036c13a3)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:43:50.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6862" for this suite. 06/09/23 11:43:50.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:43:50.454
Jun  9 11:43:50.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename dns 06/09/23 11:43:50.455
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:50.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:50.511
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6123.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6123.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 06/09/23 11:43:50.519
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6123.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6123.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 06/09/23 11:43:50.519
STEP: creating a pod to probe /etc/hosts 06/09/23 11:43:50.519
STEP: submitting the pod to kubernetes 06/09/23 11:43:50.52
Jun  9 11:43:50.535: INFO: Waiting up to 15m0s for pod "dns-test-741079dd-6637-4eee-ab41-1872b06f70da" in namespace "dns-6123" to be "running"
Jun  9 11:43:50.539: INFO: Pod "dns-test-741079dd-6637-4eee-ab41-1872b06f70da": Phase="Pending", Reason="", readiness=false. Elapsed: 3.964837ms
Jun  9 11:43:52.546: INFO: Pod "dns-test-741079dd-6637-4eee-ab41-1872b06f70da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010223643s
Jun  9 11:43:54.551: INFO: Pod "dns-test-741079dd-6637-4eee-ab41-1872b06f70da": Phase="Running", Reason="", readiness=true. Elapsed: 4.015153324s
Jun  9 11:43:54.551: INFO: Pod "dns-test-741079dd-6637-4eee-ab41-1872b06f70da" satisfied condition "running"
STEP: retrieving the pod 06/09/23 11:43:54.551
STEP: looking for the results for each expected name from probers 06/09/23 11:43:54.558
Jun  9 11:43:54.624: INFO: DNS probes using dns-6123/dns-test-741079dd-6637-4eee-ab41-1872b06f70da succeeded

STEP: deleting the pod 06/09/23 11:43:54.624
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  9 11:43:54.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6123" for this suite. 06/09/23 11:43:54.691
------------------------------
• [4.248 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:43:50.454
    Jun  9 11:43:50.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename dns 06/09/23 11:43:50.455
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:50.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:50.511
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6123.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6123.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     06/09/23 11:43:50.519
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6123.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6123.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     06/09/23 11:43:50.519
    STEP: creating a pod to probe /etc/hosts 06/09/23 11:43:50.519
    STEP: submitting the pod to kubernetes 06/09/23 11:43:50.52
    Jun  9 11:43:50.535: INFO: Waiting up to 15m0s for pod "dns-test-741079dd-6637-4eee-ab41-1872b06f70da" in namespace "dns-6123" to be "running"
    Jun  9 11:43:50.539: INFO: Pod "dns-test-741079dd-6637-4eee-ab41-1872b06f70da": Phase="Pending", Reason="", readiness=false. Elapsed: 3.964837ms
    Jun  9 11:43:52.546: INFO: Pod "dns-test-741079dd-6637-4eee-ab41-1872b06f70da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010223643s
    Jun  9 11:43:54.551: INFO: Pod "dns-test-741079dd-6637-4eee-ab41-1872b06f70da": Phase="Running", Reason="", readiness=true. Elapsed: 4.015153324s
    Jun  9 11:43:54.551: INFO: Pod "dns-test-741079dd-6637-4eee-ab41-1872b06f70da" satisfied condition "running"
    STEP: retrieving the pod 06/09/23 11:43:54.551
    STEP: looking for the results for each expected name from probers 06/09/23 11:43:54.558
    Jun  9 11:43:54.624: INFO: DNS probes using dns-6123/dns-test-741079dd-6637-4eee-ab41-1872b06f70da succeeded

    STEP: deleting the pod 06/09/23 11:43:54.624
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:43:54.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6123" for this suite. 06/09/23 11:43:54.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:43:54.703
Jun  9 11:43:54.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename configmap 06/09/23 11:43:54.705
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:54.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:54.739
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-7c91c13b-2427-4f9f-917c-8b7ba33555f2 06/09/23 11:43:54.745
STEP: Creating a pod to test consume configMaps 06/09/23 11:43:54.756
Jun  9 11:43:54.791: INFO: Waiting up to 5m0s for pod "pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9" in namespace "configmap-5543" to be "Succeeded or Failed"
Jun  9 11:43:54.799: INFO: Pod "pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.23857ms
Jun  9 11:43:56.806: INFO: Pod "pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015715526s
Jun  9 11:43:58.805: INFO: Pod "pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014516464s
STEP: Saw pod success 06/09/23 11:43:58.805
Jun  9 11:43:58.806: INFO: Pod "pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9" satisfied condition "Succeeded or Failed"
Jun  9 11:43:58.811: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9 container agnhost-container: <nil>
STEP: delete the pod 06/09/23 11:43:58.82
Jun  9 11:43:58.842: INFO: Waiting for pod pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9 to disappear
Jun  9 11:43:58.855: INFO: Pod pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun  9 11:43:58.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5543" for this suite. 06/09/23 11:43:58.864
------------------------------
• [4.171 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:43:54.703
    Jun  9 11:43:54.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename configmap 06/09/23 11:43:54.705
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:54.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:54.739
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-7c91c13b-2427-4f9f-917c-8b7ba33555f2 06/09/23 11:43:54.745
    STEP: Creating a pod to test consume configMaps 06/09/23 11:43:54.756
    Jun  9 11:43:54.791: INFO: Waiting up to 5m0s for pod "pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9" in namespace "configmap-5543" to be "Succeeded or Failed"
    Jun  9 11:43:54.799: INFO: Pod "pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.23857ms
    Jun  9 11:43:56.806: INFO: Pod "pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015715526s
    Jun  9 11:43:58.805: INFO: Pod "pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014516464s
    STEP: Saw pod success 06/09/23 11:43:58.805
    Jun  9 11:43:58.806: INFO: Pod "pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9" satisfied condition "Succeeded or Failed"
    Jun  9 11:43:58.811: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9 container agnhost-container: <nil>
    STEP: delete the pod 06/09/23 11:43:58.82
    Jun  9 11:43:58.842: INFO: Waiting for pod pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9 to disappear
    Jun  9 11:43:58.855: INFO: Pod pod-configmaps-50f02af6-2e7b-432f-9b71-ef8d5c1721f9 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:43:58.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5543" for this suite. 06/09/23 11:43:58.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:43:58.876
Jun  9 11:43:58.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename dns 06/09/23 11:43:58.877
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:58.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:58.903
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 06/09/23 11:43:58.909
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7138.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7138.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 06/09/23 11:43:58.919
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7138.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7138.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 06/09/23 11:43:58.919
STEP: creating a pod to probe DNS 06/09/23 11:43:58.919
STEP: submitting the pod to kubernetes 06/09/23 11:43:58.919
Jun  9 11:43:58.935: INFO: Waiting up to 15m0s for pod "dns-test-af5722a9-6803-4960-8995-4c862c1e190e" in namespace "dns-7138" to be "running"
Jun  9 11:43:58.940: INFO: Pod "dns-test-af5722a9-6803-4960-8995-4c862c1e190e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.69693ms
Jun  9 11:44:00.950: INFO: Pod "dns-test-af5722a9-6803-4960-8995-4c862c1e190e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015011132s
Jun  9 11:44:02.947: INFO: Pod "dns-test-af5722a9-6803-4960-8995-4c862c1e190e": Phase="Running", Reason="", readiness=true. Elapsed: 4.012697674s
Jun  9 11:44:02.947: INFO: Pod "dns-test-af5722a9-6803-4960-8995-4c862c1e190e" satisfied condition "running"
STEP: retrieving the pod 06/09/23 11:44:02.947
STEP: looking for the results for each expected name from probers 06/09/23 11:44:02.978
Jun  9 11:44:03.011: INFO: DNS probes using dns-7138/dns-test-af5722a9-6803-4960-8995-4c862c1e190e succeeded

STEP: deleting the pod 06/09/23 11:44:03.011
STEP: deleting the test headless service 06/09/23 11:44:03.028
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun  9 11:44:03.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7138" for this suite. 06/09/23 11:44:03.063
------------------------------
• [4.199 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:43:58.876
    Jun  9 11:43:58.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename dns 06/09/23 11:43:58.877
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:43:58.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:43:58.903
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 06/09/23 11:43:58.909
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7138.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7138.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     06/09/23 11:43:58.919
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7138.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7138.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     06/09/23 11:43:58.919
    STEP: creating a pod to probe DNS 06/09/23 11:43:58.919
    STEP: submitting the pod to kubernetes 06/09/23 11:43:58.919
    Jun  9 11:43:58.935: INFO: Waiting up to 15m0s for pod "dns-test-af5722a9-6803-4960-8995-4c862c1e190e" in namespace "dns-7138" to be "running"
    Jun  9 11:43:58.940: INFO: Pod "dns-test-af5722a9-6803-4960-8995-4c862c1e190e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.69693ms
    Jun  9 11:44:00.950: INFO: Pod "dns-test-af5722a9-6803-4960-8995-4c862c1e190e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015011132s
    Jun  9 11:44:02.947: INFO: Pod "dns-test-af5722a9-6803-4960-8995-4c862c1e190e": Phase="Running", Reason="", readiness=true. Elapsed: 4.012697674s
    Jun  9 11:44:02.947: INFO: Pod "dns-test-af5722a9-6803-4960-8995-4c862c1e190e" satisfied condition "running"
    STEP: retrieving the pod 06/09/23 11:44:02.947
    STEP: looking for the results for each expected name from probers 06/09/23 11:44:02.978
    Jun  9 11:44:03.011: INFO: DNS probes using dns-7138/dns-test-af5722a9-6803-4960-8995-4c862c1e190e succeeded

    STEP: deleting the pod 06/09/23 11:44:03.011
    STEP: deleting the test headless service 06/09/23 11:44:03.028
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:44:03.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7138" for this suite. 06/09/23 11:44:03.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:44:03.077
Jun  9 11:44:03.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename emptydir 06/09/23 11:44:03.078
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:44:03.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:44:03.111
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 06/09/23 11:44:03.118
Jun  9 11:44:03.146: INFO: Waiting up to 5m0s for pod "pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b" in namespace "emptydir-5091" to be "Succeeded or Failed"
Jun  9 11:44:03.152: INFO: Pod "pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.398129ms
Jun  9 11:44:05.159: INFO: Pod "pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01322682s
Jun  9 11:44:07.158: INFO: Pod "pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011412092s
Jun  9 11:44:09.159: INFO: Pod "pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013021454s
STEP: Saw pod success 06/09/23 11:44:09.159
Jun  9 11:44:09.159: INFO: Pod "pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b" satisfied condition "Succeeded or Failed"
Jun  9 11:44:09.164: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b container test-container: <nil>
STEP: delete the pod 06/09/23 11:44:09.184
Jun  9 11:44:09.211: INFO: Waiting for pod pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b to disappear
Jun  9 11:44:09.217: INFO: Pod pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun  9 11:44:09.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5091" for this suite. 06/09/23 11:44:09.228
------------------------------
• [SLOW TEST] [6.167 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:44:03.077
    Jun  9 11:44:03.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename emptydir 06/09/23 11:44:03.078
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:44:03.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:44:03.111
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 06/09/23 11:44:03.118
    Jun  9 11:44:03.146: INFO: Waiting up to 5m0s for pod "pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b" in namespace "emptydir-5091" to be "Succeeded or Failed"
    Jun  9 11:44:03.152: INFO: Pod "pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.398129ms
    Jun  9 11:44:05.159: INFO: Pod "pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01322682s
    Jun  9 11:44:07.158: INFO: Pod "pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011412092s
    Jun  9 11:44:09.159: INFO: Pod "pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013021454s
    STEP: Saw pod success 06/09/23 11:44:09.159
    Jun  9 11:44:09.159: INFO: Pod "pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b" satisfied condition "Succeeded or Failed"
    Jun  9 11:44:09.164: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-4hkw9 pod pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b container test-container: <nil>
    STEP: delete the pod 06/09/23 11:44:09.184
    Jun  9 11:44:09.211: INFO: Waiting for pod pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b to disappear
    Jun  9 11:44:09.217: INFO: Pod pod-da848975-59d4-4ca5-a8fa-7bfad0d9f13b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:44:09.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5091" for this suite. 06/09/23 11:44:09.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:44:09.244
Jun  9 11:44:09.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename sched-pred 06/09/23 11:44:09.246
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:44:09.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:44:09.274
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jun  9 11:44:09.279: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  9 11:44:09.298: INFO: Waiting for terminating namespaces to be deleted...
Jun  9 11:44:09.304: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-4hkw9 before test
Jun  9 11:44:09.321: INFO: calico-kube-controllers-75b856575b-4vmmr from calico-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.321: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun  9 11:44:09.321: INFO: calico-node-7bzns from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.321: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 11:44:09.321: INFO: csi-node-driver-qw6d7 from calico-system started at 2023-06-09 06:03:15 +0000 UTC (2 container statuses recorded)
Jun  9 11:44:09.321: INFO: 	Container calico-csi ready: true, restart count 0
Jun  9 11:44:09.321: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun  9 11:44:09.321: INFO: coredns-5c4bbd897-mndk4 from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.321: INFO: 	Container coredns ready: true, restart count 0
Jun  9 11:44:09.321: INFO: coredns-5c4bbd897-vswmt from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.321: INFO: 	Container coredns ready: true, restart count 0
Jun  9 11:44:09.321: INFO: kube-proxy-f8glt from kube-system started at 2023-06-09 06:01:45 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.321: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 11:44:09.321: INFO: snapshot-controller-df74f7b6c-pdcpl from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.321: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun  9 11:44:09.321: INFO: smtx-elf-csi-driver-node-plugin-dbgrm from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
Jun  9 11:44:09.321: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 11:44:09.321: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  9 11:44:09.321: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 11:44:09.321: INFO: tigera-operator-5948566997-9t6st from sks-system started at 2023-06-09 06:02:52 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.321: INFO: 	Container tigera-operator ready: true, restart count 0
Jun  9 11:44:09.321: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-4r82f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 11:44:09.321: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 11:44:09.321: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 11:44:09.321: INFO: pod-service-account-mountsa-mountspec from svcaccounts-8388 started at 2023-06-09 11:43:36 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.321: INFO: 	Container token-test ready: false, restart count 0
Jun  9 11:44:09.321: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-q5bjm before test
Jun  9 11:44:09.336: INFO: calico-node-vpfwq from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.336: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 11:44:09.336: INFO: calico-typha-866dbffd5c-r6nf8 from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.336: INFO: 	Container calico-typha ready: true, restart count 0
Jun  9 11:44:09.336: INFO: csi-node-driver-k2xdl from calico-system started at 2023-06-09 06:04:17 +0000 UTC (2 container statuses recorded)
Jun  9 11:44:09.336: INFO: 	Container calico-csi ready: true, restart count 0
Jun  9 11:44:09.336: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun  9 11:44:09.336: INFO: kube-proxy-b8r9m from kube-system started at 2023-06-09 06:02:23 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.336: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 11:44:09.336: INFO: smtx-elf-csi-driver-controller-plugin-7bb788b57-vhsgd from sks-system started at 2023-06-09 06:04:22 +0000 UTC (6 container statuses recorded)
Jun  9 11:44:09.336: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  9 11:44:09.336: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 11:44:09.336: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun  9 11:44:09.336: INFO: 	Container csi-resizer ready: true, restart count 0
Jun  9 11:44:09.336: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jun  9 11:44:09.336: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 11:44:09.336: INFO: smtx-elf-csi-driver-node-plugin-cs9pp from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
Jun  9 11:44:09.336: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 11:44:09.336: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  9 11:44:09.336: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 11:44:09.336: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-crx45 from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 11:44:09.336: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 11:44:09.336: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 11:44:09.336: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-8388 started at 2023-06-09 11:43:36 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.336: INFO: 	Container token-test ready: false, restart count 0
Jun  9 11:44:09.336: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-qdprq before test
Jun  9 11:44:09.357: INFO: calico-node-5cjkc from calico-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.357: INFO: 	Container calico-node ready: true, restart count 0
Jun  9 11:44:09.357: INFO: calico-typha-866dbffd5c-n5p5b from calico-system started at 2023-06-09 06:03:07 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.357: INFO: 	Container calico-typha ready: true, restart count 0
Jun  9 11:44:09.357: INFO: csi-node-driver-f9pz9 from calico-system started at 2023-06-09 11:24:43 +0000 UTC (2 container statuses recorded)
Jun  9 11:44:09.357: INFO: 	Container calico-csi ready: true, restart count 0
Jun  9 11:44:09.357: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun  9 11:44:09.357: INFO: kube-proxy-gpttb from kube-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.357: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  9 11:44:09.357: INFO: smtx-elf-csi-driver-node-plugin-9plgj from sks-system started at 2023-06-09 06:04:22 +0000 UTC (3 container statuses recorded)
Jun  9 11:44:09.357: INFO: 	Container csi-driver ready: true, restart count 0
Jun  9 11:44:09.357: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  9 11:44:09.357: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  9 11:44:09.357: INFO: sonobuoy from sonobuoy started at 2023-06-09 10:18:23 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.357: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  9 11:44:09.357: INFO: sonobuoy-e2e-job-09e3c2818710427f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 11:44:09.357: INFO: 	Container e2e ready: true, restart count 0
Jun  9 11:44:09.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 11:44:09.357: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-f89bf from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
Jun  9 11:44:09.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  9 11:44:09.357: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  9 11:44:09.357: INFO: pod-service-account-defaultsa from svcaccounts-8388 started at 2023-06-09 11:43:36 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.357: INFO: 	Container token-test ready: false, restart count 0
Jun  9 11:44:09.357: INFO: pod-service-account-mountsa from svcaccounts-8388 started at 2023-06-09 11:43:36 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.357: INFO: 	Container token-test ready: false, restart count 0
Jun  9 11:44:09.357: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-8388 started at 2023-06-09 11:43:36 +0000 UTC (1 container statuses recorded)
Jun  9 11:44:09.357: INFO: 	Container token-test ready: false, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/09/23 11:44:09.358
Jun  9 11:44:09.373: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6253" to be "running"
Jun  9 11:44:09.380: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.959472ms
Jun  9 11:44:11.386: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.012569509s
Jun  9 11:44:11.386: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/09/23 11:44:11.39
STEP: Trying to apply a random label on the found node. 06/09/23 11:44:11.408
STEP: verifying the node has the label kubernetes.io/e2e-ea97f8e9-f8c5-47bd-8b9f-8563bd3b7074 95 06/09/23 11:44:11.439
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 06/09/23 11:44:11.444
Jun  9 11:44:11.454: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6253" to be "not pending"
Jun  9 11:44:11.461: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.104142ms
Jun  9 11:44:13.467: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.012081649s
Jun  9 11:44:13.467: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.255.64.104 on the node which pod4 resides and expect not scheduled 06/09/23 11:44:13.467
Jun  9 11:44:13.476: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6253" to be "not pending"
Jun  9 11:44:13.483: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.48472ms
Jun  9 11:44:15.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012094515s
Jun  9 11:44:17.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013972639s
Jun  9 11:44:19.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013741076s
Jun  9 11:44:21.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01322988s
Jun  9 11:44:23.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013781132s
Jun  9 11:44:25.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.016322625s
Jun  9 11:44:27.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.014276312s
Jun  9 11:44:29.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015768625s
Jun  9 11:44:31.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016020103s
Jun  9 11:44:33.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.015731238s
Jun  9 11:44:35.500: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.024179193s
Jun  9 11:44:37.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.014029942s
Jun  9 11:44:39.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.015670398s
Jun  9 11:44:41.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.020951335s
Jun  9 11:44:43.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.013192053s
Jun  9 11:44:45.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.014421676s
Jun  9 11:44:47.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.013245591s
Jun  9 11:44:49.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012924293s
Jun  9 11:44:51.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012695598s
Jun  9 11:44:53.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.013868529s
Jun  9 11:44:55.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.013665055s
Jun  9 11:44:57.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.015548357s
Jun  9 11:44:59.503: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.027105808s
Jun  9 11:45:01.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.013882793s
Jun  9 11:45:03.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013459311s
Jun  9 11:45:05.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01356524s
Jun  9 11:45:07.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.015359429s
Jun  9 11:45:09.504: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.028022802s
Jun  9 11:45:11.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.014452942s
Jun  9 11:45:13.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015302308s
Jun  9 11:45:15.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.0149612s
Jun  9 11:45:17.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.014831884s
Jun  9 11:45:19.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.014744616s
Jun  9 11:45:21.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.017439847s
Jun  9 11:45:23.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.013818028s
Jun  9 11:45:25.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.014992354s
Jun  9 11:45:27.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.013381513s
Jun  9 11:45:29.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.01422486s
Jun  9 11:45:31.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.015679348s
Jun  9 11:45:33.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.015579635s
Jun  9 11:45:35.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015451153s
Jun  9 11:45:37.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.016275106s
Jun  9 11:45:39.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.013321444s
Jun  9 11:45:41.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.01357451s
Jun  9 11:45:43.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.013467746s
Jun  9 11:45:45.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.020120684s
Jun  9 11:45:47.502: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.026174115s
Jun  9 11:45:49.501: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.025291027s
Jun  9 11:45:51.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.020192191s
Jun  9 11:45:53.500: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.023907324s
Jun  9 11:45:55.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.031485703s
Jun  9 11:45:57.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.016083849s
Jun  9 11:45:59.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.01985965s
Jun  9 11:46:01.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.015231702s
Jun  9 11:46:03.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.012849214s
Jun  9 11:46:05.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.015105874s
Jun  9 11:46:07.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01667717s
Jun  9 11:46:09.499: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.022684988s
Jun  9 11:46:11.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013291754s
Jun  9 11:46:13.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012706743s
Jun  9 11:46:15.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.014007302s
Jun  9 11:46:17.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.016334754s
Jun  9 11:46:19.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.013729926s
Jun  9 11:46:21.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.017777671s
Jun  9 11:46:23.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.013084548s
Jun  9 11:46:25.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.014849833s
Jun  9 11:46:27.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.018401767s
Jun  9 11:46:29.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.017572902s
Jun  9 11:46:31.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.014175758s
Jun  9 11:46:33.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.017850934s
Jun  9 11:46:35.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.01491668s
Jun  9 11:46:37.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.014643851s
Jun  9 11:46:39.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.01280545s
Jun  9 11:46:41.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.014040947s
Jun  9 11:46:43.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.014340387s
Jun  9 11:46:45.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.012593797s
Jun  9 11:46:47.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.015544633s
Jun  9 11:46:49.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.015686582s
Jun  9 11:46:51.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.015209923s
Jun  9 11:46:53.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.016290196s
Jun  9 11:46:55.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.014676217s
Jun  9 11:46:57.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.016872429s
Jun  9 11:46:59.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.013652913s
Jun  9 11:47:01.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.016128337s
Jun  9 11:47:03.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.020861839s
Jun  9 11:47:05.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.014097464s
Jun  9 11:47:07.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.013594928s
Jun  9 11:47:09.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.014714178s
Jun  9 11:47:11.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.018524804s
Jun  9 11:47:13.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.013921462s
Jun  9 11:47:15.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.01342592s
Jun  9 11:47:17.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.012521126s
Jun  9 11:47:19.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.016528888s
Jun  9 11:47:21.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.012568757s
Jun  9 11:47:23.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.054934202s
Jun  9 11:47:25.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.01546719s
Jun  9 11:47:27.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.012187289s
Jun  9 11:47:29.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.012577241s
Jun  9 11:47:31.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.012267531s
Jun  9 11:47:33.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.014883466s
Jun  9 11:47:35.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.014211501s
Jun  9 11:47:37.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.01387439s
Jun  9 11:47:39.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.012778336s
Jun  9 11:47:41.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.019860768s
Jun  9 11:47:43.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.011819024s
Jun  9 11:47:45.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.012601566s
Jun  9 11:47:47.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.016392229s
Jun  9 11:47:49.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.012652615s
Jun  9 11:47:51.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.01227647s
Jun  9 11:47:53.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.01638801s
Jun  9 11:47:55.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.014105171s
Jun  9 11:47:57.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.012313497s
Jun  9 11:47:59.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.016011944s
Jun  9 11:48:01.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.017587423s
Jun  9 11:48:03.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.014213132s
Jun  9 11:48:05.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.014085982s
Jun  9 11:48:07.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.0144071s
Jun  9 11:48:09.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.016195621s
Jun  9 11:48:11.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.012660642s
Jun  9 11:48:13.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.018192082s
Jun  9 11:48:15.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.016436163s
Jun  9 11:48:17.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.019205024s
Jun  9 11:48:19.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.013667321s
Jun  9 11:48:21.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.015558754s
Jun  9 11:48:23.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.017084201s
Jun  9 11:48:25.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.014088977s
Jun  9 11:48:27.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.01319271s
Jun  9 11:48:29.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.013797433s
Jun  9 11:48:31.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.012589372s
Jun  9 11:48:33.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.013749254s
Jun  9 11:48:35.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.013077386s
Jun  9 11:48:37.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.016395803s
Jun  9 11:48:39.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.014581664s
Jun  9 11:48:41.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.013207962s
Jun  9 11:48:43.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.014535548s
Jun  9 11:48:45.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.014707556s
Jun  9 11:48:47.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.014126536s
Jun  9 11:48:49.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.015770617s
Jun  9 11:48:51.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.015867595s
Jun  9 11:48:53.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.021059274s
Jun  9 11:48:55.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.013494965s
Jun  9 11:48:57.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.015013433s
Jun  9 11:48:59.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.013699136s
Jun  9 11:49:01.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.020264732s
Jun  9 11:49:03.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.014700517s
Jun  9 11:49:05.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.015576038s
Jun  9 11:49:07.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.013192615s
Jun  9 11:49:09.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.015292062s
Jun  9 11:49:11.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012095196s
Jun  9 11:49:13.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.015161671s
Jun  9 11:49:13.499: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.022459784s
STEP: removing the label kubernetes.io/e2e-ea97f8e9-f8c5-47bd-8b9f-8563bd3b7074 off the node sks-test-v1-26.4-workergroup-4hkw9 06/09/23 11:49:13.499
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ea97f8e9-f8c5-47bd-8b9f-8563bd3b7074 06/09/23 11:49:13.522
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:49:13.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6253" for this suite. 06/09/23 11:49:13.559
------------------------------
• [SLOW TEST] [304.326 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:44:09.244
    Jun  9 11:44:09.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename sched-pred 06/09/23 11:44:09.246
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:44:09.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:44:09.274
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jun  9 11:44:09.279: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun  9 11:44:09.298: INFO: Waiting for terminating namespaces to be deleted...
    Jun  9 11:44:09.304: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-4hkw9 before test
    Jun  9 11:44:09.321: INFO: calico-kube-controllers-75b856575b-4vmmr from calico-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.321: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jun  9 11:44:09.321: INFO: calico-node-7bzns from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.321: INFO: 	Container calico-node ready: true, restart count 0
    Jun  9 11:44:09.321: INFO: csi-node-driver-qw6d7 from calico-system started at 2023-06-09 06:03:15 +0000 UTC (2 container statuses recorded)
    Jun  9 11:44:09.321: INFO: 	Container calico-csi ready: true, restart count 0
    Jun  9 11:44:09.321: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun  9 11:44:09.321: INFO: coredns-5c4bbd897-mndk4 from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.321: INFO: 	Container coredns ready: true, restart count 0
    Jun  9 11:44:09.321: INFO: coredns-5c4bbd897-vswmt from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.321: INFO: 	Container coredns ready: true, restart count 0
    Jun  9 11:44:09.321: INFO: kube-proxy-f8glt from kube-system started at 2023-06-09 06:01:45 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.321: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  9 11:44:09.321: INFO: snapshot-controller-df74f7b6c-pdcpl from kube-system started at 2023-06-09 06:03:15 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.321: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun  9 11:44:09.321: INFO: smtx-elf-csi-driver-node-plugin-dbgrm from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
    Jun  9 11:44:09.321: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 11:44:09.321: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun  9 11:44:09.321: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 11:44:09.321: INFO: tigera-operator-5948566997-9t6st from sks-system started at 2023-06-09 06:02:52 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.321: INFO: 	Container tigera-operator ready: true, restart count 0
    Jun  9 11:44:09.321: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-4r82f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 11:44:09.321: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 11:44:09.321: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  9 11:44:09.321: INFO: pod-service-account-mountsa-mountspec from svcaccounts-8388 started at 2023-06-09 11:43:36 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.321: INFO: 	Container token-test ready: false, restart count 0
    Jun  9 11:44:09.321: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-q5bjm before test
    Jun  9 11:44:09.336: INFO: calico-node-vpfwq from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.336: INFO: 	Container calico-node ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: calico-typha-866dbffd5c-r6nf8 from calico-system started at 2023-06-09 06:02:58 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.336: INFO: 	Container calico-typha ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: csi-node-driver-k2xdl from calico-system started at 2023-06-09 06:04:17 +0000 UTC (2 container statuses recorded)
    Jun  9 11:44:09.336: INFO: 	Container calico-csi ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: kube-proxy-b8r9m from kube-system started at 2023-06-09 06:02:23 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.336: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: smtx-elf-csi-driver-controller-plugin-7bb788b57-vhsgd from sks-system started at 2023-06-09 06:04:22 +0000 UTC (6 container statuses recorded)
    Jun  9 11:44:09.336: INFO: 	Container csi-attacher ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: 	Container csi-resizer ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: smtx-elf-csi-driver-node-plugin-cs9pp from sks-system started at 2023-06-09 06:04:23 +0000 UTC (3 container statuses recorded)
    Jun  9 11:44:09.336: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-crx45 from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 11:44:09.336: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  9 11:44:09.336: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-8388 started at 2023-06-09 11:43:36 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.336: INFO: 	Container token-test ready: false, restart count 0
    Jun  9 11:44:09.336: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-26.4-workergroup-qdprq before test
    Jun  9 11:44:09.357: INFO: calico-node-5cjkc from calico-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.357: INFO: 	Container calico-node ready: true, restart count 0
    Jun  9 11:44:09.357: INFO: calico-typha-866dbffd5c-n5p5b from calico-system started at 2023-06-09 06:03:07 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.357: INFO: 	Container calico-typha ready: true, restart count 0
    Jun  9 11:44:09.357: INFO: csi-node-driver-f9pz9 from calico-system started at 2023-06-09 11:24:43 +0000 UTC (2 container statuses recorded)
    Jun  9 11:44:09.357: INFO: 	Container calico-csi ready: true, restart count 0
    Jun  9 11:44:09.357: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun  9 11:44:09.357: INFO: kube-proxy-gpttb from kube-system started at 2023-06-09 06:03:05 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.357: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun  9 11:44:09.357: INFO: smtx-elf-csi-driver-node-plugin-9plgj from sks-system started at 2023-06-09 06:04:22 +0000 UTC (3 container statuses recorded)
    Jun  9 11:44:09.357: INFO: 	Container csi-driver ready: true, restart count 0
    Jun  9 11:44:09.357: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun  9 11:44:09.357: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun  9 11:44:09.357: INFO: sonobuoy from sonobuoy started at 2023-06-09 10:18:23 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.357: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun  9 11:44:09.357: INFO: sonobuoy-e2e-job-09e3c2818710427f from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 11:44:09.357: INFO: 	Container e2e ready: true, restart count 0
    Jun  9 11:44:09.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 11:44:09.357: INFO: sonobuoy-systemd-logs-daemon-set-d0b6c9e0aae544b6-f89bf from sonobuoy started at 2023-06-09 10:18:24 +0000 UTC (2 container statuses recorded)
    Jun  9 11:44:09.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun  9 11:44:09.357: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun  9 11:44:09.357: INFO: pod-service-account-defaultsa from svcaccounts-8388 started at 2023-06-09 11:43:36 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.357: INFO: 	Container token-test ready: false, restart count 0
    Jun  9 11:44:09.357: INFO: pod-service-account-mountsa from svcaccounts-8388 started at 2023-06-09 11:43:36 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.357: INFO: 	Container token-test ready: false, restart count 0
    Jun  9 11:44:09.357: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-8388 started at 2023-06-09 11:43:36 +0000 UTC (1 container statuses recorded)
    Jun  9 11:44:09.357: INFO: 	Container token-test ready: false, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/09/23 11:44:09.358
    Jun  9 11:44:09.373: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6253" to be "running"
    Jun  9 11:44:09.380: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.959472ms
    Jun  9 11:44:11.386: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.012569509s
    Jun  9 11:44:11.386: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/09/23 11:44:11.39
    STEP: Trying to apply a random label on the found node. 06/09/23 11:44:11.408
    STEP: verifying the node has the label kubernetes.io/e2e-ea97f8e9-f8c5-47bd-8b9f-8563bd3b7074 95 06/09/23 11:44:11.439
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 06/09/23 11:44:11.444
    Jun  9 11:44:11.454: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6253" to be "not pending"
    Jun  9 11:44:11.461: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.104142ms
    Jun  9 11:44:13.467: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.012081649s
    Jun  9 11:44:13.467: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.255.64.104 on the node which pod4 resides and expect not scheduled 06/09/23 11:44:13.467
    Jun  9 11:44:13.476: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6253" to be "not pending"
    Jun  9 11:44:13.483: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.48472ms
    Jun  9 11:44:15.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012094515s
    Jun  9 11:44:17.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013972639s
    Jun  9 11:44:19.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013741076s
    Jun  9 11:44:21.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01322988s
    Jun  9 11:44:23.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013781132s
    Jun  9 11:44:25.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.016322625s
    Jun  9 11:44:27.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.014276312s
    Jun  9 11:44:29.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015768625s
    Jun  9 11:44:31.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016020103s
    Jun  9 11:44:33.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.015731238s
    Jun  9 11:44:35.500: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.024179193s
    Jun  9 11:44:37.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.014029942s
    Jun  9 11:44:39.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.015670398s
    Jun  9 11:44:41.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.020951335s
    Jun  9 11:44:43.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.013192053s
    Jun  9 11:44:45.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.014421676s
    Jun  9 11:44:47.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.013245591s
    Jun  9 11:44:49.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012924293s
    Jun  9 11:44:51.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012695598s
    Jun  9 11:44:53.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.013868529s
    Jun  9 11:44:55.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.013665055s
    Jun  9 11:44:57.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.015548357s
    Jun  9 11:44:59.503: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.027105808s
    Jun  9 11:45:01.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.013882793s
    Jun  9 11:45:03.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013459311s
    Jun  9 11:45:05.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01356524s
    Jun  9 11:45:07.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.015359429s
    Jun  9 11:45:09.504: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.028022802s
    Jun  9 11:45:11.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.014452942s
    Jun  9 11:45:13.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015302308s
    Jun  9 11:45:15.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.0149612s
    Jun  9 11:45:17.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.014831884s
    Jun  9 11:45:19.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.014744616s
    Jun  9 11:45:21.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.017439847s
    Jun  9 11:45:23.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.013818028s
    Jun  9 11:45:25.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.014992354s
    Jun  9 11:45:27.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.013381513s
    Jun  9 11:45:29.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.01422486s
    Jun  9 11:45:31.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.015679348s
    Jun  9 11:45:33.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.015579635s
    Jun  9 11:45:35.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015451153s
    Jun  9 11:45:37.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.016275106s
    Jun  9 11:45:39.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.013321444s
    Jun  9 11:45:41.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.01357451s
    Jun  9 11:45:43.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.013467746s
    Jun  9 11:45:45.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.020120684s
    Jun  9 11:45:47.502: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.026174115s
    Jun  9 11:45:49.501: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.025291027s
    Jun  9 11:45:51.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.020192191s
    Jun  9 11:45:53.500: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.023907324s
    Jun  9 11:45:55.508: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.031485703s
    Jun  9 11:45:57.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.016083849s
    Jun  9 11:45:59.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.01985965s
    Jun  9 11:46:01.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.015231702s
    Jun  9 11:46:03.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.012849214s
    Jun  9 11:46:05.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.015105874s
    Jun  9 11:46:07.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01667717s
    Jun  9 11:46:09.499: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.022684988s
    Jun  9 11:46:11.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013291754s
    Jun  9 11:46:13.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012706743s
    Jun  9 11:46:15.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.014007302s
    Jun  9 11:46:17.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.016334754s
    Jun  9 11:46:19.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.013729926s
    Jun  9 11:46:21.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.017777671s
    Jun  9 11:46:23.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.013084548s
    Jun  9 11:46:25.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.014849833s
    Jun  9 11:46:27.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.018401767s
    Jun  9 11:46:29.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.017572902s
    Jun  9 11:46:31.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.014175758s
    Jun  9 11:46:33.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.017850934s
    Jun  9 11:46:35.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.01491668s
    Jun  9 11:46:37.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.014643851s
    Jun  9 11:46:39.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.01280545s
    Jun  9 11:46:41.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.014040947s
    Jun  9 11:46:43.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.014340387s
    Jun  9 11:46:45.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.012593797s
    Jun  9 11:46:47.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.015544633s
    Jun  9 11:46:49.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.015686582s
    Jun  9 11:46:51.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.015209923s
    Jun  9 11:46:53.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.016290196s
    Jun  9 11:46:55.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.014676217s
    Jun  9 11:46:57.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.016872429s
    Jun  9 11:46:59.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.013652913s
    Jun  9 11:47:01.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.016128337s
    Jun  9 11:47:03.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.020861839s
    Jun  9 11:47:05.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.014097464s
    Jun  9 11:47:07.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.013594928s
    Jun  9 11:47:09.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.014714178s
    Jun  9 11:47:11.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.018524804s
    Jun  9 11:47:13.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.013921462s
    Jun  9 11:47:15.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.01342592s
    Jun  9 11:47:17.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.012521126s
    Jun  9 11:47:19.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.016528888s
    Jun  9 11:47:21.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.012568757s
    Jun  9 11:47:23.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.054934202s
    Jun  9 11:47:25.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.01546719s
    Jun  9 11:47:27.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.012187289s
    Jun  9 11:47:29.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.012577241s
    Jun  9 11:47:31.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.012267531s
    Jun  9 11:47:33.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.014883466s
    Jun  9 11:47:35.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.014211501s
    Jun  9 11:47:37.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.01387439s
    Jun  9 11:47:39.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.012778336s
    Jun  9 11:47:41.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.019860768s
    Jun  9 11:47:43.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.011819024s
    Jun  9 11:47:45.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.012601566s
    Jun  9 11:47:47.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.016392229s
    Jun  9 11:47:49.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.012652615s
    Jun  9 11:47:51.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.01227647s
    Jun  9 11:47:53.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.01638801s
    Jun  9 11:47:55.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.014105171s
    Jun  9 11:47:57.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.012313497s
    Jun  9 11:47:59.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.016011944s
    Jun  9 11:48:01.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.017587423s
    Jun  9 11:48:03.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.014213132s
    Jun  9 11:48:05.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.014085982s
    Jun  9 11:48:07.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.0144071s
    Jun  9 11:48:09.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.016195621s
    Jun  9 11:48:11.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.012660642s
    Jun  9 11:48:13.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.018192082s
    Jun  9 11:48:15.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.016436163s
    Jun  9 11:48:17.495: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.019205024s
    Jun  9 11:48:19.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.013667321s
    Jun  9 11:48:21.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.015558754s
    Jun  9 11:48:23.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.017084201s
    Jun  9 11:48:25.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.014088977s
    Jun  9 11:48:27.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.01319271s
    Jun  9 11:48:29.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.013797433s
    Jun  9 11:48:31.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.012589372s
    Jun  9 11:48:33.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.013749254s
    Jun  9 11:48:35.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.013077386s
    Jun  9 11:48:37.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.016395803s
    Jun  9 11:48:39.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.014581664s
    Jun  9 11:48:41.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.013207962s
    Jun  9 11:48:43.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.014535548s
    Jun  9 11:48:45.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.014707556s
    Jun  9 11:48:47.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.014126536s
    Jun  9 11:48:49.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.015770617s
    Jun  9 11:48:51.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.015867595s
    Jun  9 11:48:53.497: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.021059274s
    Jun  9 11:48:55.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.013494965s
    Jun  9 11:48:57.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.015013433s
    Jun  9 11:48:59.490: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.013699136s
    Jun  9 11:49:01.496: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.020264732s
    Jun  9 11:49:03.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.014700517s
    Jun  9 11:49:05.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.015576038s
    Jun  9 11:49:07.489: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.013192615s
    Jun  9 11:49:09.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.015292062s
    Jun  9 11:49:11.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012095196s
    Jun  9 11:49:13.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.015161671s
    Jun  9 11:49:13.499: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.022459784s
    STEP: removing the label kubernetes.io/e2e-ea97f8e9-f8c5-47bd-8b9f-8563bd3b7074 off the node sks-test-v1-26.4-workergroup-4hkw9 06/09/23 11:49:13.499
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-ea97f8e9-f8c5-47bd-8b9f-8563bd3b7074 06/09/23 11:49:13.522
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:49:13.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6253" for this suite. 06/09/23 11:49:13.559
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:49:13.571
Jun  9 11:49:13.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename webhook 06/09/23 11:49:13.573
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:49:13.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:49:13.614
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/09/23 11:49:13.639
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:49:13.93
STEP: Deploying the webhook pod 06/09/23 11:49:13.948
STEP: Wait for the deployment to be ready 06/09/23 11:49:13.974
Jun  9 11:49:13.986: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/09/23 11:49:16.007
STEP: Verifying the service has paired with the endpoint 06/09/23 11:49:16.029
Jun  9 11:49:17.036: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 06/09/23 11:49:17.418
STEP: Creating a configMap that does not comply to the validation webhook rules 06/09/23 11:49:17.488
STEP: Deleting the collection of validation webhooks 06/09/23 11:49:17.524
STEP: Creating a configMap that does not comply to the validation webhook rules 06/09/23 11:49:17.709
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:49:17.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6101" for this suite. 06/09/23 11:49:17.95
STEP: Destroying namespace "webhook-6101-markers" for this suite. 06/09/23 11:49:17.994
------------------------------
• [4.443 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:49:13.571
    Jun  9 11:49:13.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename webhook 06/09/23 11:49:13.573
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:49:13.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:49:13.614
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/09/23 11:49:13.639
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/09/23 11:49:13.93
    STEP: Deploying the webhook pod 06/09/23 11:49:13.948
    STEP: Wait for the deployment to be ready 06/09/23 11:49:13.974
    Jun  9 11:49:13.986: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/09/23 11:49:16.007
    STEP: Verifying the service has paired with the endpoint 06/09/23 11:49:16.029
    Jun  9 11:49:17.036: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 06/09/23 11:49:17.418
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/09/23 11:49:17.488
    STEP: Deleting the collection of validation webhooks 06/09/23 11:49:17.524
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/09/23 11:49:17.709
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:49:17.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6101" for this suite. 06/09/23 11:49:17.95
    STEP: Destroying namespace "webhook-6101-markers" for this suite. 06/09/23 11:49:17.994
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:49:18.015
Jun  9 11:49:18.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 11:49:18.017
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:49:18.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:49:18.05
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jun  9 11:49:18.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-9402 version'
Jun  9 11:49:18.140: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jun  9 11:49:18.140: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:13:53Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:05:35Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 11:49:18.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9402" for this suite. 06/09/23 11:49:18.151
------------------------------
• [0.147 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:49:18.015
    Jun  9 11:49:18.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 11:49:18.017
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:49:18.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:49:18.05
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jun  9 11:49:18.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-9402 version'
    Jun  9 11:49:18.140: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jun  9 11:49:18.140: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:13:53Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:05:35Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:49:18.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9402" for this suite. 06/09/23 11:49:18.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:49:18.165
Jun  9 11:49:18.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename podtemplate 06/09/23 11:49:18.166
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:49:18.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:49:18.268
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 06/09/23 11:49:18.274
STEP: Replace a pod template 06/09/23 11:49:18.29
Jun  9 11:49:18.309: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jun  9 11:49:18.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-7584" for this suite. 06/09/23 11:49:18.321
------------------------------
• [0.170 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:49:18.165
    Jun  9 11:49:18.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename podtemplate 06/09/23 11:49:18.166
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:49:18.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:49:18.268
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 06/09/23 11:49:18.274
    STEP: Replace a pod template 06/09/23 11:49:18.29
    Jun  9 11:49:18.309: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:49:18.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-7584" for this suite. 06/09/23 11:49:18.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:49:18.335
Jun  9 11:49:18.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename limitrange 06/09/23 11:49:18.337
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:49:18.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:49:18.379
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-8cq5t" in namespace "limitrange-5459" 06/09/23 11:49:18.385
STEP: Creating another limitRange in another namespace 06/09/23 11:49:18.395
Jun  9 11:49:18.419: INFO: Namespace "e2e-limitrange-8cq5t-3502" created
Jun  9 11:49:18.419: INFO: Creating LimitRange "e2e-limitrange-8cq5t" in namespace "e2e-limitrange-8cq5t-3502"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-8cq5t" 06/09/23 11:49:18.428
Jun  9 11:49:18.434: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-8cq5t" in "limitrange-5459" namespace 06/09/23 11:49:18.434
Jun  9 11:49:18.445: INFO: LimitRange "e2e-limitrange-8cq5t" has been patched
STEP: Delete LimitRange "e2e-limitrange-8cq5t" by Collection with labelSelector: "e2e-limitrange-8cq5t=patched" 06/09/23 11:49:18.445
STEP: Confirm that the limitRange "e2e-limitrange-8cq5t" has been deleted 06/09/23 11:49:18.458
Jun  9 11:49:18.458: INFO: Requesting list of LimitRange to confirm quantity
Jun  9 11:49:18.463: INFO: Found 0 LimitRange with label "e2e-limitrange-8cq5t=patched"
Jun  9 11:49:18.463: INFO: LimitRange "e2e-limitrange-8cq5t" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-8cq5t" 06/09/23 11:49:18.463
Jun  9 11:49:18.471: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jun  9 11:49:18.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5459" for this suite. 06/09/23 11:49:18.478
STEP: Destroying namespace "e2e-limitrange-8cq5t-3502" for this suite. 06/09/23 11:49:18.49
------------------------------
• [0.181 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:49:18.335
    Jun  9 11:49:18.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename limitrange 06/09/23 11:49:18.337
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:49:18.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:49:18.379
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-8cq5t" in namespace "limitrange-5459" 06/09/23 11:49:18.385
    STEP: Creating another limitRange in another namespace 06/09/23 11:49:18.395
    Jun  9 11:49:18.419: INFO: Namespace "e2e-limitrange-8cq5t-3502" created
    Jun  9 11:49:18.419: INFO: Creating LimitRange "e2e-limitrange-8cq5t" in namespace "e2e-limitrange-8cq5t-3502"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-8cq5t" 06/09/23 11:49:18.428
    Jun  9 11:49:18.434: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-8cq5t" in "limitrange-5459" namespace 06/09/23 11:49:18.434
    Jun  9 11:49:18.445: INFO: LimitRange "e2e-limitrange-8cq5t" has been patched
    STEP: Delete LimitRange "e2e-limitrange-8cq5t" by Collection with labelSelector: "e2e-limitrange-8cq5t=patched" 06/09/23 11:49:18.445
    STEP: Confirm that the limitRange "e2e-limitrange-8cq5t" has been deleted 06/09/23 11:49:18.458
    Jun  9 11:49:18.458: INFO: Requesting list of LimitRange to confirm quantity
    Jun  9 11:49:18.463: INFO: Found 0 LimitRange with label "e2e-limitrange-8cq5t=patched"
    Jun  9 11:49:18.463: INFO: LimitRange "e2e-limitrange-8cq5t" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-8cq5t" 06/09/23 11:49:18.463
    Jun  9 11:49:18.471: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:49:18.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5459" for this suite. 06/09/23 11:49:18.478
    STEP: Destroying namespace "e2e-limitrange-8cq5t-3502" for this suite. 06/09/23 11:49:18.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:49:18.518
Jun  9 11:49:18.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 11:49:18.519
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:49:18.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:49:18.556
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jun  9 11:49:18.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 06/09/23 11:49:21.434
Jun  9 11:49:21.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 create -f -'
Jun  9 11:49:22.620: INFO: stderr: ""
Jun  9 11:49:22.620: INFO: stdout: "e2e-test-crd-publish-openapi-261-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun  9 11:49:22.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 delete e2e-test-crd-publish-openapi-261-crds test-foo'
Jun  9 11:49:22.851: INFO: stderr: ""
Jun  9 11:49:22.851: INFO: stdout: "e2e-test-crd-publish-openapi-261-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun  9 11:49:22.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 apply -f -'
Jun  9 11:49:23.181: INFO: stderr: ""
Jun  9 11:49:23.181: INFO: stdout: "e2e-test-crd-publish-openapi-261-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun  9 11:49:23.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 delete e2e-test-crd-publish-openapi-261-crds test-foo'
Jun  9 11:49:23.317: INFO: stderr: ""
Jun  9 11:49:23.317: INFO: stdout: "e2e-test-crd-publish-openapi-261-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 06/09/23 11:49:23.317
Jun  9 11:49:23.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 create -f -'
Jun  9 11:49:24.377: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 06/09/23 11:49:24.377
Jun  9 11:49:24.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 create -f -'
Jun  9 11:49:25.548: INFO: rc: 1
Jun  9 11:49:25.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 apply -f -'
Jun  9 11:49:25.902: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 06/09/23 11:49:25.902
Jun  9 11:49:25.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 create -f -'
Jun  9 11:49:26.236: INFO: rc: 1
Jun  9 11:49:26.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 apply -f -'
Jun  9 11:49:26.670: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 06/09/23 11:49:26.67
Jun  9 11:49:26.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 explain e2e-test-crd-publish-openapi-261-crds'
Jun  9 11:49:27.088: INFO: stderr: ""
Jun  9 11:49:27.088: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-261-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 06/09/23 11:49:27.089
Jun  9 11:49:27.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 explain e2e-test-crd-publish-openapi-261-crds.metadata'
Jun  9 11:49:27.464: INFO: stderr: ""
Jun  9 11:49:27.464: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-261-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun  9 11:49:27.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 explain e2e-test-crd-publish-openapi-261-crds.spec'
Jun  9 11:49:27.816: INFO: stderr: ""
Jun  9 11:49:27.816: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-261-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun  9 11:49:27.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 explain e2e-test-crd-publish-openapi-261-crds.spec.bars'
Jun  9 11:49:28.132: INFO: stderr: ""
Jun  9 11:49:28.132: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-261-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 06/09/23 11:49:28.132
Jun  9 11:49:28.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 explain e2e-test-crd-publish-openapi-261-crds.spec.bars2'
Jun  9 11:49:28.441: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:49:31.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7379" for this suite. 06/09/23 11:49:31.261
------------------------------
• [SLOW TEST] [12.754 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:49:18.518
    Jun  9 11:49:18.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 11:49:18.519
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:49:18.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:49:18.556
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jun  9 11:49:18.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 06/09/23 11:49:21.434
    Jun  9 11:49:21.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 create -f -'
    Jun  9 11:49:22.620: INFO: stderr: ""
    Jun  9 11:49:22.620: INFO: stdout: "e2e-test-crd-publish-openapi-261-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jun  9 11:49:22.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 delete e2e-test-crd-publish-openapi-261-crds test-foo'
    Jun  9 11:49:22.851: INFO: stderr: ""
    Jun  9 11:49:22.851: INFO: stdout: "e2e-test-crd-publish-openapi-261-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jun  9 11:49:22.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 apply -f -'
    Jun  9 11:49:23.181: INFO: stderr: ""
    Jun  9 11:49:23.181: INFO: stdout: "e2e-test-crd-publish-openapi-261-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jun  9 11:49:23.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 delete e2e-test-crd-publish-openapi-261-crds test-foo'
    Jun  9 11:49:23.317: INFO: stderr: ""
    Jun  9 11:49:23.317: INFO: stdout: "e2e-test-crd-publish-openapi-261-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 06/09/23 11:49:23.317
    Jun  9 11:49:23.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 create -f -'
    Jun  9 11:49:24.377: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 06/09/23 11:49:24.377
    Jun  9 11:49:24.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 create -f -'
    Jun  9 11:49:25.548: INFO: rc: 1
    Jun  9 11:49:25.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 apply -f -'
    Jun  9 11:49:25.902: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 06/09/23 11:49:25.902
    Jun  9 11:49:25.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 create -f -'
    Jun  9 11:49:26.236: INFO: rc: 1
    Jun  9 11:49:26.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 --namespace=crd-publish-openapi-7379 apply -f -'
    Jun  9 11:49:26.670: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 06/09/23 11:49:26.67
    Jun  9 11:49:26.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 explain e2e-test-crd-publish-openapi-261-crds'
    Jun  9 11:49:27.088: INFO: stderr: ""
    Jun  9 11:49:27.088: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-261-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 06/09/23 11:49:27.089
    Jun  9 11:49:27.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 explain e2e-test-crd-publish-openapi-261-crds.metadata'
    Jun  9 11:49:27.464: INFO: stderr: ""
    Jun  9 11:49:27.464: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-261-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jun  9 11:49:27.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 explain e2e-test-crd-publish-openapi-261-crds.spec'
    Jun  9 11:49:27.816: INFO: stderr: ""
    Jun  9 11:49:27.816: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-261-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jun  9 11:49:27.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 explain e2e-test-crd-publish-openapi-261-crds.spec.bars'
    Jun  9 11:49:28.132: INFO: stderr: ""
    Jun  9 11:49:28.132: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-261-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 06/09/23 11:49:28.132
    Jun  9 11:49:28.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=crd-publish-openapi-7379 explain e2e-test-crd-publish-openapi-261-crds.spec.bars2'
    Jun  9 11:49:28.441: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:49:31.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7379" for this suite. 06/09/23 11:49:31.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:49:31.273
Jun  9 11:49:31.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename deployment 06/09/23 11:49:31.274
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:49:31.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:49:31.319
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jun  9 11:49:31.342: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun  9 11:49:36.350: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/09/23 11:49:36.35
Jun  9 11:49:36.351: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun  9 11:49:38.364: INFO: Creating deployment "test-rollover-deployment"
Jun  9 11:49:38.408: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun  9 11:49:40.427: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun  9 11:49:40.447: INFO: Ensure that both replica sets have 1 created replica
Jun  9 11:49:40.465: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun  9 11:49:40.494: INFO: Updating deployment test-rollover-deployment
Jun  9 11:49:40.494: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun  9 11:49:42.529: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun  9 11:49:42.547: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun  9 11:49:42.593: INFO: all replica sets need to contain the pod-template-hash label
Jun  9 11:49:42.593: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:49:44.607: INFO: all replica sets need to contain the pod-template-hash label
Jun  9 11:49:44.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:49:46.625: INFO: all replica sets need to contain the pod-template-hash label
Jun  9 11:49:46.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:49:48.640: INFO: all replica sets need to contain the pod-template-hash label
Jun  9 11:49:48.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:49:50.609: INFO: all replica sets need to contain the pod-template-hash label
Jun  9 11:49:50.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  9 11:49:52.619: INFO: 
Jun  9 11:49:52.619: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun  9 11:49:52.645: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-862  43a34f9c-96b1-4cba-a7f1-a539458fef2e 117311 2 2023-06-09 11:49:38 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-09 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:49:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005809e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-09 11:49:38 +0000 UTC,LastTransitionTime:2023-06-09 11:49:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-06-09 11:49:52 +0000 UTC,LastTransitionTime:2023-06-09 11:49:38 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  9 11:49:52.652: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-862  7a1aedd3-776b-4efc-b4d2-46f2a706c9b9 117301 2 2023-06-09 11:49:40 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 43a34f9c-96b1-4cba-a7f1-a539458fef2e 0xc003bac317 0xc003bac318}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43a34f9c-96b1-4cba-a7f1-a539458fef2e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:49:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bac3c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  9 11:49:52.652: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun  9 11:49:52.652: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-862  412ef49c-9384-48dc-9702-29f96c29139c 117310 2 2023-06-09 11:49:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 43a34f9c-96b1-4cba-a7f1-a539458fef2e 0xc003bac1e7 0xc003bac1e8}] [] [{e2e.test Update apps/v1 2023-06-09 11:49:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:49:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43a34f9c-96b1-4cba-a7f1-a539458fef2e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:49:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003bac2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  9 11:49:52.652: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-862  d801013d-5952-49c6-988b-ded8f30a62e8 117235 2 2023-06-09 11:49:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 43a34f9c-96b1-4cba-a7f1-a539458fef2e 0xc003bac437 0xc003bac438}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43a34f9c-96b1-4cba-a7f1-a539458fef2e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:49:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bac4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  9 11:49:52.662: INFO: Pod "test-rollover-deployment-6c6df9974f-22ftw" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-22ftw test-rollover-deployment-6c6df9974f- deployment-862  8f6063bd-e268-4fd7-aab3-cc3dc925239c 117255 0 2023-06-09 11:49:40 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:0bcf8779f5ce2e70d0db06c62b76167131eab3bb8845a41df081840b437efc85 cni.projectcalico.org/podIP:172.26.90.60/32 cni.projectcalico.org/podIPs:172.26.90.60/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 7a1aedd3-776b-4efc-b4d2-46f2a706c9b9 0xc0048062a7 0xc0048062a8}] [] [{kube-controller-manager Update v1 2023-06-09 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a1aedd3-776b-4efc-b4d2-46f2a706c9b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:49:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:49:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.90.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f9j7k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f9j7k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:49:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:49:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:49:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:49:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.26.90.60,StartTime:2023-06-09 11:49:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:49:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://801d66fd756a69f56ce745d204f72015ee6de1c5a3f1f9c9b13f90d703ac5cdc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.90.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun  9 11:49:52.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-862" for this suite. 06/09/23 11:49:52.679
------------------------------
• [SLOW TEST] [21.429 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:49:31.273
    Jun  9 11:49:31.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename deployment 06/09/23 11:49:31.274
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:49:31.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:49:31.319
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jun  9 11:49:31.342: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jun  9 11:49:36.350: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/09/23 11:49:36.35
    Jun  9 11:49:36.351: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jun  9 11:49:38.364: INFO: Creating deployment "test-rollover-deployment"
    Jun  9 11:49:38.408: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jun  9 11:49:40.427: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jun  9 11:49:40.447: INFO: Ensure that both replica sets have 1 created replica
    Jun  9 11:49:40.465: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jun  9 11:49:40.494: INFO: Updating deployment test-rollover-deployment
    Jun  9 11:49:40.494: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jun  9 11:49:42.529: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jun  9 11:49:42.547: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jun  9 11:49:42.593: INFO: all replica sets need to contain the pod-template-hash label
    Jun  9 11:49:42.593: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:49:44.607: INFO: all replica sets need to contain the pod-template-hash label
    Jun  9 11:49:44.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:49:46.625: INFO: all replica sets need to contain the pod-template-hash label
    Jun  9 11:49:46.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:49:48.640: INFO: all replica sets need to contain the pod-template-hash label
    Jun  9 11:49:48.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:49:50.609: INFO: all replica sets need to contain the pod-template-hash label
    Jun  9 11:49:50.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 9, 11, 49, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 9, 11, 49, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun  9 11:49:52.619: INFO: 
    Jun  9 11:49:52.619: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun  9 11:49:52.645: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-862  43a34f9c-96b1-4cba-a7f1-a539458fef2e 117311 2 2023-06-09 11:49:38 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-09 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:49:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005809e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-09 11:49:38 +0000 UTC,LastTransitionTime:2023-06-09 11:49:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-06-09 11:49:52 +0000 UTC,LastTransitionTime:2023-06-09 11:49:38 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun  9 11:49:52.652: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-862  7a1aedd3-776b-4efc-b4d2-46f2a706c9b9 117301 2 2023-06-09 11:49:40 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 43a34f9c-96b1-4cba-a7f1-a539458fef2e 0xc003bac317 0xc003bac318}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43a34f9c-96b1-4cba-a7f1-a539458fef2e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:49:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bac3c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun  9 11:49:52.652: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jun  9 11:49:52.652: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-862  412ef49c-9384-48dc-9702-29f96c29139c 117310 2 2023-06-09 11:49:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 43a34f9c-96b1-4cba-a7f1-a539458fef2e 0xc003bac1e7 0xc003bac1e8}] [] [{e2e.test Update apps/v1 2023-06-09 11:49:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:49:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43a34f9c-96b1-4cba-a7f1-a539458fef2e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:49:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003bac2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun  9 11:49:52.652: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-862  d801013d-5952-49c6-988b-ded8f30a62e8 117235 2 2023-06-09 11:49:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 43a34f9c-96b1-4cba-a7f1-a539458fef2e 0xc003bac437 0xc003bac438}] [] [{kube-controller-manager Update apps/v1 2023-06-09 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43a34f9c-96b1-4cba-a7f1-a539458fef2e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-09 11:49:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bac4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun  9 11:49:52.662: INFO: Pod "test-rollover-deployment-6c6df9974f-22ftw" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-22ftw test-rollover-deployment-6c6df9974f- deployment-862  8f6063bd-e268-4fd7-aab3-cc3dc925239c 117255 0 2023-06-09 11:49:40 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:0bcf8779f5ce2e70d0db06c62b76167131eab3bb8845a41df081840b437efc85 cni.projectcalico.org/podIP:172.26.90.60/32 cni.projectcalico.org/podIPs:172.26.90.60/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 7a1aedd3-776b-4efc-b4d2-46f2a706c9b9 0xc0048062a7 0xc0048062a8}] [] [{kube-controller-manager Update v1 2023-06-09 11:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a1aedd3-776b-4efc-b4d2-46f2a706c9b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-09 11:49:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-09 11:49:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.90.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f9j7k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f9j7k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-26.4-workergroup-4hkw9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:49:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:49:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:49:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-09 11:49:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.26.90.60,StartTime:2023-06-09 11:49:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-09 11:49:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://801d66fd756a69f56ce745d204f72015ee6de1c5a3f1f9c9b13f90d703ac5cdc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.90.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:49:52.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-862" for this suite. 06/09/23 11:49:52.679
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:49:52.704
Jun  9 11:49:52.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 11:49:52.707
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:49:52.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:49:52.749
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2560 06/09/23 11:49:52.754
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/09/23 11:49:52.805
STEP: creating service externalsvc in namespace services-2560 06/09/23 11:49:52.806
STEP: creating replication controller externalsvc in namespace services-2560 06/09/23 11:49:52.843
I0609 11:49:52.856147      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2560, replica count: 2
I0609 11:49:55.913470      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 06/09/23 11:49:55.928
Jun  9 11:49:55.995: INFO: Creating new exec pod
Jun  9 11:49:56.049: INFO: Waiting up to 5m0s for pod "execpodqzkhq" in namespace "services-2560" to be "running"
Jun  9 11:49:56.059: INFO: Pod "execpodqzkhq": Phase="Pending", Reason="", readiness=false. Elapsed: 9.929338ms
Jun  9 11:49:58.069: INFO: Pod "execpodqzkhq": Phase="Running", Reason="", readiness=true. Elapsed: 2.019700462s
Jun  9 11:49:58.069: INFO: Pod "execpodqzkhq" satisfied condition "running"
Jun  9 11:49:58.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-2560 exec execpodqzkhq -- /bin/sh -x -c nslookup nodeport-service.services-2560.svc.cluster.local'
Jun  9 11:49:58.391: INFO: stderr: "+ nslookup nodeport-service.services-2560.svc.cluster.local\n"
Jun  9 11:49:58.391: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-2560.svc.cluster.local\tcanonical name = externalsvc.services-2560.svc.cluster.local.\nName:\texternalsvc.services-2560.svc.cluster.local\nAddress: 10.109.206.155\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2560, will wait for the garbage collector to delete the pods 06/09/23 11:49:58.391
Jun  9 11:49:58.478: INFO: Deleting ReplicationController externalsvc took: 20.122609ms
Jun  9 11:49:58.578: INFO: Terminating ReplicationController externalsvc pods took: 100.208889ms
Jun  9 11:50:00.646: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 11:50:00.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2560" for this suite. 06/09/23 11:50:00.705
------------------------------
• [SLOW TEST] [8.016 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:49:52.704
    Jun  9 11:49:52.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 11:49:52.707
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:49:52.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:49:52.749
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-2560 06/09/23 11:49:52.754
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/09/23 11:49:52.805
    STEP: creating service externalsvc in namespace services-2560 06/09/23 11:49:52.806
    STEP: creating replication controller externalsvc in namespace services-2560 06/09/23 11:49:52.843
    I0609 11:49:52.856147      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2560, replica count: 2
    I0609 11:49:55.913470      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 06/09/23 11:49:55.928
    Jun  9 11:49:55.995: INFO: Creating new exec pod
    Jun  9 11:49:56.049: INFO: Waiting up to 5m0s for pod "execpodqzkhq" in namespace "services-2560" to be "running"
    Jun  9 11:49:56.059: INFO: Pod "execpodqzkhq": Phase="Pending", Reason="", readiness=false. Elapsed: 9.929338ms
    Jun  9 11:49:58.069: INFO: Pod "execpodqzkhq": Phase="Running", Reason="", readiness=true. Elapsed: 2.019700462s
    Jun  9 11:49:58.069: INFO: Pod "execpodqzkhq" satisfied condition "running"
    Jun  9 11:49:58.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-2560 exec execpodqzkhq -- /bin/sh -x -c nslookup nodeport-service.services-2560.svc.cluster.local'
    Jun  9 11:49:58.391: INFO: stderr: "+ nslookup nodeport-service.services-2560.svc.cluster.local\n"
    Jun  9 11:49:58.391: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-2560.svc.cluster.local\tcanonical name = externalsvc.services-2560.svc.cluster.local.\nName:\texternalsvc.services-2560.svc.cluster.local\nAddress: 10.109.206.155\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2560, will wait for the garbage collector to delete the pods 06/09/23 11:49:58.391
    Jun  9 11:49:58.478: INFO: Deleting ReplicationController externalsvc took: 20.122609ms
    Jun  9 11:49:58.578: INFO: Terminating ReplicationController externalsvc pods took: 100.208889ms
    Jun  9 11:50:00.646: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:50:00.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2560" for this suite. 06/09/23 11:50:00.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:50:00.724
Jun  9 11:50:00.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename endpointslicemirroring 06/09/23 11:50:00.725
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:50:00.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:50:00.769
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 06/09/23 11:50:00.817
Jun  9 11:50:00.836: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 06/09/23 11:50:02.844
STEP: mirroring deletion of a custom Endpoint 06/09/23 11:50:02.871
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jun  9 11:50:02.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-391" for this suite. 06/09/23 11:50:02.905
------------------------------
• [2.195 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:50:00.724
    Jun  9 11:50:00.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename endpointslicemirroring 06/09/23 11:50:00.725
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:50:00.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:50:00.769
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 06/09/23 11:50:00.817
    Jun  9 11:50:00.836: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 06/09/23 11:50:02.844
    STEP: mirroring deletion of a custom Endpoint 06/09/23 11:50:02.871
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:50:02.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-391" for this suite. 06/09/23 11:50:02.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:50:02.919
Jun  9 11:50:02.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename var-expansion 06/09/23 11:50:02.921
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:50:02.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:50:02.956
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Jun  9 11:50:02.976: INFO: Waiting up to 2m0s for pod "var-expansion-906e8780-1b1d-4602-9d00-757229a230bb" in namespace "var-expansion-5292" to be "container 0 failed with reason CreateContainerConfigError"
Jun  9 11:50:02.984: INFO: Pod "var-expansion-906e8780-1b1d-4602-9d00-757229a230bb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.10575ms
Jun  9 11:50:04.991: INFO: Pod "var-expansion-906e8780-1b1d-4602-9d00-757229a230bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015003091s
Jun  9 11:50:04.991: INFO: Pod "var-expansion-906e8780-1b1d-4602-9d00-757229a230bb" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jun  9 11:50:04.991: INFO: Deleting pod "var-expansion-906e8780-1b1d-4602-9d00-757229a230bb" in namespace "var-expansion-5292"
Jun  9 11:50:05.012: INFO: Wait up to 5m0s for pod "var-expansion-906e8780-1b1d-4602-9d00-757229a230bb" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  9 11:50:09.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5292" for this suite. 06/09/23 11:50:09.042
------------------------------
• [SLOW TEST] [6.143 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:50:02.919
    Jun  9 11:50:02.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename var-expansion 06/09/23 11:50:02.921
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:50:02.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:50:02.956
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Jun  9 11:50:02.976: INFO: Waiting up to 2m0s for pod "var-expansion-906e8780-1b1d-4602-9d00-757229a230bb" in namespace "var-expansion-5292" to be "container 0 failed with reason CreateContainerConfigError"
    Jun  9 11:50:02.984: INFO: Pod "var-expansion-906e8780-1b1d-4602-9d00-757229a230bb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.10575ms
    Jun  9 11:50:04.991: INFO: Pod "var-expansion-906e8780-1b1d-4602-9d00-757229a230bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015003091s
    Jun  9 11:50:04.991: INFO: Pod "var-expansion-906e8780-1b1d-4602-9d00-757229a230bb" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jun  9 11:50:04.991: INFO: Deleting pod "var-expansion-906e8780-1b1d-4602-9d00-757229a230bb" in namespace "var-expansion-5292"
    Jun  9 11:50:05.012: INFO: Wait up to 5m0s for pod "var-expansion-906e8780-1b1d-4602-9d00-757229a230bb" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:50:09.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5292" for this suite. 06/09/23 11:50:09.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:50:09.064
Jun  9 11:50:09.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename svcaccounts 06/09/23 11:50:09.065
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:50:09.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:50:09.111
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Jun  9 11:50:09.150: INFO: created pod
Jun  9 11:50:09.150: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4245" to be "Succeeded or Failed"
Jun  9 11:50:09.164: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 14.059477ms
Jun  9 11:50:11.190: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039844578s
Jun  9 11:50:13.175: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024754204s
STEP: Saw pod success 06/09/23 11:50:13.175
Jun  9 11:50:13.175: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jun  9 11:50:43.176: INFO: polling logs
Jun  9 11:50:43.202: INFO: Pod logs: 
I0609 11:50:10.126029       1 log.go:198] OK: Got token
I0609 11:50:10.127352       1 log.go:198] validating with in-cluster discovery
I0609 11:50:10.128027       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0609 11:50:10.128076       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4245:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1686312009, NotBefore:1686311409, IssuedAt:1686311409, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4245", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cd69ef85-e92d-4e3f-8286-1dfe2c9aba0b"}}}
I0609 11:50:10.175196       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0609 11:50:10.187024       1 log.go:198] OK: Validated signature on JWT
I0609 11:50:10.187144       1 log.go:198] OK: Got valid claims from token!
I0609 11:50:10.187178       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4245:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1686312009, NotBefore:1686311409, IssuedAt:1686311409, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4245", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cd69ef85-e92d-4e3f-8286-1dfe2c9aba0b"}}}

Jun  9 11:50:43.202: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun  9 11:50:43.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4245" for this suite. 06/09/23 11:50:43.225
------------------------------
• [SLOW TEST] [34.174 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:50:09.064
    Jun  9 11:50:09.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename svcaccounts 06/09/23 11:50:09.065
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:50:09.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:50:09.111
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Jun  9 11:50:09.150: INFO: created pod
    Jun  9 11:50:09.150: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4245" to be "Succeeded or Failed"
    Jun  9 11:50:09.164: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 14.059477ms
    Jun  9 11:50:11.190: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039844578s
    Jun  9 11:50:13.175: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024754204s
    STEP: Saw pod success 06/09/23 11:50:13.175
    Jun  9 11:50:13.175: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jun  9 11:50:43.176: INFO: polling logs
    Jun  9 11:50:43.202: INFO: Pod logs: 
    I0609 11:50:10.126029       1 log.go:198] OK: Got token
    I0609 11:50:10.127352       1 log.go:198] validating with in-cluster discovery
    I0609 11:50:10.128027       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0609 11:50:10.128076       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4245:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1686312009, NotBefore:1686311409, IssuedAt:1686311409, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4245", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cd69ef85-e92d-4e3f-8286-1dfe2c9aba0b"}}}
    I0609 11:50:10.175196       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0609 11:50:10.187024       1 log.go:198] OK: Validated signature on JWT
    I0609 11:50:10.187144       1 log.go:198] OK: Got valid claims from token!
    I0609 11:50:10.187178       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4245:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1686312009, NotBefore:1686311409, IssuedAt:1686311409, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4245", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cd69ef85-e92d-4e3f-8286-1dfe2c9aba0b"}}}

    Jun  9 11:50:43.202: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:50:43.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4245" for this suite. 06/09/23 11:50:43.225
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:50:43.238
Jun  9 11:50:43.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename disruption 06/09/23 11:50:43.239
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:50:43.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:50:43.272
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 06/09/23 11:50:43.288
STEP: Waiting for all pods to be running 06/09/23 11:50:43.338
Jun  9 11:50:43.361: INFO: running pods: 0 < 3
Jun  9 11:50:45.370: INFO: running pods: 1 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun  9 11:50:47.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8598" for this suite. 06/09/23 11:50:47.4
------------------------------
• [4.175 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:50:43.238
    Jun  9 11:50:43.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename disruption 06/09/23 11:50:43.239
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:50:43.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:50:43.272
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 06/09/23 11:50:43.288
    STEP: Waiting for all pods to be running 06/09/23 11:50:43.338
    Jun  9 11:50:43.361: INFO: running pods: 0 < 3
    Jun  9 11:50:45.370: INFO: running pods: 1 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:50:47.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8598" for this suite. 06/09/23 11:50:47.4
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:50:47.413
Jun  9 11:50:47.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename services 06/09/23 11:50:47.416
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:50:47.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:50:47.464
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-9628 06/09/23 11:50:47.471
STEP: creating service affinity-clusterip-transition in namespace services-9628 06/09/23 11:50:47.472
STEP: creating replication controller affinity-clusterip-transition in namespace services-9628 06/09/23 11:50:47.507
I0609 11:50:47.529298      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9628, replica count: 3
I0609 11:50:50.580448      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  9 11:50:50.600: INFO: Creating new exec pod
Jun  9 11:50:50.614: INFO: Waiting up to 5m0s for pod "execpod-affinity8vnlj" in namespace "services-9628" to be "running"
Jun  9 11:50:50.620: INFO: Pod "execpod-affinity8vnlj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.397467ms
Jun  9 11:50:52.630: INFO: Pod "execpod-affinity8vnlj": Phase="Running", Reason="", readiness=true. Elapsed: 2.016405977s
Jun  9 11:50:52.630: INFO: Pod "execpod-affinity8vnlj" satisfied condition "running"
Jun  9 11:50:53.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-9628 exec execpod-affinity8vnlj -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jun  9 11:50:53.956: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun  9 11:50:53.956: INFO: stdout: ""
Jun  9 11:50:53.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-9628 exec execpod-affinity8vnlj -- /bin/sh -x -c nc -v -z -w 2 10.104.145.35 80'
Jun  9 11:50:54.173: INFO: stderr: "+ nc -v -z -w 2 10.104.145.35 80\nConnection to 10.104.145.35 80 port [tcp/http] succeeded!\n"
Jun  9 11:50:54.173: INFO: stdout: ""
Jun  9 11:50:54.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-9628 exec execpod-affinity8vnlj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.104.145.35:80/ ; done'
Jun  9 11:50:54.620: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n"
Jun  9 11:50:54.620: INFO: stdout: "\naffinity-clusterip-transition-sb22w\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-sb22w\naffinity-clusterip-transition-sb22w\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-sb22w\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-mxdrk\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-mxdrk\naffinity-clusterip-transition-sb22w\naffinity-clusterip-transition-mxdrk\naffinity-clusterip-transition-sb22w\naffinity-clusterip-transition-sb22w\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76"
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-sb22w
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-sb22w
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-sb22w
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-sb22w
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-mxdrk
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-mxdrk
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-sb22w
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-mxdrk
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-sb22w
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-sb22w
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-9628 exec execpod-affinity8vnlj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.104.145.35:80/ ; done'
Jun  9 11:50:54.996: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n"
Jun  9 11:50:54.997: INFO: stdout: "\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76"
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
Jun  9 11:50:54.997: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9628, will wait for the garbage collector to delete the pods 06/09/23 11:50:55.017
Jun  9 11:50:55.086: INFO: Deleting ReplicationController affinity-clusterip-transition took: 13.573842ms
Jun  9 11:50:55.187: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.497654ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun  9 11:50:57.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9628" for this suite. 06/09/23 11:50:57.912
------------------------------
• [SLOW TEST] [10.521 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:50:47.413
    Jun  9 11:50:47.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename services 06/09/23 11:50:47.416
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:50:47.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:50:47.464
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-9628 06/09/23 11:50:47.471
    STEP: creating service affinity-clusterip-transition in namespace services-9628 06/09/23 11:50:47.472
    STEP: creating replication controller affinity-clusterip-transition in namespace services-9628 06/09/23 11:50:47.507
    I0609 11:50:47.529298      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9628, replica count: 3
    I0609 11:50:50.580448      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun  9 11:50:50.600: INFO: Creating new exec pod
    Jun  9 11:50:50.614: INFO: Waiting up to 5m0s for pod "execpod-affinity8vnlj" in namespace "services-9628" to be "running"
    Jun  9 11:50:50.620: INFO: Pod "execpod-affinity8vnlj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.397467ms
    Jun  9 11:50:52.630: INFO: Pod "execpod-affinity8vnlj": Phase="Running", Reason="", readiness=true. Elapsed: 2.016405977s
    Jun  9 11:50:52.630: INFO: Pod "execpod-affinity8vnlj" satisfied condition "running"
    Jun  9 11:50:53.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-9628 exec execpod-affinity8vnlj -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jun  9 11:50:53.956: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jun  9 11:50:53.956: INFO: stdout: ""
    Jun  9 11:50:53.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-9628 exec execpod-affinity8vnlj -- /bin/sh -x -c nc -v -z -w 2 10.104.145.35 80'
    Jun  9 11:50:54.173: INFO: stderr: "+ nc -v -z -w 2 10.104.145.35 80\nConnection to 10.104.145.35 80 port [tcp/http] succeeded!\n"
    Jun  9 11:50:54.173: INFO: stdout: ""
    Jun  9 11:50:54.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-9628 exec execpod-affinity8vnlj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.104.145.35:80/ ; done'
    Jun  9 11:50:54.620: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n"
    Jun  9 11:50:54.620: INFO: stdout: "\naffinity-clusterip-transition-sb22w\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-sb22w\naffinity-clusterip-transition-sb22w\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-sb22w\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-mxdrk\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-mxdrk\naffinity-clusterip-transition-sb22w\naffinity-clusterip-transition-mxdrk\naffinity-clusterip-transition-sb22w\naffinity-clusterip-transition-sb22w\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76"
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-sb22w
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-sb22w
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-sb22w
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-sb22w
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-mxdrk
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-mxdrk
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-sb22w
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-mxdrk
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-sb22w
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-sb22w
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.620: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=services-9628 exec execpod-affinity8vnlj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.104.145.35:80/ ; done'
    Jun  9 11:50:54.996: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.145.35:80/\n"
    Jun  9 11:50:54.997: INFO: stdout: "\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76\naffinity-clusterip-transition-frm76"
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Received response from host: affinity-clusterip-transition-frm76
    Jun  9 11:50:54.997: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9628, will wait for the garbage collector to delete the pods 06/09/23 11:50:55.017
    Jun  9 11:50:55.086: INFO: Deleting ReplicationController affinity-clusterip-transition took: 13.573842ms
    Jun  9 11:50:55.187: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.497654ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:50:57.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9628" for this suite. 06/09/23 11:50:57.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:50:57.938
Jun  9 11:50:57.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename cronjob 06/09/23 11:50:57.939
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:50:58.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:50:58.007
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 06/09/23 11:50:58.012
STEP: Ensuring a job is scheduled 06/09/23 11:50:58.023
STEP: Ensuring exactly one is scheduled 06/09/23 11:51:02.031
STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/09/23 11:51:02.038
STEP: Ensuring no more jobs are scheduled 06/09/23 11:51:02.047
STEP: Removing cronjob 06/09/23 11:56:02.06
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun  9 11:56:02.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5724" for this suite. 06/09/23 11:56:02.082
------------------------------
• [SLOW TEST] [304.155 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:50:57.938
    Jun  9 11:50:57.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename cronjob 06/09/23 11:50:57.939
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:50:58.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:50:58.007
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 06/09/23 11:50:58.012
    STEP: Ensuring a job is scheduled 06/09/23 11:50:58.023
    STEP: Ensuring exactly one is scheduled 06/09/23 11:51:02.031
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/09/23 11:51:02.038
    STEP: Ensuring no more jobs are scheduled 06/09/23 11:51:02.047
    STEP: Removing cronjob 06/09/23 11:56:02.06
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:56:02.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5724" for this suite. 06/09/23 11:56:02.082
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:56:02.093
Jun  9 11:56:02.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename var-expansion 06/09/23 11:56:02.095
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:56:02.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:56:02.126
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 06/09/23 11:56:02.131
Jun  9 11:56:02.143: INFO: Waiting up to 2m0s for pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12" in namespace "var-expansion-4787" to be "running"
Jun  9 11:56:02.147: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.564359ms
Jun  9 11:56:04.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012120416s
Jun  9 11:56:06.153: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010128473s
Jun  9 11:56:08.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01249703s
Jun  9 11:56:10.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012635321s
Jun  9 11:56:12.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011362226s
Jun  9 11:56:14.157: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013811128s
Jun  9 11:56:16.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011722073s
Jun  9 11:56:18.164: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 16.02081615s
Jun  9 11:56:20.157: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013943059s
Jun  9 11:56:22.153: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010636756s
Jun  9 11:56:24.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011172884s
Jun  9 11:56:26.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 24.012937115s
Jun  9 11:56:28.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012611412s
Jun  9 11:56:30.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 28.011248619s
Jun  9 11:56:32.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011156269s
Jun  9 11:56:34.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011865906s
Jun  9 11:56:36.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 34.01224364s
Jun  9 11:56:38.153: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010601533s
Jun  9 11:56:40.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011760415s
Jun  9 11:56:42.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010756206s
Jun  9 11:56:44.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 42.011219227s
Jun  9 11:56:46.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 44.012132193s
Jun  9 11:56:48.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 46.011466961s
Jun  9 11:56:50.175: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 48.032384258s
Jun  9 11:56:52.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011222301s
Jun  9 11:56:54.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 52.012090625s
Jun  9 11:56:56.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 54.012286194s
Jun  9 11:56:58.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 56.011781645s
Jun  9 11:57:00.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012345312s
Jun  9 11:57:02.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012499323s
Jun  9 11:57:04.157: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014154374s
Jun  9 11:57:06.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.01294023s
Jun  9 11:57:08.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011305964s
Jun  9 11:57:10.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.013057616s
Jun  9 11:57:12.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.01238498s
Jun  9 11:57:14.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012344862s
Jun  9 11:57:16.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.012461963s
Jun  9 11:57:18.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.013288468s
Jun  9 11:57:20.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.012795509s
Jun  9 11:57:22.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.011143532s
Jun  9 11:57:24.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011884546s
Jun  9 11:57:26.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012041049s
Jun  9 11:57:28.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01099174s
Jun  9 11:57:30.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.01237111s
Jun  9 11:57:32.157: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.014405748s
Jun  9 11:57:34.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011813729s
Jun  9 11:57:36.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01179888s
Jun  9 11:57:38.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.012344896s
Jun  9 11:57:40.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.011452324s
Jun  9 11:57:42.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.0134959s
Jun  9 11:57:44.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012049614s
Jun  9 11:57:46.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.012737793s
Jun  9 11:57:48.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013675341s
Jun  9 11:57:50.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.01269707s
Jun  9 11:57:52.158: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.015644493s
Jun  9 11:57:54.157: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.014207453s
Jun  9 11:57:56.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.012328243s
Jun  9 11:57:58.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012933165s
Jun  9 11:58:00.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.011979055s
Jun  9 11:58:02.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010885956s
Jun  9 11:58:02.160: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01726041s
STEP: updating the pod 06/09/23 11:58:02.16
Jun  9 11:58:02.683: INFO: Successfully updated pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12"
STEP: waiting for pod running 06/09/23 11:58:02.683
Jun  9 11:58:02.683: INFO: Waiting up to 2m0s for pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12" in namespace "var-expansion-4787" to be "running"
Jun  9 11:58:02.690: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 7.059502ms
Jun  9 11:58:04.701: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Running", Reason="", readiness=true. Elapsed: 2.017975266s
Jun  9 11:58:04.701: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12" satisfied condition "running"
STEP: deleting the pod gracefully 06/09/23 11:58:04.701
Jun  9 11:58:04.702: INFO: Deleting pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12" in namespace "var-expansion-4787"
Jun  9 11:58:04.727: INFO: Wait up to 5m0s for pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun  9 11:58:36.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4787" for this suite. 06/09/23 11:58:36.772
------------------------------
• [SLOW TEST] [154.696 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:56:02.093
    Jun  9 11:56:02.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename var-expansion 06/09/23 11:56:02.095
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:56:02.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:56:02.126
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 06/09/23 11:56:02.131
    Jun  9 11:56:02.143: INFO: Waiting up to 2m0s for pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12" in namespace "var-expansion-4787" to be "running"
    Jun  9 11:56:02.147: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.564359ms
    Jun  9 11:56:04.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012120416s
    Jun  9 11:56:06.153: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010128473s
    Jun  9 11:56:08.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01249703s
    Jun  9 11:56:10.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012635321s
    Jun  9 11:56:12.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011362226s
    Jun  9 11:56:14.157: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013811128s
    Jun  9 11:56:16.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011722073s
    Jun  9 11:56:18.164: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 16.02081615s
    Jun  9 11:56:20.157: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013943059s
    Jun  9 11:56:22.153: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010636756s
    Jun  9 11:56:24.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011172884s
    Jun  9 11:56:26.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 24.012937115s
    Jun  9 11:56:28.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012611412s
    Jun  9 11:56:30.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 28.011248619s
    Jun  9 11:56:32.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011156269s
    Jun  9 11:56:34.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011865906s
    Jun  9 11:56:36.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 34.01224364s
    Jun  9 11:56:38.153: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010601533s
    Jun  9 11:56:40.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011760415s
    Jun  9 11:56:42.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010756206s
    Jun  9 11:56:44.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 42.011219227s
    Jun  9 11:56:46.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 44.012132193s
    Jun  9 11:56:48.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 46.011466961s
    Jun  9 11:56:50.175: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 48.032384258s
    Jun  9 11:56:52.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011222301s
    Jun  9 11:56:54.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 52.012090625s
    Jun  9 11:56:56.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 54.012286194s
    Jun  9 11:56:58.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 56.011781645s
    Jun  9 11:57:00.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012345312s
    Jun  9 11:57:02.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012499323s
    Jun  9 11:57:04.157: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014154374s
    Jun  9 11:57:06.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.01294023s
    Jun  9 11:57:08.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011305964s
    Jun  9 11:57:10.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.013057616s
    Jun  9 11:57:12.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.01238498s
    Jun  9 11:57:14.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012344862s
    Jun  9 11:57:16.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.012461963s
    Jun  9 11:57:18.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.013288468s
    Jun  9 11:57:20.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.012795509s
    Jun  9 11:57:22.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.011143532s
    Jun  9 11:57:24.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011884546s
    Jun  9 11:57:26.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012041049s
    Jun  9 11:57:28.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01099174s
    Jun  9 11:57:30.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.01237111s
    Jun  9 11:57:32.157: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.014405748s
    Jun  9 11:57:34.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011813729s
    Jun  9 11:57:36.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01179888s
    Jun  9 11:57:38.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.012344896s
    Jun  9 11:57:40.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.011452324s
    Jun  9 11:57:42.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.0134959s
    Jun  9 11:57:44.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012049614s
    Jun  9 11:57:46.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.012737793s
    Jun  9 11:57:48.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013675341s
    Jun  9 11:57:50.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.01269707s
    Jun  9 11:57:52.158: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.015644493s
    Jun  9 11:57:54.157: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.014207453s
    Jun  9 11:57:56.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.012328243s
    Jun  9 11:57:58.156: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012933165s
    Jun  9 11:58:00.155: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.011979055s
    Jun  9 11:58:02.154: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010885956s
    Jun  9 11:58:02.160: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01726041s
    STEP: updating the pod 06/09/23 11:58:02.16
    Jun  9 11:58:02.683: INFO: Successfully updated pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12"
    STEP: waiting for pod running 06/09/23 11:58:02.683
    Jun  9 11:58:02.683: INFO: Waiting up to 2m0s for pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12" in namespace "var-expansion-4787" to be "running"
    Jun  9 11:58:02.690: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Pending", Reason="", readiness=false. Elapsed: 7.059502ms
    Jun  9 11:58:04.701: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12": Phase="Running", Reason="", readiness=true. Elapsed: 2.017975266s
    Jun  9 11:58:04.701: INFO: Pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12" satisfied condition "running"
    STEP: deleting the pod gracefully 06/09/23 11:58:04.701
    Jun  9 11:58:04.702: INFO: Deleting pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12" in namespace "var-expansion-4787"
    Jun  9 11:58:04.727: INFO: Wait up to 5m0s for pod "var-expansion-6c0de51e-e9b4-4987-b7f2-caf31434ea12" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:58:36.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4787" for this suite. 06/09/23 11:58:36.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:58:36.79
Jun  9 11:58:36.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename downward-api 06/09/23 11:58:36.792
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:58:36.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:58:36.827
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 06/09/23 11:58:36.835
Jun  9 11:58:36.859: INFO: Waiting up to 5m0s for pod "downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677" in namespace "downward-api-6565" to be "Succeeded or Failed"
Jun  9 11:58:36.871: INFO: Pod "downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677": Phase="Pending", Reason="", readiness=false. Elapsed: 11.974959ms
Jun  9 11:58:38.879: INFO: Pod "downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01930812s
Jun  9 11:58:40.882: INFO: Pod "downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02285958s
Jun  9 11:58:42.884: INFO: Pod "downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024810284s
STEP: Saw pod success 06/09/23 11:58:42.884
Jun  9 11:58:42.885: INFO: Pod "downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677" satisfied condition "Succeeded or Failed"
Jun  9 11:58:42.895: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677 container dapi-container: <nil>
STEP: delete the pod 06/09/23 11:58:42.924
Jun  9 11:58:42.968: INFO: Waiting for pod downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677 to disappear
Jun  9 11:58:43.107: INFO: Pod downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun  9 11:58:43.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6565" for this suite. 06/09/23 11:58:43.118
------------------------------
• [SLOW TEST] [6.341 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:58:36.79
    Jun  9 11:58:36.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename downward-api 06/09/23 11:58:36.792
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:58:36.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:58:36.827
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 06/09/23 11:58:36.835
    Jun  9 11:58:36.859: INFO: Waiting up to 5m0s for pod "downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677" in namespace "downward-api-6565" to be "Succeeded or Failed"
    Jun  9 11:58:36.871: INFO: Pod "downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677": Phase="Pending", Reason="", readiness=false. Elapsed: 11.974959ms
    Jun  9 11:58:38.879: INFO: Pod "downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01930812s
    Jun  9 11:58:40.882: INFO: Pod "downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02285958s
    Jun  9 11:58:42.884: INFO: Pod "downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024810284s
    STEP: Saw pod success 06/09/23 11:58:42.884
    Jun  9 11:58:42.885: INFO: Pod "downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677" satisfied condition "Succeeded or Failed"
    Jun  9 11:58:42.895: INFO: Trying to get logs from node sks-test-v1-26.4-workergroup-qdprq pod downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677 container dapi-container: <nil>
    STEP: delete the pod 06/09/23 11:58:42.924
    Jun  9 11:58:42.968: INFO: Waiting for pod downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677 to disappear
    Jun  9 11:58:43.107: INFO: Pod downward-api-cbd00e0b-fa61-4446-9daf-6c8fa7c10677 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:58:43.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6565" for this suite. 06/09/23 11:58:43.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:58:43.132
Jun  9 11:58:43.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename pods 06/09/23 11:58:43.133
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:58:43.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:58:43.238
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 06/09/23 11:58:43.244
STEP: setting up watch 06/09/23 11:58:43.245
STEP: submitting the pod to kubernetes 06/09/23 11:58:43.351
STEP: verifying the pod is in kubernetes 06/09/23 11:58:43.376
STEP: verifying pod creation was observed 06/09/23 11:58:43.407
Jun  9 11:58:43.407: INFO: Waiting up to 5m0s for pod "pod-submit-remove-ccdbf23d-2ab2-4209-9871-d926e332276b" in namespace "pods-9838" to be "running"
Jun  9 11:58:43.485: INFO: Pod "pod-submit-remove-ccdbf23d-2ab2-4209-9871-d926e332276b": Phase="Pending", Reason="", readiness=false. Elapsed: 77.39431ms
Jun  9 11:58:45.493: INFO: Pod "pod-submit-remove-ccdbf23d-2ab2-4209-9871-d926e332276b": Phase="Running", Reason="", readiness=true. Elapsed: 2.085980391s
Jun  9 11:58:45.493: INFO: Pod "pod-submit-remove-ccdbf23d-2ab2-4209-9871-d926e332276b" satisfied condition "running"
STEP: deleting the pod gracefully 06/09/23 11:58:45.501
STEP: verifying pod deletion was observed 06/09/23 11:58:45.516
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun  9 11:58:47.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9838" for this suite. 06/09/23 11:58:47.95
------------------------------
• [4.831 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:58:43.132
    Jun  9 11:58:43.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename pods 06/09/23 11:58:43.133
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:58:43.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:58:43.238
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 06/09/23 11:58:43.244
    STEP: setting up watch 06/09/23 11:58:43.245
    STEP: submitting the pod to kubernetes 06/09/23 11:58:43.351
    STEP: verifying the pod is in kubernetes 06/09/23 11:58:43.376
    STEP: verifying pod creation was observed 06/09/23 11:58:43.407
    Jun  9 11:58:43.407: INFO: Waiting up to 5m0s for pod "pod-submit-remove-ccdbf23d-2ab2-4209-9871-d926e332276b" in namespace "pods-9838" to be "running"
    Jun  9 11:58:43.485: INFO: Pod "pod-submit-remove-ccdbf23d-2ab2-4209-9871-d926e332276b": Phase="Pending", Reason="", readiness=false. Elapsed: 77.39431ms
    Jun  9 11:58:45.493: INFO: Pod "pod-submit-remove-ccdbf23d-2ab2-4209-9871-d926e332276b": Phase="Running", Reason="", readiness=true. Elapsed: 2.085980391s
    Jun  9 11:58:45.493: INFO: Pod "pod-submit-remove-ccdbf23d-2ab2-4209-9871-d926e332276b" satisfied condition "running"
    STEP: deleting the pod gracefully 06/09/23 11:58:45.501
    STEP: verifying pod deletion was observed 06/09/23 11:58:45.516
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:58:47.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9838" for this suite. 06/09/23 11:58:47.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:58:47.964
Jun  9 11:58:47.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename kubectl 06/09/23 11:58:47.965
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:58:47.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:58:48.004
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 06/09/23 11:58:48.011
Jun  9 11:58:48.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 create -f -'
Jun  9 11:58:49.104: INFO: stderr: ""
Jun  9 11:58:49.104: INFO: stdout: "pod/pause created\n"
Jun  9 11:58:49.104: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun  9 11:58:49.104: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2669" to be "running and ready"
Jun  9 11:58:49.112: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821822ms
Jun  9 11:58:49.112: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'sks-test-v1-26.4-workergroup-qdprq' to be 'Running' but was 'Pending'
Jun  9 11:58:51.121: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.016256908s
Jun  9 11:58:51.121: INFO: Pod "pause" satisfied condition "running and ready"
Jun  9 11:58:51.121: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 06/09/23 11:58:51.121
Jun  9 11:58:51.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 label pods pause testing-label=testing-label-value'
Jun  9 11:58:51.238: INFO: stderr: ""
Jun  9 11:58:51.238: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 06/09/23 11:58:51.238
Jun  9 11:58:51.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 get pod pause -L testing-label'
Jun  9 11:58:51.327: INFO: stderr: ""
Jun  9 11:58:51.327: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 06/09/23 11:58:51.327
Jun  9 11:58:51.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 label pods pause testing-label-'
Jun  9 11:58:51.442: INFO: stderr: ""
Jun  9 11:58:51.442: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 06/09/23 11:58:51.442
Jun  9 11:58:51.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 get pod pause -L testing-label'
Jun  9 11:58:51.550: INFO: stderr: ""
Jun  9 11:58:51.550: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 06/09/23 11:58:51.55
Jun  9 11:58:51.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 delete --grace-period=0 --force -f -'
Jun  9 11:58:51.677: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  9 11:58:51.677: INFO: stdout: "pod \"pause\" force deleted\n"
Jun  9 11:58:51.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 get rc,svc -l name=pause --no-headers'
Jun  9 11:58:51.803: INFO: stderr: "No resources found in kubectl-2669 namespace.\n"
Jun  9 11:58:51.803: INFO: stdout: ""
Jun  9 11:58:51.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  9 11:58:51.905: INFO: stderr: ""
Jun  9 11:58:51.905: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun  9 11:58:51.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2669" for this suite. 06/09/23 11:58:51.923
------------------------------
• [3.998 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:58:47.964
    Jun  9 11:58:47.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename kubectl 06/09/23 11:58:47.965
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:58:47.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:58:48.004
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 06/09/23 11:58:48.011
    Jun  9 11:58:48.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 create -f -'
    Jun  9 11:58:49.104: INFO: stderr: ""
    Jun  9 11:58:49.104: INFO: stdout: "pod/pause created\n"
    Jun  9 11:58:49.104: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jun  9 11:58:49.104: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2669" to be "running and ready"
    Jun  9 11:58:49.112: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821822ms
    Jun  9 11:58:49.112: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'sks-test-v1-26.4-workergroup-qdprq' to be 'Running' but was 'Pending'
    Jun  9 11:58:51.121: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.016256908s
    Jun  9 11:58:51.121: INFO: Pod "pause" satisfied condition "running and ready"
    Jun  9 11:58:51.121: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 06/09/23 11:58:51.121
    Jun  9 11:58:51.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 label pods pause testing-label=testing-label-value'
    Jun  9 11:58:51.238: INFO: stderr: ""
    Jun  9 11:58:51.238: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 06/09/23 11:58:51.238
    Jun  9 11:58:51.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 get pod pause -L testing-label'
    Jun  9 11:58:51.327: INFO: stderr: ""
    Jun  9 11:58:51.327: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 06/09/23 11:58:51.327
    Jun  9 11:58:51.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 label pods pause testing-label-'
    Jun  9 11:58:51.442: INFO: stderr: ""
    Jun  9 11:58:51.442: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 06/09/23 11:58:51.442
    Jun  9 11:58:51.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 get pod pause -L testing-label'
    Jun  9 11:58:51.550: INFO: stderr: ""
    Jun  9 11:58:51.550: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 06/09/23 11:58:51.55
    Jun  9 11:58:51.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 delete --grace-period=0 --force -f -'
    Jun  9 11:58:51.677: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun  9 11:58:51.677: INFO: stdout: "pod \"pause\" force deleted\n"
    Jun  9 11:58:51.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 get rc,svc -l name=pause --no-headers'
    Jun  9 11:58:51.803: INFO: stderr: "No resources found in kubectl-2669 namespace.\n"
    Jun  9 11:58:51.803: INFO: stdout: ""
    Jun  9 11:58:51.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3267009506 --namespace=kubectl-2669 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun  9 11:58:51.905: INFO: stderr: ""
    Jun  9 11:58:51.905: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:58:51.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2669" for this suite. 06/09/23 11:58:51.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:58:51.967
Jun  9 11:58:51.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename gc 06/09/23 11:58:51.969
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:58:52.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:58:52.02
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 06/09/23 11:58:52.028
STEP: delete the rc 06/09/23 11:58:57.061
STEP: wait for all pods to be garbage collected 06/09/23 11:58:57.073
STEP: Gathering metrics 06/09/23 11:59:02.086
Jun  9 11:59:02.120: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" in namespace "kube-system" to be "running and ready"
Jun  9 11:59:02.127: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh": Phase="Running", Reason="", readiness=true. Elapsed: 6.365972ms
Jun  9 11:59:02.127: INFO: The phase of Pod kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh is Running (Ready = true)
Jun  9 11:59:02.127: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" satisfied condition "running and ready"
Jun  9 11:59:02.195: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun  9 11:59:02.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6277" for this suite. 06/09/23 11:59:02.204
------------------------------
• [SLOW TEST] [10.247 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:58:51.967
    Jun  9 11:58:51.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename gc 06/09/23 11:58:51.969
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:58:52.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:58:52.02
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 06/09/23 11:58:52.028
    STEP: delete the rc 06/09/23 11:58:57.061
    STEP: wait for all pods to be garbage collected 06/09/23 11:58:57.073
    STEP: Gathering metrics 06/09/23 11:59:02.086
    Jun  9 11:59:02.120: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" in namespace "kube-system" to be "running and ready"
    Jun  9 11:59:02.127: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh": Phase="Running", Reason="", readiness=true. Elapsed: 6.365972ms
    Jun  9 11:59:02.127: INFO: The phase of Pod kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh is Running (Ready = true)
    Jun  9 11:59:02.127: INFO: Pod "kube-controller-manager-sks-test-v1-26.4-controlplane-qtlkh" satisfied condition "running and ready"
    Jun  9 11:59:02.195: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:59:02.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6277" for this suite. 06/09/23 11:59:02.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:59:02.215
Jun  9 11:59:02.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename watch 06/09/23 11:59:02.218
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:59:02.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:59:02.27
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 06/09/23 11:59:02.275
STEP: creating a new configmap 06/09/23 11:59:02.277
STEP: modifying the configmap once 06/09/23 11:59:02.321
STEP: closing the watch once it receives two notifications 06/09/23 11:59:02.337
Jun  9 11:59:02.337: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8646  a1c1fcbc-6300-41fe-83d6-e7582dda2d0e 120352 0 2023-06-09 11:59:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-09 11:59:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  9 11:59:02.337: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8646  a1c1fcbc-6300-41fe-83d6-e7582dda2d0e 120353 0 2023-06-09 11:59:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-09 11:59:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 06/09/23 11:59:02.337
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 06/09/23 11:59:02.372
STEP: deleting the configmap 06/09/23 11:59:02.376
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 06/09/23 11:59:02.483
Jun  9 11:59:02.483: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8646  a1c1fcbc-6300-41fe-83d6-e7582dda2d0e 120354 0 2023-06-09 11:59:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-09 11:59:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  9 11:59:02.483: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8646  a1c1fcbc-6300-41fe-83d6-e7582dda2d0e 120355 0 2023-06-09 11:59:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-09 11:59:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun  9 11:59:02.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8646" for this suite. 06/09/23 11:59:02.491
------------------------------
• [0.313 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:59:02.215
    Jun  9 11:59:02.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename watch 06/09/23 11:59:02.218
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:59:02.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:59:02.27
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 06/09/23 11:59:02.275
    STEP: creating a new configmap 06/09/23 11:59:02.277
    STEP: modifying the configmap once 06/09/23 11:59:02.321
    STEP: closing the watch once it receives two notifications 06/09/23 11:59:02.337
    Jun  9 11:59:02.337: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8646  a1c1fcbc-6300-41fe-83d6-e7582dda2d0e 120352 0 2023-06-09 11:59:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-09 11:59:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  9 11:59:02.337: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8646  a1c1fcbc-6300-41fe-83d6-e7582dda2d0e 120353 0 2023-06-09 11:59:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-09 11:59:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 06/09/23 11:59:02.337
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 06/09/23 11:59:02.372
    STEP: deleting the configmap 06/09/23 11:59:02.376
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 06/09/23 11:59:02.483
    Jun  9 11:59:02.483: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8646  a1c1fcbc-6300-41fe-83d6-e7582dda2d0e 120354 0 2023-06-09 11:59:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-09 11:59:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun  9 11:59:02.483: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8646  a1c1fcbc-6300-41fe-83d6-e7582dda2d0e 120355 0 2023-06-09 11:59:02 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-09 11:59:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:59:02.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8646" for this suite. 06/09/23 11:59:02.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:59:02.529
Jun  9 11:59:02.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename conformance-tests 06/09/23 11:59:02.53
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:59:02.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:59:02.583
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 06/09/23 11:59:02.589
Jun  9 11:59:02.589: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jun  9 11:59:02.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-7215" for this suite. 06/09/23 11:59:02.608
------------------------------
• [0.107 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:59:02.529
    Jun  9 11:59:02.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename conformance-tests 06/09/23 11:59:02.53
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:59:02.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:59:02.583
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 06/09/23 11:59:02.589
    Jun  9 11:59:02.589: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:59:02.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-7215" for this suite. 06/09/23 11:59:02.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/09/23 11:59:02.651
Jun  9 11:59:02.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 11:59:02.652
STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:59:02.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:59:02.76
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 06/09/23 11:59:02.766
Jun  9 11:59:02.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
STEP: mark a version not serverd 06/09/23 11:59:08.122
STEP: check the unserved version gets removed 06/09/23 11:59:08.147
STEP: check the other version is not changed 06/09/23 11:59:10.236
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun  9 11:59:14.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4696" for this suite. 06/09/23 11:59:14.493
------------------------------
• [SLOW TEST] [11.852 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/09/23 11:59:02.651
    Jun  9 11:59:02.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: Building a namespace api object, basename crd-publish-openapi 06/09/23 11:59:02.652
    STEP: Waiting for a default service account to be provisioned in namespace 06/09/23 11:59:02.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/09/23 11:59:02.76
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 06/09/23 11:59:02.766
    Jun  9 11:59:02.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3267009506
    STEP: mark a version not serverd 06/09/23 11:59:08.122
    STEP: check the unserved version gets removed 06/09/23 11:59:08.147
    STEP: check the other version is not changed 06/09/23 11:59:10.236
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun  9 11:59:14.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4696" for this suite. 06/09/23 11:59:14.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jun  9 11:59:14.506: INFO: Running AfterSuite actions on node 1
Jun  9 11:59:14.506: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jun  9 11:59:14.506: INFO: Running AfterSuite actions on node 1
    Jun  9 11:59:14.506: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.113 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6048.282 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h40m48.809701097s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

